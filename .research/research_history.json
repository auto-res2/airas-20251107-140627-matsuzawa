{
  "research_topic": "Improving fine-tuning performance of language models.",
  "queries": [
    "parameter-efficient fine-tuning",
    "low-rank adaptation",
    "adapter-based tuning",
    "layer-wise lr decay",
    "hyperparameter optimization"
  ],
  "research_study_list": [
    {
      "title": "Spectral Adapter: Fine-Tuning in Spectral Space",
      "abstract": "Recent developments in Parameter-Efficient Fine-Tuning (PEFT) methods for\npretrained deep neural networks have captured widespread interest. In this\nwork, we study the enhancement of current PEFT methods by incorporating the\nspectral information of pretrained weight matrices into the fine-tuning\nprocedure. We investigate two spectral adaptation mechanisms, namely additive\ntuning and orthogonal rotation of the top singular vectors, both are done via\nfirst carrying out Singular Value Decomposition (SVD) of pretrained weights and\nthen fine-tuning the top spectral space. We provide a theoretical analysis of\nspectral fine-tuning and show that our approach improves the rank capacity of\nlow-rank adapters given a fixed trainable parameter budget. We show through\nextensive experiments that the proposed fine-tuning model enables better\nparameter efficiency and tuning performance as well as benefits multi-adapter\nfusion.",
      "full_text": "Spectral Adapter: Fine-Tuning in Spectral Space Fangzhao Zhang Electrical Engineering Stanford University zfzhao@stanford.edu Mert Pilanci Electrical Engineering Stanford University pilanci@stanford.edu Abstract Recent developments in Parameter-Efficient Fine-Tuning (PEFT) methods for pre- trained deep neural networks have captured widespread interest. In this work, we study the enhancement of current PEFT methods by incorporating the spectral information of pretrained weight matrices into the fine-tuning procedure. We investigate two spectral adaptation mechanisms, namely additive tuning and or- thogonal rotation of the top singular vectors, both are done via first carrying out Singular Value Decomposition (SVD) of pretrained weights and then fine-tuning the top spectral space. We provide a theoretical analysis of spectral fine-tuning and show that our approach improves the rank capacity of low-rank adapters given a fixed trainable parameter budget. We show through extensive experiments that the proposed fine-tuning model enables better parameter efficiency and tun- ing performance as well as benefits multi-adapter fusion. Code is released at https://github.com/pilancilab/spectral_adapter. 1 Introduction Size of language and vision model undergoes a drastic explosion in recent days and results in billions of parameters up to date. While fine-tuning has been used a lot for adapting pretrained large models to various downstream tasks, fine-tuning tasks become increasingly hard with current size of pretrained models due to the huge demand of computing resource. Meanwhile, exchange and storing of fine- tuned models are also expensive given their enormous size. To alleviate these rising problems for fine-tuning large pretrained models, a recent line of research has digged into the Parameter-Efficient Fine-Tuning (PEFT) model family and harnessed great attention. A high-level philosophy behind those PEFT methods is to train a reduced number of parameters compared to full fine-tuning, which instantly saves computing resource and enables light-weight fine-tuned model exchange. Among all PEFT methods, Low-Rank Adaptation (LoRA) [ 20] model is a huge success attributed to its simplicity and effectiveness. Specifically, LoRA proposes to tune an additive trainable low-rank matrix and brings zero inference latency after merging the adapter into pretrained model weights. Since its emergence, numerous variants of LoRA have been developed. For instance, AdaLoRA [65], IncreLoRA [62], and DyLoRA [ 54] propose to dynamically adjust LoRA rank distribution for improving tuning efficiency, QLoRA [10] combines LoRA with model quantization to further save computing resource, LoRA+ [ 16] and PrecLoRA [ 61] study the optimization landscape of LoRA training, and more recent variant DoRA [32] decomposes pretrained weights into magnitude and direction components and applies LoRA for direction tuning, see Apppendix A for a more comprehensive review of different LoRA variants. Other PEFT methods such as Orthogonal Fine- Tuning (OFT) proposes to multiply pretrained weights by tunable orthogonal matrices for preservation of hypersphere energy between pretrained neurons. Though these different PEFT methods focus on improving fine-tuning efficiency with reduced parameters, rare attention has been paid to utilize pretrained model weights’ information beyond its magnitude in the fine-tuning procedure. Prior research in statistical machine learning such as [36] has studied the Empirical Spectral Distribu- tion (ESD) of deep models’ weight matrices and found that the ESDs for larger model weights are arXiv:2405.13952v2  [cs.LG]  4 Nov 2024Figure 1: Training loss of fine-tuning Llama3 8B model with Orca Math dataset [38] and evaluation score on GSM8K benchmark [7]. We follow experimental setup in [53], see Appendix F.1 for details. All methods except full fine-tuning maintain approximately 0.23% trainable parameters. usually more structured and contain indicative information to distinguish between different training stages. More recent work such as [3] investigates the \"dark matter\" effect of bottom spectral space of model weights and recognizes its critical role in attention sink phenomenon observed in [57]. Both work contributes to decrypting spectral information of model weights and sheds light on building insightful understanding of the connection between weight matrices’ spectral information and model performance. In this work, we explore further the value of model weights’ spectral pattern and unravel its effectiveness in enhancing fine-tuning tasks. We showcase via extensive empirical observation that integration of spectral information of pretrained model weights improves current PEFT methods’ parameter efficiency, tuning effect, and arises as a natural solution to multi-adapter fusion problems. Moreover, the suggested fine-tuning model maintains better practicality compared to prior spectral tuning models, which will be investigated further below. Though any technique for weight fine-tuning can be directly applied to fine-tune singular vector matrices of pretrained model weights, we investigate two specific forms of such extension, namely additive tuning and orthogonal rotating the top singular vector space, which we address as Spectral AdapterA and Spectral AdapterR respectively in later content. The spectral adaptation mechanisms being considered are formally depicted in Section 2. As a warmup, to show that incorporating spectral information is indeed helpful, Figure 1 displays the training loss of fine-tuning Llama3 8B model on HuggingFace Orca Math dataset and validation score on GSM8K benchmark, from which it can be clearly observed that Spectral AdapterA performs superior to recent variants of PEFT methods and behaves closest to full fine-tuning, here we follow experimental setup in [53], see Appendix F.1 for details and more investigation. In below, we first introduce the fine-tuning model being studied in Section 2 and we then provide some theoretic insights in Section 3. After that, we detail the advantage of our spectral adapter in enhancing fine-tuning result, improving model’s parameter efficiency, and helping with multi-adapter fusion as well as address any concern with respect to practicality issues in Section 4. Conclusion and future work is discussed in Section 5. For sake of page limitation, literature review is deferred to Appendix A. To summarize, the proposed spectral adaptation mechanism demonstrates the first attempt to fine-tune spectral space of pretrained model weights in a parameter-efficient and storage-economic way which improves current PEFT methods from aspects involving tuning results, parameter efficiency, and multi-adapter fusion. We hope this work serves as a building block and motivates further and deeper insightful investigation for exploring spectral structure of pretrained model weights, which becomes increasingly meaningful especially in current large model regime. 2 Spectral Adapter: Incorporating Spectral Information into Fine-Tuning Motivated by the intrinsic low-rank of weight shifts in fine-tuning procedure studied in [1], LoRA [20] proposes to add a low-rank factorized trainable matrix to pretrained model weights and tune only these additive parameters for downstream task adaptation, which usually injects far fewer trainable parameters compared to full fine-tuning and results in light-weight tuned adapters. LoRA serves as an outstanding representative of PEFT family and is now widely-used for different fine-tuning tasks. 2Figure 2: Compared to LoRA which proposes to add low-rank trainable matrices to pretrained weights, we study two types of spectral adapters: Spectral AdapterA considers additively tuning the top columns of singular vector matrices and Spectral AdapterR considers orthogonally rotating the top columns of singular vector matrices. Inspired by the parameter efficiency of LoRA and the close connection between matrix rank and its spectral representation, here we study two spectral fine-tuning mechanisms, both are completed via first carrying out Singular Value Decomposition (SVD) of pretrained model weights and then fine- tuning the top columns of singular vector matrices obtained via the SVD. More precisely, consider a pretrained weight matrix with its spectral representation of form W =USV T , we define additive spectral adapter as Spectral AdapterA(W) ∶=[U1 +AU U2]S[V1 +AV V2], and correspondingly the rotational version Spectral AdapterR(W) ∶=[U1RU U2]S[V1RV V2], where U1, V1 denote the top- r columns of U and V and U2, V2 denote the rest of the columns. A =(AU , AV ) consists of trainable matrices of shape same as (U1, V1) and R =(RU , RV ) consists of two trainable orthogonal matrices of shape r by r such that RT U RU =RT V RV =I. As we show in later sections, the orthogonality constraint is efficiently handled with the Cayley parameterization, see Section 4.3 for details. The proposed fine-tuning model architecture can be visualized from Figure 2. Here Spectral AdapterA more resembles LoRA as it is of additive form while Spectral AdapterR more resembles prior Orthogonal Fine-Tuning (OFT) method which we compare further in Section 4. To ensure zero initialization as often done for PEFT methods, we initialize AU and AV both at zero. For rotational spectral adapter, we initialize RU and RV as identity matrices. A more thorough literature review suggests that prior work considering tuning model weights’ spectral representation (FSGAN[ 47], SVDiff [ 15]) has been proposed for alleviating overfitting when fine-tuning different vision models. These methods only look at tuning the singular values of flattened CNN weights and thus have fixed amount of trainable parameters. Moreover, these methods require storing all U, Sand V during training while only the diagonal vector of S is tuned, which nearly doubles the storage requirement compared to pretraining when fine-tuning on downstream tasks. Contrarily, we consider incorporating spectral information in generic fine-tuning procedure for different layers (flattened CNN weights, dense linear weights, etc.) and our method enables flexible parameter budget choices by varying values of r. Methodology-wise, we consider tuning the top-r columns of U and V by additive and rotational tuning, both requiring only these top columns to be stored additionally and the left part can be merged into a single weight matrix. See Section 4.4 for more investigation on practicality of the proposed method. 3 Theoretical Insights After introducing the model architecture of spectral adapter we consider, the main question now remains whether tuning the spectral representation of pretrained weights is indeed an improvement over existing PEFT methods. Before we step into our empirical observations, we first provide some theoretical insights for the proposed spectral adaptation mechanism. In this section, we show advantage of our spectral adapter method compared to LoRA from two theoretic perspectives by 3analyzing both the rank capacity of the adapters (Section 3.1) and the subspace alignment of pretrained weight matrices (Section 3.2). Specifically, we will see that Spectral AdapterA has larger rank capacity than LoRA adapter, which indicates the tuned weight has more adaptation freedom and thus is more desirable. Moreover, the dominant spectral direction of pretrained weight matrix identifies more ideal neuron alignment under the setting we consider in Section 3.2, which justifies the robustness of tuning top singular vectors in our spectral adapter. In Appendix D, we show that Spectral AdapterA is approximately equivalent to DoRA [32] for vector-form weights. 3.1 Adapter Rank Capacity For any pretrained weight matrixW, suppose that the adapter is given by the parameterizationfθ(W) where θ represents trainable weights. For instance with LoRA adapter, fθ(W) =W +ABT , where θ ={A, B} is trainable. We define the rank capacity of an adapter fθ(W) as follows: R(fθ; W) ∶= max θ rank(fθ(W))−min θ rank(fθ(W)), which describes the range of matrix ranks the tuned weight can achieve given a specific adapter form. Then, the following lemma shows that Spectral AdapterA has twice the rank capacity of LoRA adapter under an equal number of trainable parameters. Lemma 3.1. Suppose that W ∈Rn×m is an arbitrary full row-rank matrix and n ≤m without loss of generality. Consider rank-r LoRA and rank-r additive spectral adapter, which have an equal number of trainable parameters. We have R(LoRA; W) =r, R(Spectral AdapterA; W) =2r. See Appendix B for proof. Therefore when pretrained model weight matrix is close to full row-rank, as what has been observed in [20], Spectral AdapterA has nearly double rank capacity compared to LoRA adapter. Furthermore, some prior work explicitly imposes low-rank constraint when training original NNs [50, 43, 66, 22, 68, 24, 9]. Using LoRA adapter to fine-tune such pretrained model weights would destroy their rank constraints while applying spectral adapter preserves the constraints. Next we proceed to show that top spectral space of pretrained weight matrices is more aligned with ideal neuron direction under a simple setting via subspace decomposition analysis of pretrained model weights. This observation corroborates our choice of tuning top singular vectors in our proposed spectral adaptation mechanism. Empirically, we observe that tuning top directions performs superior to tuning bottom ones, see Appendix F.3 and F.5.1 for related experiments. 3.2 Weight Subspace Alignment Figure 3: Top singu- lar vector of pretrained weight recognizes more ideal neuron direction. Il- lustration plot for Section 3.2. Consider two-layer ReLU network with m hidden nodes and univariate output. For squared loss objective, we can write out the training problem explicitly as min W(1),W(2) ∥(XW (1))+W(2) −y∥2 2 +β(∥W(1)∥2 F +∥W(2)∥2 2), where X ∈ Rn×d is the data matrix, (W(1) ∈ Rd×m, W(2) ∈ Rm) are first and second layer weights respectively and y ∈Rn is the label vector. For better visualization, we take d = 3. Consider the case that all data points lie on xy−plane, which mimics the usual observation that data points occupy a low-dimensional manifold. Then we can decompose each first layer neuron W(1) j ∈ Rd into W(1) j = wj1 +wj2 where wj1 ∈ R(X), wj2 ⊥ R(X). With simple algebra, for non-zero weight decay which is often the default setting for current deep learning optimizers, one can derive wj2 =0 and thus W(1) j =wj1 ∈R(X). Therefore all optimal neurons lie also in xy−plane. However, due to optimization errors, some of the trained neurons might be slightly deviated from xy−plane, as illustrated in Figure 3, where ui indicates pretrained neuron directions, though most of them lie in xy−plane, some might deviate (i.e., u4). u⋆ indicates the top singular vector direction of pretrained weight W(1) which here recognizes the xy−plane orientation, and thus fine-tuning u⋆ is noiseless and is expected to be more robust. 44 Empirical Results: The Impact of Spectral Information We experiment our proposed spectral adapter with fine-tuning large language models and diffusion models and compare against various recent PEFT methods. From language model experiments, we observe that Spectral Adapter A performs superior to various PEFT baselines and harnesses higher scores on different benchmarks, which again verifies the effectiveness of incorporating spectral information into the fine-tuning procedure, see Section 4.1 for details. For diffusion model experiments, we will see that the advantage of spectral adapter comes in two-fold: Spectral AdapterA offers a natural solution to existing problems in multi-adapter fusion procedure and Spectral AdapterR manifests finer-grained parameter budgets as well as better parameter efficiency, see Section 4.2 and 4.3 respectively. For a fair comparison with all baselines, we use their official implementation and follow hyperparameter setting in their original reports as long as available. See each individual section for corresponding experimental details. All experiments are done with NVIDIA RTX A6000 GPU. 4.1 Language Model Fine-Tuning: Enhancing Fine-Tuning Results with Spectral Adapter A For large language model experiments, we present experimental results for fine-tuning DeBERTaV3- base model (185M) and Mistral model (7B) on GLUE and GSM8K tasks respectively. Our Spectral AdapterA method achieves superior tuning results compared to various recent PEFT methods in most experiments. DeBERTaV3-base Experiment. Table 1 shows fine-tuning results of DeBERTaV3-base model on GLUE benchmarks with various PEFT methods. For a fair comparison, we use official implemen- tations for LoRA, DoRA, OFT and AdaLoRA in HuggingFace PEFT library, with hyperparameter setting for LoRA [20] and AdaLoRA [65] following their original reports. We use same hyperpa- rameter setting as LoRA for DoRA and follow the setting used in BOFT [33], a variant of OFT, for OFT experiments. We abbreviate Spectral AdapterA as SpectralA for presentation simplicity and we tune hyperparameters for Spectral AdapterA. See Appendix F.2 for hyperparameter details and F.3 for loss/validation plot comparison. We fine-tune all q, k, vmatrices in attention layers. Our Spectral AdapterA achieves highest average score and best scores for most tasks with fewest trainable parameters. Method # Param GLUE MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B Avg. LoRAr=24 0.72% 88.87 95.06 87.00 65.84 91.87 91.45 81.22 90.43 86.47 DoRAr=24 0.73% 88.91 95.29 88.72 65.84 92.01 91.51 80.14 90.10 86.57 OFTr=4 0.72% 89.16 95.06 87.74 66.75 93.28 91.33 78.70 89.72 86.47 AdaLoRAr=24 1.07% 89.44 94.95 89.70 63.06 93.17 91.48 83.75 91.22 87.10 SpectralA r=24 0.72% 89.79 95.75 90.19 69.44 93.35 91.65 83.39 90.64 88.03 Table 1: Accuracy comparison of fine-tuning DeBERTaV3-base with various PEFT methods on GLUE benchmarks. SpectralA is abbreviation for Spectral AdapterA. See Section 4.1 for experimental details. Mistral 7B Experiment. We experiment our Spectral Adapter A with Mistral 7B model [23] fine-tuned for GSM8K task [ 7]. Since all baseline model reports include no fine- tuning tasks with the Mistral family, we use official implementations of all baseline meth- ods for comparison and we fix learning rate to be 2.5e − 5 for all methods following [ 51]. Method #Param GSM8K Pre-Trained − 37.91 ±1.34 LoRAr=8 0.16% 44.81 ±1.37 DoRAr=8 0.17% 43.82 ±1.37 SpectralA r=8 0.16% 49.73 ±1.38 Table 2: Accuracy comparison of fine-tuning Mis- tral 7B model with different PEFT methods on GSM8K benchmark. See Section 4.1 for experi- mental details. We take r = 8 for LoRA, DoRA and Spectral AdapterA to maintain approximately same num- ber of trainable parameters for all methods. Ta- ble 2 presents the accuracy comparison where SpectralA stands for Spectral Adapter A. From the result, we observe that our Spectral AdapterA scores higher than both LoRA and DoRA by a large margin and increases the pretrained model baseline significantly, which verifies the effective- ness of the proposed spectral adaptation mecha- nism. See Appendix F.4 for more about experi- mental details. Note for a different learning rate, DoRA performs better than LoRA while still worse than our method, see also Appendix F.4 for details. 54.2 Diffusion Model Fusion: Improving Multi-Object Fine-Tuning with Spectral Adapter A Figure 4: Distributing different concept tunings along different spectral space helps with identity preservation in multi-adapter fusion, see Section 4.2 for details. Multi-adapter fusion is a current bottleneck in diffusion model fine-tuning tasks with LoRA adapters. Simply adding different LoRA adapters tuned for distinct objects will result in problems involving identity loss and concept binding [12]. To tackle this toughness, different methods emerge such as Gradient Fusion [12] and Orthogonal Adaptation [42]. Specifically, Orthogonal Adaptation method proposes to fix LoRA parameter B to have orthogonal basis and train A solely. Experiments there show that merging LoRA weights with such orthogonal basis helps preserving individual object characteristics compared to its non-orthogonal counterpart. In Orthogonal Adaptation [ 42], the authors maintain B by manually keeping large orthogonal matrices for different layer sizes and sample r columns from corresponding orthogonal matrix to form B for each LoRA adapter. With knowledge from random matrix theory, such sampled matrices are likely to have orthogonal basis. Notably, our Spectral AdapterA naturally operates on orthogonal singular vectors and thus introduces an elegant solution to multi-adapter fusion problems by distributing different concept tunings along different columns of singular vector matrices, which maps to wireless communications where the signals are distributed over non-overlapping frequencies. A subtlety here lies in the choice of column space for different fine-tuning tasks: (1) Sample-based methods can be adopted if data privacy is considered and different tuning tasks are done independently. In Appendix F.5, we show that tuning top columns manifests better generation quality compared to both tuning bottom columns and sampling random orthogonal basis as what has been done in Orthogonal Adaptation [42]. Thus there is a trade-off between high-quality generation and concept collapsing, i.e., sampling from top singular vectors is more encouraged while column overlapping between concepts happens more often compared to sampling from the whole set. (2) On the other hand, if fine-tuning tasks are not isolated and can collaborate on the column scheduling, then more deliberate tuning scheduling can be adopted, for example in a two-concept tuning task with r =4, the first concept can allocate first to fourth columns and the second concept then claims fifth to eighth columns. Figure 4 demonstrates steps for the same method for three-concept tuning task. Since we expect fine-tuned weights to stay close to original weights, though both row space and column space are tuned in spectral adapter, this adaptation mechanism approximates orthogonal-basis tuning for different objects and thus we expect it helps improving identity preservation for multi-adapter fusion. In this section, we investigate this effect via extensive diffusion model experiments. Our experiments follow [42] and build on [12] which studies multi-LoRA fusion. We experiment with multi-object tuning and face generation tasks. Due to space limitation, we present some multi-object tuning results below and we leave the rest to Appendix F.5. For all tasks, we compare against baselines including Gradient Fusion [12], Orthogonal Adaptation [42], and FedAvg [37]. We start with a simple review for these baseline methods. Baseline Review To merge different LoRA adapters, say we have a set of LoRA parameters{∆θ1, . . . ,∆θn} where ∆θi = AiBT i and pretrained parameter θ0, FedAvg [ 37] proposes to merge them in to a single parameter by taking a weighted average as θmerged =θ0 +∑i λi∆θi, where λi is the weight attached to parameter ∆θi and is usually taken to satisfy ∑i λi = 1, i.e., θmerged is a convex combination of individual adapters. Gradient Fusion [12] instead considers solving an auxiliary optimization problem of form θmerged =argminθ ∑n i=1 ∥(θ0 +∆θi)Xi −θXi∥2 F where Xi represents the input activation of the i-th concept. Orthogonal Adaptation [42] follows FedAvg method and replaces original LoRA 6Figure 5: Generation results of Chilloutmix diffusion model [8] with different fused adapters tuned on three custom animal concepts. See Section 4.2 for details. parameters with orthogonal-based LoRA adapters. For our method, to merge different spectral adapters, let θ0 = U0S0V T 0 denote the spectral representation of pretrained model weight. Given a set of spectral adapters {(Ui, Vi), . . . ,(Un, Vn)} with zero-padding to make the shape the same as (U0, V0), we follow FedAvg and compute θmerged = (U0 +∑i λiUi)S0(V0 +∑i λiVi)T . In the following experiments, we take λi =1/n as in [42] for all FedAvg, Orthogonal Adaptation, and our Spectral AdapterA fusion. Notably, all FedAvg, Orthogonal Adaptation, and our Spectral AdapterA fusion can be done approximately instantly while Gradient Fusion usually takes around 10 ∼ 15 minutes for solving its auxiliary optimization problems for all concept adapters. Multi-Object Generation We follow default training setting in [ 12] and fine-tune the Chilloutmix diffusion model [ 8] on three custom animal concepts, see original animals in \"reference\" in Figure 5. For better spatial alignment, we adopt T2I-Adapter [39] with sketch condition and we set guidance equal to one, see also \"reference\" in Figure 5 for the sketch condition being used. LoRA rank r =8 is adopted. For baseline comparisons, we use original code for Gradient Fusion [ 12] and Orthogonal Adaptation [42]. We adapt code of Gradient Fusion for FedAvg method since there is no official implementation available. Custom animal name is replaced with special token < Vanimal> for fine-tuning. For our Spectral AdapterA, we follow the method depicted in Figure 4 and tune first, second, and third top eighth columns of singular vector matrices for different animal concepts. Figure 5 shows the generation results with different methods for selected prompts. Notably, baseline methods sometimes fail to capture the custom animal concepts while Spectral AdapterA recognizes all custom animals and generates visually satisfactory images. For better measurement, we also compute the alignment scores for each generated image with both reference images and prompt texts. It can be witnessed that our method achieves better alignment scores compared to baselines. See Appendix F.7 for details on alignment score computation. 4.3 Diffusion Model Expressiveness: Improving Parameter Efficiency with Spectral AdapterR Spectral AdapterR is closely connected to prior Orthogonal Fine-Tuning (OFT ) [45] method which proposes to multiply the pretrained model weights by trainable orthogonal matrices in the fine- tuning procedure. Motivation behind OFT is to preserve hyperspherical energy which characterizes the pairwise neuron relationship on the unit hypersphere. Unlike OFT which orthogonally rotates neurons, Spectral Adapter R multiplies the top- r columns of singular vector space U and V by orthogonal trainable matrices. For our implementation, several options are available for maintaining a trainable orthogonal matrix such as adding an orthogonality penalty in the objective function considered in [65] or via Cayley parameterization considered in [ 45]. We follow [ 45] and adopt Cayley parameterization which is supported by Pytorch [44]. Specifically, the orthogonal matrix R is 7constructed via R =(I +Q)(I −Q)−1 with a skew-symmetric matrix Q maintained as (A −AT )/2 where A is our trainable parameter. Compared to adding an auxiliary orthogonality penalty, this parametrization is exact and thus the SVD form is preserved after tuning with Spectral AdapterR and can be adopted directly for subsequent fine-tuning tasks, which we state formally as a lemma below: Lemma 4.1. With the Cayley parametrization, Spectral AdapterR is an exact rotation operation and thus preserves the structure of the SVD of the fine-tuned weight. Subsequent fine-tunings can be applied consequently without recomputing the SVD each time. See Appendix C for the proof of above lemma. Unlike LoRA which requires number of trainable parameters to scale with weight size, when tuning top-r columns of U an V , Spectral AdapterR only requires two trainable matrices of size r ×r and thus can be more parameter-efficient especially for large pretrained weight. For common weight size such as W ∈ R1024×1024, LoRA with only r = 1 introduces same number of trainable parameters as Spectral AdapterR with r =32. For a thorough analysis on parameter efficiency improvement brought by Spectral AdapterR, we here also compare with different variants of LoRA which are proposed for trainable parameter savings. We review all baselines in detail below. Baseline Review We compare our Spectral Adapter R with LoRA [ 20], SVDiff [ 15], LiDB [ 48], OFT [ 45], and VeRA [25]. Though the other methods are proposed for vision model tuning, VeRA is originally proposed for LLM tuning and we extend it here to diffusion model tuning due to its parameter efficiency. Consider a pretrained weight W ∈Rn×n, SVDiff originally proposes to tune all singular values of flattened CNN weights, here we extend it to tune all singular values of text encoder and U-Net weights for our comparison, thus trainable parameter attached to W will be of size n and is nonadjustable. LiDB stands for Lightweight Dreambooth and proposes to cut down trainable parameter budget by introducing auxiliary frozen matrixAaux ∈Rn×a and Baux ∈Rb×n, then it mimics LoRA but uses AauxABT Baux in replace of ABT with trainable (A ∈ Ra×r, B∈ Rb×r). Thus with a, b< n, LiDB requires (a +b)r < 2nr trainable parameters. In below, we use a = 50, b= 100 as default in [48]. OFT multiplies the weight matrix by a trainable orthogonal matrix via Cayley parametrization discussed above, thus its complete version requires n2 trainable parameters. For parameter efficiency, OFT proposes to use block-diagonal trainable matrix with all diagonal blocks being orthogonal. Thus with r diagonal blocks, the number of trainable parameter will be r ×(n/r)2. Method Granularity #Param Auxiliary Param LoRA / ∞ 2nr∝n noSVDiff / 1 n∝n noLiDB / ∞ (a+b)r∝r yes OFT / #factors ofn1 (n/r)2 ∝nr no VeRA / ∞ n+r∝n yes Spectral AdapterR , n 2r2 ∝r no 1 Ceiling operation is ignored for this count. Table 3: Baseline methods comparison for parameter effi- ciency. Granularity indicates number of trainable parameter budgets available. See Section 4.3 for details. Further reduction of trainable parame- ter is achieved via sharing the diagonal blocks, which demands only (n/r)2 parameters. In below comparison, we use this shared block-diagonal version for best parameter efficiency of OFT. VeRA proposes to use ΛaAΛbBT in replace of ABT where Λa and Λb are diagonal matrices of size n ×n and r ×r respectively. Thus the total num- ber of trainable parameters by VeRA is (n +r) ∝n. Table 3 compares dif- ferent properties across all methods, where n represents weight size and r represents rank for all methods except for OFT, where r denotes number of diagonal blocks. Parameter Efficiency We fine-tune the Chilloumix diffusion model [8] with various PEFT methods on custom vase concept and present the generation results for prompt \"a <Vvase> on a table\" in Figure 6 for various trainable parameter budgets, where grey dash denotes that the corresponding parameter budget is unobtainable with a given adapter no matter how the hyperparameter is chosen and empty entry without grey dash 8Figure 6: Generation results for prompt “a <Vvase> on a table” after fine-tuning Chilloutmix diffusion model [8] on custom vase images with different PEFT methods. See Section 4.3 for details. represents that there is a way to achieve the corresponding parameter budget though the generation result is skipped for better visualization. We follow default LoRA implementation in [12] for LoRA baseline and adjust it for all other methods. From Figure 6, it can be observed that LoRA, OFT, and LiDB start to generate vase close to custom vase with at least 200k trainable parameters. SVDiff and VeRA are unable to generate ideal vase images even if scaled to large parameter budget. On the contrary, Spectral AdapterR starts to recognize the custom vase concept with only 20k trainable parameters and has finer-grained parameter choices compared to other methods, i.e., notably Spectral AdapterR can have as few as1k parameters while other methods start with at least tens of thousands of trainable parameters. In a word, Spectral AdapterR enjoys finer-grained parameter budget choices and manifests better visual quality with fewer parameters, thus achieves enhanced parameter efficiency compared to various other PEFT methods. Figure 7: Generation results for prompt “a yellow <Vchair>” after fine-tuning Chilloutmix diffusion model [8] on custom chair images with different PEFT methods. Spectral R is abbreviation for Spectral AdapterR. See Section 4.3 for details. Figure 7 above presents generation results of Chilloutmix diffusion model [8] tuned on custom chair concept with different methods under various parameter budgets. The prompt used is \"a yellow <Vchair>\". See \"reference\" in Figure 7 for original chair images. From the generation results, it can be observed that LoRA generates reasonable chairs for all rank r =1, 2, 3 though it already induces 273k parameters even if rank is set to 1. OFT and VeRA start to recognize custom chair with >100k parameters. SVDiff has a single fixed trainable parameter budget of size around 100k. LiDB forms a competitive candidate and generates satisfactory images with smallest trainable parameter budget among all baseline methods. However, our Spectral AdapterR still generates images better aligned to 9reference images with as few as 20k trainable parameters and has finer-grained parameter budget choices compared to LiDB. See Appendix F.6 for hyperparameter setting and Appendix F.7 for alignment score computation details. 4.4 Final Note: A Closer Look at SVD Cost Figure 8: Runtime and GPU storage cost plot. See Section 4.4 for details. To alleviate the concerns with respect to online training cost and show that our pro- posed method is very practical, we provide runtime and GPU storage cost bar plot in Figure 8, which shows runtime and GPU storage cost for LoRA and for our Spec- tral AdapterA when used for fine-tuning diffusion model in Section 4.2 and Mistral 7B model in Section 4.1. Here we adopt rank r = 8 for both LoRA and Spectral AdapterA. It can be observed that our Spec- tral Adapter A introduces negligible run- time and storage overhead for current large model size. Modern numerical tools such as randomized SVD [13] can also be exploited for further runtime reduction and the SVD procedure can be paral- lelized when multiple machines are available. See Appendix E for further investigation. 5 Conclusion and Limitations In this work, we investigate the incorporation of spectral information of pretrained model weights into current PEFT models by introducing a spectral adaptation mechanism which updates only the top singular vectors of pretrained weights. We investigate the additive and rotational variants of such spectral adaptation mechanism. Theoretically, we show the motivation of tuning top singular vectors by comparing the rank capacity of different fine-tuning models and carrying out weight decomposition of pretrained model layers. Empirically, we verify the superiority of our proposed spectral adaptation method compared to various recent PEFT methods from different aspects via extensive experiments. To our best knowledge, this is the first work considering incorporating spectral information as a practical generic paradigm for fine-tuning tasks and enhances fine-tuning results, parameter efficiency, as well as benefits multi-adapter fusion of existing PEFT methods. For future work, fine-tuning spectral representation of different components, i.e., only the attention layer, of current large models is also worth studying. Other PEFT methods such as AdaLoRA [65] can also be dynamically combined with spectral adaptation. A limitation of the current work remains in the choice of tuning top spectral space. Though its validity has been theoretically verified under simple settings, further investigation on tuning different columns of singular vector matrices is critical to understanding the role of spectral information in fine-tuning procedure. Besides, fine-tuning spectral representation of different components, i.e., only the attention layer, of current large models is also worth studying. Moreover, the time consumption of singular value decomposition procedure increases as model grows larger and thus faster singular value decomposition method also benefits. 106 Acknowledgement This work was supported in part by the National Science Foundation (NSF) under Grant DMS- 2134248; in part by the NSF CAREER Award under Grant CCF-2236829; in part by the U.S. Army Research Office Early Career Award under Grant W911NF-21-1-0242; and in part by the Office of Naval Research under Grant N00014-24-1-2164. References [1] A. Aghajanyan, L. Zettlemoyer, and S. Gupta. Intrinsic dimensionality explains the effectiveness of language model fine-tuning, 2020. [2] A. Asai, M. Salehi, M. E. Peters, and H. Hajishirzi. Attempt: Parameter-efficient multi-task tuning via attentional mixtures of soft prompts, 2022. [3] N. Cancedda. Spectral filters, dark signals, and attention sinks, 2024. [4] A. Chavan, Z. Liu, D. Gupta, E. Xing, and Z. Shen. One-for-all: Generalized lora for parameter- efficient fine-tuning, 2023. [5] Y . Chen, D. Hazarika, M. Namazifar, Y . Liu, D. Jin, and D. Hakkani-Tur. Empowering parameter-efficient transfer learning by recognizing the kernel structure in self-attention. arXiv preprint arXiv:2205.03720, 2022. [6] A. Chronopoulou, M. E. Peters, A. Fraser, and J. Dodge. Adaptersoup: Weight averaging to improve generalization of pretrained language models, 2023. [7] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems, 2021. [8] C. M. Creator. Chilloutmix diffusion model. https://civitai.com/models/6424/chilloutmix. [9] M. Denil, B. Shakibi, L. Dinh, M. Ranzato, and N. de Freitas. Predicting parameters in deep learning, 2014. [10] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer. Qlora: Efficient finetuning of quantized llms, 2023. [11] A. Edalati, M. Tahaei, I. Kobyzev, V . P. Nia, J. J. Clark, and M. Rezagholizadeh. Krona: Parameter efficient tuning with kronecker adapter, 2022. [12] Y . Gu, X. Wang, J. Z. Wu, Y . Shi, C. Yunpeng, Z. Fan, W. Xiao, R. Zhao, S. Chang, W. Wu, Y . Ge, S. Ying, and M. Z. Shou. Mix-of-show: Decentralized low-rank adaptation for multi-concept customization of diffusion models. arXiv preprint arXiv:2305.18292, 2023. [13] N. Halko, P.-G. Martinsson, and J. A. Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions, 2010. [14] K. Hambardzumyan, H. Khachatrian, and J. May. Warp: Word-level adversarial reprogramming, 2021. [15] L. Han, Y . Li, H. Zhang, P. Milanfar, D. Metaxas, and F. Yang. Svdiff: Compact parameter space for diffusion fine-tuning, 2023. [16] S. Hayou, N. Ghosh, and B. Yu. Lora+: Efficient low rank adaptation of large models, 2024. [17] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, and G. Neubig. Towards a unified view of parameter-efficient transfer learning, 2022. [18] S. He, R.-Z. Fan, L. Ding, L. Shen, T. Zhou, and D. Tao. Mera: Merging pretrained adapters for few-shot learning. arXiv preprint arXiv:2308.15982, 2023. [19] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. de Laroussilhe, A. Gesmundo, M. At- tariyan, and S. Gelly. Parameter-efficient transfer learning for nlp, 2019. 11[20] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models, 2021. [21] C. Huang, Q. Liu, B. Y . Lin, T. Pang, C. Du, and M. Lin. Lorahub: Efficient cross-task generalization via dynamic lora composition, 2024. [22] M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up convolutional neural networks with low rank expansions. arXiv preprint arXiv:1405.3866, 2014. [23] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed. Mistral 7b, 2023. [24] M. Khodak, N. Tenenholtz, L. Mackey, and N. Fusi. Initialization and regularization of factorized neural layers, 2022. [25] D. J. Kopiczko, T. Blankevoort, and Y . M. Asano. Vera: Vector-based random matrix adaptation, 2024. [26] T. Lei, J. Bai, S. Brahma, J. Ainslie, K. Lee, Y . Zhou, N. Du, V . Zhao, Y . Wu, B. Li, et al. Conditional adapters: Parameter-efficient transfer learning with fast inference. Advances in Neural Information Processing Systems, 36, 2024. [27] B. Lester, R. Al-Rfou, and N. Constant. The power of scale for parameter-efficient prompt tuning, 2021. [28] X. L. Li and P. Liang. Prefix-tuning: Optimizing continuous prompts for generation, 2021. [29] Y . Li, Y . Yu, C. Liang, P. He, N. Karampatziakis, W. Chen, and T. Zhao. Loftq: Lora-fine- tuning-aware quantization for large language models, 2023. [30] Z. Lin, A. Madotto, and P. Fung. Exploring versatile generative language model via parameter- efficient transfer learning. arXiv preprint arXiv:2004.03829, 2020. [31] Q. Liu, X. Wu, X. Zhao, Y . Zhu, D. Xu, F. Tian, and Y . Zheng. Moelora: An moe-based parameter efficient fine-tuning method for multi-task medical applications, 2023. [32] S.-Y . Liu, C.-Y . Wang, H. Yin, P. Molchanov, Y .-C. F. Wang, K.-T. Cheng, and M.-H. Chen. Dora: Weight-decomposed low-rank adaptation, 2024. [33] W. Liu, Z. Qiu, Y . Feng, Y . Xiu, Y . Xue, L. Yu, H. Feng, Z. Liu, J. Heo, S. Peng, Y . Wen, M. J. Black, A. Weller, and B. Schölkopf. Parameter-efficient orthogonal finetuning via butterfly factorization, 2023. [34] X. Liu, Y . Zheng, Z. Du, M. Ding, Y . Qian, Z. Yang, and J. Tang. Gpt understands, too, 2023. [35] R. K. Mahabadi, S. Ruder, M. Dehghani, and J. Henderson. Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks, 2021. [36] C. H. Martin and M. W. Mahoney. Implicit self-regularization in deep neural networks: Evidence from random matrix theory and implications for learning, 2018. [37] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication- efficient learning of deep networks from decentralized data, 2023. [38] A. Mitra, H. Khanpour, C. Rosset, and A. Awadallah. Orca-math: Unlocking the potential of slms in grade school math, 2024. [39] C. Mou, X. Wang, L. Xie, Y . Wu, J. Zhang, Z. Qi, Y . Shan, and X. Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models, 2023. [40] mrm8488. Lora finetune deberta-v3 huggingface blog, 2021. Available at https://huggingface.co/mrm8488/deberta-v3-small-finetuned-mnli/commits/main. [41] J. Pfeiffer, A. Kamath, A. Rücklé, K. Cho, and I. Gurevych. Adapterfusion: Non-destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247, 2020. 12[42] R. Po, G. Yang, K. Aberman, and G. Wetzstein. Orthogonal adaptation for modular customiza- tion of diffusion models, 2023. [43] D. Povey, G. Cheng, Y . Wang, K. Li, H. Xu, M. A. Yarmohammadi, and S. Khudanpur. Semi- orthogonal low-rank matrix factorization for deep neural networks. In Interspeech, 2018. [44] pytorch group. Pytorch orthogonal parameterization method implementation, 2023. [45] Z. Qiu, W. Liu, H. Feng, Y . Xue, Y . Feng, Z. Liu, D. Zhang, A. Weller, and B. Schölkopf. Controlling text-to-image diffusion by orthogonal finetuning, 2023. [46] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from natural language supervision, 2021. [47] E. Robb, W.-S. Chu, A. Kumar, and J.-B. Huang. Few-shot adaptation of generative adversarial networks, 2020. [48] N. Ruiz, Y . Li, V . Jampani, W. Wei, T. Hou, Y . Pritch, N. Wadhwa, M. Rubinstein, and K. Aberman. Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models, 2023. [49] A. Rücklé, G. Geigle, M. Glockner, T. Beck, J. Pfeiffer, N. Reimers, and I. Gurevych. Adapter- drop: On the efficiency of adapters in transformers, 2021. [50] T. N. Sainath, B. Kingsbury, V . Sindhwani, E. Arisoy, and B. Ramabhadran. Low-rank matrix factorization for deep neural network training with high-dimensional output targets. In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 6655–6659, 2013. [51] H. Skogström. Lora finetune mistral 7b valohai blog, 2024. https://valohai.com/blog/finetune- mistral/. [52] A. Tang, L. Shen, Y . Luo, Y . Zhan, H. Hu, B. Du, Y . Chen, and D. Tao. Parameter efficient multi-task model fusion with partial linearization, 2023. [53] K. Turgutlu. Answer.ai qdora report, 2024. https://www.answer.ai/posts/2024-04-26-fsdp-qdora- llama3.html. [54] M. Valipour, M. Rezagholizadeh, I. Kobyzev, and A. Ghodsi. Dylora: Parameter efficient tuning of pre-trained models using dynamic search-free low-rank adaptation, 2023. [55] T. Vu, B. Lester, N. Constant, R. Al-Rfou, and D. Cer. Spot: Better frozen model adaptation through soft prompt transfer. arXiv preprint arXiv:2110.07904, 2021. [56] Z. Wang, R. Panda, L. Karlinsky, R. Feris, H. Sun, and Y . Kim. Multitask prompt tuning enables parameter-efficient transfer learning, 2023. [57] G. Xiao, Y . Tian, B. Chen, S. Han, and M. Lewis. Efficient streaming language models with attention sinks, 2024. [58] L. Xu, H. Xie, S.-Z. J. Qin, X. Tao, and F. L. Wang. Parameter-efficient fine-tuning methods for pretrained language models: A critical review and assessment, 2023. [59] Y . Xu, L. Xie, X. Gu, X. Chen, H. Chang, H. Zhang, Z. Chen, X. Zhang, and Q. Tian. Qa-lora: Quantization-aware low-rank adaptation of large language models, 2023. [60] A. X. Yang, M. Robeyns, X. Wang, and L. Aitchison. Bayesian low-rank adaptation for large language models, 2024. [61] F. Zhang and M. Pilanci. Riemannian preconditioned lora for fine-tuning foundation models, 2024. [62] F. F. Zhang, L. Li, J.-C. Chen, Z. Jiang, B. Wang, and Y . Qian. Increlora: Incremental parameter allocation method for parameter-efficient fine-tuning. ArXiv, abs/2308.12043, 2023. 13[63] L. Zhang, L. Zhang, S. Shi, X. Chu, and B. Li. Lora-fa: Memory-efficient low-rank adaptation for large language models fine-tuning, 2023. [64] M. Zhang, H. Chen, C. Shen, Z. Yang, L. Ou, X. Yu, and B. Zhuang. Loraprune: Pruning meets low-rank parameter-efficient fine-tuning, 2023. [65] Q. Zhang, M. Chen, A. Bukharin, N. Karampatziakis, P. He, Y . Cheng, W. Chen, and T. Zhao. Adalora: Adaptive budget allocation for parameter-efficient fine-tuning, 2023. [66] Y . Zhang, E. Chuangsuwanich, and J. Glass. Extracting deep neural network bottleneck features using low-rank matrix factorization. In2014 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 185–189. IEEE, 2014. [67] H. Zhao, H. Tan, and H. Mei. Tiny-attention adapter: Contexts are more important than the number of parameters, 2022. [68] Y . Zhao, J. Li, and Y . Gong. Low-rank plus diagonal adaptation for deep neural networks. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5005–5009. IEEE, 2016. [69] Y . Zhu, J. Feng, C. Zhao, M. Wang, and L. Li. Counter-interference adapter for multilingual machine translation, 2021. [70] B. Zi, X. Qi, L. Wang, J. Wang, K.-F. Wong, and L. Zhang. Delta-lora: Fine-tuning high-rank parameters with the delta of low-rank matrices, 2023. 14Appendix A Prior Work Here we provide an overview of recent PEFT methods. Dating back to 2019, Houlsby et al. [ 19] develop the idea of parameter-efficient fine-tuning and introduce Adapter model, which injects trainable components between pretrained model layers, though the number of trainable parameters has been reduced due to the small size of adapters, this method incurs inference latency and is thus not desirable. Later improvement of Adapter fine-tuning focuses on improving inference latency [49, 26], fusing multiple adapters [6, 41, 18], modifying adapter model architecture [67], introducing parallelism [17, 69], and creating task-specific and layer-specific adapter [ 35, 30]. Another line of fine-tuning is prompt-tuning [27] which usually adds the trainable components into the prompt. Variants of prompt-tuning involve WARP [14], prefix-tuning [28], P-tuning [34], and ATTEMPT [2] which consider injecting different forms of trainable components. Multitask prompt-tuning is considered in [55, 56]. The more relevant PEFT methods to our spectral adaptation mechanism involves LoRA [20] and OFT [45], which inspires our Spectral AdapterA and Spectral AdapterR respectively. LoRA originates from the observation that model fine-tuning is intrinsically low-rank [1]. Variants of LoRA involve different methods proposing dynamic allocation of LoRA rank budgets [54, 62, 65, 5]. LoRA has been combined with model pruning [64] and quantization [10, 59, 29]. Some other variants further cut down the trainable parameter budget or activation storage by modifying LoRA model [25, 11, 63]. DoRA [32] fixes LoRA’s low-rank limitation by decomposing pretrained model weights and isolating their magnitudes. Laplace-LoRA [ 60] incorporates Bayesian inference into LoRA parameters to improve calibration. LoRAHub [21], MOELoRA [31], and L-LoRA [52] consider multitask LoRA. Delta-LoRA [70] updates pretrained weights simultaneously from information of LoRA parameters. GLoRA [4] generalizes LoRA by introducing a prompt module. Another line of variants focuses on analyzing the optimization scheme of LoRA model [ 61, 16]. OFT studies the multiplicative fine-tuning and its variant BOFT [33] improves OFT by utilizing butterfly parametrization for better information delivery efficiency. [58] offers a comprehensive review of recent development of PEFT methods. B Rank Capacity Proof Proof. Consider weight matrix W ∈ Rn×m with n ≤ m of full row rank. For LoRA parameter A ∈Rm×r, B∈Rn×r with n ≥r, final weight matrix W +ABT has rank in [n −r, n]. With Spectral AdapterA parameters AS ∈ Rm×r, BS ∈ Rn×r where n ≥ 2r. Let Xr denote the first r columns of any matrix X and X−r denote the rest columns, final weight matrix ((Ur +AS)Sr(Vr +BS)T )+ U−rS−rV T −r has rank in [n−2r, n]. Therefore, R(LoRA; W) =r and R(Spectral AdapterA; W) = 2r can be derived trivially. C Cayley Parameterization Proof Proof. With any trainable square matrix A, we set Q = (A −AT )/2 and thus Q = −QT and Q is skew-symmetric thereby. Now we show that for any skew-symmetric Q, (I +Q)(I −Q)−1 is orthogonal. Let O =(I +Q)(I −Q)−1, then OT O =((I +Q)(I −Q)−1)T (I +Q)(I −Q)−1 =(I −QT )−1(I +QT )(I +Q)(I −Q)−1 by Q skew-symmetric, =(I +Q)−1(I −Q)(I +Q)(I −Q)−1 since (I −Q) and (I +Q) have same eigen-basis and are commutable, =I, which shows that the Cayley parametrization is exact and no re-SVD is needed for orthogonality preservation. 15D Connection to DoRA In DoRA [32], the authors observe that plain LoRA method tends to either increase or decrease the magnitude and direction updates proportionally and thus lacks ability to make slight direction change together with large magnitude change, to come across this limitation, the authors propose to decompose pretrained model weights into magnitude and direction and update them separately. The magnitude is replaced with a trainable scalar and the direction is updated with original LoRA method. Experiments in [32] show that such decomposition helps improve effectiveness of LoRA significantly. Here we show that our Spectral AdapterA is closely connected to the weight decomposition trick used in DoRA when pretrained model weight is of vector form. We note that in DoRA, after the weight decomposition, each column becomes unit-length while in Spectral AdapterA, we also operates on matrices with unit-length columns. Specifically, consider a pretrained model weight w0 ∈Rn×1, then DoRA becomes w =w w0 +ba ∥w0 +ba∥2 , where w is a trainable scalar initialized at ∥w0∥2. band a are trainable parameters of size n ×1 and 1 ×1 respectively, with ba =0 at initialization. Comparably, Spectral AdapterA becomes w =( w0 ∥w0∥2 +a′)∥w0∥2(1 +b′), with trainable vectora′ ∈Rn×1 and trainable scalarb′ both initialized at zero. We can thus equivalently view ∥w0∥2(1 +b′) as a single trainable scalar initialized at ∥w0∥2, which then plays the role of magnitude adapter as w in DoRA. a′ is adopted for directional adaptation since it directly operates on the normalized base vector. E Cost Investigation (More Detailed) Here we address the potential concern about the overhead of our proposed spectral adaptation mechanism. Firstly, we note that spectral adapter introduces similar number of trainable parameters and can be merged into original model weights, thus it is lightweight for sharing and introduces no additional inference latency, which preserves the strengths of additive fine-tuning methods. Therefore, the major overhead concern exists in the runtime and GPU storage overhead during online training. Note our method involves only matrix multiplication in the forward procedure and thus should run as quick as LoRA. Though the SVD procedure can bring additional runtime overhead, it needs to be done only once for a single model and can be reused for later fine-tuning on various downstream tasks. Besides, modern numerical tools such as randomized SVD [ 13] can also be exploited and the SVD procedure can be parallelized when multiple machines are available. As for GPU storage, unlike SVDiff [15] where all SVD components are required for training procedure thus introducing significant GPU storage burden, our method requires only the top spectral space to be stored additionally and consumes similar GPU storage to LoRA for relatively small tuning ranks (which is usually the case). F Supplemental Materials for Experiments F.1 Experimental Setup for Figure 1 For Figure 1 experiments, we follow QDoRA [53] experimental setup for fine-tuning Llama3 8B model, where all k_proj, q_proj, v_proj, up_proj, down_proj, and gate_proj weights are tuned. We adopt the same data processing method and train on 10K Orca Math data (shuffled) as in [53]. We fix learning rate as 1e −5 for all methods as in QDoRA and train for one epoch with batch size 8. r =8 is adopted for LoRA, DoRA, AdaLoRA, and Spectral AdapterA while for OFT, we set number of diagonal blocks to be 800 to maintain similar amount of trainable parameters. LoRA alpha is set to be 16 following DoRA [32] convention and AdaLoRA hyperparameter is set following what has been used for MNLI benchmark in the original AdaLoRA report [65] with regularization set to 1e −3 which we find works better. For evaluation, we test on GSM8K [7] benchmark for exact matching. For more comparisons, Figure 9 provides training loss for smaller rank r = 4 (oft_r = 1600) and larger rank r =64 (oft_r =95). All settings are the same except that LoRA alpha is always kept as 16Figure 9: More experiments with Llama3 8B model with different number of trainable parameters. In the left plot, the training loss of LoRA and DoRA overlaps. See Appendix F.1 for details. twice as rank number. From Figure 9 we can observe that though increasing trainable parameters closes the gap between different tuning methods, our spectral adapter method is always superior to other PEFT methods and stays closest to full fine-tuning. F.2 Hyperparameter Setting for DeBERTaV3-base Experiment (Section 4.1) Dataset learning rate batch size #epochs optimizer weight decay MNLI 1e −4 32 1 AdamW 0.01 RTE 3e −4 32 10 AdamW 0.01 QNLI 1e −4 32 1 AdamW 0.01 MRPC 7e −4 32 13 AdamW 0.01 QQP 1e −4 32 10 AdamW 0.01 SST-2 1e −4 32 5 AdamW 0.01 CoLA 3e −4 32 8 AdamW 0.01 STS-B 5e −4 32 30 AdamW 0.01 Table 4: Hyperparameters for DeBERTaV3-base model fine-tuning with Spectral AdapterA in Section 4.1 Table 4 shows the hyperparameter setting for our Spectral AdapterA used for fine-tuning DeBERTaV3- base model in Section 4.1. We set number of diagonal blocks to be 4 and enable block sharing for OFT to maintain similar amount of trainable parameters. F.3 More About DeBERTaV3-base Experiment Left plot in Figure 10 presents the training loss and validation score comparisons of LoRA, SVDiff and our Spectral AdapterA for fine-tuning DeBERTaV3-base model on CoLA benchmark. We set learning rates for both LoRA and Spectral AdapterA as what has been used in popular public blog [40] for LoRA fine-tuning with DeBERTaV3-base model, which is not tuned in favor of our method. For SVDiff, since it is originally proposed for vision model tuning, we extend it to this experiment by tuning all singular values of pretrained weights. We find the same learning rate leads to poor fine-tuning results with SVDiff, we thus pick the best learning rate among [1e −3, 1e −4, 1e −5] according to validation performance and set learning rate to be 1e −3. We use r = 8 for LoRA and Spectral AdapterA. From Figure 10, it can be observed that Spectral AdapterA achieves better training and validation performance compared to both LoRA and SVDiff. Interestingly, in LoRA [20], the authors provide a correlation analysis between the LoRA additive component △W = ABT and original pretrained weight matrix W (see Section H.3 in [ 20]), and they find that the additive component does not contain the top singular directions of W. The authors therefore conclude that the learned LoRA component amplifies \"task-specific\" directions which are not emphasized in the pretrained weight matrix. Naively, this seems to suggest that tuning top singular subspace of pretrained weights is not ideal and one should identify the desired \"task-specific\" directions to improve LoRA. Here we show that this is not the case and fine-tuning top directions provides a significant improvement to LoRA. In the right plot of Figure 10 above, we experiment 17Figure 10: Left plot presents training loss and validation results for fine-tuning DeBERTaV3-base model with LoRA, SVDiff, and Spectral AdapterA on CoLA benchmark. Right plot compares the same statistics between LoRA and spectral adapter with top ranks and bottom ranks tuned respectively. tuning the top eighth rank and the bottom eighth rank of singular vector space in our Spectral AdapterA, which we present as \"Spectral Top\" and \"Spectral Bottom\" respectively. Remarkably, \"Spectral Top\" converges faster and scores higher than LoRA, which is then superior to \"Spectral Bottom\". This result unravels the fact that tuning different part of spectral space brings different tuning effect and tuning the top columns of singular vector space improves LoRA tuning significantly. See Section 3 for more theoretic insights. F.4 Hyperparameter Setting for Mistral 7B Experiment (Section 4.1) Method lr lora alpha batch size #epochs lora dropout weight decay LoRA 2.5e −5 16 4 2 0.05 0.01 DoRA 2.5e −5 16 4 2 0.05 0.01 Spectral AdapterA 2.5e −5 - 4 2 - 0.01 Table 5: Hyperparameters for Mistral 7B model fine-tuning task in Section 4.1 Table 5 shows training hyperparameter setting for fine-tuning Mistral 7B model in Section 4.1. We train with bfloat16 precision and fine-tune all q_proj, k_proj, v_proj, o_proj, and gate_proj weights. We evaluate with lm-evaluation-harness [47]. Table 6 shows accuracy comparison of different tuning methods with learning rate 1e −5. Our Spectral AdapterA still exceeds both LoRA and DoRA. F.5 Supplemental Materials for Multi-Adapter Fusion Experiment (Section 4.2) F.5.1 Comparison of Single Object Generation We present more experimental results to show that Spectral AdapterA with top ranks tuned behaves at least as good as LoRA with same parameter budget and is better than Orthogonal Adaptation [42], which is likely due to that Orthogonal Adaptation fixes LoRA parameter B and thus has limited expressiveness. We also show that tuning bottom ranks in spectral adapter behaves worse than all other methods. Figure 11 shows generation results for custom toy concept tuning, where Orthogonal Adaptation and Spectral AdapterA (bottom) generate inaccurate happy-face octopus, sad-face octopus, and green tortoise. Figure 12 shows generation results for custom animal concept tuning, where Orthogonal Adaptation and Spectral AdapterA (bottom) sometimes miss first dog concept. Method #Param GSM8K Pre-Trained − 38.82 LoRAr=8 0.16% 43.29 ±1.36 DoRAr=8 0.17% 43.52 ±1.37 SpectralA r=8 0.16% 46.47 ±1.37 Table 6: Supplemental experiments of fine-tuning Mistral 7B model with different PEFT methods with a different learning rate on GSM8K benchmark. See Section F.4 for experimental details. 18Figure 11: Generation results for single toy concept tuning with LoRA, Orthogonal Adaptation, and Spectral AdapterA with top and bottom ranks tuned respectively. F.5.2 More Multi-Adapter Fusion Generation Results Here we present more results for multi-adapter fusion generation. Figure 13 shows generation results for multi-object generation for custom toy concepts and Figure 14 presents generation results for multi-character generation for three computer scientists. See below for experimental details. Multi-Object Generation. As in Section 4.2, we fine-tune Chilloutmix diffusion model [8] on four custom toy concepts, see \"reference\" in Figure 13 for original toy images. We use r =8 for all methods and tune first, second, third, and fourth top eighth columns of singular vector space of pretrained weights for first, second, third, and fourth toys in our Spectral AdapterA. We follow all default experimental settings in [ 12] and tune all embedding layer, U-Net, and text-encoder. For better spatial alignment, we employ T2I-Adapter with sketch condition listed in \"reference\" in Figure 13. We randomly select three scenes and prompt fused-adapters for the results, see \"prompts\" in Figure 13 for individual prompt being used. From Figure 13, it can be observed that FedAvg and Orthogonal Adaptation generate unsatisfactory happy-face octopus and green tortoise toys. On the contrary, our spectral adapter generates high-quality images similar to Gradient Fusion while saving 19Figure 12: Generation results for single animal concept tuning with LoRA, Orthogonal Adaptation, and Spectral AdapterA with top and bottom ranks tuned respectively. Figure 13: Generation results of Chilloutmix diffusion model [8] tuned on four custom toy concepts with different fused adapters. See Appendix F.5.2 for details. much more time. Multi-Character Generation. We also experiment fine-tuning Chilloutmix diffusion model [ 8] with photos of three computer scientists Yoshua Bengio, Yann LeCun, and Geoffrey Hinton. As in multi-object generation, we use r = 8 for all methods and tune first, second, and third top eighth columns of singular vector space of pretrained weights for Bengio, Lecun, and Hinton in our Spectral AdapterA. We use T2I-Adapter [ 39] with keypose condition. See \"reference\" in Figure 14 for scientists’ photos and keypose condition being used. Figure 14 shows generation results for prompt 20\"<Vbengio> and <Vlecun> and <Vhinton>, standing near a lake, 4K, high quality, high resolution\" with different fused adapters, from which it can be observed that our spectral adapter generates picture of most consistent styles across characters and renders all scientists’ faces clearly. Figure 14: Generation results of Chilloutmix diffusion model [8] tuned on photos of three computer scientists with different fused adapters. See Appendix F.5.2 for details. F.6 Supplemental Materials for Parameter Efficiency Experiment (Section 4.3) In this section, we present more tuning results with various parameter budgets for parameter efficiency experiment studied in Section 4.3, see Section 4.3 for baseline method explanation. Table 7 shows the learning rates used for each baseline method and Table 8 shows learning rates used for our method, the rest experimental settings are default as in [12]. Method text encoder lr unet lr LoRA 1e −5 1e −4 VeRA (r =1) 1e −3 1e −4 VeRA (r =1024, 4096) 5e −3 1e −4 OFTA 1e −5 1e −4 LiDB 5e −4 1e −4 SVDiff 1e −3 1e −4 Table 7: Hyperparameters for baseline methods for diffusion model fine-tuning task in Section 4.3 Method vase chair table text unet text unet text unet Spectral AdapterR (r =2, 40) 1e −3 1e −2 1e −2 1e −2 1e −3 1e −2 Spectral AdapterR (r =4) 5e −3 5e −3 1e −3 1e −2 Spectral AdapterR (r =8) 5e −4 5e −2 1e −3 1e −2 1e −3 1e −2 Spectral AdapterR (r =16) 1e −2 1e −3 1e −3 1e −2 Spectral AdapterR (r =24) 1e −4 1e −2 1e −3 1e −3 1e −4 1e −2 Spectral AdapterR (r =32) 1e −4 5e −2 Table 8: Hyperparameters for Spectral AdapterR for diffusion model fine-tuning task in Section 4.3 Figure 15 shows generation results of Chilloutmix diffusion model [8] fine-tuned on custom table concept with different methods under various parameter budgets. The prompt used is “a <Vtable>”. LoRA generates acceptable images for all rank r =1, 2, 3 though it starts with 273k parameters even if rank is set to 1. OFT generates desirable images only for parameter budget > 400k. VeRA and LiDB start to generate reasonable images with >300k trainable parameters and SVDiff has only a single fixed parameter budget. Meanwhile, our Spectral AdapterR recognizes the shape of custom table with as few as 6k parameters and produces ideal images since 100k parameters. See Appendix F.7 for alignment score computation details. 21Figure 15: Generation results for prompt “a <Vtable>” after fine-tuning Chilloutmix diffusion model [8] on custom table images with different PEFT methods. Spectral R is abbreviation for Spectral AdapterR. See Appendix F.6 for details. F.7 Alignment Score Computation For better quantitative measurement, we compute alignment scores for our Figure 5,6,7,15 results. Specifically, we first compute CLIP [46] embedding for all generated/reference images and prompt texts, then we compute the cosine similarity between generated images’ embedding and reference images’ embedding to serve as their alignment score. Likewise, text score stands for cosine similarity between generated images’ embeddings and their corresponding prompt texts’ embeddings. Intuition here is that if an image is close to another image (or text), their CLIP vectors are expected to stay close as well. For Figure 5 alignment score computation, we crop each generated image vertically into three columns, then we compute their alignment scores to each corresponding reference animal, we finally take the mean of these three scores. For Figure 6, 7, 15 scores, we compute average score over three random trials, with each trial consisting of 8 generated images. 22",
      "meta_data": {
        "arxiv_id": "2405.13952v2",
        "authors": [
          "Fangzhao Zhang",
          "Mert Pilanci"
        ],
        "published_date": "2024-05-22T19:36:55Z",
        "pdf_url": "https://arxiv.org/pdf/2405.13952v2.pdf",
        "github_url": "https://github.com/pilancilab/spectral_adapter"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Spectral Adapter, a novel Parameter-Efficient Fine-Tuning (PEFT) method that enhances existing PEFT techniques by incorporating spectral information of pretrained weight matrices into the fine-tuning process. It proposes two mechanisms: Spectral AdapterA (additive tuning) and Spectral AdapterR (orthogonal rotation) of the top singular vectors obtained via Singular Value Decomposition (SVD). The theoretical analysis demonstrates that Spectral AdapterA doubles the rank capacity of LoRA given the same trainable parameter budget. Experimentally, Spectral Adapter shows superior parameter efficiency, improved tuning performance in language and diffusion models, and effectively addresses multi-adapter fusion challenges by distributing concept tunings along different spectral spaces.",
        "methodology": "The Spectral Adapter methodology involves two main mechanisms applied after performing Singular Value Decomposition (SVD) on pretrained weight matrices (W = USV^T). Spectral AdapterA (additive) modifies the top-r columns of U and V (U1, V1) by adding trainable matrices AU and AV, resulting in W' = [U1 + AU U2]S[V1 + AV V2]^T. Spectral AdapterR (rotational) orthogonally rotates the top-r columns of U and V using trainable orthogonal matrices RU and RV, defined as W' = [U1 RU U2]S[V1 RV V2]^T. The orthogonality constraint for RU and RV is efficiently handled using Cayley parameterization (R = (I + Q)(I - Q)^-1, where Q is a skew-symmetric matrix derived from a trainable parameter A as (A - A^T)/2). Both methods focus on tuning the top spectral space, with AU and AV initialized to zero, and RU and RV to identity matrices, respectively. Theoretical insights support tuning top singular vectors by analyzing adapter rank capacity and weight subspace alignment.",
        "experimental_setup": "The proposed Spectral Adapter was evaluated on both large language models (LLMs) and diffusion models against various PEFT baselines. For LLMs, DeBERTaV3-base (185M) was fine-tuned on GLUE benchmarks, and Mistral 7B was fine-tuned on the GSM8K task. For diffusion models, Chilloutmix was fine-tuned for multi-object fusion and parameter efficiency experiments. Multi-object generation involved custom animal and toy concepts, as well as multi-character generation for computer scientists, utilizing T2I-Adapter with sketch/keypose conditions. Parameter efficiency was assessed by fine-tuning on custom vase, chair, and table concepts. Baselines included LoRA, DoRA, OFT, AdaLoRA, SVDiff, LiDB, VeRA, Gradient Fusion, Orthogonal Adaptation, and FedAvg, with their official implementations and reported hyperparameters used where available. Evaluation metrics included training loss, validation scores (accuracy for LLMs), visual quality for diffusion models, and quantitative alignment scores computed using CLIP embeddings for images and texts. All experiments were conducted on NVIDIA RTX A6000 GPUs.",
        "limitations": "The primary limitation lies in the current fixed choice of tuning only the top spectral space. While theoretically supported under simplified conditions, further in-depth investigation into tuning different columns or regions of singular vector matrices is crucial to fully comprehend the intricate role of spectral information in fine-tuning. Additionally, the time cost associated with the Singular Value Decomposition (SVD) procedure increases with larger model sizes, which could pose a practical challenge, despite potential solutions like randomized SVD and parallelization.",
        "future_research_directions": "Future work could explore fine-tuning the spectral representation of specific model components, such as only the attention layer in large models. There's also potential to dynamically combine Spectral Adapter with other adaptive PEFT methods like AdaLoRA. A deeper investigation into tuning various columns of singular vector matrices, beyond just the top ones, is recommended to further understand the impact of spectral information. Furthermore, research into developing faster Singular Value Decomposition (SVD) methods would be beneficial to alleviate the computational overhead as models continue to grow in size.",
        "experimental_code": "class SpectralLinearLayer_OFT(nn.Module):    def __init__(self, name, original_module, rank=4, alpha=1, top=True, idx=0, revised_r=-1):        rank = 8        super().__init__()        self.name = name        if original_module.__class__.__name__ == 'Conv2d':            self.conv = True            in_channels, out_channels = original_module.in_channels, original_module.out_channels        else:            self.conv = False            in_channels, out_channels = original_module.in_features, original_module.out_features        W = original_module.weight.data.view(out_channels, in_channels)        U, S, V = torch.svd(W)        self.U = torch.nn.Parameter(U, requires_grad=False)        self.S = torch.nn.Parameter(S, requires_grad=False)        self.V = torch.nn.Parameter(V, requires_grad=False)        self.spectral_A = torch.nn.Parameter(torch.zeros(revised_r,revised_r), requires_grad=True)        self.spectral_B = torch.nn.Parameter(torch.zeros(revised_r,revised_r), requires_grad=True)        self.spectral_C = torch.nn.Parameter(torch.ones(revised_r), requires_grad=True)        original_module.forward = self.forward        self.original_module = original_module        self.top = top        self.idx = idx        assert revised_r>0        self.rank = revised_r    def cayley(self, data: torch.Tensor) -> torch.Tensor:        r, _ = data.shape        skew = 0.5 * (data - data.T)        I = torch.eye(r, device=data.device)        Q = torch.mm(I - skew, torch.inverse(I + skew))        return Q    def forward(self, hidden_states):        if self.top:            pad_U = self.U.clone()            pad_U[:,self.idx*self.rank:(self.idx+1)*self.rank] = self.U[:,self.idx*self.rank:(self.idx+1)*self.rank]@self.cayley(self.spectral_A)            pad_S = self.S.clone()            pad_S[self.idx*self.rank:(self.idx+1)*self.rank] = self.S[self.idx*self.rank:(self.idx+1)*self.rank]*self.spectral_C            pad_V = self.V.clone()            pad_V[:,self.idx*self.rank:(self.idx+1)*self.rank] = self.V[:,self.idx*self.rank:(self.idx+1)*self.rank]@self.cayley(self.spectral_B)        else:            raise Exception('')        pad_W = pad_U@pad_S.diag()@pad_V.T        if self.conv :            raise Exception('')        else:            return F.linear(hidden_states, pad_W, bias=self.original_module.bias)def merge_spectraloft_into_weight(original_state_dict, lora_state_dict, model_type, alpha, top=True, idx=0):    def get_spectral_A_name(original_layer_name):        if model_type == 'text_encoder':            spectral_A_name = original_layer_name.replace('q_proj.weight', 'q_proj.spectral_A')                 .replace('k_proj.weight', 'k_proj.spectral_A')                 .replace('v_proj.weight', 'v_proj.spectral_A')                 .replace('out_proj.weight', 'out_proj.spectral_A')                 .replace('fc1.weight', 'fc1.spectral_A')                 .replace('fc2.weight', 'fc2.spectral_A')        else:            spectral_A_name = k.replace('to_q.weight', 'to_q.spectral_A')                 .replace('to_k.weight', 'to_k.spectral_A')                 .replace('to_v.weight', 'to_v.spectral_A')                 .replace('to_out.0.weight', 'to_out.0.spectral_A')                 .replace('ff.net.0.proj.weight', 'ff.net.0.proj.spectral_A')                 .replace('ff.net.2.weight', 'ff.net.2.spectral_A')                 .replace('proj_out.weight', 'proj_out.spectral_A')                 .replace('proj_in.weight', 'proj_in.spectral_A')        return spectral_A_name        def cayley(data):        r, _ = data.shape        skew = 0.5 * (data - data.T)        I = torch.eye(r, device=data.device)        Q = torch.mm(I - skew, torch.inverse(I + skew))        return Q    assert model_type in ['unet', 'text_encoder']    new_state_dict = copy.deepcopy(original_state_dict)    load_cnt = 0    for k in new_state_dict.keys():        spectral_A_name = get_spectral_A_name(k)        spectral_B_name = spectral_A_name.replace('spectral_A', 'spectral_B')        spectral_C_name = spectral_A_name.replace('spectral_A', 'spectral_C')        U_name = spectral_A_name.replace('spectral_A', 'U')        S_name = spectral_A_name.replace('spectral_A', 'S')        V_name = spectral_A_name.replace('spectral_A', 'V')        if spectral_B_name in lora_state_dict:            load_cnt += 1            original_params = new_state_dict[k]            spectral_A_params = lora_state_dict[spectral_A_name].to(original_params.device)            spectral_B_params = lora_state_dict[spectral_B_name].to(original_params.device)            spectral_C_params = lora_state_dict[spectral_C_name].to(original_params.device)            U_params = lora_state_dict[U_name].to(original_params.device)            S_params = lora_state_dict[S_name].to(original_params.device)            V_params = lora_state_dict[V_name].to(original_params.device)            r = spectral_A_params.shape[0]            if top:                pad_U = U_params                 pad_U[:,idx*r:(idx+1)*r] = U_params[:,idx*r:(idx+1)*r]@(alpha*(cayley(spectral_A_params)-torch.eye(r).to(spectral_A_params.device))+torch.eye(r).to(spectral_A_params.device))                pad_V = V_params                pad_V[:,idx*r:(idx+1)*r] = V_params[:,idx*r:(idx+1)*r]@(alpha*(cayley(spectral_B_params)-torch.eye(r).to(spectral_A_params.device))+torch.eye(r).to(spectral_A_params.device))                pad_S = S_params                 pad_S[idx*r:(idx+1)*r] = S_params[idx*r:(idx+1)*r]*(alpha*(spectral_C_params-torch.ones(r).to(spectral_A_params.device))+torch.ones(r).to(spectral_A_params.device))            else:                raise Exception('')            if len(original_params.shape) == 4:                raise Exception('')            else:                spectral_param = pad_U@pad_S.diag()@pad_V.T            new_state_dict[k] = spectral_param    print(f'load {load_cnt} Spectrals of {model_type}')    return new_state_dict",
        "experimental_info": "The method performs Singular Value Decomposition (SVD) on the pretrained weight matrices (W = USV^T). It then applies modifications to the top-`rank` spectral components of U, S, and V. Specifically, trainable matrices `spectral_A` and `spectral_B` are used to orthogonally rotate the top-`rank` columns of U and V, respectively. These matrices are initialized to zeros, and Cayley parametrization (R = (I - Q)(I + Q)^-1, where Q is a skew-symmetric matrix derived from `spectral_A` or `spectral_B`) is employed to ensure orthogonality. A trainable vector `spectral_C`, initialized to ones, is used to scale the top-`rank` singular values. During the merging process, an `alpha` parameter is applied to scale the *delta* changes (i.e., `Cayley(A) - I`, `Cayley(B) - I`, and `C - 1`) before reconstructing the final weight matrix. The effective rank of adaptation (referred to as `revised_r` in the code) is dynamically determined by the configuration (`text_encoder_cfg['lora_cfg']['rank']` or `unet_cfg['lora_cfg']['rank']`) provided during initialization. This implementation is designed for linear layers and does not support convolutional layers."
      }
    },
    {
      "title": "Parameter-Efficient Fine-Tuning Design Spaces",
      "abstract": "Parameter-efficient fine-tuning aims to achieve performance comparable to\nfine-tuning, using fewer trainable parameters. Several strategies (e.g.,\nAdapters, prefix tuning, BitFit, and LoRA) have been proposed. However, their\ndesigns are hand-crafted separately, and it remains unclear whether certain\ndesign patterns exist for parameter-efficient fine-tuning. Thus, we present a\nparameter-efficient fine-tuning design paradigm and discover design patterns\nthat are applicable to different experimental settings. Instead of focusing on\ndesigning another individual tuning strategy, we introduce parameter-efficient\nfine-tuning design spaces that parameterize tuning structures and tuning\nstrategies. Specifically, any design space is characterized by four components:\nlayer grouping, trainable parameter allocation, tunable groups, and strategy\nassignment. Starting from an initial design space, we progressively refine the\nspace based on the model quality of each design choice and make greedy\nselection at each stage over these four components. We discover the following\ndesign patterns: (i) group layers in a spindle pattern; (ii) allocate the\nnumber of trainable parameters to layers uniformly; (iii) tune all the groups;\n(iv) assign proper tuning strategies to different groups. These design patterns\nresult in new parameter-efficient fine-tuning methods. We show experimentally\nthat these methods consistently and significantly outperform investigated\nparameter-efficient fine-tuning strategies across different backbone models and\ndifferent tasks in natural language processing.",
      "full_text": "PARAMETER -EFFICIENT FINE -TUNING DESIGN SPACES Jiaao Chen†∗, Aston Zhang‡, Xingjian Shi‡, Mu Li‡, Alex Smola‡, Diyi Yang⋄ †Georgia Institute of Technology,‡Amazon Web Services, ⋄Stanford University ABSTRACT Parameter-efﬁcient ﬁne-tuning aims to achieve performance comparable to ﬁne-tuning, using fewer trainable parameters. Several strategies (e.g., Adapters, preﬁx tuning, BitFit, and LoRA) have been proposed. However, their designs are hand-crafted separately, and it remains unclear whether cer- tain design patterns exist for parameter-efﬁcient ﬁne-tuning. Thus, we present a parameter-efﬁcient ﬁne-tuning design paradigm and discover design patterns that are applicable to different experi- mental settings. Instead of focusing on designing another individual tuning strategy, we introduce parameter-efﬁcient ﬁne-tuning design spaces that parameterize tuning structures and tuning strate- gies. Speciﬁcally, any design space is characterized by four components: layer grouping, trainable parameter allocation, tunable groups, and strategy assignment. Starting from an initial design space, we progressively reﬁne the space based on the model quality of each design choice and make greedy selection at each stage over these four components. We discover the following design patterns: (i) group layers in a spindle pattern; (ii) allocate the number of trainable parameters to layers uni- formly; (iii) tune all the groups; (iv) assign proper tuning strategies to different groups. These design patterns result in new parameter-efﬁcient ﬁne-tuning methods. We show experimentally that these methods consistently and signiﬁcantly outperform investigated parameter-efﬁcient ﬁne-tuning strategies across different backbone models and different tasks in natural language processing1. 1 Introduction Large pretrained models have achieved the state-of-the-art performances across a wide variety of downstream natural language processing tasks through ﬁne-tuning on task-speciﬁc labeled data [Devlin et al., 2019, Liu et al., 2019, Yang et al., 2019, Joshi et al., 2019, Sun et al., 2019, Clark et al., 2019, Lewis et al., 2020a, Bao et al., 2020, He et al., 2020, Raffel et al., 2020, Ziems et al., 2022]. However, ﬁne-tuning all the parameters and storing them separately for different tasks is expensive in terms of computation and storage overhead (e.g., 355M parameters for RoBERTa [Liu et al., 2019] and 175B parameters for GPT- 3 [Brown et al., 2020]). This makes it difﬁcult to deploy in real-world natural language processing (NLP) systems composed of multiple tasks. To adapt general knowledge in pretrained models to speciﬁc down-stream tasks in a more parameter-efﬁcient way, various strategies have been proposed where only a small number of (extra) parameters are learned while the remaining pretrained parameters are frozen [Houlsby et al., 2019a, Pfeiffer et al., 2021, Li and Liang, 2021, Brown et al., 2020, Lester et al., 2021a, Schick and Sch ¨utze, 2021, Ziems et al., 2022]. Adapter tuning [Houlsby et al., 2019a] is among the earliest strategies to steer pretrained models with a limited number of parameters. It inserts adapters (small neural modules) to each layer of the pretrained network and only the adapters are trained at the ﬁne-tuning time. Inspired by the success of prompting methods that control pretrained language models through textual prompts [Brown et al., 2020], preﬁx tuning [Li and Liang, 2021] and prompt tuning [Lester et al., 2021b] prepend additional tunable tokens to the input or hidden layers and only train these soft prompts when ﬁne-tuning on downstream tasks. BitFit [Zaken et al., 2021] updates the bias terms in pretrained models while freezing the remaining parameters. LoRA [Hu et al., 2021] decomposes attention weight gradients into low-rank matrices to reduce the number of trainable parameters. With promising results from such research, He et al. [2022] proposed a uniﬁed view of these existing strategies and ∗Work done during an internship at Amazon Web Services. Correspondence to Jiaao Chen<jiaaochen@gatech.edu> and Aston Zhang <astonz@amazon.com>. 1Code is available at: https://github.com/amazon-science/peft-design-spaces . arXiv:2301.01821v1  [cs.CL]  4 Jan 2023P P P L P L A B L A B L… Layer Grouping P L Strategy Assignment Trainable Parameter Allocation Tunable Groups p ⇥ p Figure 1: A parameter-efﬁcient ﬁne-tuning design space. It is characterized by (i) layer grouping (how to group consecutive layers), (ii) trainable parameter allocation (how to allocate the number of trainable parameters to layers), (iii) tunable groups (which groups will be ﬁnetuned), and (iv) strategy assignment (how to assign proper strategies, such as among Adapter, Preﬁx, BitFit, and LoRA, to groups). illustrated differences and connections among them. Like its antecedents, the resulting method is stillequally assigned to different pretrained layers. Despite being effective, most parameter-efﬁcient ﬁne-tuning strategies have been developed via manual design pro- cesses, without much consideration of whether design patterns exist across these different strategies and how such patterns might apply to different backbone models and downstream tasks. Moreover, different strategies are usually applied separately; thus, it is unclear which strategy works best when and where [Mao et al., 2022], as well as how these different strategies reinforce or complement each other. In this light, our goal is to understand the parameter- efﬁcient ﬁne-tuning design in a more comprehensive view and discover design patterns that are both interpretable and applicable across different experimental settings. Instead of designing yet another individual strategy that is equally applied to different pretrained layers, we introduce parameter-efﬁcient ﬁne-tuning design spaces that parameterize both tuning structures and strategies. More con- cretely, any of these design spaces is characterized by four major components as shown in Figure 1: layer grouping, trainable parameter allocation, tunable groups, and strategy assignment. Starting from a relatively unconstrained parameter-efﬁcient ﬁne-tuning design space, we progressively reﬁne the space by comparing the overall quality of models randomly sampled from design spaces enforced with different constraints (e.g., each group has the same number of layers). Throughout the experimental process, we discover several design patterns for parameter-efﬁcient ﬁne-tuning, such as group layers in a spindle pattern, allocate the number of trainable parameters to layers uniformly, tune all the groups, and assign proper tuning strategies to different groups. We fur- ther introduce new parameter-efﬁcient ﬁne-tuning methods that adopt all these discovered design patterns. Extensive experiments show that our methods consistently outperform investigated parameter-efﬁcient ﬁne-tuning strategies. Al- though we use T5 [Raffel et al., 2020] and classiﬁcation tasks as the working example, we ﬁnd that our methods with all these discovered design patters are applicable to other backbones (e.g., RoBERTa [Liu et al., 2019], BART [Lewis et al., 2020b], and XLNet [Yang et al., 2019]) and different natural language processing tasks (e.g., summarization, machine translation, and eight SuperGLUE datasets). Our contributions can be summarized as follows: (i) We introduce parameter-efﬁcient ﬁne-tuning design spaces. (ii) Based on these design spaces, we discover several design patterns in parameter-efﬁcient ﬁne-tuning via comprehen- sive experiments. (iii) Our discovered design patterns lead to parameter-efﬁcient ﬁne-tuning methods, consistently outperforming investigated parameter-efﬁcient ﬁne-tuning strategies across different backbone models and different NLP tasks. 22 Related Work Our work is closely related to and built upon the research about the network design spaces and parameter-efﬁcient ﬁne-tuning. We discuss the connections and differences below. Network Design Spaces A lot of works designed neural network models via an ad-hoc discovery of new design choices that improve performances [Radosavovic et al., 2019], such as the use of deeper architectures or residuals. Recently, there have been works [Radosavovic et al., 2020, You et al., 2020, Radosavovic et al., 2019] performing at the design space level to discover new design principles for convolutional neural networks [Radosavovic et al., 2020] and graph neural networks [You et al., 2020]. Inspired by this line of research, we focus on the design space perspective to rethink parameter-efﬁcient ﬁne-tuning, with the goal of discovering design patterns that are applicable to different experimental settings. Parameter-Efﬁcient Fine-Tuning for NLP As pretrained models grow in size, storing ﬁne-tuned models becomes exceedingly expensive, and ﬁne-tuning becomes infeasible for those without extremely high compute resources. A growing body of research has been devoted to ﬁnding parameter-efﬁcient alternatives for adapting large-scale pre- trained models with reduced memory and storage costs. Houlsby et al. [2019b] proposed to adapt large models using bottleneck layers (with skip-connections) between each layer. This idea has been extended in many domains [Stick- land and Murray, 2019, Pfeiffer et al., 2020, Rebufﬁ et al., 2017, Lin et al., 2020]. Other works have aimed to avoid introducing additional parameters by identifying and training only a subset of all model parameters [Zhao et al., 2020, Guo et al., 2020, Mallya et al., 2018, Radiya-Dixit and Wang, 2020, Sung et al., 2021, Zaken et al., 2021]. Recent works also explored the idea of rank decomposition based on parameterized hypercomplex multiplications via the Kro- necker product [Zhang et al., 2021a] and injecting trainable rank decomposition matrices into each layer [Hu et al., 2021, Karimi Mahabadi et al., 2021]. Li and Liang [2021] introduced preﬁx-tuning that prepends a set of preﬁxes to autoregressive language models or prepends preﬁxes for both encoders and decoders. The preﬁx parameters are updated while the pretrained parameters are ﬁxed. Lester et al. [2021a] proposed a similar method, but only added virtual tokens at the embedding layer of large-scale models rather than discrete prompts [Deng et al., 2022, Zhong et al., 2022]. Bari et al. [2022] proposed semi-parametric prompt tuning that converges more easily, where memory prompts are input-adaptive without the need for tuning. Recently, He et al. [2022] and Ding et al. [2022] proposed a uniﬁed view of the existing parameter-efﬁcient ﬁne-tuning strategies and illustrated the difference and connections among them. Mao et al. [2022] also introduced a uniﬁed framework to combine different methods through mixture- of-experts. In contrast to these aforementioned works that assign their individual method equally to different pretrained layers, we focus on more general design spaces of parameter-efﬁcient ﬁne-tuning. This could provide a more comprehensive view of parameter-efﬁcient ﬁne-tuning in terms of both the tuning structures and tuning strategies. Through experiments where we progressively reﬁne design spaces, we discover design patterns for parameter-efﬁcient ﬁne-tuning. 3 Components of Design Spaces When deﬁning design spaces of parameter-efﬁcient ﬁne-tuning, we aim to cover key design components and provide a representative set of choices in each design component. Note that our goal is not to enumerate all possible design spaces, but to demonstrate how the use of design spaces can help inform parameter-efﬁcient ﬁne-tuning research. Concretely, in our work, the parameter-efﬁcient ﬁne-tuning design spaces are formed by a representative set of choices in parameter-efﬁcient ﬁne-tuning, which consists of the following four components: (i) layer grouping, (ii) trainable parameter allocation, (iii) tunable groups, and (iv) strategy assignment. Following the illustrated design space exam- ple in Figure 1, we describe these four design components in detail below and will explore their design choices in Section 4. Layer Grouping Different layers in pretrained models capture different information and behave differently. For example, Jawahar et al. [2019] found that the {3, 4, 5, 6, 7, 9, 12}-th layers have the most representation power in BERT and every layer captures a different type of information ranging from the surface, syntactic, to the semantic level representation of text. For instance, the 9th layer has predictive power for semantic tasks such as checking random swapping of coordinated clausal conjuncts, while the 3rd layer performs best in surface tasks like predicting sentence length. Therefore when adapting these pretrained models to downstream tasks, how to group layers with similar behaviors together is critical to the design and application of proper parameter-efﬁcient ﬁne-tuning strategies. For this design component, we study the patterns of how to group consecutive layers in pretrained models (e.g., transformer layers in T5) during the ﬁne-tuning process. 3Trainable Parameter Allocation In parameter-efﬁcient ﬁne-tuning, the total number of trainable parameters is usually preset, such as a small portion of the total number of parameters in the pretrained models. We will study different design choices for how to allocate a predeﬁned number of trainable parameters to layers. Tunable Groups Zaken et al. [2021] found that not all the parameters need to be tuned during ﬁne-tuning on the downstream tasks. For instance, BitFit [Zaken et al., 2021] only updates the bias parameters in pretrained models while freezing the remaining parameters. Thus, we study which groups need to be learned during parameter-efﬁcient ﬁne-tuning to attain better performances. Strategy Assignment In order to improve the parameter efﬁciency, different sets of strategies [Li and Liang, 2021, Lester et al., 2021a, Houlsby et al., 2019a, Hu et al., 2021] have been proposed where only a small number of (ex- tra) parameters are tuned and the remaining parameters in these pretrained models are frozen to adapt their general knowledge to speciﬁc down-stream tasks. Inspired by effectiveness of offering architectural ﬂexibility [Zhang et al., 2021a,b], we hypothesize that different groups might beneﬁt from different proper strategies (or combinations) for capturing different types of information. More formally, given a set of individual strategies Afor assignment, for any group Gi, assign a subset Ui ⊂A to each layer in Gi. 4 Discovering Design Patterns Building on these four different design components of PEFT design spaces, we will start from a relatively uncon- strained design space and progressively discover the design patterns. 4.1 Design Space Experimental Setup We ﬁrst describe our experimental setup for discovering the design patterns. Note that our process is generic for other tasks and future pretrained backbone models. Datasets Our process for discovering design patterns of PEFT is based on the average performances on the widely- used GLUE benchmark [Wang et al., 2018]. It covers a wide range of natural language understanding tasks. First, single-sentence tasks include (i) Stanford Sentiment Treebank (SST-2) and (ii) Corpus of Linguistic Acceptability (CoLA). Second, similarity and paraphrase tasks include (i) Quora Question Pairs (QQP), (ii) Semantic Textual Sim- ilarity Benchmark (STS-B), and (iii) Microsoft Research Paraphrase Corpus (MRPC). Third, inference tasks include (i) Multi-Genre Natural Language Inference (MNLI), (ii) Question Natural Language Inference (QNLI), and (iii) Rec- ognizing Textual Entailment (RTE). To compare performances, the Matthews correlation is measured for CoLA; the Spearman correlation is used for STS-B, and accuracy is measured for the rest GLUE tasks. Pretrained Backbone Models and Model Settings We use T5-base/3b [Raffel et al., 2020] as the main pretrained backbone models for discovering design patterns via our PEFT design spaces. We use Hugging Face 2 for our imple- mentations and follow the default settings. During the exploration, we set the total number of trainable parameters (in the percentage of that in the backbone model) to 0.5% by following He et al. [2022]. 4.2 Discovering Design Patterns Using T5-base In this subsection, we describe the empirical process for discovering the design patterns using T5-base (pretrained backbone model) as the working example. Each PEFT design space (denoted as Si) consists of a set of models ( Si- models) that satisfy constraints characterizing the space with respect to layer grouping, trainable parameter allocation, tunable groups, and strategy assignment. To discover design patterns, we start from a relatively unconstrained PEFT design space ( S0). Then we progressively reﬁne design spaces (from S0 to S1:4) by comparing overall quality of models in design spaces enforced with different constraints (e.g., each group has the same number of layers). To quantify the overall quality of models in any design space Si with a low-compute, low-epoch regime [Radosavovic et al., 2020], we randomly sample 100 models from Si, ﬁne-tune with 3 epochs 3, and compute the average of the GLUE average performances. 2https://huggingface.co/docs/transformers/index 3We set the low epoch by observing whether it is enough for models to obtain stable performances to draw consistent conclusions (See Table 7 in the Appendix). 4We emphasize that our goal is to demonstrate how the perspective of design spaces can help inform PEFT research, rather than to ﬁnd out the “best” design space or method. For computational efﬁciency, it is beyond the scope of this work to enumerate all possible constraints with respect to the design space components (Section 3). 4.2.1 The Initial S0 Design Space The initial relatively unconstrained design space S0 consists of all models without constraints on the design space components (Section 3). Individual PEFT strategies consist of Adapter, Preﬁx, BitFit, and LoRA. One can think of this S0 design space as a set of random models ( S0-models) with random design patterns. Speciﬁcally, without grouping constraints, each layer of the pretrained layer has a half chance to be tuned: if tuned, random strategies (or combinations) with a random amount of trainable parameters are assigned to that layer. Before comparing more subtle design patterns such as how to properly assign tunable strategies among Adapter, Preﬁx, BitFit, and LoRA, we begin with exploring how to group layers and how to allocate the total number of trainable parameters to layers. 4.2.2 The S1 Design Space with Additional Grouping Constraints Inspired by Radosavovic et al. [2020], we also consider 4 groups (G1, . . . , G4, in the order of forward pass) in the experiments 4. Denote by Ni the number of layers in Gi. As illustrated in Figure 2, we compare the following layer grouping patterns: (i) Increasing (Ni+1 > Ni): the number of layers in groups gradually increases; (ii) Uniform (Ni+1 = Ni): the number of layers in groups is the same; (iii) Decreasing (Ni+1 < Ni): the number of layers in groups gradually decreases; (iv) Spindle (N1 < N2 = N3 > N4): the numbers of layers in groups at both ends are smaller; and (v) Bottleneck (N1 > N2 = N3 < N4): the numbers of layers in groups at both ends are bigger. Figure 2: Layer grouping patterns, where the horizontal and vertical axes represent groups (G1, . . . , G4) and numbers of layers in groups. These layer grouping patterns lead to 5 different design spaces. Any of these 5 design spaces consists of all models in the S0 design space that satisfy one of these grouping pattern constraints. To compare the overall model qualities of different design spaces, we (i) randomly sample 100 models from the S0 design space that satisfy each grouping pattern constraint (Figure 2); (ii) ﬁne-tune with 3 epochs; and (iii) compute the average performances for each design space. We will follow this procedure as we progressively add new constraints later. The averaged performances are shown in Table 1 5. We ﬁnd that models from the design space with the spindle grouping pattern (Figure 2) consistently outperform those from the other design spaces across all the 8 GLUE tasks. This may be due to the complexities of information captured in different layers of large pretrained models, which favor information adaptation in the discovered layer grouping pattern. From now on, we will group layers in a spindle pattern. We refer to S0 with this additional design pattern as the new S1 design space. 4.2.3 The S2 Design Space with Additional Parameter Constraints We continue to explore design patterns in trainable parameter allocation to reﬁne the S1 design space. Denote by ni the number of trainable parameters for the i-th layer of the pretrained backbone model, we compare the following design patterns: (i) Increasing (ni+1 ≥ni): the number of trainable parameters in every layer gradually increases (or remains the same); (ii) Uniform (ni+1 = ni): the number of trainable parameters in every layer is the same; and (iii) Decreasing (ni+1 ≤ni): the number of trainable parameters in every layer gradually decreases (or remains the same). Following the procedure described in Section 4.2.2, we obtain 100 models for each of these 3 new design spaces. Table 2 reports the average performances of these 3 design spaces. The uniform allocation design pattern obtains the highest GLUE average performance, making this relatively simple, interpretable design pattern favorable. 4The experimental results with 8 groups are shown in the Table 16 in the Appendix. 5The training time for the step is shown in the Table 18 in the Appendix. 5Table 1: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different layer grouping constraints to the S0 design space. Layer Grouping SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg S0-models 76.9 70.1 72.5 73.3 63.6 71.7 73.8 24.3 65.7 Increasing 85.3 74.9 77.2 77.5 66.8 76.2 76.0 33.0 70.8 Uniform 84.8 73.7 78.1 78.6 68.5 77.8 79.2 36.1 72.1 Decreasing 81.9 72.1 78.3 76.7 67.3 75.9 78.6 28.7 70.0 Spindle 86.9 75.5 79.8 79.4 69.8 78.3 80.1 37.3 73.3 Bottleneck 84.5 74.6 76.9 78.1 69.2 76.2 78.6 32.1 71.3 Table 2: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different parameter allocation constraints to the S1 design space. Param Allocation SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg Increasing 87.2 77.9 79.4 78.7 71.6 77.6 81.4 32.0 73.2 Uniform 87.8 77.4 80.1 80.5 73.9 78.1 80.4 34.3 74.0 Decreasing 86.4 75.8 78.4 77.0 70.4 77.1 78.7 35.8 72.4 We will allocate the number of trainable parameters to layers uniformly. We refer to S1 with this additional design pattern as the new S2 design space. 4.2.4 The S3 Design Space with Additional Tunable Group Constraints Before digging into the strategy assignment design patterns, it is necessary to examine which groups need to be tuned. After all, it is only meaningful to study assigning strategies to different groups after we ﬁnd out which groups need to be ﬁne-tuned. As shown in Table 3, we explore various design patterns in tunable groups to further constrain the S2 design space. Based on the GLUE average performances, we ﬁnd that all the groups need to be tuned to obtain the best performances. This suggests that all the groups of pretrained layers have captured useful information that should be adapted to the downstream tasks. We will tune all the groups. We refer to S2 with this additional design pattern as the new S3 design space. 4.2.5 The S4 Design Space with Additional Strategy Constraints Finally, we study the subtle design pattern with respect to assigning proper strategies by further constraining the derived S3 design space. Speciﬁcally, each design space consists of models that assign a subset of {Adapter (A), Preﬁx (P), BitFit (B), and LoRA (L) }to all layers of any group Gi (i = 1, . . . ,4). We begin by adding different G1 strategy assignment constraints to the S3 space. Following the same pattern discovery procedure (Section 4.2.2), we discover strategy assignment patterns for G1. Then we progressively add Gi (i >1) strategy assignment constraints together with the discovered strategy assignment patterns for all Gj (j = 1, . . . , i−1) to the S3 space. Due to space limit, we present results of this process in the Appendix ( G1 in Table 8, G2 Table 9, G3 in Table 10, and G4 in Table 11), which suggests strategy assignment ofG1-(A, L) – G2-(A, P) – G3-(A, P, B) –G4-(P, B, L) for the T5-base pretrained backbone model. We will assign the discovered proper tuning strategies to groups.We refer to S3 with this additional design pattern as the new S4 design space, which consists of the ﬁnal S4-model. 4.3 Discovering Design Patterns Using T5-3b We then repeat the above process on T5-3b to examine if the design patterns we discovered using smaller models (T5- base) still apply when we use larger models. The results are shown in Table 12 (layer grouping), Table 13 (trainable parameter allocation), Table 14 (tunable groups) and Table 15 (strategy assignment) in the Appendix. We observe that the design patterns still apply when larger models like T5-3b are used: (i) grouping layers in a spindle pattern (Table 12), (ii) uniformly allocating the number of trainable parameters to layers (Table 13), (iii) tuning all the groups 6Table 3: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different tunable group constraints to the S2 design space. Tunable Groups SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G1 82.6 72.1 77.6 70.6 65.3 71.9 77.6 27.6 68.2 G2 83.3 72.8 77.5 72.8 63.6 72.8 77.5 27.5 68.4 G3 83.6 73.3 78.2 73.3 66.4 71.3 77.9 22.9 68.4 G4 83.2 73.0 77.9 73.7 63.9 72.0 77.9 27.9 68.7 G1, G2 83.5 73.2 78.0 75.4 67.7 73.2 78.0 28.0 69.6 G3, G4 87.8 74.6 78.3 76.9 68.6 74.3 78.3 28.3 70.7 G1, G2, G3 86.0 75.8 79.0 77.8 71.8 78.8 79.0 33.0 72.6 G2, G3, G4 85.2 76.6 79.1 78.6 70.1 77.6 79.1 31.9 72.2 G1,G2,G3,G4 88.3 77.4 82.1 81.5 74.9 79.4 81.4 34.3 74.9 Table 4: Performances of different tuning methods on the GLUE datasets using the T5-base (upper part) and T5-3b (lower part) pretrained backbone models, respectively. The results are averaged over 20 random runs (with standard deviations as subscripts). The S4-model and the S4-3b-model perform signiﬁcantly better than the second-best PEFT methods in all the eight datasets at the signiﬁcance level p <0.05(∗) or even p <0.01(∗∗). Method SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Average full 95.2 87.1 93.7 89.4 80.1 89.4 90.7 51.1 84.5 Adapter 94.6 85.5 89.8 86.7 75.3 86.7 89.1 59.2 83.3 Preﬁx 94.0 81.6 87.8 83.4 64.3 83.1 84.8 34.0 76.6 BitFit 94.4 84.5 90.6 88.3 74.3 86.6 90.1 57.7 83.3 LoRA 94.8 84.7 91.6 88.5 75.8 86.3 88.7 51.5 82.7 S4-model 95.5∗∗ 1.7 87.6∗∗ 1.0 92.7∗∗ 1.1 88.8∗∗ 1.0 80.4∗ 2.3 87.4∗ 2.0 91.2∗∗ 2.4 62.2∗ 3.2 85.7 full 97.4 91.4 96.3 89.7 91.1 90.6 92.5 67.1 89.5 Adapter 96.3 89.9 94.7 87.8 83.4 90 89.7 65.2 87.1 Preﬁx 96.3 82.8 88.9 85.5 78.3 83.5 85.4 42.7 80.4 BitFit 95.8 89.5 93.5 88.5 86.2 90.7 88.6 64.2 87.1 LoRA 96.2 90.6 94.9 89.1 91.2 91.1 91.1 67.4 88.9 S4-3b-model 97.2∗∗ 1.8 91.6∗∗ 1.2 96.6∗∗ 1.0 89.5∗∗ 1.5 91.5∗ 2.8 91.5∗ 2.5 91.9∗ 2.0 69.7∗ 3.4 89.9 (Table 14), and (iv) tuning different groups with proper strategies (Table 15). For T5-3b, the discovered proper strategy assignment is G1-(P, L) –G2-(A, L) – G3-(P, B, L) –G4-(A, P, B). We refer to the ﬁnal design space asS4-3b and the ﬁnal model in this space as S4-3b-model. 5 Evaluation The S4-model (Section 4.2.5) and S4-3b-model (Section 4.3) adopt all the design patterns that have been discovered by using T5-base and T5-3b, respectively. As a result, they are both new methods of PEFT. We will evaluate their effectiveness when applied to different pretrained backbone models and different NLP tasks. 5.1 Experimental Setup Datasets Besides the GLUE datasets [Wang et al., 2018] (Section 4.1), we further evaluate our methods on two generation tasks used by He et al. [2022]: (i) Abstractive Summarization using XSum [Narayan et al., 2018], and (ii) Machine Translation using the WMT 2016 en-ro dataset [Bojar et al., 2016]. We report ROUGE scores [Lin, 2004] on the XSum test set, and BLEU scores [Papineni et al., 2002] on the en-ro test set. Models and Model Settings We mainly compare our methods with the following baselines: (i) Full Fine-tuning (full): it ﬁne-tunes all the model parameters in the pretrained models; (ii) Adapter [Houlsby et al., 2019a]: it adds adapter modules to each transformer layer; (iii) Preﬁx [Li and Liang, 2021]: it optimizes a set of small continuous vectors prepended to transformer layers; (iv) BitFit [Zaken et al., 2021]: it only updates the bias terms in pretrained models; (v) LoRA [Hu et al., 2021]: it decomposes the attention weight into low-rank matrices to reduce the number of trainable parameters. Besides T5 [Raffel et al., 2020], we additionally apply our methods to other backbone models 7Table 5: Performances of different tuning methods on GLUE datasets using the RoBERTa-base (upper part) and RoBERTa-large (lower part) pretrained backbone models. The results are averaged over 20 random runs (with standard deviations as subscripts). Here we also include two baselines: (i) S0-model, where all the designs are randomly selected for RoBERTa as in the S0 design space; (ii) S3-model, where strategies are randomly assigned to different RoBERTa layer groups as in the S3 design space. The S4-model and S4-3b-model perform signiﬁcantly better than the second-best PEFT methods in all the eight datasets at the signiﬁcance level p <0.05(∗) or even p <0.01(∗∗). Method SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Average full 94.8 87.6 92.8 91.9 80.8 90.3 90.2 63.6 86.5 Adapter 94.2 87.1 93.1 90.2 71.5 89.7 88.5 60.8 84.4 Preﬁx 94.0 86.8 91.3 90.5 74.5 90.3 88.2 61.5 84.6 BitFit 93.7 84.8 91.3 84.5 77.8 90.8 90.0 61.8 84.3 LoRA 94.9 87.5 93.1 90.8 83.1 90.0 89.6 62.6 86.4 S0-model 94.2 95.3 90.4 90.6 75.6 89.6 88.0 60.9 85.6 S3-model 94.3 87.2 92.8 91.0 81.8 90.3 89.2 63.2 86.2 S4-model 94.81.6 87.8∗∗ 0.8 93.4∗∗ 1.3 91.6∗ 1.2 85.8∗∗ 1.8 90.4∗ 2.0 90.0∗∗ 1.8 63.2∗ 3.5 87.1 full 96.4 90.2 94.7 92.2 86.6 92.4 90.9 68.0 88.9 Adapter 96.6 90.5 94.8 91.7 80.1 92.1 90.9 67.8 88.1 Preﬁx 95.7 87.6 92.1 88.7 82.3 89.6 87.4 62.8 85.7 BitFit 96.1 88.0 93.4 90.2 86.2 90.9 92.7 64.2 87.7 LoRA 96.2 90.6 94.7 91.6 87.4 92.0 89.7 68.2 88.8 S0-model 95.5 86.5 92.3 89.8 84.6 89.2 86.3 61.2 85.6 S3-model 96.3 89.4 93.8 90.2 85.9 90.8 90.9 63.4 87.6 S4-3b-model 96.6∗∗ 1.3 90.8∗ 1.1 95.1∗∗ 0.8 92.0∗∗ 1.2 87.22.8 92.3∗ 2.2 91.8∗∗ 1.8 68.4∗ 3.2 89.3 including RoBERTa-base/large [Liu et al., 2019] and BART-base/large [Lewis et al., 2020a]. We use the default settings. We set the total number of trainable parameters (in the percentage of that in the backbone model) by following He et al. [2022]. Speciﬁcally, this value is set to 0.5% for Adapter, Preﬁx, LoRA, and our methods, and 0.1% for BitFit. For all the experiments, we followed Liu et al. [2019] to set the linear decay scheduler with a warmup ratio of 0.06 for training. The batch size was 128 for base models and 64 for large models. The maximum learning rate was 5e −5 and the maximum number of training epochs was set to be either 5 or 10. All the experiments were performed using 8 A100 GPUs. 5.2 Effectiveness on GLUE with T5 Backbones Table 6: Performances of different tuning methods on generation tasks (XSUM and en-ro) using the BART-base (upper part) and BART-large (lower part) pretrained backbone models. Method XSUM(R-1/2/L) en-ro (BLEU) full 40.5/19.2/34.8 34.5 Adapter 37.7/17.9/33.1 33.3 Preﬁx 38.2/18.4/32.4 33.8 BitFit 37.2/17.5/31.4 33.2 LoRA 38.9/18.6/33.5 33.6 PA 39.3/18.7/33.8 33.8 S4-model 40.2/19.3/34.2 34.1 full 45.1/22.3/37.2 37.9 Adapter 43.8/20.8/35.7 35.3 Preﬁx 43.4/20.4/35.5 35.6 BitFit 42.8/18.7/33.2 35.2 LoRA 42.9/19.4/34.8 35.8 PA 43.9/20.6/35.6 36.4 S4-3b-model 44.3/21.7/36.8 37.2 With our discovered design patterns, we ﬁne-tune T5-base (S4-model) and T5-3b ( S4-3b-model) on GLUE and compare them with all the baseline methods. The results are shown in Table 4, where the key measure is the GLUE average performance (last column). We ﬁnd that our S4-model and S4- 3b-model consistently outperform the investigated methods in the key measure. By tuning only 0.5% parameters, our methods even outperform the full ﬁne-tuning baseline where all the parameters are tuned, indicating the effectiveness of our discov- ered PEFT design patterns. 5.3 General Effectiveness on GLUE with RoBERTa Backbones We directly apply the S4-model and S4-3b-model (adopting design patterns discovered using T5- base and T5-3b) to ﬁne-tune the RoBERTa-base and RoBERTa-large pretrained backbone models (with no extra discovery process), respectively. We keep all the other settings the same and evaluate them on GLUE datasets. We also compare with variant methods randomly sampled from two de- 8sign spaces: (i) S0-model, where all the designs are randomly selected for RoBERTa as in S0; (ii) S3-model, where strategies are randomly assigned to different RoBERTa layer groups as in S3. Table 5 shows that (i) the design pat- terns (adopted by S4-model and S4-3b-model) discovered using T5 models are applicable to the RoBERTa backbone models and outperform the investigated methods in GLUE average performances with no extra discovery process;(ii) improved performances fromS0-models, S3-models, to S4-(3b)-models support adding more constraints in the pattern discovery process (Section 4). 5.4 General Effectiveness on Generation Tasks with BART Backbones Like in Section 5.3, we further directly apply the S4-model and S4-3b-model (adopting design patterns discovered using T5-base and T5-3b) to ﬁne-tune the BART-base and BART-large pretrained backbone models (without additional discovery process.), respectively. We evaluate the models on two generation tasks: summarization (XSUM) and machine translation (en-ro) following He et al. [2022]. We also compare with PA (parallel adapter) using the same number of trainable parameters [He et al., 2022]. Table 6 shows that our methods, although adopting design patterns discovered from classiﬁcation tasks using T5, still outperform investigated PEFT strategies on generation tasks with different BART backbones. 6 Conclusion PEFT adapts knowledge in pretrained models to down-stream tasks in a more parameter-efﬁcient fashion. Instead of focusing on designing another strategy in the ﬁrst place, we introduced PEFT design spaces. We empirically discovered several design patterns in PEFT. These design patterns led to new PEFT methods. Experiments showed that these methods consistently outperform investigated PEFT strategies across different backbone models and different tasks in natural language processing. References Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional trans- formers for language understanding. In NAACL-HLT, 2019. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. InAdvances in neural information processing systems, pages 5754–5764, 2019. Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. Spanbert: Improving pre-training by representing and predicting spans. Transactions of the Association for Computational Linguistics , 8:64–77, 2019. Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, and Hua Wu. Ernie: Enhanced representation through knowledge integration. arXiv preprint arXiv:1904.09223, 2019. Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training text encoders as discriminators rather than generators. In International Conference on Learning Representations, 2019. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, transla- tion, and comprehension. SCL, 2020a. Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Songhao Piao, Jianfeng Gao, Ming Zhou, et al. Unilmv2: Pseudo-masked language models for uniﬁed language model pre-training. arXiv preprint arXiv:2002.12804, 2020. Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention. arXiv preprint arXiv:2006.03654, 2020. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer, 2020. Caleb Ziems, Jiaao Chen, Camille Harris, Jessica Anderson, and Diyi Yang. V ALUE: Understanding dialect disparity in NLU. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 91: Long Papers) , pages 3701–3720, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.258. URL https://aclanthology.org/2022.acl-long.258. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efﬁcient transfer learning for NLP. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning , vol- ume 97 of Proceedings of Machine Learning Research , pages 2790–2799. PMLR, 09–15 Jun 2019a. URL http://proceedings.mlr.press/v97/houlsby19a.html. Jonas Pfeiffer, Aishwarya Kamath, Andreas R ¨uckl´e, Kyunghyun Cho, and Iryna Gurevych. AdapterFusion: Non- destructive task composition for transfer learning. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume , pages 487–503, Online, April 2021. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2021.eacl-main.39. Xiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimizing continuous prompts for generation, 2021. Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efﬁcient prompt tuning, 2021a. Timo Schick and Hinrich Sch ¨utze. Exploiting cloze-questions for few-shot text classiﬁcation and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume , pages 255–269, Online, April 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.20. URL https://aclanthology.org/2021.eacl-main.20. Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efﬁcient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 3045–3059, Online and Punta Cana, Dominican Republic, November 2021b. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.243. URL https://aclanthology.org/2021.emnlp-main.243. Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitﬁt: Simple parameter-efﬁcient ﬁne-tuning for transformer- based masked language-models, 2021. URL https://arxiv.org/abs/2106.10199. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. URL https://arxiv.org/abs/2106.09685. Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a uniﬁed view of parameter-efﬁcient transfer learning. In International Conference on Learning Representations, 2022. Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Scott Yih, and Madian Khabsa. UniPELT: A uniﬁed framework for parameter-efﬁcient language model tuning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 6253–6264, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.433. URL https: //aclanthology.org/2022.acl-long.433. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoy- anov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language genera- tion, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Com- putational Linguistics , pages 7871–7880, Online, July 2020b. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.703. URL https://www.aclweb.org/anthology/2020.acl-main.703. Ilija Radosavovic, Justin Johnson, Saining Xie, Wan-Yen Lo, and Piotr Doll ´ar. On network design spaces for visual recognition, 2019. URL https://arxiv.org/abs/1905.13214. Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll ´ar. Designing network design spaces, 2020. URL https://arxiv.org/abs/2003.13678. Jiaxuan You, Rex Ying, and Jure Leskovec. Design space for graph neural networks, 2020. URL https://arxiv. org/abs/2011.08843. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efﬁcient transfer learning for nlp. In International Conference on Machine Learning, pages 2790–2799. PMLR, 2019b. Asa Cooper Stickland and Iain Murray. Bert and pals: Projected attention layers for efﬁcient adaptation in multi-task learning. In International Conference on Machine Learning, pages 5986–5995. PMLR, 2019. 10Jonas Pfeiffer, Aishwarya Kamath, Andreas R ¨uckl´e, Kyunghyun Cho, and Iryna Gurevych. Adapterfusion: Non- destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247, 2020. Sylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters. arXiv preprint arXiv:1705.08045, 2017. Zhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model via parameter- efﬁcient transfer learning. arXiv preprint arXiv:2004.03829, 2020. Mengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, and Hinrich Sch ¨utze. Masking as an efﬁcient alternative to ﬁnetuning for pretrained language models. arXiv preprint arXiv:2004.12406, 2020. Demi Guo, Alexander M Rush, and Yoon Kim. Parameter-efﬁcient transfer learning with diff pruning. arXiv preprint arXiv:2012.07463, 2020. Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggyback: Adapting a single network to multiple tasks by learning to mask weights. In Proceedings of the European Conference on Computer Vision (ECCV), pages 67–82, 2018. Evani Radiya-Dixit and Xin Wang. How ﬁne can ﬁne-tuning be? learning efﬁcient language models. In International Conference on Artiﬁcial Intelligence and Statistics, pages 2435–2443. PMLR, 2020. Yi-Lin Sung, Varun Nair, and Colin A Raffel. Training neural networks with ﬁxed sparse masks. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Pro- cessing Systems, volume 34, pages 24193–24205. Curran Associates, Inc., 2021. URL https://proceedings. neurips.cc/paper/2021/file/cb2653f548f8709598e8b5156738cc51-Paper.pdf. Aston Zhang, Yi Tay, Shuai Zhang, Alvin Chan, Anh Tuan Luu, Siu Hui, and Jie Fu. Beyond fully-connected lay- ers with quaternions: Parameterization of hypercomplex multiplications with 1/n parameters. In International Conference on Learning Representations, 2021a. Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efﬁcient low-rank hypercomplex adapter layers. Advances in Neural Information Processing Systems, 34:1022–1035, 2021. Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric P. Xing, and Zhiting Hu. Rlprompt: Optimizing discrete text prompts with reinforcement learning, 2022. URL https: //arxiv.org/abs/2205.12548. Wanjun Zhong, Yifan Gao, Ning Ding, Zhiyuan Liu, Ming Zhou, Jiahai Wang, Jian Yin, and Nan Duan. Improving task generalization via uniﬁed schema prompt, 2022. URL https://arxiv.org/abs/2208.03229. M Saiful Bari, Aston Zhang, Shuai Zheng, Xingjian Shi, Yi Zhu, Shaﬁq Joty, and Mu Li. Spt: Semi-parametric prompt tuning for multitask prompted learning. arXiv preprint arXiv:2212.10929, 2022. Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao, Xiaozhi Wang, Zhiyuan Liu, Hai-Tao Zheng, Jianfei Chen, Yang Liu, Jie Tang, Juanzi Li, and Maosong Sun. Delta tuning: A comprehensive study of parameter efﬁcient methods for pre-trained language models, 2022. URL https://arxiv.org/abs/2203.06904. Ganesh Jawahar, Benoˆıt Sagot, and Djam´e Seddah. What does BERT learn about the structure of language? In Pro- ceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3651–3657, Florence, Italy, July 2019. Association for Computational Linguistics. Aston Zhang, Yi Tay, Yikang Shen, Alvin Chan, and Shuai Zhang. Self-instantiated recurrent units with dynamic soft recursion. Advances in Neural Information Processing Systems, 34:6503–6514, 2021b. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In BlackboxNLP@EMNLP, 2018. Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don’t give me the details, just the summary! topic-aware convo- lutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1797–1807, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1206. URL https://aclanthology.org/D18-1206. Ondˇrej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Ji- meno Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Aur ´elie N ´ev´eol, Mariana Neves, Martin Popel, Matt Post, Raphael Rubino, Carolina Scarton, Lucia Specia, Marco Turchi, Karin Verspoor, and Mar- cos Zampieri. Findings of the 2016 conference on machine translation. In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, pages 131–198, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/W16-2301. URL https://aclanthology.org/W16-2301. 11Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74–81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https: //aclanthology.org/W04-1013. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In ACL, 2002. 12A More Experimental Results Table 7: Average performances (low-compute, low-epoch regime: 100 random models, tuning epochs = 1, 2, 3, 4, 20 for ﬁve different blocks) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different grouping constraints to the S0 design space. Grouping Patterns SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg 1 epochs Increasing 73.2 63.3 67.8 68.8 63.8 67.2 64.1 11.0 59.9 Uniform 72.8 64.1 63.4 63.4 62.5 69.8 65.8 12.1 59.2 Decreasing 72.4 63.2 65.1 69.8 59.3 62.7 63.6 18.7 59.4 Spindle 72.6 64.8 66.8 71.1 62.1 62.3 64.8 12.3 59.6 Bottleneck 72.2 63.7 65.3 68.3 61.2 63.2 66.6 12.1 59.0 2 epochs Increasing 76.2 69.3 73.2 76.5 65.8 72.2 74.0 21.0 66.0 Uniform 74.8 70.9 74.1 75.6 66.5 73.4 71.2 22.1 66.1 Decreasing 71.4 70.1 72.1 76.8 64.3 71.7 73.6 18.7 64.8 Spindle 76.6 71.9 71.8 74.4 67.5 73.5 71.8 22.3 66.2 Bottleneck 74.2 71.1 69.6 73.3 65.2 73.3 73.6 24.1 65.5 3 epochs Increasing 85.3 74.9 77.2 77.5 66.8 76.2 76.0 33.0 70.8 Uniform 84.8 73.7 78.1 78.6 68.5 77.8 79.2 36.1 72.1 Decreasing 81.9 72.1 78.3 76.7 67.3 75.9 78.6 28.7 69.9 Spindle 86.9 75.5 79.8 79.4 69.8 78.3 80.1 47.3 74.6 Bottleneck 84.5 74.6 76.9 78.1 69.2 76.2 78.6 32.1 71.3 4 epochs Increasing 88.3 78.5 80.2 80.5 70.8 80.2 80.0 37.0 74.4 Uniform 88.8 78.9 81.9 81.5 71.5 80.8 81.4 39.1 75.4 Decreasing 87.6 74.1 80.8 81.7 79.3 78.9 79.6 38.7 75.1 Spindle 89.6 79.8 83.6 82.8 71.8 81.3 82.1 39.3 76.3 Bottleneck 86.5 77.6 82.7 81.1 70.2 70.9 81.6 36.1 73.3 20 epochs Increasing 92.3 83.3 86.2 82.5 71.8 82.2 84.0 51.0 79.1 Uniform 92.8 83.9 86.1 83.6 72.5 83.8 84.2 52.1 79.9 Decreasing 91.4 82.1 85.1 83.1 69.3 81.7 83.6 48.7 78.1 Spindle 93.6 84.8 87.8 84.4 73.5 84.3 85.8 52.3 80.8 Bottleneck 92.1 82.6 85.6 83.3 71.2 83.2 84.6 52.1 79.3 B General Effectiveness on SuperGLUE with XLNet Backbones We also directly use the S4-model and S4-3b-model (adopting design patterns discovered using T5-base and T5-3b) to ﬁne-tune the XLNet-base and XLNet-large pretrained backbone models without any extra discovery process. We keep all the other settings the same and evaluate them on SuperGLUE datasets. Table 17 reiterates the fact that our PEFT design patterns discovered from T5 models are generelizable to the XLNet backbone models and outperform the investigated methods in other tasks (SuperGLUE) with no additional discovery process. C On the Discovery Sequence In this work, we follow the discovery sequence of “grouping patterns – trainable parameter allocation – tunable groups – strategy assignment”: 1. To explore and understand the design patterns in all the layers in large pre-trained models in scale, it is necessary and more efﬁcient to study the layers in the unit of groups. So we start with the grouping patterns. 13Table 8: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different G1 strategy assignment con- straints to the S3 design space. Strategy Assignment SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G1-Adapter (A) 89.8 83.5 84.9 80.8 72.5 80.8 78.5 37.7 76.1 G1-Preﬁx (P) 89.3 83.1 84.4 80.1 70.1 80.0 77.6 33.0 74.7 G1-BitFit (B) 89.0 82.9 84.1 81.4 72.0 81.1 77.0 30.8 74.8 G1-LoRA (L) 89.9 83.6 85.0 81.1 71.8 81.0 78.8 35.3 75.8 G1-(P, L) 89.1 82.8 85.1 81.2 71.9 81.5 79.1 35.0 75.7 G1-(A, P) 89.8 82.8 84.8 81.1 72.2 81.3 79.2 36.4 75.9 G1-(A, L) 89.6 83.8 85.6 81.3 72.9 81.7 79.5 36.8 76.4 G1-(A, P, L) 89.6 83.5 85.2 81.5 72.2 81.4 79.2 35.2 75.9 G1-(P, B, L) 89.3 83.6 85.5 81.6 72.3 81.0 78.8 35.7 76.0 G1-(A, P, B) 89.2 83.3 84.8 81.8 72.5 81.1 78.6 35.6 75.8 G1-(A, B, L) 89.8 83.4 84.8 81.1 72.6 81.6 79.4 34.8 75.9 G1-(A, P, B, L) 90.0 83.1 85.3 81.6 72.6 81.4 79.2 36.5 76.1 Table 9: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different G2 strategy assignment con- straints with G1-(L, A) to the S3 design space. Strategy Assignment SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G2-Adapter (A) 91.6 84.3 85.5 82.3 73.5 82.8 81.3 38.8 77.5 G2-Preﬁx (P) 89.6 84.0 86.5 81.5 73.3 82.5 80.5 36.2 76.7 G2-BitFit (B) 91.2 83.6 85.7 82.9 72.6 82.6 80.8 33.1 76.5 G2-LoRA (L) 91.4 84.4 86.1 82.0 72.8 81.8 81.6 39.8 77.4 G2-(P, L) 91.6 84.6 86.8 81.8 73.8 82.8 82.0 38.5 77.7 G2-(A, P) 92.2 84.2 87.1 82.2 74.4 83.0 82.5 40.8 78.3 G2-(A, L) 92.0 84.4 86.5 81.8 73.6 82.6 82.2 40.1 77.9 G2-(A, P, L) 91.8 84.8 86.8 81.8 74.1 83.0 82.1 37.9 77.7 G2-(P, B, L) 91.6 84.1 87.1 82.0 74.0 82.9 82.4 35.8 77.4 G2-(A, P, B) 91.8 84.2 86.8 82.1 73.7 83.3 82.2 41.2 78.1 G2-(A, B, L) 92.2 84.3 86.1 82.0 74.1 83.2 82.0 37.6 77.6 G2-(A, P, B, L) 92.0 84.1 87.0 81.9 74.2 83.1 81.3 42.4 78.1 2. Once ﬁguring out the optimal grouping patterns, it is then important to explore how to allocate the trainable parameters to these different groups in order to study more subtle designs with fair comparisons (e.g., this would allow comparing different patterns of strategy assignments without the impact from different trainable parameters.). 3. Next, it becomes inﬂuential to examine which groups need to be learned during ﬁne-tuning before we dig into the strategy assignment patterns. Because it is only meaningful to study assigning strategies to different groups after we ﬁgure out which groups need to be learned. 4. Finally, we study the tuning strategy assignment, which is the most subtle design. 14Table 10: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different G3 strategy assignment constraints with G1-(L, A) – G2-(P, A) to the S3 design space. Strategy Assignment SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G3-Adapter (A) 92.5 85.3 87.5 83.3 73.9 84.0 83.8 44.9 79.4 G3-Preﬁx (P) 91.5 84.7 86.7 82.6 74.2 83.8 82.9 40.5 78.4 G3-BitFit (B) 91.9 84.3 87.0 82.0 73.6 84.1 83.3 36.1 77.8 G3-LoRA (L) 92.8 85.4 87.8 83.5 74.7 82.4 84.0 44.0 79.3 G3-(P, L) 93.0 85.2 88.3 83.8 75.2 84.4 84.2 37.9 79.0 G3-(A, P) 92.4 85.6 88.1 83.6 75.0 84.2 84.0 41.8 79.3 G3-(A, L) 92.0 85.9 88.2 83.1 75.3 84.3 83.9 42.2 79.4 G3-(A, P, L) 92.6 86.0 87.5 83.4 75.6 84.6 83.5 43.9 79.6 G3-(P, B, L) 92.7 85.8 87.2 83.7 75.2 84.5 83.8 40.8 79.2 G3-(A, P, B) 93.3 85.8 88.6 84.0 75.5 84.9 84.1 42.1 79.8 G3-(A, B, L) 93.7 86.5 88.0 83.2 75.8 84.2 84.2 39.7 79.4 G3-(A, P, B, L) 93.3 85.6 87.7 83.8 75.2 84.3 84.4 41.6 79.4 Table 11: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different G4 strategy assignment constraints with G1-(A, L) – G2-(A, P) – G3-(A, P, B) to the S3 design space. Strategy Assignment SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G4-Adapter (A) 93.8 85.8 88.6 84.8 76.3 85.8 86.0 48.5 81.2 G4-Preﬁx (P) 93.5 85.2 88.3 83.6 76.8 85.3 85.6 44.8 80.3 G4-BitFit (B) 94.1 85.3 88.9 84.4 77.1 85.4 86.2 46.1 80.9 G4-LoRA (L) 94.0 86.0 89.2 85.0 77.2 85.5 85.8 47.7 81.3 G4-(P, L) 94.3 86.2 89.3 85.8 78.0 86.0 88.2 47.2 81.8 G4-(A, P) 94.1 86.2 89.6 85.4 77.9 86.2 86.9 45.3 81.4 G4-(A, L) 94.2 85.9 89.2 85.5 77.8 86.2 88.0 46.8 81.7 G4-(A, P, L) 94.1 85.8 88.8 85.7 77.4 86.5 87.9 44.8 81.3 G4-(P, B, L) 94.6 86.4 90.4 86.1 78.2 86.8 88.5 47.2 82.3 G4-(A, P, B) 94.5 86.0 89.6 86.0 78.0 86.2 88.1 44.8 81.6 G4-(A, B, L) 94.3 86.4 89.2 85.6 78.2 86.4 88.3 46.6 81.9 G4-(A, P, B, L) 94.2 86.2 89.2 85.9 78.5 86.1 88.0 45.3 81.6 Table 12: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-3b pretrained backbone model. We compare adding different layer grouping constraints to the S0 design space. Grouping Patterns SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg S0-models 80.3 72.1 74.7 72.8 76.9 75.2 71.0 32.2 69.4 Increasing 84.4 75.7 83.0 78.3 82.7 80.3 76.3 42.1 75.3 Uniform 86.8 77.1 82.6 76.2 83.8 81.6 77.3 48.9 76.8 Decreasing 83.2 74.3 81.8 77.3 82.8 79.9 76.5 40.8 74.5 Spindle 88.6 78.8 83.7 77.7 84.2 80.9 78.3 44.6 77.1 Bottleneck 86.3 77.0 82.2 75.6 83.3 80.2 77.1 41.5 75.4 15Table 13: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-3b pretrained backbone model. We compare adding different layer parameter constraints to the S1 design space. Parameter Allocation SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg Increasing 90.3 79.3 84.9 79.3 85.2 82.8 79.2 50.1 78.9 Uniform 90.6 80.8 84.6 79.7 85.5 82.4 78.9 50.8 79.1 Decreasing 88.6 78.2 83.5 78.1 84.4 81.5 78.1 49.6 77.7 Table 14: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-3b pretrained backbone model. We compare adding different tuning groups constraints to the S2 design space. Tunable Groups SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G1 88.3 78.3 82.2 77.4 82.1 80.7 76.1 49.4 76.8 G2 89.1 78.8 82.1 77.2 82.3 81.2 76.4 49.6 77.1 G3 89.6 78.5 82.6 78.1 83.8 81.9 77.4 48.7 77.5 G4 89.8 79.3 82.7 77.9 83.5 81.9 77.9 48.5 77.1 G1, G2 90.1 80.2 83.4 78.5 84.3 82.4 78.5 51.1 78.5 G3, G4 90.5 80.6 83.8 78.7 84.2 83 78.2 50.3 78.6 G1, G2, G3 90.6 80.3 84.9 79.3 84.7 82.9 79.3 50.2 79.0 G2, G3, G4 90.8 80.9 84.6 79.1 85.1 83.1 79.1 49.2 78.9 G1, G2, G3, G4 91.1 81.4 85.2 80.4 85.9 83.5 80.0 51.6 79.9 16Table 15: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-3b pretrained backbone model. We compare adding different strategy assignment con- straints following the process in Section 4.2.5. Strategy Assignment SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G1-Adapter (A) 91.1 81.4 86.1 80.5 86.7 83.3 80.1 50.8 80.0 G1-Preﬁx (P) 90.8 81.1 85.5 80.2 86.2 83.1 79.8 50.2 79.6 G1-BitFit (B) 90.2 81.3 85.1 79.6 85.8 82.8 79.6 49.5 79.2 G1-LoRA (L) 91.4 81.9 86.2 80.8 86.4 83.9 80.8 49.6 80.0 G1-(P, L) 91.8 82.9 86.8 81.3 87.1 84.2 81.6 52.3 81.0 G1-(A, P) 91.3 81.9 86.4 81.1 85.6 83.7 80.7 52.8 80.1 G1-(A, L) 91.6 82.3 86.1 81.5 85.8 84.9 81.5 51.8 80.6 G1-(A, P, L) 91.1 81.7 85.8 81.2 86.4 84.2 80.9 52.3 80.4 G1-(P, B, L) 91.5 82.8 86.3 81.4 86.1 83.6 81.2 51.5 80.5 G1-(A, P, B) 91.3 82.3 86.7 80.8 86.8 84.3 80.7 51.8 80.5 G1-(A, B, L) 91.7 82.5 86.2 81.3 86.3 84.6 81.3 51.7 80.7 G1-(A, P, B, L) 91.6 82.3 86.2 81.1 86.6 84.2 81.1 51.1 80.5 G2-Adapter (A) 92.1 82.5 86.4 81.8 87.2 84.8 81.8 53.8 81.3 G2-Preﬁx (P) 91.8 83.1 87.2 81.6 86.2 84.4 81.1 52.8 81.0 G2-BitFit (B) 91.2 82.1 86.4 81.1 86.3 84.6 80.3 53.1 80.6 G2-LoRA (L) 92.6 82.9 87.5 81.3 87.4 85.1 81.9 52.2 81.4 G2-(P, L) 91.6 82.7 87.6 81.6 87.8 85.3 82.1 52.8 81.4 G2-(A, P) 92.1 83.3 87.5 81.9 87.4 85.5 81.8 53.1 81.5 G2-(A, L) 92.5 83.7 88.1 82.2 87.4 85.7 82.9 53.6 82.1 G2-(A, P, L) 92.3 83.4 87.4 81.6 87.1 85.3 81.4 53.2 81.4 G2-(P, B, L) 91.8 83.1 87.4 81.5 87.2 85.1 82.7 53.8 81.5 G2-(A, P, B) 91.5 82.6 87.8 81.3 86.5 85.2 82.1 54.2 81.4 G2-(A, B, L) 92.6 83.5 87.2 82 87.3 86.5 82.5 52.8 81.8 G2-(A, P, B, L) 92.8 83.2 87.6 81.6 87.5 85.5 82.4 51.2 81.5 G3-Adapter (A) 92.6 84.1 88.3 81.8 87.8 85.4 82.8 55.2 82.2 G3-Preﬁx (P) 92.1 83.3 87.6 81.4 87.1 85.4 82.6 53.5 81.6 G3-BitFit (B) 92.4 83.9 88.4 82.1 87.2 85.8 82.4 53.3 81.9 G3-LoRA (L) 93.1 84.3 87.7 82.4 87.8 86.2 83.1 54.3 82.3 G3-(P, L) 92.8 84.1 88.7 82.6 88.2 86.2 83.3 54.7 82.6 G3-(A, P) 93.1 83.8 89.1 82.3 88.1 85.8 82.6 55.1 82.5 G3-(A, L) 92.7 84.5 88.4 82.8 88.2 86.1 83.5 54.6 82.6 G3-(A, P, L) 92.8 84.6 88.1 82.5 87.7 85.5 83.2 53.8 82.3 G3-(P, B, L) 93.6 84.9 89.3 83.1 88.2 86.5 83.9 55.8 83.2 G3-(A, P, B) 93.3 83.9 88.5 82.2 88.4 86.2 83.5 55.3 82.6 G3-(A, B, L) 93.4 84.2 88.9 82.6 87.8 85.8 84.2 54.9 82.7 G3-(A, P, B, L) 92.2 84.4 88.7 82.3 88.5 86.2 84.2 54.2 82.5 G4-Adapter (A) 92.8 85.2 89.1 83.5 87.8 86.5 84.2 56.3 83.2 G4-Preﬁx (P) 92.8 84.6 89.5 82.6 87.4 86.5 83.8 55.8 82.8 G4-BitFit (B) 93.8 84.9 89.5 83.3 88.7 86.8 84.4 55.2 83.3 G4-LoRA (L) 93.3 84.7 89.3 82.7 88.3 86.2 82.7 54.7 82.7 G4-(P, L) 93.8 85.3 89.6 83.6 88.6 86.8 84.6 56.3 83.5 G4-(A, P) 93.8 84.9 89.8 84.3 88.5 86.6 84.8 56.7 83.6 G4-(A, L) 93.7 85.6 89.5 84.1 88.2 86.6 85.2 55.4 83.5 G4-(A, P, L) 94.2 85.2 89.6 83.9 88.2 86.4 84.9 55.9 83.5 G4-(P, B, L) 93.8 85.9 89.8 83.6 88.6 86.9 85.2 56.3 83.7 G4-(A, P, B) 94.4 85.7 90.1 84.8 88.9 87.2 85.3 57.3 84.2 G4-(A, B, L) 93.8 85.3 89.5 84.1 88.8 86.7 85.5 56.6 83.7 G4-(A, P, B, L) 94.1 85.4 89.7 84.4 88.5 86.5 85.2 56.8 83.8 17Table 16: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different layer grouping constraints to the S0 design space. Layer grouping is based on 8 groups. Layer Grouping SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg S0-models 76.9 70.1 72.5 73.3 63.6 71.7 73.8 24.3 65.7 Increasing 83.2 74 .1 76 .6 77 .1 67 .7 76.8 74.7 30.0 70.0 Uniform 83.6 73.4 78.0 77.9 68.2 76.4 78.6 34.2 71.3 Decreasing 80.3 71.6 77.4 75.5 67.0 75.3 77.2 26.4 68.9 Spindle 86.2 74.3 79.1 78.6 68.5 77.4 79.5 35.1 72.3 Bottleneck 83.2 73.1 75.8 77.6 67.9 75.3 78.2 31.4 70.3 Table 17: Performances of different tuning methods on the SuperGLUE datasets using the XLNet-base (upper part) and XLNet-large (lower part) pretrained backbone models, respectively. The results are averaged over 10 random runs. The S4-model and S4-3b-model perform signiﬁcantly better than the second-best PEFT methods in all the eight datasets at the signiﬁcance level p <0.05 (*) or even p <0.01 (**). Method BoolQ CB COPA MultiRC ReCoRD RTE WiC WSC Average Adapter 72.8 71.3/78.0 64.0 67.0/24.5 71.0/71.8 76.2 65.0 60.8 66.2 Preﬁx 72.0 70.5/77.0 63.3 66.4/23.8 69.9/71.0 75.5 64.4 60.8 65.9 BitFit 71.8 70.0/76.2 62.8 65.8/22.6 69.4/70.6 74.5 64.8 60.6 65.2 LoRA 72.2 71.1/77.8 64.7 67.4/24.8 70.8/71.3 76.8 65.1 61.1 66.4 S4-model 73.8∗∗ 71.7/78.4∗ 65.9∗∗ 68.2/25.5∗∗ 71.1/72.0∗ 78.4∗∗ 65.8∗ 62.6∗ 67.5 Adapter 74.4 71.4/81.1 67.4 68.8/26.4 71.7/72.4 80.8 68.0 64.6 68.8 Preﬁx 72.4 70.0/78.3 66.9 68.8/25.8 70.9/71.2 78.8 66.9 64.0 67.7 BitFit 71.1 70.7/79.8 68.0 68.6/25.4 71.1/71.6 80.4 67.2 64.3 68.1 LoRA 74.1 72.1/80.9 67.9 69.1/26.8 72.0/72.8 81.0 67.8 64.4 69.0 S4-3b-model 76.8∗∗ 74.6/81.9∗∗ 68.6∗∗ 69.5/27.1∗ 72.4/73.3∗ 81.2∗ 68.2∗∗ 64.8∗ 69.7 Table 18: Total training time (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model with 8 A100 GPUs from S0 to S1. SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA 18 mins 22 mins 20 mins 40 mins 8 mins 12 mins 8 mins 6 mins 18",
      "meta_data": {
        "arxiv_id": "2301.01821v1",
        "authors": [
          "Jiaao Chen",
          "Aston Zhang",
          "Xingjian Shi",
          "Mu Li",
          "Alex Smola",
          "Diyi Yang"
        ],
        "published_date": "2023-01-04T21:00:18Z",
        "pdf_url": "https://arxiv.org/pdf/2301.01821v1.pdf",
        "github_url": "https://github.com/amazon-science/peft-design-spaces"
      },
      "llm_extracted_info": {
        "main_contributions": "The main research problem addresses the lack of systematic understanding of parameter-efficient fine-tuning (PEFT) strategies, which are often hand-crafted without clear design patterns across different strategies, backbone models, and tasks. The paper introduces a PEFT design paradigm by defining parameter-efficient fine-tuning design spaces, characterized by four components: layer grouping, trainable parameter allocation, tunable groups, and strategy assignment. Key contributions include the empirical discovery of several design patterns: (i) grouping layers in a spindle pattern; (ii) uniformly allocating the number of trainable parameters to layers; (iii) tuning all groups; and (iv) assigning proper tuning strategies to different groups. These discovered design patterns lead to new PEFT methods (S4-model and S4-3b-model) that consistently and significantly outperform existing individual PEFT strategies across various backbone models (T5, RoBERTa, BART, XLNet) and diverse NLP tasks (GLUE, XSum summarization, WMT machine translation, SuperGLUE).",
        "methodology": "The methodology involves defining parameter-efficient fine-tuning design spaces with four components: layer grouping, trainable parameter allocation, tunable groups, and strategy assignment (using Adapter, Prefix, BitFit, LoRA). The discovery process starts from an unconstrained initial design space (S0) and progressively refines it by adding constraints based on model quality. A greedy selection approach is used at each stage over the four components. To quantify model quality in any design space, 100 models are randomly sampled from that space, fine-tuned for 3 epochs (low-compute, low-epoch regime), and their average GLUE performance is computed. Specific patterns explored include: 5 layer grouping patterns (Increasing, Uniform, Decreasing, Spindle, Bottleneck), 3 parameter allocation patterns (Increasing, Uniform, Decreasing), various combinations of tunable groups, and iterative strategy assignment for each group.",
        "experimental_setup": "Design patterns were discovered primarily using T5-base and T5-3b as backbone models. The discovery process utilized the GLUE benchmark (SST-2, CoLA, QQP, STS-B, MRPC, MNLI, QNLI, RTE), with evaluation metrics including Matthews correlation (CoLA), Spearman correlation (STS-B), and accuracy for others. For discovery, the total trainable parameters were set to 0.5% of the backbone model. The discovered S4-model and S4-3b-model were then extensively evaluated for general effectiveness on: (i) GLUE with T5-base/3b and RoBERTa-base/large backbones; (ii) Generation tasks (Abstractive Summarization with XSum, Machine Translation with WMT 2016 en-ro) using BART-base/large backbones; and (iii) SuperGLUE datasets with XLNet-base/large backbones. Baselines included Full Fine-tuning, Adapter, Prefix, BitFit, LoRA, and sometimes S0-model (random designs) and S3-model (random strategy assignment). Evaluation parameters included 0.5% trainable parameters for most PEFT methods (0.1% for BitFit), linear decay scheduler (warmup 0.06), batch size 128 (base) / 64 (large), max learning rate 5e-5, and 5 or 10 training epochs. All experiments were performed using 8 A100 GPUs, with results averaged over 20 random runs (10 for SuperGLUE) and statistical significance reported.",
        "limitations": "The study's primary limitation is that its goal was not to enumerate all possible design spaces or discover the single 'best' design space/method, but rather to demonstrate the utility of the design space perspective in informing PEFT research. Consequently, the discovered patterns, while effective, might not represent a globally exhaustive or optimal set, especially given computational constraints that limited the search space. The pattern discovery process relied on a low-compute, low-epoch regime (3 epochs), which is a simplification for efficiency, despite checks for performance stability. Additionally, the specific discovery sequence (grouping, then parameter allocation, then tunable groups, then strategy assignment) was a chosen heuristic and not necessarily the only or universally optimal order for exploring the design space.",
        "future_research_directions": "Not explicitly mentioned, but implied by the work are potential future research directions. These could include exploring more complex or granular design choices for each of the four components (layer grouping, parameter allocation, tunable groups, strategy assignment), investigating alternative discovery sequences or automated search methods (e.g., using neural architecture search techniques) within the design spaces to find potentially superior patterns. Further research could also delve into the theoretical reasons behind the effectiveness of the discovered patterns and apply this design space methodology to other model architectures or domains beyond NLP to assess its generality. Evaluating the discovered patterns on an even wider and more diverse range of models and tasks would also be a valuable extension.",
        "experimental_code": "# Adapters\nif is_torch_available():\n    _import_structure[\"adapters\"] = [\n        \"ADAPTER_CACHE\",\n        \"ADAPTER_CONFIG_MAP\",\n        \"ADAPTERFUSION_CONFIG_MAP\",\n        \"ADAPTER_MODEL_MAPPING\",\n        \"DEFAULT_ADAPTER_CONFIG\",\n        \"DEFAULT_ADAPTERFUSION_CONFIG\",\n        \"MODEL_WITH_HEADS_MAPPING\",\n        \"AdapterArguments\",\n        \"AdapterConfig\",\n        \"AdapterConfigBase\",\n        \"AdapterFusionConfig\",\n        \"AdapterInfo\",\n        \"AdapterLayer\",\n        \"AdapterLayerBase\",\n        \"AdapterSetup\",\n        \"AdapterTrainer\",\n        \"AdapterType\",\n        \"AutoAdapterModel\",\n        \"AutoModelWithHeads\",\n        \"BartAdapterModel\",\n        \"BartModelWithHeads\",\n        \"BeitAdapterModel\",\n        \"BertAdapterModel\",\n        \"BertModelWithHeads\",\n        \"CompacterConfig\",\n        \"CompacterPlusPlusConfig\",\n        \"ConfigUnion\",\n        \"DebertaAdapterModel\",\n        \"DebertaV2AdapterModel\",\n        \"DistilBertAdapterModel\",\n        \"DistilBertModelWithHeads\",\n        \"DynamicAdapterFusionConfig\",\n        \"EmbeddingAdaptersMixin\",\n        \"ForwardContext\",\n        \"GPT2AdapterModel\",\n        \"GPT2ModelWithHeads\",\n        \"GPTJAdapterModel\",\n        \"HoulsbyConfig\",\n        \"HoulsbyInvConfig\",\n        \"IA3Config\",\n        \"InvertibleAdaptersMixin\",\n        \"LoRAConfig\",\n        \"MAMConfig\",\n        \"MBartAdapterModel\",\n        \"MBartModelWithHeads\",\n        \"ModelAdaptersConfig\",\n        \"ModelAdaptersMixin\",\n        \"ModelWithFlexibleHeadsAdaptersMixin\",\n        \"ModelWithHeadsAdaptersMixin\",\n        \"MultiLingAdapterArguments\",\n        \"ParallelConfig\",\n        \"PfeifferConfig\",\n        \"PfeifferInvConfig\",\n        \"PrefixTuningConfig\",\n        \"RobertaAdapterModel\",\n        \"RobertaModelWithHeads\",\n        \"Seq2SeqAdapterTrainer\",\n        \"StaticAdapterFusionConfig\",\n        \"T5AdapterModel\",\n        \"T5ModelWithHeads\",\n        \"PEFTConfig\",\n        \"ViTAdapterModel\",\n        \"XLMRobertaAdapterModel\",\n        \"XLMRobertaModelWithHeads\",\n        \"get_adapter_config_hash\",\n        \"get_adapter_info\",\n        \"list_adapters\",\n    ]",
        "experimental_info": "The provided repository content (setup.py, __init__.py, etc.) primarily contains package setup, module import structures, utility functions, benchmarking infrastructure, and CLI commands. It does not include specific experimental settings such as the number of models sampled (e.g., 100 models), the number of fine-tuning epochs (e.g., 3 epochs), the performance metric (e.g., average GLUE performance), or the definitions of specific layer grouping/parameter allocation patterns (e.g., Increasing, Uniform, Decreasing, Spindle, Bottleneck) as described in the \"Method\" section."
      }
    },
    {
      "title": "ReFT: Representation Finetuning for Language Models",
      "abstract": "Parameter-efficient finetuning (PEFT) methods seek to adapt large neural\nmodels via updates to a small number of weights. However, much prior\ninterpretability work has shown that representations encode rich semantic\ninformation, suggesting that editing representations might be a more powerful\nalternative. We pursue this hypothesis by developing a family of Representation\nFinetuning (ReFT) methods. ReFT methods operate on a frozen base model and\nlearn task-specific interventions on hidden representations. We define a strong\ninstance of the ReFT family, Low-rank Linear Subspace ReFT (LoReFT), and we\nidentify an ablation of this method that trades some performance for increased\nefficiency. Both are drop-in replacements for existing PEFTs and learn\ninterventions that are 15x--65x more parameter-efficient than LoRA. We showcase\nLoReFT on eight commonsense reasoning tasks, four arithmetic reasoning tasks,\ninstruction-tuning, and GLUE. In all these evaluations, our ReFTs deliver the\nbest balance of efficiency and performance, and almost always outperform\nstate-of-the-art PEFTs. We release a generic ReFT training library publicly at\nhttps://github.com/stanfordnlp/pyreft.",
      "full_text": "ReFT: Representation Finetuning for Language Models Zhengxuan Wu∗† Aryaman Arora∗† Zheng Wang† Atticus Geiger‡ Dan Jurafsky† Christopher D. Manning† Christopher Potts† †Stanford University ‡Pr(Ai)2R Group {wuzhengx,aryamana,peterwz,atticusg}@stanford.edu {jurafsky,manning,cgpotts}@stanford.edu Abstract Parameter-efficient finetuning (PEFT) methods seek to adapt large neural models via updates to a small number of weights. However, much prior interpretability work has shown that representations encode rich semantic information, suggesting that editing representations might be a more powerful alternative. We pursue this hypothesis by developing a family of Representation Finetuning (ReFT) methods. ReFT methods operate on a frozen base model and learn task-specific interventions on hidden representations. We define a strong instance of the ReFT family, Low-rank Linear Subspace ReFT (LoReFT), and we identify an ablation of this method that trades some performance for increased efficiency. Both are drop-in replacements for existing PEFTs and learn interventions that are 15×–65×more parameter-efficient than LoRA. We showcase LoReFT on eight commonsense rea- soning tasks, four arithmetic reasoning tasks, instruction-tuning, and GLUE. In all these evaluations, our ReFTs deliver the best balance of efficiency and performance, and almost always outperform state-of-the-art PEFTs. We release a generic ReFT training library publicly at https://github.com/stanfordnlp/pyreft. 1 Introduction Pretrained language models (LMs) are frequently finetuned to adapt them to new domains or tasks [Dai and Le, 2015]. With finetuning, a single base model can be adapted to a variety of tasks given only small amounts of in-domain data. However, finetuning large LMs is expensive. Parameter- efficient finetuning (PEFT) methods propose to address the high costs of full finetuning by updating a small number of weights. This reduces memory usage and training time, and PEFTs achieve similar performance to full finetuning in many settings [Hu et al., 2023]. A hallmark of current state-of-the-art PEFTs is that they modify weights rather than representations. However, much prior interpretability work has shown that representations encode rich semantic information, suggesting that editing representations might be a more powerful alternative to weight updates. In this paper, we pursue this hypothesis by developing and motivating Representation Finetuning (ReFT). Instead of adapting model weights, ReFT methods train interventions that manipulate a small fraction of model representations in order to steer model behaviors to solve downstream tasks at inference time. ReFT methods are drop-in replacements for weight-based PEFTs. This approach is inspired by recent work in LM interpretability that intervenes on representations to find faithful causal mechanisms [Geiger et al., 2023b] and to steer model behaviours at inference time [Turner et al., 2023, Li et al., 2024], and it can be seen as a generalisation of the representation-editing work of Wu et al. [2024a], Turner et al. [2023], and Zou et al. [2023] (see appendix B for details). *Equal contribution. Preprint. Under review. arXiv:2404.03592v3  [cs.CL]  22 May 2024Commonsense LLaMA 7B  LLaMA 13B  Llama- 2 7B  Llama- 3 8B Instruct -tuning Llama- 2 7B Paramet ers P er f ormance Arit hmetic LLaMA 7B  LLaMA 13B GLUE R oBERT a-base  R oBERT a-lar ge Figure 1: Parameter count vs. performance for LoReFT and other PEFTs across four benchmarks when applied to LLaMA, Llama-2, Llama-3, and RoBERTa models. Despite training far fewer parameters than existing PEFTs, LoReFT achieves competitive or even state-of-the-art performance on all tasks. Its value is most apparent for the largest models in our evaluations. Note: FT is full-parameter finetuning, which is not a PEFT or ReFT method. Additional results are in section 4. We focus on a strong and highly efficient instance of the ReFT family that we call Low-rank Linear Subspace ReFT (LoReFT). LoReFT is a parametrisation of ReFT that intervenes on hidden representations in the linear subspace spanned by a low-rank projection matrix, building directly on the distributed alignment search (DAS) method of Geiger et al. [2023b] and Wu et al. [2023]. We also identify an ablation of this method (DiReFT) that trades some performance for increased efficiency. We evaluate our ReFTs on LLaMA-family models and small-scale LMs against existing PEFTs on standard benchmarks from four domains: commonsense reasoning, arithmetic reasoning, instruction-following, and natural language understanding. Compared to LoRA, we find that LoReFT uses 15×–65×times fewer parameters while achieving state-of-the-art performance on commonsense reasoning, instruction-following, and natural language understanding against the strongest PEFTs. These findings indicate that ReFT methods are worthy of further exploration, as they may emerge as more efficient and effective alternatives to weight-based PEFTs. 2 Related work Parameter-efficient finetuning methods (PEFTs). PEFTs train a fraction of the model’s parameters to adapt it to downstream tasks. We classify PEFTs into three categories: 1. Adapter-based methods train additional modules (e.g. fully-connected layers) on top of the frozen pretrained model. Series adapters insert components between LM attention or MLP layers [Houlsby et al., 2019, Pfeiffer et al., 2020, Wang et al., 2022, He et al., 2022b, Fu et al., 2021], while parallel adapters add modules alongside existing components [He et al., 2022a]. Since adapters add new components that cannot be easily folded into existing model weights, they impose an additional burden at inference time.1 2. LoRA [Hu et al., 2022] and DoRA [Liu et al., 2024c] use low-rank matrices to approximate additive weight updates during training, and require no additional overhead during inference since the weight updates can be merged into the model. These are the strongest PEFTs currently.2 3. Prompt-based methods add randomly-initialised soft tokens to the input (usually as a prefix) and train their embeddings while keeping the LM weights frozen [Li and Liang, 2021]. These 1Several very recent papers introduce new adapter architectures but do not benchmark them on the tasks we consider, or they perform hyperparameter-tuning in a different setup than done in this work. These include: LLaMA-Adapter [Zhang et al., 2024b], LLaMA-Adapter v2 [Gao et al., 2023], Aligner [Ziheng et al., 2023]. 2Additional methods not studied in this work: AutoLoRA [Zhang et al., 2024c], ResLoRA [Shi et al., 2024], SiRA [Zhu et al., 2023]. 2methods are often far from optimal compared to other PEFTs, and come at the cost of significant inference overhead. A variant of this method where hidden-layer activations are also tuned was introduced as a baseline in Hu et al. [2022], with better performance. Representation editing. Recent work on activation steering and representation engineering shows that adding fixed or task-specific steering vectors [Subramani et al., 2022, Turner et al., 2023, Zou et al., 2023, Liu et al., 2024b, V ogel, 2024, Li et al., 2024] or applying concept erasure [Ravfogel et al., 2022, Belrose et al., 2023, Avitan et al., 2024, Singh et al., 2024] to the residual stream can enable a degree of control over pretrained LM generations without the need for resource-intensive finetuning [Wu et al., 2024a]. The success of these methods affirms that representations induced by pretrained LMs carry rich semantic structure. Interventional interpretability. Much recent work has used interventions on model-internal states to test hypotheses about how LMs implement various behaviours. In particular, interventions on linear subspaces of representations have provided increasing evidence that human-interpretable concepts are encoded linearly [Smolensky, 1986, Rumelhart et al., 1986, McClelland et al., 1986]. This includes linguistic features such as gender and number [Lasri et al., 2022, Wang et al., 2023, Hanna et al., 2023, Chintam et al., 2023, Yamakoshi et al., 2023, Hao and Linzen, 2023, Chen et al., 2023, Amini et al., 2023, Guerner et al., 2023, Arora et al., 2024], logical and mathematical reasoning [Wu et al., 2023], entity attributes [Huang et al., 2024], and a number of other domains [Mikolov et al., 2013, Elhage et al., 2022, Park et al., 2023, Nanda et al., 2023, Guerner et al., 2023]. 3 ReFT We now define the ReFT family of methods. To do this, we first summarize the core motivation, which emerges from work on intervention-based model interpretability. We then show how this leads directly to Low-rank Linear Subspace ReFT (LoReFT). Finally, we generalize this to a family of ReFT methods. Appendix A provides a brief overview of our generic ReFT training library. To keep the presentation simple, we assume throughout that our target model is a Transformer- based [Vaswani et al., 2017] LM that produces contextualised representations of sequences of tokens. Given a sequence of n input tokens x = (x1, . . . , xn), the model first embeds these into a list of representations h(0) =(h(0) 1 , . . . ,h(0) n ). Then, m layers successively compute the j-th list of hidden representations h(j) as a function of the previous list of hidden representations h(j−1). Each hidden representation is a vector h ∈Rd. The LM uses the final hidden representations h(m) to produce its predictions. In our experiments, we consider both autoregressive LMs and masked LMs [Devlin et al., 2019]. An autoregressive LM predicts p(xn+1 ∣ x1, . . . , xn) =softmax (Wh(m) n ), while a masked LM predicts p(xi ∣ x1, . . . , xi−1, xi+1, . . . , xn) =softmax (Wh(m) i ), where W is a learned matrix mapping from representations to logits over the vocabulary space. 3.1 Motivation In interpretability research, the framework of causal abstraction [Geiger et al., 2021] usesinterchange interventions to establish the causal role of representations in deep learning models. An interchange intervention fixes a representation to the value it would take if a counterfactual input were processed by the model. Experiments investigating how such interventions affect model behavior form the evidence for claims about the causal role of a representation and the concept it encodes. To test whether a concept is encoded in a linear subspace of a representation, one may use a dis- tributed interchange intervention (DII) [Geiger et al., 2023b].3 Let b be the hidden representation created at row i and column k when our model processes input b, and let s be the corresponding representation when that same model processes input s. A distributed interchange intervention on b given a counterfactual source representation s is then defined as DII(b, s, R) =b +R⊺(Rs −Rb) (1) where R ∈ Rr×d is a low-rank projection matrix with orthonormal rows, d is the representation dimensionality, and r is the dimensionality of the subspace we are intervening on. We learn the subspace R using distributed alignment search (DAS), which finds the subspace that maximises the 3This notion of subspace intervention was also independently discovered by Guerner et al. [2023]. 3R eFT Int er v ention t his is some t e xt LoR eFT edit subspace (r o ws of R) RW h h h Φ(h) b -+ + R T edit r estrict ed  t o subspace edit  v ect or Figure 2: Illustration of ReFT. (1) The left panel depicts an intervention I: the intervention function Φ is applied to hidden representations at positions P in layer l. (2) The right panel depicts the intervention function used in LoReFT, which finds an edit vector that only modifies the representation in the linear subspace spanned by the rows of R. Specifically, we show how a rank-2 LoReFT operates on 3-dimensional hidden representations. probability of the expected counterfactual output after intervention [Geiger et al., 2023b]. DAS is highly expressive, and can effectively localize concepts within model representations [Wu et al., 2023, Arora et al., 2024, Wu et al., 2024c, Huang et al., 2024]. This suggests that subspace representation interventions could also be a powerful tool for model control. 3.2 Two low-rank ReFT instantiations LoReFT. The formulation of DII in eq. (1) immediately suggests a way to control model generations via interventions. The guiding intuition is that we can learn how to perform interventions that steer the model towards predicting our task labels. The resulting method, Low-rank Linear Subspace ReFT (LoReFT), is defined by the following variant of eq. (1): ΦLoReFT(h) =h +R⊺(Wh +b −Rh) (2) This is identical to eq. (1), except we use a learned projected source Rs =Wh +b. LoReFT thus edits the representation in the r-dimensional subspace spanned by the rows of R to take on the values obtained from our linear projection Wh +b. We depict this operation in fig. 2. The learned parameters are ϕ = {R, W, b}; the parameters of the LM are frozen. As with DII, R ∈ Rr×d is a low-rank matrix with orthonormal rows where d is the hidden-state dimensionality and r ≤d is the rank of the subspace. We further define a linear projection W ∈Rr×d and bias vector b ∈Rr. DiReFT. In addition, we define an ablation of LoReFT which removes the orthogonality constraint and the difference operation, reducing training time: ΦDiReFT(h) =h +W⊺ 2 (W1h +b) (3) Both W1, W2 ∈Rr×d are low-rank projection matrices. Note that eq. (3) resembles LoRA, and thus DiReFT can be thought of as LoRA applied directly to hidden representations at certain positions.4 Empirical evidence from previous work suggests that adding orthogonal constraints to LoRA weights increases performance [Liu et al., 2024d]. (Appendix E reports results for additional ablations of LoReFT.) Training objective. We consider both generation tasks using decoder-only or encoder–decoder LMs and classification tasks using encoder-only models. The pretrained language model induces a distribution over token sequences p(⋅). We denote the model that results from the ReFT intervention Φ on p(⋅) as pΦ(⋅) with trainable parameters ϕ. To simplify notation, we refer to the hidden representations produced by the LM on input x as h(x), and those by the intervened LM as hΦ(x). For generation tasks, our training objective is language modelling. Given an input sequence x = (x1, . . . , xn) with n tokens as the prompt, the goal is to predict the output sequencey =(y1, . . . , ym) 4LoRA is not applicable to the residual stream, which is weightless. LoRA can be configured to apply only to the attention layer output projection matrix, which is similar to our residual stream intervention. However, previous works found that applying LoRA only to attention layers is sub-optimal [Hu et al., 2023]. 4with m tokens. We minimise the cross-entropy loss with teacher-forcing over all output positions. min ϕ {− m ∑ i=1 log pΦ (yi ∣ xy<i)} (4) For single-label classification tasks, we add a classification head Hθ(⋅) with parameters θ that takes the final-layer representation at the first token (CLS) as input and outputs a distribution over classes. H has the learned parameters θ ={Wo, bo, Wd, bd}. Hθ(⋅∣ h) =softmax (Wo(tanh(Wdh(m) 1 +bd))+bo) (5) We learn the parameters of the head and those of the intervention function Φ. We minimise the cross-entropy loss of the target class y given input x: min ϕ,θ {−log Hθ(y ∣ hΦ(x))} (6) 3.3 The ReFT family of methods It is straightforward to generalise the above intervention functions to define a family of intervention- based representation finetuning methods. We first define a general notion of intervention, i.e. the modification of hidden representations during the model forward pass: Definition 3.1. An intervention I is a tuple ⟨Φ, P, l⟩ that encapsulates a single inference-time modification of the representations computed by a Transformer-based LM. The three components of an intervention are (1) the intervention function Φ ∶Rd → Rd with learned parameters ϕ, (2) a set of input positions P ⊆{1, . . . , n} that the intervention is applied to, and (3) the layer l ∈{1, . . . , m} at which the intervention is applied. We implement the intervention I as the following operation that overwrites some representations h: h(l) ← (Φ (h(l) p ) if p ∈P else h(l) p )p∈1,...,n (7) The intervention is applied immediately after the computation of h(l) and thus affects the representa- tions computed in later layers h(l+1), . . . ,h(m). Figure 2 provides a schematic overview of an intervention. A ReFT is then defined as a constrained set of non-overlapping interventions: Definition 3.2. A ReFT method is a set of f interventions I = {I1, . . . , If }. We enforce that for any two interventions Ij, Ik ∈I such that they operate on the same layer lj =lk, their intervention positions must be disjoint, i.e. Pj ∩Pk =∅. The parameters (ϕ1, . . . , ϕf ) of all of the intervention functions are independent. ReFT is thus a generic framework encompassing interventions on hidden representations during the model forward pass. In appendix B, we show how a variety of existing inference-time intervention methods can be described within this framework. 4 Experiments To evaluate our ReFTs against existing PEFTs, we conduct experiments across four diverse NLP benchmarks covering more than 20 datasets (extensive details on our datasets are in appendix C). Our goal is to provide a rich picture of how LoReFT and DiReFT perform in different scenarios. We experiment with both masked and autoregressive LMs at different scales, ranging from RoBERTa- base [Liu et al., 2019] with 125M to LLaMA models [Touvron et al., 2023a,b] with 13B parameters. We benchmark against existing PEFTs such as prefix-tuning [Li and Liang, 2021], adapter-tuning with both Series Adapters and Parallel Adapters, BitFit [Ben Zaken et al., 2022], RED [Wu et al., 2024a], LoRA [Hu et al., 2022], and DoRA [Liu et al., 2024c]. Our comparisons focus on both performance and parameter efficiency. In our comparisons, we use hyperparameter-tuned scores from previous works when possible. We load our base LMs in torch.bfloat16 to save memory. All of our experiments are run with a single GPU: NVIDIA A100 40G/80G or RTX 6000. Examples of raw model generations are in appendix I. 54.1 Hyperparameter configuration For our experiments, we must decide how many interventions to learn and which layers and input positions to apply each one on. We propose learning interventions on a fixed number of p prefix and s suffix positions in the prompt. Specifically, we tune four hyperparameters: 1. The number of prefix positions p to intervene on, i.e. positions {1, . . . , p}. 2. The number of suffix positions s to intervene on, i.e. positions {n −s +1, . . . , n}. 3. Which set of layers L to intervene on. 4. Whether or not to tie intervention parameters ϕ across different positions in the same layer. This simplifies the hyperparameter search space; compared to LoRA, the only additional consideration is which positions to intervene on. Since the number of positions edited is constant, LoReFT and DiReFT contribute a fixed additional inference cost that does not scale with prompt length. Given the positions P ={1, . . . , p}∪{n −s +1, . . . , n}, we define the untied and tied variants: Iuntied ={⟨Φ, {p}, l⟩ ∣p ∈P, l∈L} Itied ={⟨Φ, P, l⟩ ∣l ∈L} Additionally, when applying LoReFT and DiReFT to a prompt with length n where n <p +s, we set p ← min(p, ⌊n/2⌋) and s ← min(s, ⌈n/2⌉) and do not apply the truncated interventions in Iuntied. We also tune neural-network training hyperparameters. Unlike previous work [Hu et al., 2022, 2023, Liu et al., 2024c] where hyperparameter tuning may involve optimising performance directly on test sets, we only tune our hyperparameters on development sets which do not contain any overlapping examples with the test sets of our tasks. We further describe hyperparameter tuning for each benchmark in appendix D.1. 4.2 Commonsense reasoning We replicate the experimental setup in Hu et al. [2023] and finetune LLaMA-1 7B/13B, Llama-2 7B, and Llama-3 8B 5 on a combined dataset of eight commonsense reasoning tasks (COMMONSENSE 170K ). We report scores on each task’s test set individually. We compare with PEFTs benchmarked in Hu et al. [2023] as well as the identical experiment reported in Liu et al. [2024c] for DoRA. Datasets. Our benchmark contains eight commonsense reasoning datasets, including BoolQ [Clark et al., 2019], PIQA [Bisk et al., 2020], SIQA [Sap et al., 2019], HellaSwag [Zellers et al., 2019], WinoGrande [Sakaguchi et al., 2021], ARC-e, ARC-c [Clark et al., 2018], and OBQA [Mihaylov et al., 2018]. Examples are formulated as multiple-choice problems where the model needs to directly generate the correct choice without rationales. We use the same prompt template as in Hu et al. [2023] with additional string normalisation (removing leading and trailing whitespace). Hyperparameter tuning. We do not do hyperparameter selection based on test set results. Rather, we use the hyperparameter settings of the model that performs best on a development set created from the GSM8K training set, except we use a lower number of epochs (6 instead of 12) because the COMMONSENSE 170K training set is more than 20 times larger than GSM8K . This allows us to tune relevant hyperparamters, and also serves to test the robustness of these settings across different domains. We additionally report scores on 3 epochs in appendix D.3. Results. We report results in table 1. LoReFT sets state-of-the-art performance on the commonsense reasoning tasks, outperforming all other methods by a considerable margin. While being more compute-efficient, DiReFT achieves only slightly worse performance consistently. 4.3 Arithmetic reasoning Similar to the previous experiment, we follow the experimental setup in Hu et al. [2023] and finetune LLaMA-1 7B and 13B on a combined dataset of seven arithmetic reasoning tasks with LM-generated chain-of-thought steps (MATH10K) and report scores on four of the tasks’ test sets. We only evaluate correctness on the final numeric or multiple-choice answer. 5Llama-3 8B appeared on April 18, 2024, and thus we had time to complete only commonsense reasoning experiments with this model. Liu et al. [2024c] report corresponding results for LoRA and DoRA. 6Table 1: Accuracy comparison of LLaMA-1 7B/13B, Llama-2 7B and Llama-3 8B against existing PEFT methods on eight commonsense reasoning datasets. ∗Performance results of all baseline methods are taken from Liu et al. [2024c]. We report averaged performance of three runs with distinct random seeds for our method. For our methods, Param. (%) is calculated by dividing the number of trainable parameters by the number of parameters of the base LM. Model PEFT Params (%) Accuracy(↑) BoolQ PIQA SIQA HellaS. WinoG. ARC-e ARC-c OBQA Avg. ChatGPT∗ — — 73.1 85.4 68.5 78.5 66.1 89.8 79.9 74.8 77.0 LLaMA-7B PrefT∗ 0.039% 64.3 76.8 73.9 42.1 72.1 72.9 54.0 60.6 64.6 AdapterS∗ 1.953% 63.0 79.2 76.3 67.9 75.7 74.5 57.1 72.4 70.8 AdapterP∗ 3.542% 67.9 76.4 78.8 69.8 78.9 73.7 57.3 75.2 72.3LoRA∗ 0.826% 68.9 80.7 77.4 78.1 78.8 77.8 61.3 74.8 74.7DoRA (half)∗ 0.427% 70.0 82.6 79.7 83.2 80.6 80.6 65.4 77.6 77.5DoRA∗ 0.838% 68.5 82.9 79.6 84.8 80.8 81.4 65.8 81.0 78.1 DiReFT (ours) 0.031% 69.5 83.0 79.0 92.5 80.5 82.2 68.0 77.5 79.0LoReFT (ours) 0.031% 69.3 84.4 80.3 93.1 84.2 83.2 68.2 78.9 80.2 LLaMA-13B PrefT∗ 0.031% 65.3 75.4 72.1 55.2 68.6 79.5 62.9 68.0 68.4 AdapterS∗ 1.586% 71.8 83.0 79.2 88.1 82.4 82.5 67.3 81.8 79.5 AdapterP∗ 2.894% 72.5 84.9 79.8 92.1 84.7 84.2 71.2 82.4 81.5LoRA∗ 0.670% 72.1 83.5 80.5 90.5 83.7 82.8 68.3 82.4 80.5DoRA (half)∗ 0.347% 72.5 85.3 79.9 90.1 82.9 82.7 69.7 83.6 80.8DoRA∗ 0.681% 72.4 84.9 81.5 92.4 84.2 84.2 69.6 82.8 81.5 DiReFT (ours) 0.025% 71.3 86.1 80.8 94.6 83.6 85.5 72.9 82.7 82.2LoReFT (ours) 0.025% 72.1 86.3 81.8 95.1 87.2 86.2 73.7 84.2 83.3 Llama-2 7B LoRA∗ 0.826% 69.8 79.9 79.5 83.6 82.6 79.8 64.7 81.0 77.6DoRA (half)∗ 0.427% 72.0 83.1 79.9 89.1 83.0 84.5 71.0 81.2 80.5DoRA∗ 0.838% 71.8 83.7 76.0 89.1 82.6 83.7 68.2 82.4 79.7 DiReFT (ours) 0.031% 70.8 83.6 80.2 93.6 82.1 84.8 70.4 81.5 80.9LoReFT (ours) 0.031% 71.1 83.8 80.8 94.3 84.5 85.6 72.2 82.3 81.8 Llama-3 8B LoRA∗ 0.700% 70.8 85.2 79.9 91.7 84.3 84.2 71.2 79.0 80.8DoRA (half)∗ 0.361% 74.5 88.8 80.3 95.5 84.7 90.1 79.1 87.2 85.0DoRA∗ 0.710% 74.6 89.3 79.9 95.5 85.6 90.5 80.4 85.8 85.2 DiReFT (ours) 0.026% 73.4 88.7 81.0 95.6 85.5 91.8 81.8 85.4 85.4LoReFT (ours) 0.026% 75.1 90.2 82.0 96.3 87.4 92.4 81.6 87.5 86.6 Hyperparameter tuning. We use the same hyperparameter settings as for the Commonsense Rea- soning benchmark, but with 12 epochs for training. We also report scores on 3 epochs. Datasets. Our benchmark contains four datasets for math world problems, including AQuA [Ling et al., 2017], GSM8K [Cobbe et al., 2021], MAWPS [Koncel-Kedziorski et al., 2016], and SV AMP [Pa- tel et al., 2021]. Models need to generate chain-of-thought [Wei et al., 2022] before the final answer. We use the same prompt template and hyperparameter settings as in the previous experiment. Results. We report results in table 2. We find that both LoReFT and DiReFT do not perform as well at arithmetic reasoning tasks compared to LoRA and adapters, but do outperform prefix-tuning. Our results suggest that our ReFTs may have more trouble on chain-of-thought reasoning than the single-step commonsense reasoning tasks due to the length of generations (greater length necessarily reduces the effect of the intervention) and overall greater difficulty of the task. Our results show that our ReFTs perform better with the 13B model than the 7B model, which suggests that our methods scale with model size. Overall, we note that the arithmetic reasoning results show a lot of variation, with no single method emerging as a clear winner across all of them. 4.4 Instruction-following Base LMs require instruction finetuning to follow human prompts [Ouyang et al., 2022]. We follow the experimental setup in Wu et al. [2024a] and finetune Llama-2 7B with Ultrafeedback [Cui et al., 2023]. We compare against full parameter finetuning, LoRA, and RED. For evaluation, we use Alpaca-Eval v1.0 [Li et al., 2023], which computes the win-rate against text-davinci-003 using GPT-4 as the annotator. We use the same prompt template as in Taori et al. [2023]. Datasets. Ultrafeedback is high-quality instruction dataset where responses are generated via scoring a diverse set of model responses from a list of candidates (e.g. ChatGPT and Bard). The score is calculated as a weighted score of instruction-following, truthfulness, honesty, and helpfulness. 7Table 2: Accuracy comparison of LLaMA-1 7B/13B against existing PEFT methods on four arithmetic reasoning datasets. ∗Performance results of all baseline methods are taken from Hu et al. [2023]. We report averaged performance of three runs with distinct random seeds for our method. Model PEFT Params (%) Accuracy(↑) AQuA GSM8K MA WPS SV AMP Avg. LLaMA-7B PrefT∗ 0.039% 14.2 24.4 63.4 38.1 35.0 AdapterS∗ 1.953% 15.0 33.3 77.7 52.3 44.6 AdapterP∗ 3.542% 18.1 35.3 82.4 49.6 46.4 LoRA∗ 0.826% 18.9 37.5 79.0 52.1 46.9 DiReFT (ours) 0.031% 21.3 24.1 74.5 42.7 40.6 LoReFT (ours) 0.031% 21.4 26.0 76.2 46.8 42.6 LLaMA-13B PrefT∗ 0.031% 15.7 31.1 66.8 41.4 38.8 AdapterS∗ 1.586% 22.0 44.0 78.6 50.8 48.9 AdapterP∗ 2.894% 20.5 43.3 81.1 55.7 50.2 LoRA∗ 0.670% 18.5 47.5 83.6 54.6 51.1 DiReFT (ours) 0.025% 20.5 35.8 80.8 54.8 48.0 LoReFT (ours) 0.025% 23.6 38.1 82.4 54.2 49.6 Table 3: Instruction tuning evaluation results for instruction-tuned Llama-2 7B with Alpaca-Eval v1.0. We report averaged performance of two runs with distinct random seeds for our method. half denotes our runs with half of the rank; 1K denotes our runs with a low-resource setting where there is only 1K training examples. †Performance results of baseline methods are taken from Li et al. [2023]. ∗Performance results of baseline methods are taken from Wu et al. [2024a]. ‡It takes 18 minutes to train our Llama-2 Chat 7B on 1K examples using a single A100 40G GPU with ≈1MB parameters on disk. Model & PEFT Params (%) Win-rate (↑) GPT-3.5 Turbo 1106† — 86.30 Llama-2 Chat 13B† — 81.10 Llama-2 Chat 7B† — 71.40 Llama-2 7B & FT∗ 100% 80.93 Llama-2 7B & LoRA∗ 0.1245% 81.48 Llama-2 7B & RED∗ 0.0039% 81.69 Llama-2 7B & DiReFT (ours) 0.0039% 84.85 Llama-2 7B & LoReFT (ours) 0.0039% 85.60 Llama-2 7B & LoReFT (ours, half) 0.0019% 84.12 Llama-2 7B & LoReFT (ours, 1K)‡ 0.0039% 81.91 Some of the best 7B and 13B chat-models (e.g. UltraLM-13B [Ding et al., 2023]) are finetuned with Ultrafeedback. Hyperparameter tuning. We do hyperparameter-tuning on the unseen instruction-following dataset Alpaca-52K [Taori et al., 2023] with only LLaMA-7B to prevent test-set hill-climbing. We then use the hyperparameter settings of our best performing model to finetune on Ultrafeedback. For hyperparameter tuning, we use Alpaca-Eval v1.0 with GPT-4 turbo as the annotator for fast turnaround, which also prevents overfitting with GPT-4 as a judge. Results. We report results in table 3. When matched in parameter count to the previous most parameter-efficient PEFT (RED) and trained on Llama-2 7B, LoReFT outperforms all reported finetuning methods (including full finetuning) and achieves a win-rate within 1% of GPT-3.5 Turbo 1106. Furthermore, after halving the parameter count or using only 1/64-th of the data, LoReFT still outperforms other finetuning methods. This result shows that LoReFT can succeed at long-form text generation. DiReFT is again slightly worse than LoReFT but is highly competitive. 6 6We release our ReFT weights (<1MB) of our instruction-tuned model through HuggingFace and provide a tutorial at https://github.com/stanfordnlp/pyreft/blob/main/examples/chat. 8Table 4: Accuracy comparison of RoBERTa-base and RoBERTa-large against existing PEFT methods on the GLUE benchmark. ∗Performance results of all baseline methods are taken from Wu et al. [2024a]. We report averaged performance of five runs with distinct random seeds for our method. Model PEFT Params (%) Accuracy(↑) MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B Avg. base FT 100% 87.3 94.4 87.9 62.4 92.5 91.7 78.3 90.6 85.6 Adapter∗ 0.318% 87.0 93.3 88.4 60.9 92.5 90.5 76.5 90.5 85.0 LoRA∗ 0.239% 86.6 93.9 88.7 59.7 92.6 90.4 75.3 90.3 84.7 AdapterFNN∗ 0.239% 87.1 93.0 88.8 58.5 92.0 90.2 77.7 90.4 84.7 BitFit∗ 0.080% 84.7 94.0 88.0 54.0 91.0 87.3 69.8 89.5 82.3 RED∗ 0.016% 83.9 93.9 89.2 61.0 90.7 87.2 78.0 90.4 84.3 DiReFT (ours) 0.015% 82.5 92.6 88.3 58.6 91.3 86.4 76.4 89.3 83.2 LoReFT (ours) 0.015% 83.1 93.4 89.2 60.4 91.2 87.4 79.0 90.0 84.2 large FT 100% 88.8 96.0 91.7 68.2 93.8 91.5 85.8 92.6 88.6 Adapter∗ 0.254% 90.1 95.2 90.5 65.4 94.6 91.4 85.3 91.5 88.0 LoRA∗ 0.225% 90.2 96.0 89.8 65.5 94.7 90.7 86.3 91.7 88.1 AdapterFNN∗ 0.225% 90.3 96.1 90.5 64.4 94.3 91.3 84.8 90.2 87.7 RED∗ 0.014% 89.5 96.0 90.3 68.1 93.5 88.8 86.2 91.3 88.0 DiReFT (ours) 0.014% 88.7 95.4 88.5 66.7 93.9 88.1 86.9 91.2 87.4 LoReFT (ours) 0.014% 89.2 96.2 90.1 68.0 94.1 88.5 87.5 91.6 88.2 4.5 Natural language understanding We evaluate LoReFT on the GLUE benchmark [Wang et al., 2018] against existing PEFTs. We use this set of experiments to show LoReFT works well even with small-scale LMs, and can improve representations for classification tasks and not just text generation. We finetune RoBERTa-base (125M) as well as RoBERTa-large (350M) on GLUE, a sequence classification benchmark for natural language understanding (NLU) which covers domains such as sentiment classification and natural language inference. Details about the GLUE benchmark can be found in its original paper. We follow Wu et al. [2024a] for proper evaluation on GLUE validation set: we split the validation set into two sets guarded by a random seed, and we pick the best model with highest in-training validation accuracy to evaluate on the other held-out half for testing accuracy. Hyperparameter tuning. We tune our hyperparameters for each task separately. which is standard for PEFTs. To avoid overfitting to random seeds, we hyperparameter-tune our models with a constant seed, and report averaged results over that and four additional unseen seeds. We describe hyperparameter tuning experiments in Appendix D.1. Results. We report results in table 4. LoReFT obtains comparable performance with PEFT methods on both model sizes when parameter matched with RED, the previous most parameter-efficient PEFT for this task. Furthermore, DiReFT achieves worse performance than most of the PEFTs suggesting LoReFT is a better choice when LM is small. Full results with standard deviation is in table 13. We additionally compare against VeRA [Kopiczko et al., 2024] in appendix D.3. 5 Limitations Due to limited resources, we mainly explored the LLaMA-family of models. In future work, we hope to explore the effectiveness of ReFT on other model families as well as vision–language models such as LLaV A [Liu et al., 2024a]. The capabilities of ReFT have not yet been fully explored due to the large hyperparameter search space; we are interested in automating this search. We provide some initial explorations of LM personalisation with ReFT in a few-shot setting in appendix G.2. We hope to explore why ReFT works, and we provide some of our early explorations focused on memorisation (appendix F.1, appendix F.2). We are also investigating whether learned orthogonal subspaces can be composed together without adaptation. Some encouraging initial findings are in appendix G.1. ReFT, abstraction, and generation. Neural network interpretability research often struggles to contribute directly to improving models. With ReFT, we have shown one way to overcome this challenge. The ReFT framework is rooted in work on causal abstraction [Geiger et al., 2023a] for model interpretability, and LoReFT builds directly on the distributed interchange intervention method 9of Geiger et al. [2023b] and Wu et al. [2023]. See also the interchange intervention training (IIT) method of Geiger et al. [2022], Wu et al. [2022], Huang et al. [2023c]. In a similar vein, recent work also uses representation-based editing of the Transformer stream to steer model behavior [Li et al., 2024, Zou et al., 2023]. ReFT advances this line of work by showing one way that such steering can be learned, rather than being merely a post hoc analysis step. The precise ways in which ReFT works deserve deeper exploration. Although these methods intervene on representations, the causal effect of such interventions may only emerge in the model’s upstream computations. In other words, the power of ReFT may come from the fact that it creates new causal pathways or modifies the strength of some existing ones. We leave it to future research to track these effects, and perhaps to explore more structured ReFTs to modify complex causal pathways in LMs. ReFT and model interpretability. ReFT relies on insights from work on interpretability, and it may also be able to contribute insights back to that field. In particular, LoReFT shows that training a set of low-rank interventions on selected residual streams can induce a base LM to follow instructions (section 4.4). In other words, a linear subspace distributed across a set of neurons can achieve generalised control over a vast number of tasks. This is a serious challenge to work seeing to interpret individual neurons in isolation (for related criticisms, see Huang et al. 2023b). The success of ReFT suggests to us a quite different approach to interperetability, one that starts from the assumption that neurons will play different roles in different contexts. Evaluation practices in PEFT research. In this work, we hyperparameter-tune ReFT on develop- ment sets that do not overlap with the test set. Unfortunately, a considerable portion of the literature on PEFTs directly hill-climbs performance on test sets. This results in overfitting to specific tasks, which gives practitioners less certainty about the real-world performance of different methods and impedes fair comparison. We hope that future work can introduce benchmarks for evaluating PEFTs and ReFTs. These should allow for compute- or time-matched hyperparameter-tuning comparisons, and they should disallow any kind of tuning or model selection based on the test set. 6 Conclusion We propose a strong alternative to PEFTs, LoReFT, and we identify an ablation of this method, DiReFT, that trades some performance for increased efficiency. Overall, LoReFT achieves strong per- formance across benchmarks from four domains while being 15×–65×more efficient than LoRA. No- tably, LoReFT establishes new state-of-the-art performance on commonsense reasoning, instruction- following, and natural language understanding against the strongest PEFTs. We also show how our method can be described under a generic framework – ReFT. ReFT is a new approach to finetuning that is more powerful, more parameter-efficient, and more interpretable than any existing PEFTs. Acknowledgements We thank Jing Huang for helpful discussion in designing our memorisation tests as well as writing. We thank Chenglei Si, Harshit Joshi, Jordan Juravsky, Julie Kallini, Ken Liu, Rohan Pandey, Jiuding Sun, Leonard Tang, Tristan Thrush, Shengguang Wu, Qinan Yu, Yanzhe Zhang, Amir Zur, and Shiqi Chen for helpful discussion about the project and comments on the manuscript. References Afra Amini, Tiago Pimentel, Clara Meister, and Ryan Cotterell. Naturalistic causal probing for morpho-syntax. Transactions of the Association for Computational Linguistics, 11:384–403, 2023. doi: 10.1162/tacl_a_00554. URL https://aclanthology.org/2023.tacl-1.23. Aryaman Arora, Dan Jurafsky, and Christopher Potts. CausalGym: Benchmarking causal inter- pretability methods on linguistic tasks. arXiv:2402.12560, 2024. URL https://arxiv.org/abs/ 2402.12560. Matan Avitan, Ryan Cotterell, Yoav Goldberg, and Shauli Ravfogel. What changed? Converting representational interventions to natural language. arXiv:2402.11355, 2024. URL https://arxiv. org/abs/2402.11355. 10Nora Belrose, David Schneider-Joseph, Shauli Ravfogel, Ryan Cotterell, Edward Raff, and Stella Biderman. LEACE: Perfect linear concept erasure in closed form. Advances in Neural Information Processing Systems, 36, 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/ file/d066d21c619d0a78c5b557fa3291a8f4-Paper-Conference.pdf. Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. BitFit: Simple parameter-efficient fine- tuning for transformer-based masked language-models. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) , pages 1–9, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-short.1. URL https: //aclanthology.org/2022.acl-short.1. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. PIQA: Reasoning about physical commonsense in natural language. InProceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 7432–7439, 2020. URL https://arxiv.org/abs/1911.11641. Lewis Carroll. Alice’s Adventures in Wonderland. Macmillan, London, 1865. Angelica Chen, Ravid Schwartz-Ziv, Kyunghyun Cho, Matthew L. Leavitt, and Naomi Saphra. Sudden drops in the loss: Syntax acquisition, phase transitions, and simplicity bias in MLMs. arXiv:2309.07311, 2023. URL https://arxiv.org/abs/2309.07311v4. Abhijith Chintam, Rahel Beloch, Willem Zuidema, Michael Hanna, and Oskar van der Wal. Identifying and adapting transformer-components responsible for gender bias in an English language model. In Yonatan Belinkov, Sophie Hao, Jaap Jumelet, Najoung Kim, Arya Mc- Carthy, and Hosein Mohebbi, editors, Proceedings of the 6th BlackboxNLP Workshop: Ana- lyzing and Interpreting Neural Networks for NLP, pages 379–394, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.blackboxnlp-1.29. URL https://aclanthology.org/2023.blackboxnlp-1.29. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan- guage Technologies, Volume 1 (Long and Short Papers), pages 2924–2936, Minneapolis, Min- nesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1300. URL https://aclanthology.org/N19-1300. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? Try ARC, the AI2 reasoning challenge. arXiv:1803.05457, 2018. URL https://arxiv.org/abs/1803.05457. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv:2110.14168, 2021. URL https://arxiv.org/abs/2110.14168. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. UltraFeedback: Boosting language models with high-quality feedback. arXiv:2310.01377, 2023. URL https://arxiv.org/abs/2310.01377. Andrew M. Dai and Quoc V . Le. Semi-supervised sequence learning. In Advances in Neural Information Processing Systems , volume 28. Curran Associates, Inc., 2015. URL https:// proceedings.neurips.cc/paper/2015/hash/7137debd45ae4d0ab9aa953017286b20-Abstract.html. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/ N19-1423. 11Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 3029–3051, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.183. URL https://aclanthology.org/2023.emnlp-main.183. Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg, and Christopher Olah. Toy models of superpo- sition. Transformer Circuits Thread, 2022. URL https://transformer-circuits.pub/2022/toy_ model/index.html. Stanislav Fort. Scaling laws for adversarial attacks on language model activations, 2023. URL http://arxiv.org/abs/2312.02780. Cheng Fu, Hanxian Huang, Xinyun Chen, Yuandong Tian, and Jishen Zhao. Learn-to-Share: A hardware-friendly transfer learning framework exploiting computation and parameter sharing. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 3469–3479. PMLR, 2021. URL http://proceedings.mlr. press/v139/fu21a.html. Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al. LLaMA-Adapter v2: Parameter-efficient visual instruction model. arXiv:2304.15010, 2023. URL https://arxiv.org/abs/2304.15010. Atticus Geiger, Hanson Lu, Thomas Icard, and Christopher Potts. Causal abstractions of neural networks. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 9574–9586. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/ 4f5c422f4d49a5a807eda27434231040-Paper.pdf. Atticus Geiger, Zhengxuan Wu, Hanson Lu, Josh Rozner, Elisa Kreiss, Thomas Icard, Noah Good- man, and Christopher Potts. Inducing causal structure for interpretable neural networks. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 7324–7338. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/geiger22a.html. Atticus Geiger, Chris Potts, and Thomas Icard. Causal abstraction for faithful model interpretation. arXiv:2301.04709, 2023a. URL https://arxiv.org/abs/2301.04709. Atticus Geiger, Zhengxuan Wu, Christopher Potts, Thomas Icard, and Noah D. Goodman. Find- ing alignments between interpretable causal variables and distributed neural representations. arXiv:2303.02536, 2023b. URL https://arxiv.org/abs/2303.02536. Clément Guerner, Anej Svete, Tianyu Liu, Alexander Warstadt, and Ryan Cotterell. A geometric notion of causal probing. arXiv:2307.15054, 2023. URL https://arxiv.org/abs/2307.15054. Michael Hanna, Yonatan Belinkov, and Sandro Pezzelle. When language models fall in love: Animacy processing in transformer language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , pages 12120–12135, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.744. URL https://aclanthology.org/2023.emnlp-main.744. Sophie Hao and Tal Linzen. Verb conjugation in transformers is determined by linear encodings of subject number. In Houda Bouamor, Juan Pino, and Kalika Bali, editors,Findings of the Association for Computational Linguistics: EMNLP 2023 , pages 4531–4539, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.300. URL https://aclanthology.org/2023.findings-emnlp.300. 12Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, 2022a. URL https://openreview.net/ forum?id=0RDcd5Axok. Shwai He, Liang Ding, Daize Dong, Jeremy Zhang, and Dacheng Tao. SparseAdapter: An easy approach for improving the parameter-efficiency of adapters. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Findings of the Association for Computational Linguistics: EMNLP 2022 , pages 2184–2190, Abu Dhabi, United Arab Emirates, December 2022b. As- sociation for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.160. URL https://aclanthology.org/2022.findings-emnlp.160. Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. Learning to solve arithmetic word problems with verb categorization. In Alessandro Moschitti, Bo Pang, and Walter Daelemans, editors, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 523–533, Doha, Qatar, October 2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1058. URL https://aclanthology.org/ D14-1058. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA , volume 97 of Proceedings of Machine Learning Research, pages 2790–2799. PMLR, 2019. URL http://proceedings.mlr.press/v97/houlsby19a.html. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In The Tenth In- ternational Conference on Learning Representations, ICLR 2022 , Virtual Event, 2022. URL https://openreview.net/forum?id=nZeVKeeFYf9. Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Soujanya Poria, and Roy Lee. LLM-adapters: An adapter family for parameter-efficient fine-tuning of large language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5254–5276, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main. 319. URL https://aclanthology.org/2023.emnlp-main.319. Chengsong Huang, Qian Liu, Bill Yuchen Lin, Tianyu Pang, Chao Du, and Min Lin. LoraHub: Efficient cross-task generalization via dynamic lora composition. arXiv:2307.13269, 2023a. URL https://arxiv.org/abs/2307.13269. Jing Huang, Atticus Geiger, Karel D’Oosterlinck, Zhengxuan Wu, and Christopher Potts. Rigorously assessing natural language explanations of neurons. In Yonatan Belinkov, Sophie Hao, Jaap Jumelet, Najoung Kim, Arya McCarthy, and Hosein Mohebbi, editors, Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pages 317–331, Singapore, December 2023b. Association for Computational Linguistics. doi: 10.18653/v1/2023. blackboxnlp-1.24. URL https://aclanthology.org/2023.blackboxnlp-1.24. Jing Huang, Zhengxuan Wu, Kyle Mahowald, and Christopher Potts. Inducing character-level structure in subword-based language models with type-level interchange intervention training. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023 , pages 12163–12180, Toronto, Canada, July 2023c. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.770. URL https: //aclanthology.org/2023.findings-acl.770. Jing Huang, Christopher Potts Zhengxuan Wu, Mor Geva, and Atticus Geiger. RA VEL: Evaluating interpretability methods on disentangling language model representations. arXiv:2402.17700, 2024. URL https://arxiv.org/abs/2402.17700. Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. Parsing algebraic word problems into equations.Transactions of the Association for Computational 13Linguistics, 3:585–597, 2015. doi: 10.1162/tacl_a_00160. URL https://aclanthology.org/ Q15-1042. Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. MAWPS: A math word problem repository. In Kevin Knight, Ani Nenkova, and Owen Rambow, editors, Proceedings of the 2016 Conference of the North American Chapter of the Association for Com- putational Linguistics: Human Language Technologies, pages 1152–1157, San Diego, Califor- nia, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1136. URL https://aclanthology.org/N16-1136. Dawid Jan Kopiczko, Tijmen Blankevoort, and Yuki M Asano. VeRA: Vector-based random matrix adaptation. In The Twelfth International Conference on Learning Representations, ICLR 2024, 2024. URL https://openreview.net/forum?id=NjNfLdxr3A. Karim Lasri, Tiago Pimentel, Alessandro Lenci, Thierry Poibeau, and Ryan Cotterell. Prob- ing for the usage of grammatical number. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Com- putational Linguistics (Volume 1: Long Papers) , pages 8818–8831, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.603. URL https://aclanthology.org/2022.acl-long.603. Hector Levesque, Ernest Davis, and Leora Morgenstern. The Winograd Schema Challenge. In Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning, 2012. URL https://cdn.aaai.org/ocs/4492/4492-21843-1-PB.pdf . Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. Inference-time intervention: Eliciting truthful answers from a language model. Advances in Neural Information Processing Systems, 36, 2024. URL https://proceedings.neurips.cc/paper_files/paper/2023/ hash/81b8390039b7302c909cb769f8b6cd93-Abstract-Conference.html. Margaret Li, Suchin Gururangan, Tim Dettmers, Mike Lewis, Tim Althoff, Noah A. Smith, and Luke Zettlemoyer. Branch-train-merge: Embarrassingly parallel training of expert language models. arXiv:2208.03306, 2022. URL https://arxiv.org/abs/2208.03306. Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582–4597, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.353. URL https://aclanthology.org/2021.acl-long.353. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. AlpacaEval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval, 2023. Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale gener- ation: Learning to solve and explain algebraic word problems. arXiv:1705.04146, 2017. URL https://arxiv.org/abs/1705.04146. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in Neural Information Processing Systems, 36, 2024a. URL https://arxiv.org/abs/2304.08485. Sheng Liu, Haotian Ye, Lei Xing, and James Zou. In-context vectors: Making in context learning more effective and controllable through latent space steering. arXiv:2311.06668, 2024b. URL https://arxiv.org/abs/2311.06668. Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. DoRA: Weight-decomposed low-rank adaptation. arXiv:2402.09353, 2024c. URL https://arxiv.org/abs/2402.09353. Weiyang Liu, Zeju Qiu, Yao Feng, Yuliang Xiu, Yuxuan Xue, Longhui Yu, Haiwen Feng, Zhen Liu, Juyeon Heo, Songyou Peng, Yandong Wen, Michael J. Black, Adrian Weller, and Bern- hard Schölkopf. Parameter-efficient orthogonal finetuning via butterfly factorization. In The 14Twelfth International Conference on Learning Representations, ICLR 2024 , 2024d. URL https://openreview.net/forum?id=7NzgkEdGyr. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. arXiv:1907.11692, 2019. URL https://arxiv.org/abs/1907.11692. James L. McClelland, David E. Rumelhart, and PDP Research Group.Parallel Distributed Processing: Explorations in the Microstructure of Cognition, volume 2: Psychological and Biological Models. MIT Press, 1986. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? A new dataset for open book question answering. arXiv:1809.02789, 2018. URL https://arxiv.org/abs/1809.02789. Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space word representations. In Lucy Vanderwende, Hal Daumé III, and Katrin Kirchhoff, editors, Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Lin- guistics: Human Language Technologies, pages 746–751, Atlanta, Georgia, June 2013. Association for Computational Linguistics. URL https://aclanthology.org/N13-1090. Neel Nanda, Andrew Lee, and Martin Wattenberg. Emergent linear representations in world models of self-supervised sequence models. In Yonatan Belinkov, Sophie Hao, Jaap Jumelet, Najoung Kim, Arya McCarthy, and Hosein Mohebbi, editors, Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pages 16–30, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.blackboxnlp-1.2. URL https: //aclanthology.org/2023.blackboxnlp-1.2. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems , 35: 27730–27744, 2022. URL https://arxiv.org/abs/2203.02155. Kiho Park, Yo Joong Choe, and Victor Veitch. The linear representation hypothesis and the geometry of large language models. arXiv:2311.03658, 2023. URL https://arxiv.org/abs/2311.03658. Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve sim- ple math word problems? In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2080–2094, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.168. URL https://aclanthology.org/2021.naacl-main.168. Jonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Sebastian Ruder. MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7654–7673, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.617. URL https://aclanthology. org/2020.emnlp-main.617. Shauli Ravfogel, Michael Twiton, Yoav Goldberg, and Ryan D. Cotterell. Linear adversarial concept erasure. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 18400–18421, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/ravfogel22a.html. Subhro Roy and Dan Roth. Solving general arithmetic word problems. In Lluís Màrquez, Chris Callison-Burch, and Jian Su, editors, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1743–1752, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1202. URL https://aclanthology.org/ D15-1202. 15David E. Rumelhart, James L. McClelland, and PDP Research Group.Parallel Distributed Processing: Explorations in the Microstructure of Cognition, volume 1: Foundations. MIT Press, 1986. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An adversarial Winograd Schema Challenge at scale. Communications of the ACM, 64(9):99–106, 2021. URL https://arxiv.org/abs/1907.10641. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQa: Common- sense reasoning about social interactions. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP- IJCNLP), pages 4463–4473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1454. URL https://aclanthology.org/D19-1454. Shuhua Shi, Shaohan Huang, Minghui Song, Zhoujun Li, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, and Qi Zhang. ResLoRA: Identity residual mapping in low-rank adaption. arXiv:2402.18039, 2024. URL https://arxiv.org/abs/2402.18039. Shashwat Singh, Shauli Ravfogel, Jonathan Herzig, Roee Aharoni, Ryan Cotterell, and Ponnu- rangam Kumaraguru. MiMiC: Minimally modified counterfactuals in the representation space. arXiv:2402.09631, 2024. URL https://arxiv.org/abs/2402.09631. Paul Smolensky. Neural and conceptual interpretation of PDP models. In Parallel Distributed Processing: Explorations in the Microstructure of Cognition , volume 2: Psychological and Biological Models, pages 390–431. MIT Press/Bradford Books, Cambridge, MA, 1986. Nishant Subramani, Nivedita Suresh, and Matthew E. Peters. Extracting latent steering vectors from pretrained language models. arXiv:2205.05124, 2022. URL https://arxiv.org/abs/2205.05124. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford Alpaca: An instruction-following LLaMA model. https://github.com/tatsu-lab/stanford_alpaca, 2023. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and efficient foundation language models. arXiv:2302.13971, 2023a. URL https://arxiv.org/abs/2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cris- tian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023b. URL https://arxiv.org/abs/2307.09288. Alex Turner, Lisa Thiergart, David Udell, Gavin Leech, Ulisse Mini, and Monte MacDiarmid. Activation addition: Steering language models without optimization. arXiv:2308.10248, 2023. URL https://arxiv.org/abs/2308.10248. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors,Advances in Neural Information Processing Systems, volume 30, pages 5998–6008. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf . Theia V ogel. repeng, 2024. URLhttps://github.com/vgel/repeng/. 16Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Tal Linzen, Grzegorz Chrupała, and Afra Alishahi, editors, Proceedings of the 2018 EMNLP Workshop Black- boxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353–355, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://aclanthology.org/W18-5446. Kevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretability in the wild: a circuit for indirect object identification in GPT-2 small. In The Eleventh International Conference on Learning Representations, ICLR 2023 , Kigali, Rwanda, 2023. URL https://openreview.net/pdf?id=NpsVSN6o4ul. Yaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan Awadallah, and Jianfeng Gao. AdaMix: Mixture-of-adaptations for parameter-efficient model tuning. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors,Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 5744–5760, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.388. URL https://aclanthology.org/2022.emnlp-main.388. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824–24837, 2022. URL https://arxiv.org/abs/ 2201.11903. Muling Wu, Wenhao Liu, Xiaohua Wang, Tianlong Li, Changze Lv, Zixuan Ling, Jianhao Zhu, Cenyuan Zhang, Xiaoqing Zheng, and Xuanjing Huang. Advancing parameter efficiency in fine- tuning via representation editing. arXiv:2402.15179, 2024a. URL https://arxiv.org/abs/2402. 15179. Zhengxuan Wu, Atticus Geiger, Joshua Rozner, Elisa Kreiss, Hanson Lu, Thomas Icard, Christo- pher Potts, and Noah Goodman. Causal distillation for language models. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz, editors, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguis- tics: Human Language Technologies , pages 4288–4295, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.318. URL https://aclanthology.org/2022.naacl-main.318. Zhengxuan Wu, Atticus Geiger, Christopher Potts, and Noah D. Goodman. Interpretability at scale: Identifying causal mechanisms in Alpaca. In Advances in Neural Information Processing Systems, volume 36, 2023. URL https://papers.neurips.cc/paper_files/paper/2023/file/ f6a8b109d4d4fd64c75e94aaf85d9697-Paper-Conference.pdf. Zhengxuan Wu, Atticus Geiger, Aryaman Arora, Jing Huang, Zheng Wang, Noah D. Goodman, Christopher D. Manning, and Christopher Potts. pyvene: A library for understanding and improving PyTorch models via interventions. In arXiv:2403.07809, 2024b. URL https://arxiv.org/abs/ 2403.07809. Zhengxuan Wu, Atticus Geiger, Jing Huang, Aryaman Arora, Thomas Icard, Christopher Potts, and Noah D. Goodman. A reply to Makelov et al. (2023)’s “interpretability illusion” arguments. arXiv:2401.12631, 2024c. URL https://arxiv.org/abs/2401.12631. Takateru Yamakoshi, James McClelland, Adele Goldberg, and Robert Hawkins. Causal in- terventions expose implicit situation models for commonsense language understanding. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023 , pages 13265–13293, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.839. URL https://aclanthology.org/2023.findings-acl.839. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine really finish your sentence? arXiv:1905.07830, 2019. URL https://arxiv.org/abs/1905.07830. 17Feiyu Zhang, Liangzhi Li, Junhao Chen, Zhouqiang Jiang, Bowen Wang, and Yiming Qian. IncreLoRA: Incremental parameter allocation method for parameter-efficient fine-tuning. arXiv:2308.12043, 2023. URL https://arxiv.org/abs/2308.12043. Jinghan Zhang, Shiqi Chen, Junteng Liu, and Junxian He. Composing parameter-efficient modules with arithmetic operation. Advances in Neural Information Processing Systems, 36, 2024a. URL https://arxiv.org/abs/2306.14870. Renrui Zhang, Jiaming Han, Chris Liu, Aojun Zhou, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. LLaMA-Adapter: Efficient fine-tuning of large language models with zero-initialized attention. In The Twelfth International Conference on Learning Representations, Vienna, Austria, 2024b. URL https://openreview.net/forum?id=d4UiXAHN2W. Ruiyi Zhang, Rushi Qiang, Sai Ashish Somayajula, and Pengtao Xie. AutoLoRA: Automatically tuning matrix ranks in low-rank adaptation based on meta learning. arXiv:2403.09113, 2024c. URL https://arxiv.org/abs/2403.09113. Ming Zhong, Yelong Shen, Shuohang Wang, Yadong Lu, Yizhu Jiao, Siru Ouyang, Donghan Yu, Jiawei Han, and Weizhu Chen. Multi-LoRA composition for image generation. arXiv:2402.16843, 2024. URL https://arxiv.org/abs/2402.16843. Yun Zhu, Nevan Wichers, Chu-Cheng Lin, Xinyi Wang, Tianlong Chen, Lei Shu, Han Lu, Ca- noee Liu, Liangchen Luo, Jindong Chen, et al. SiRa: Sparse mixture of low rank adaptation. arXiv:2311.09179, 2023. URL https://arxiv.org/abs/2311.09179. Zhou Ziheng, Yingnian Wu, Song-Chun Zhu, and Demetri Terzopoulos. Aligner: One global token is worth millions of parameters when aligning large language models. arXiv:2312.05503, 2023. URL https://arxiv.org/abs/2312.05503. Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael J. Byun, Zifan Wang, Alex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, J. Zico Kolter, and Dan Hendrycks. Representation engineering: A top-down approach to AI transparency. arXiv:2310.01405, 2023. URL https://arxiv.org/abs/2310.01405. 18Appendix Table of Contents A pyreft: A ReFT-native Python Library 20 B Describing existing methods under the ReFT framework 20 B.1 RED . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 B.2 Activation addition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 B.3 RepE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 C Datasets 21 C.1 Commonsense reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 C.2 Arithmetic reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 C.3 Natural language understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 D Hyperparameters 23 D.1 Hyperparameter tuning and decoding strategy . . . . . . . . . . . . . . . . . . . . 23 D.2 Suggestions on choosing hyperparameters for ReFT . . . . . . . . . . . . . . . . . 29 D.3 Additional hyperparameter-tuning results of LoReFT . . . . . . . . . . . . . . . . 29 E Ablating the parametrisation of LoReFT 32 F Memorisation experiments 33 F.1 A single vector is worth a thousand tokens . . . . . . . . . . . . . . . . . . . . . . 33 F.2 A single vector can memorise a codebook with 256 entries . . . . . . . . . . . . . 35 G Capabilities experiments 36 G.1 Multi-task learning: Learned ReFTs are like puzzle pieces . . . . . . . . . . . . . 36 G.2 Few-shot adaptation: Adapting Llama-2-Chat to GOODY-2 with 5 examples . . 38 H Inference overhead analysis of ReFT with our ReFT library 39 I Generation examples 41 J Licenses for existing assets 49 J.1 Commonsense reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 J.2 Arithmetic reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 J.3 Instruct-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 J.4 Natural language understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 J.5 Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 19A pyreft: A ReFT-native Python Library To lower the cost of switching from PEFTs to ReFT, we release a Python library made for training and sharing ReFTs. Our library is built on top of pyvene [Wu et al., 2024b], a library for performing and training activation interventions on arbitrary PyTorch models. Any pretrained LM available on HuggingFace is supported through our library for finetuning with ReFT methods, and finetuned models can be easily uploaded to HuggingFace. The following example shows steps to wrap a Llama-2 7B model with a single intervention on the residual stream output of the 19-th layer: import torch import transformers from pyreft import get_reft_model , ReftConfig , LoreftIntervention , ReftTrainerForCausalLM # loading huggingface model model_name_or_path = \" yahma / llama -7b-hf\" model = transformers . AutoModelForCausalLM . from_pretrained ( model_name_or_path , torch_dtype = torch . bfloat16 , device_map =\" cuda \") # wrap the model with rank -1 loreft reft_config = ReftConfig ( representations ={ \" layer \": 19 , \" component \": \" block_output \", \" intervention \": LoreftIntervention ( embed_dim = model . config . hidden_size , low_rank_dimension =1) }) reft_model = get_reft_model ( model , reft_config ) reft_model . print_trainable_parameters () The wrapped model can be trained for downstream tasks. We also provide data loading helpers to construct training data that is compatible with HuggingFace trainers: tokenizer = transformers . AutoTokenizer . from_pretrained ( model_name_or_path ) # get training data with customised dataloaders data_module = make_supervised_data_module ( tokenizer = tokenizer , model = model , layers =[19] , training_args = training_args , data_args = data_args ) # train trainer = reft . ReftTrainerForCausalLM ( model = reft_model , tokenizer = tokenizer , args = training_args , ** data_module ) trainer . train () trainer . save_model ( output_dir = training_args . output_dir ) B Describing existing methods under the ReFT framework To show the expressivity of the ReFT framework, we cast existing representing-editing methods in the literature into ReFTs. General comments about expressivity of ReFT. Given that previous works have unified PEFTs under a single framework [He et al., 2022a], one may ask why not express ReFT as a PEFT method? The main reason is that PEFT frameworks lack the notion of time or sequence (see the unified PEFT view provided in Table 1 on pg. 5 of He et al., 2022a). In PEFTs, representation modifications are necessarily applied to every token in the sequence, even in recent variants such as AdaLoRA [Zhang et al., 2023]. A key aspect of ReFT is that it leverages representations over time and intervenes only on a small number of them while being effective. More importantly, the notation of time is important for future versions of ReFT that intervene on representations schematically (e.g. intervene on the first token at some early layers and then intervene on the last token at some later layers). The ability to intervene at different layer and position combinations schematically is also supported in our code. Existing PEFT libraries 7 enforce weight-based updates without supporting flexible representation-based interventions. B.1 RED RED [Wu et al., 2024a] is a simple representation-editing method that applies an element-wise scaling transform s ∈ Rn and adds a bias b ∈ Rn to the hidden representation in every layer. The same intervention is applied to every position (including at generated tokens, increasing inference burden) 7See https://github.com/huggingface/peft. 20but separate interventions are learned at each layer. In the ReFT framework, RED is defined as ΦRED(h) =s ×h +b (8) IRED ={⟨ΦRED, {1, . . . , n}, l⟩ ∣l ∈{1, . . . , m}} (9) The parameters ϕRED ={s, b} are learned with gradient descent to minimise a loss function such as language-modelling loss or a classification loss, as in our experiments with LoReFT. We believe that RED is better classified as a kind of adapter due to its application at all positions. B.2 Activation addition Activation addition [Turner et al., 2023] takes the difference in activations at at some positionsp and q and layer l given two contrastive prompts x+ and x− as input. It then adds this difference vector, scaled by a tuned constant c, to representations at all positions in layer l for some new prompt. a =h(x+)(l) p −h(x−)(l) q (10) ΦActAdd(h) =h +c ⋅a (11) IActAdd ={⟨ϕActAdd, {1, . . . , n}, l⟩} (12) B.3 RepE Zou et al. [2023] introduce several intervention methods for controlling model behaviour, which they term representation engineering. First, given a set of prompts {x1, . . . ,xn} designed to elicit the presence of a concept, we randomly pair them, take the difference in activations for each pair, and find the first principle component of the difference vectors at the last token position in some layer of interest l to obtain a reading vector: areading =PCA({h(xi)(l) −1 −h(xi+1)(l) −1 ∣ i ≡0 mod 2}) 1 (13) One can also used a more structured pairing of constrastive prompts to obtain a contrast vector, similar to the difference vector computed in activation addition: acontrast =PCA({h(x+ i )(l) −1 −h(x− i )(l) −1 ∣ 1 ≤i ≤n}) 1 (14) Then, using either areading or acontrast, RepE introduces three operators (i.e. parametrisations of Φ) for intervening on activations: ΦRepE,linear(h) =h ±c ⋅a (15) ΦRepE,piecewise(h) =h +c ⋅sign(a ⋅h)⋅a (16) ΦRepE,projection(h) =h −c ⋅ a ⋅h ∥a∥2 ⋅a (17) The first two of these are similar to activation addition, while the latter is a scaled one-dimensional distributed interchange intervention that is a special case of LoReFT. These operations are then used to intervene on some set of positions P ⊆{1, . . . , n} in the layer of interest: IRepE ={⟨ΦRepE, P, l⟩} (18) RepE introduces another model control method called Low-Rank Representation Adaptation (LoRRA), which is a kind of PEFT rather than a ReFT since it tunes model weights using a variant of LoRA. C Datasets C.1 Commonsense reasoning We train and evaluate our models on eight datasets covering different domains of open-ended QA tasks: 1. The BoolQ [Clark et al., 2019] dataset, which is a question-answering dataset for yes or no naturally occurring questions. We remove the provided passage in the dataset following previous works to ensure a fair comparison. 212. The PIQA [Bisk et al., 2020] dataset, which tests physical commonsense reasoning and requires the model to choose one of the provided actions to take based on a hypothesised scenario. 3. The SIQA [Sap et al., 2019] dataset, which focus on reasoning about people’s actions and their corresponding social consequences. 4. The HellaSwag [Zellers et al., 2019] dataset, which asks the model to choose an appropriate ending (or sentence completion) given a context. 5. The WinoGrande [Sakaguchi et al., 2021] dataset, inspired by Winograd Schema Chal- lenge [Levesque et al., 2012], asks the model to fill-in-a-blank with binary options given a sentence which requires commonsense reasoning. 6. The ARC Easy set (ARC-e [Clark et al., 2018]), which includes genuine grade-school level multiple-choice science questions 7. The ARC Challenge set (ARC-c) [Clark et al., 2018]), which is like ARC-e but designed in a way that co-occurrence methods are expected to fail to answer correctly. 8. The OBQA [Mihaylov et al., 2018] dataset, which is a knowledge-intensive and open-book QA dataset that requires multi-hop reasoning. Dataset statistics and simplified training examples from each dataset are provided in Hu et al. [2023]. Dataset statistics and simplified training examples from each dataset are provided in Hu et al. [2023]. We replicate the experimental setup in Hu et al. [2023] and finetune our models on a combined training dataset (COMMONSENSE 170K ) of the tasks mentioned above, and evaluate on their individual test set. C.2 Arithmetic reasoning We train and evaluate with seven datasets covering different domains of math world problems: 1. The AddSub [Hosseini et al., 2014] dataset, which involves solving arithmetic word prob- lems that include addition and subtraction. 2. The AQuA [Ling et al., 2017] dataset, which formulates algebraic word problems as multiple-choice problems. 3. The GSM8K [Cobbe et al., 2021] dataset, which consists of grade-school math word problems that require multi-step reasoning. 4. The MA WPS[Koncel-Kedziorski et al., 2016] dataset, which contains math word problem with varying complexity. 5. The MultiArith [Roy and Roth, 2015] dataset, which contains multi-step arithmetic prob- lems. 6. The SingleEq [Koncel-Kedziorski et al., 2015] dataset, which has grade-school math word problems that map to single equations with different length. 7. The SV AMP[Patel et al., 2021] dataset, which enhances the original Math World Prob- lem (MWP) challenge by requiring robust reasoning ability that is invariant to structural alternations of the posing problem. Dataset statistics and simplified training examples from each dataset are provided in Hu et al. [2023]. We replicate the experimental setup in Hu et al. [2023] and finetune our models on a combined training dataset (MATH10K ) of four tasks mentioned above: GSM8K, MAWPS, MAWPS-single and AQuA. Different from Hu et al. [2023], selected tasks are excluded for testing since the original paper accidentally leaks testing examples from these tasks into the training set, affecting AddSub, MultiArith and SingleEq. They are included in the MAWPS training dataset, and thus leaked into the training dataset. C.3 Natural language understanding We follow Wu et al. [2024a] for proper evaluation on the GLUE validation set. We split the validation set into two subsets, using one subset guarded by a random seed for in-training evaluation and the other for testing. Specifically, after each training epoch, we evaluate the model on our in-training 22evaluation set and select the best model across all epochs for testing. For datasets with a large validation set (i.e., QQP, MNLI, and QNLI), we select 1,000 samples for in-training evaluation. For the remaining smaller datasets, we select half of the samples for this purpose. For the evaluation metric, we use the Matthews correlation coefficient for CoLA, the Pearson correlation coefficient for STS-B, and accuracy for the other datasets. For MNLI, we report results only on the matched version. D Hyperparameters D.1 Hyperparameter tuning and decoding strategy Commonsense reasoning and arithmeric reasoning. We create a standalone development set by taking the last 300 examples from the GSM8K training set. We train our models with the remaining training set of GSM8K and select the hyperparameter settings based on model performance on the development set. We select the hyperparameters using LLaMA-7B, and apply the same settings to LLaMA-13B without additional tuning. We use a maximum sequence length of 512 for training and hyperparameter tuning, and a maximum new token number of 32 for inference. Table 5 and table 6 describes our hyperparameter search space. We use a lower number of epochs (6 instead of 12) for the commonsense reasoning benchmark because the COMMONSENSE 170K training set is more than 20 times larger than GSM8K. During inference, we use greedy decoding without sampling for the commonsense reasoning bench- mark, since it is a multi-token classification benchmark, and use the same decoding strategy as in Hu et al. [2023] for the arithmetic reasoning benchmark with a higher temperature 0.3. The reason to switch to a slightly different set of decoding hyperparameters is that the HuggingFace decoding function may throw an error due to statistical instability with close-to-zero probabilities over output tokens with beam search.8 Instruction following. We finetune LLaMA-7B on Alpaca-52K [Taori et al., 2023] to select hy- perparameters. We select the hyperparameter settings based on model performance evaluated with Alpaca-Eval v1.0 [Li et al., 2023], which calculates the win-rate over text-davinci-003 by using gpt-4-turbo as the annotator. We use a maximum sequence length of 768 for training and hyper- parameter tuning, and a maximum new token number of 2048 for inference. Table 7 describes our hyperparameter search space. During inference, we use the same decoding strategy as in RED [Wu et al., 2024a] to ensure a fair comparison. Specifically, we use greedy decoding without sampling, and use a maximum repetition n-gram size of 5 with a repetition penalty of 1.1. Natural language understanding. We conduct hyperparameter tuning with RoBERTa-base and RoBERTa-large for each task individually. We pick the hyperparameters based on testing performance on the held-out validation set with a fixed random seed of 42. We then evaluate our model with additional four unseen seeds {43, 44, 45, 46} for final results. We follow Wu et al. [2024a]’s setting for evaluation. For QQP with RoBERTa-large, there are some stochasticity in runs with the same seed, so we picked the best run out of 3 runs for any particular seed. As reported by Wu et al. [2024a], we also observe that evaluation results on RTE are unstable due to the small size of the dataset. We thus replace several random seeds as in Wu et al. [2024a] to ensure a fair comparison. In addition, we replace one or two random seeds for CoLA for stability. Table 8 describes our hyperparameter search space. Table 9 to table 12 describe our hyperparameter settings for each task. We conduct separate hyperparameter tuning for LoReFT and DiReFT to ensure a fair comparison. 8See reference ticket: https://github.com/huggingface/transformers/issues/11267. 23Table 5: Hyperparameter search space of LLaMA-1 7B models with LoReFT on the GSM8K development set with the best settings underlined. We use greedy decoding without sampling during hyperparameter tuning. Hyperparameters LLaMA-7B w/ GSM8K for LoReFT prefix+suffix position p + s {p1+s1, p3+s3, p5+s5, p7+s7, p9+s9, p11+s11} Tied weight p, s {True, False} Rank r {8, 16, 32, 64} Layer L (sep. w/ ‘;’) {0;2;4;6;10;12;14;18, 10;12;14;18;20;22;24;28, 4;6;10;12;14;18;20;22, all } Dropout {0.00, 0.05 } Optimizer AdamW LR {9 ×10−5, 1×10−4, 3×10−4, 6×10−4, 9×10−4, 1×10−3, 3×10−3} Weight decay {0 , 1×10−3, 2×10−3} LR scheduler Linear Batch size {4, 8, 16, 32 , 64} Warmup ratio {0.00, 0.06, 0.10 } Epochs {3, 6, 9, 12 , 18} Table 6: Hyperparameter search space of LLaMA-1 7B models with DiReFT on the GSM8K development set with the best settings underlined. We use greedy decoding without sampling during hyperparameter tuning. Hyperparameters LLaMA-7B w/ GSM8K for DiReFT prefix+suffix position p + s {p1+s1, p3+s3, p5+s5, p7+s7, p9+s9, p11+s11} Tied weight p, s {True, False} Rank r {8, 16, 32, 64} Layer L (sep. w/ ‘;’) {0;2;4;6;10;12;14;18, 10;12;14;18;20;22;24;28, 4;6;10;12;14;18;20;22 , all} Dropout {0.00, 0.05 } Optimizer AdamW LR {9 ×10−5, 1×10−4, 3×10−4, 6×10−4, 9×10−4, 1×10−3, 3×10−3} Weight decay {0, 1 ×10−3, 2×10−3, 6×10−3, 1×10−2, 2×10−2, 6×10−2} LR scheduler Linear Batch size {4, 8 , 16, 32, 64} Warmup ratio {0.00, 0.06 , 0.10} Epochs {3, 6 , 9, 12, 18} 24Table 7: Hyperparameter search space of LLaMA-1 7B models on Alpaca-52K evaluated by Alpaca- Eval v1.0 with the best settings underlined. We use greedy decoding without sampling during hyperparameter tuning. LoReFT and DiReFT have the same hyperparameter settings. Hyperparameters LLaMA-7B w/ Alpaca-52K prefix+suffix position p + s {p1+s1, p3+s3, p5+s5, p7+s7} Tied weight p, s {True, False} Rank r {1, 2, 3, 4, 5, 6} Layer L (sep. w/ ‘;’) {9;18, 3;9;18, 3;9;18;24 } Dropout {0.00, 0.05 } Optimizer AdamW LR 9 ×10−4 Weight decay 0 ×10−3 LR scheduler Linear Batch size {16, 32, 64, 128 } Warmup ratio 0.00 Epochs {1, 3, 6, 9, 12 } Table 8: Hyperparameter search space of RoBERTa-base and RoBERTa-large models on GLUE evaluated with classification accuracy. Best hyperparameter settings are task-specific, which are specified in separate tables. Hyperparameters RoBERTa-base and RoBERTa-large w/ GLUE prefix+suffix position p + s {p1, p3, p5, p7, p9, p11} Tied weight p, s False Rank r {1, 2} Layer L (sep. w/ ‘;’) {1;3;5;7;9;11, all} Dropout {0.00, 0.05, 0.10, 0.15, 0.20} Optimizer AdamW LR {1 ×10−4, 2×10−4, 3×10−4, 4×10−4, 5×10−4}, {6×10−4, 9×10−4, 1×10−3, 3×10−3} Weight decay {0, 1 ×10−4, 6×10−4, 1×10−3, 6×10−3, 1×10−2, 2×10−2, 4×10−2} LR scheduler Linear Batch size {16, 32, 64, 128} Warmup ratio {0, 5 ×10−3, 6×10−3, 3×10−2, 5×10−2, 6×10−2, 1×10−1, 2×10−1} Epochs {20, 30, 40, 50, 60} 25Table 9: Hyperparameter settings of RoBERTa-base models on GLUE for LoReFT. Hyperparameters MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B position p p 1 p3 p3 p3 p11 p11 p3 p3 Tied weight False Rank r 1 Layer L all Dropout 0.05 0.10 0.05 0.20 0.05 0.05 0.05 0.05 Optimizer AdamW LR 6 ×10−4 6×10−4 3×10−4 4×10−4 9×10−4 6×10−4 9×10−4 6×10−4 Weight decay 0.00 LR scheduler Linear Batch size 32 Warmup ratio 6 ×10−2 1×10−1 0 5 ×10−3 1×10−1 0 0 3 ×10−2 Epochs 40 40 40 60 20 40 60 60 Table 10: Hyperparameter settings of RoBERTa-large models on GLUE for LoReFT. Hyperparameters MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B position p p 1 p3 p3 p3 p11 p11 p3 p3 Tied weight False Rank r 1 Layer L all Dropout 0.05 0.05 0.20 0.20 0.05 0.05 0.05 0.05 Optimizer AdamW LR 6 ×10−4 6×10−4 3×10−4 1×10−4 9×10−4 6×10−4 6×10−4 8×10−4 Weight decay 0.00 LR scheduler Linear Batch size 32 Warmup ratio 0.00 0.10 0.06 0.20 0.10 0.06 0.00 0.20 Epochs 20 20 30 30 20 20 30 30 26Table 11: Hyperparameter settings of RoBERTa-base models on GLUE for DiReFT. Hyperparameters MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B position p p 1 p3 p5 p1 p11 p11 p1 p3 Tied weight False Rank r 1 Layer L all Dropout 0.05 0.10 0.05 0.00 0.05 0.05 0.00 0.05 Optimizer AdamW LR 6 ×10−4 6×10−4 3×10−4 6×10−4 9×10−4 6×10−4 9×10−4 6×10−4 Weight decay 0.00 0.00 0.00 0.04 0.00 0.00 0.04 0.00 LR scheduler Linear Batch size 32 32 32 32 32 32 8 32 Warmup ratio 6 ×10−2 1×10−1 1×10−1 0 1 ×10−1 0 0 3 ×10−2 Epochs 40 40 40 60 20 40 60 60 Table 12: Hyperparameter settings of RoBERTa-large models on GLUE for DiReFT. Hyperparameters MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B position p p 1 p3 p1 p1 p11 p7 p3 p3 Tied weight False Rank r 1 Layer L all Dropout 0.05 0.05 0.10 0.15 0.05 0.05 0.05 0.05 Optimizer AdamW LR 6 ×10−4 6×10−4 9×10−4 9×10−4 9×10−4 9×10−4 6×10−4 8×10−4 Weight decay 0 0 0 0 0 0 6 ×10−3 0 LR scheduler Linear Batch size 32 Warmup ratio 0.00 0.10 0.00 0.00 0.10 0.10 0.00 0.10 Epochs 20 20 50 60 20 20 30 30 27Table 13: Accuracy comparison of RoBERTa-base and RoBERTa-large against existing PEFT methods on the GLUE benchmark with standard deviation (SD) . ∗Performance results of all baseline methods are taken from Wu et al. [2024a]. We report averaged performance of five runs with distinct random seeds for our method. Param. (%) is calculated by dividing the number of trainable parameters (excluding the number of parameters of the classification head) with the number of parameter of the base LM. Model PEFT Params (%) Accuracy(↑) (SD) MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B Avg. base FT 100% 87.3 (0.34) 94.4(0.96) 87.9(0.91) 62.4(3.29) 92.5(0.22) 91.7(0.19) 78.3(3.20) 90.6(0.59) 85.6 Adapter∗ 0.318% 87.0(0.28) 93.3(0.40) 88.4(1.54) 60.9(3.09) 92.5(0.02) 90.5(0.08) 76.5(2.26) 90.5(0.35) 85.0LoRA∗ 0.239% 86.6(0.23) 93.9(0.49) 88.7(0.76) 59.7(4.36) 92.6(0.10) 90.4(0.08) 75.3(2.79) 90.3(0.54) 84.7AdapterFNN∗ 0.239%87.1(0.10) 93.0(0.05) 88.8(1.38) 58.5(1.69) 92.0(0.28) 90.2(0.07) 77.7(1.93) 90.4(0.31) 84.7BitFit∗ 0.080% 84.7(0.08) 94.0(0.87) 88.1(1.57) 54.0(3.07) 91.0(0.05) 87.3(0.02) 69.8(1.51) 89.5(0.35) 82.3RED∗ 0.016% 83.9(0.14) 93.9(0.31) 89.2(0.98) 61.0(2.96) 90.7(0.35) 87.2(0.17) 78.0(2.06) 90.4(0.32) 84.3 DiReFT (ours)0.015% 82.5(0.22) 92.6(0.76) 88.3(1.23) 58.6(1.99) 91.3(0.19) 86.4(0.27) 76.4(1.48) 89.3(0.56) 83.2LoReFT (ours)0.015% 83.1(0.26) 93.4(0.64) 89.2(2.62) 60.4(2.60) 91.2(0.25) 87.4(0.23) 79.0(2.76) 90.0(0.29) 84.2 large FT 100% 88.8 (0.45) 96.0(0.66) 91.7(1.73) 68.2(2.62) 93.8(0.33) 91.5(1.28) 85.8(1.40) 92.6(0.16) 88.6 Adapter∗ 0.254% 90.1(0.12) 95.2(0.48) 90.5(0.59) 65.4(2.24) 94.6(0.17) 91.4(0.13) 85.3(1.34) 91.5(0.33) 88.0LoRA∗ 0.225% 90.2(0.25) 96.0(0.85) 89.8(2.09) 65.5(2.02) 94.7(0.21) 90.7(0.91) 86.3(2.41) 91.7(0.44) 88.1AdapterFNN∗ 0.225%90.3(0.15) 96.1(0.75) 90.5(1.26) 64.4(1.56) 94.3(0.39) 91.3(0.24) 84.8(2.01) 90.2(0.24) 87.7RED∗ 0.014% 89.5(0.38) 96.0(0.48) 90.3(1.40) 68.1(1.69) 93.5(0.33) 88.8(0.11) 86.2(1.40) 91.3(0.21) 88.0 DiReFT (ours)0.014% 88.7(0.13) 95.4(0.60) 88.5(2.16) 66.7(2.21) 93.9(0.39) 88.1(0.47) 86.9(1.56) 91.2(0.29) 87.4LoReFT (ours)0.014% 89.2(0.27) 96.2(0.72) 90.1(1.17) 68.0(1.44) 94.1(0.35) 88.5(0.45) 87.5(1.49) 91.6(0.43) 88.2 28D.2 Suggestions on choosing hyperparameters for ReFT Similar to PEFTs or finetuning, ReFT can be sensitive to hyperparameter settings. Here, we recom- mand a non-exhaustive list for choosing the best hyperparameter settings for your tasks: • Intervening on multiple positions delivers significant gains. We find that intervening only on a single token position (e.g., just the first one or the last one) is always less optimal than intervening on multiple tokens. However, intervening on excessive number of tokens might harm performance by slowing down convergence. • Intervening on all layers first, and then shrink down . Intervening on all layers often provides a good baseline. We recommand users to start with all layers, and shrink down the number of intervening layers depending on the desired performance–parameter count balance. • Higher rank may not entail better performance . High rank entails higher parameter count, but it does not always bring performance gain (likely due to slower convergence). We recommend users to start with a rank that is lower than 32 (e.g. rank 4). • Tie intervention weights as much as you can. In the paper, we explore tying the interven- tion weights between prefix and suffix token positions. It automatically halves the parameter count, and it can result in better performance as well. We suspect weight sharing across layers may also help. • Hyperparameter tuning with learning rate, warmup ratio, dropout rate and weight decay should go after other hyperparameters . These classic neural-network training hyperparameters can play a role, yet they have much smaller effect than previous ones. D.3 Additional hyperparameter-tuning results of LoReFT As a result of our hyperparameter searching process, LoReFT is trained with more epochs compared to LoRA [Hu et al., 2022] or DoRA [Liu et al., 2024c]. This raises the concern whether our performance gain is purely due to the larger number of epochs. We thus rerun our experiments with the exact same number of epochs and effective batch size as LoRA or DoRA. Results are shown in table 14 and table 15. With matched hyperparameters, LoReFT shows similar results by outperforming previous methods significantly on eight commonsense reasoning datasets. Recently, VeRA was proposed as a new variant of LoRA that further reduces the number of trainable parameters while maintaining performance [Kopiczko et al., 2024]. Table 16 shows our results compared against VeRA as well as the baseline numbers reported in VeRA’s paper. We include this set of results in the appendix, given that the hyperparameter tuning process is drastically different from ours.9 The original VeRA implementation records the performance of the best epoch on the validation set, which could cause overfitting since results are selected based on test set performance. 9VeRA’s original implementation can be found athttps://openreview.net/notes/edits/attachment? id=D0dcbrnPq0&name=supplementary_material. 29Table 14: Accuracy comparison of LLaMA-7B and LLaMA-13B against existing PEFT methods on eight commonsense reasoning datasets. ∗Performance results of all baseline methods are taken from Liu et al. [2024c]. We report averaged performance of three runs with distinct random seeds for our method. For LoReFT, Param. (%) is calculated by dividing the number of trainable parameters by the number of parameters of the base LM. We include LoReFTe=3, which is trained with 3 epochs — the same number of epochs as DoRA, but with a reduced batch size of 16 to ensure an equivalent number of gradient sets. Model PEFT Params (%) Accuracy(↑) BoolQ PIQA SIQA HellaS. WinoG. ARC-e ARC-c OBQA Avg. ChatGPT∗ — — 73.1 85.4 68.5 78.5 66.1 89.8 79.9 74.8 77.0 LLaMA-7B PrefT∗ 0.039% 64.3 76.8 73.9 42.1 72.1 72.9 54.0 60.6 64.6 AdapterS∗ 1.953% 63.0 79.2 76.3 67.9 75.7 74.5 57.1 72.4 70.8 AdapterP∗ 3.542% 67.9 76.4 78.8 69.8 78.9 73.7 57.3 75.2 72.3LoRA∗ 0.826% 68.9 80.7 77.4 78.1 78.8 77.8 61.3 74.8 74.7DoRA (half)∗ 0.427% 70.0 82.6 79.7 83.2 80.6 80.6 65.4 77.6 77.5DoRA∗ 0.838% 68.5 82.9 79.6 84.8 80.8 81.4 65.8 81.0 78.1 LoReFTe=3 0.031% 68.3 83.5 79.7 92.7 82.6 83.2 67.4 78.5 79.5LoReFT (ours) 0.031% 69.3 84.4 80.3 93.1 84.2 83.2 68.2 78.9 80.2 LLaMA-13B PrefT∗ 0.031% 65.3 75.4 72.1 55.2 68.6 79.5 62.9 68.0 68.4 AdapterS∗ 1.586% 71.8 83.0 79.2 88.1 82.4 82.5 67.3 81.8 79.5 AdapterP∗ 2.894% 72.5 84.9 79.8 92.1 84.7 84.2 71.2 82.4 81.5LoRA∗ 0.670% 72.1 83.5 80.5 90.5 83.7 82.8 68.3 82.4 80.5DoRA (half)∗ 0.347% 72.5 85.3 79.9 90.1 82.9 82.7 69.7 83.6 80.8DoRA∗ 0.681% 72.4 84.9 81.5 92.4 84.2 84.2 69.6 82.8 81.5 LoReFTe=3 0.025% 72.0 85.6 82.1 94.8 85.3 86.9 73.0 85.0 83.1LoReFT (ours) 0.025% 72.1 86.3 81.8 95.1 87.2 86.2 73.7 84.2 83.3 Table 15: Accuracy comparison of LLaMA-7B and LLaMA-13B against existing PEFT methods on four arithmetic reasoning datasets. ∗Performance results of all baseline methods are taken from Hu et al. [2023]. We report averaged performance of three runs with distinct random seeds for our method. We include LoReFTe=3, which is trained with 3 epochs — the same number of epoch as DoRA, but with a reduced batch size of 16 to ensure an equivalent number of gradient sets. Model PEFT Params (%) Accuracy(↑) AQuA GSM8K MA WPS SV AMP Avg. LLaMA-7B PrefT∗ 0.039% 14.2 24.4 63.4 38.1 35.0 AdapterS∗ 1.953% 15.0 33.3 77.7 52.3 44.6 AdapterP∗ 3.542% 18.1 35.3 82.4 49.6 46.4 LoRA∗ 0.826% 18.9 37.5 79.0 52.1 46.9 LoReFTe=3 0.031% 22.4 21.6 69.5 43.6 39.3 LoReFT (ours) 0.031% 21.4 26.0 76.2 46.8 42.6 LLaMA-13B PrefT∗ 0.031% 15.7 31.1 66.8 41.4 38.8 AdapterS∗ 1.586% 22.0 44.0 78.6 50.8 48.9 AdapterP∗ 2.894% 20.5 43.3 81.1 55.7 50.2 LoRA∗ 0.670% 18.5 47.5 83.6 54.6 51.1 LoReFTe=3 0.025% 23.4 35.5 81.8 54.6 48.8 LoReFT (ours) 0.025% 23.6 38.1 82.4 54.2 49.6 30Table 16: Accuracy comparison of RoBERTa-base and RoBERTa-large against existing PEFT methods on the GLUE benchmark. ∗Performance results of all baseline methods are taken from Kopiczko et al. [2024]. To ensure a fair comparison, we report median performance of five runs with distinct random seeds for our method. Model PEFT Params (%) Accuracy(↑) SST-2 MRPC CoLA QNLI RTE STS-B Avg. base FT 100% 94.8 90.2 63.6 92.8 78.7 91.2 85.2 BitFit 0.080% 93.7 92.7 62.0 91.8 81.5 90.8 85.4 AdptD 0.239% 94.2 88.5 60.8 93.1 71.5 89.7 83.0 AdptD 0.717% 94.7 88.4 62.6 93.0 75.9 90.3 84.2 LoRA 0.239% 95.1 89.7 63.4 93.3 86.6 91.5 86.6 VeRA 0.034% 94.6 89.5 65.6 91.8 78.8 90.7 85.2 DiReFT (ours) 0.015% 92.2 88.7 59.5 91.3 77.0 89.6 83.0 LoReFT (ours) 0.015% 93.6 87.8 59.1 91.3 79.9 90.0 83.6 base AdptP 0.845% 96.1 90.2 68.3 94.8 83.8 92.1 87.6 AdptP 0.225% 96.6 89.7 67.8 94.8 80.1 91.9 86.8 AdptH 1.690% 96.2 88.7 66.5 94.7 83.4 91.0 86.8 AdptH 0.225% 96.3 87.7 66.3 94.7 72.9 91.5 84.9 LoRA-FA 1.042% 96.0 90.0 68.0 94.4 86.1 92.0 87.8 LoRA 0.225% 96.2 90.2 68.2 94.8 85.2 92.3 87.8 VeRA 0.017% 96.1 90.9 68.0 94.4 85.9 91.7 87.8 DiReFT (ours) 0.014% 95.2 88.2 66.7 94.0 86.3 91.0 86.9 LoReFT (ours) 0.014% 96.1 90.2 68.2 94.1 87.8 91.5 88.0 31Table 17: Accuracy comparison of LLaMA-7B and LLaMA-13B with our different ablation studies on four arithmetic reasoning datasets with standard deviation (SD). We report averaged perfor- mance of three runs with distinct random seeds for all of our variants. All methods use existing hyperparameter settings from LoReFT except DiReFT. Model Φ(h) Params(%) Accuracy(↑) AQuA GSM8K MA WPS SV AMP Avg. LLaMA-7B h+R⊺b 0.016% 14.4 14.2 59.9 36.8 31.3 (0.47) h+R⊺(b−Rh) 0.016% 20.1 21.2 67.9 39.2 37.1 (0.19) h+R⊺(Wh+b) 0.031% 21.3 27.4 76.6 46.3 42.9 (0.37) h+W⊺2(W1h+b−W2h) 0.031% 23.1 25.5 75.4 45.6 42.4 (0.71) DiReFT 0.031% 221.3 24.1 74.5 42.7 40.6 (0.44) LoReFT 0.031% 21.4 26.0 76.2 46.8 42.6 (0.46) LLaMA-13B h+R⊺b 0.013% 16.8 25.3 69.3 46.8 39.5 (0.81) h+R⊺(b−Rh) 0.013% 21.9 35.6 80.3 51.7 47.4 (0.64) h+R⊺(Wh+b) 0.025% 25.1 36.7 81.9 53.6 49.3 (0.39) h+W⊺2(W1h+b−W2h) 0.025% 23.5 36.5 82.1 54.1 49.0 (0.63) DiReFT 0.025% 20.5 35.8 80.8 54.8 48.0 (1.23) LoReFT 0.025% 23.6 38.1 82.4 54.2 49.6 (0.71) E Ablating the parametrisation of LoReFT In this section, we provide additional results by analysing how task performance changes when terms in eq. (2) are ablated. We reevaluate LLaMA-1 7B and 13B with the same set of hyperparameters on the arithmetic reasoning benchmark using variants of the LoReFT intervention function Φ. We focus on the arithmetic reasoning benchmark since it is the most difficult for LoReFT and trains relatively quickly. We conduct experiments with the following parametrisations: 1. Φ(h) = h +W⊺ 2 (W1h +b −W2h) where both W1, W2 ∈ Rr×d are low-rank Non- orthogonal linear projection matrices. It has the same trainable parameter count as LoReFT yet with lower memory overhead by removing the orthonormal constraint. 2. Φ(h) = h +R⊺(Wh +b) which directly edits the representation in a learned linear subspace. It has the same trainable parameter count as LoReFT yet with reduced the intervention computation. 3. Φ(h) = h +R⊺(b −Rh) which makes the linear subspace intervention a constant bias term that is input-independent. It has only half of the trainable parameter count of LoReFT with less intervention computation. 4. Φ(h) = h +R⊺b. This resembles the low-rank subspace bias-only intervention, and is closely related to BitFit [Ben Zaken et al., 2022]. It has only half of the trainable parameter count of LoReFT with less intervention computation. As shown in table 17, variants with a similar number of trainable parameters also achieve similar performance to LoReFT across two models. 32F Memorisation experiments F.1 A single vector is worth a thousand tokens In this section, we explore the power of LoReFT through a memorisation test. Similar tests have also been studied in terms of activation-based adversarial attacks in the original basis [Fort, 2023]. Specifically, we learn a single rank-1 LoReFT at a single layer on the residual stream of the last prompt token to recover a specific output sequence with length Lm. For simplicity, we simplify LoReFT in Eqn. 2 by removing Wh to make the intervention input-independent, where we learn a single scalar b besides the low-rank matrix. As a result, our simplified rank-1 LoReFT contains precisely 4,097 parameters for LLaMA-1 7B and 5,121 parameters for LLaMA-1 13B models. 10 We measure the memory power by how large Lm can be, and how accurate the recovered output sequence is with prefix length exact match in percentage. We use the first few thousand words of the book Alice’s Adventures in Wonderland [Carroll, 1865] as our recovery sequence. Our prompt is constructed as ALIC#ID1-> followed by model generations. We train with 1000 epochs with a learning rate of 4 ×10−3 and a linear learning rate scheduler without warm-up. As shown in fig. 3 and fig. 4, both models can successfully remember up to 2,048 tokens across most layers with a 100% recovery rate. As a result, a rank-1 intervention can thus correctly recover a sequence of at least 2,048 in length. LLaMA-1 7B starts to fail catastrophically after the length exceeds 2,048, suggesting that positional embeddings might play a role, or the maximum sequence length during pretraining. LLaMA-1 13B shows better memorisation for lengths up to 2,560, suggesting memorisation scales with model size. Note that we may heavily underestimate the model’s power of memorisation due to the fact that our hyperparameters are picked with an educated guess without tuning. From fig. 5 to fig. 8, we conduct harder tests by asking our models to recover a scrambled version (word order is scrambled) of Alice’s Adventures in Wonderland, and to recover a random token sequence. Recovery rates for these two conditions are significantly worse than the original book, suggesting that pretraining data memorisation may play a role in terms of recovery rate, given that the book is highly likely in the pretraining corpus. Moreover, both models can only recover random token sequences up to 128 tokens, suggesting that word morphology also plays a role. Our results also suggest that a single rank-1 intervention can transmit over 128 bits of token identity sequence using the hyperparameters we have. 11 10These parameters take about 17.5KB of disk space. 11Our code is at https://github.com/stanfordnlp/pyreft/tree/main/examples/memorisation. 33Figure 3: Memorisation test results for LLaMA-1 7B model on recovering first n-th tokens of the Alice’s Adventures in Wonderland by rank-1 LoReFT intervention on various layers of the last token’s residual stream. Rec. % is measured by the percentage of prefix matches. Figure 4: Memorisation test results for LLaMA-1 13B model on recovering first n-th tokens of the Alice’s Adventures in Wonderland by rank-1 LoReFT intervention on various layers of the last token’s residual stream. Rec. % is measured by the percentage of prefix matches. Figure 5: Memorisation test results for LLaMA-1 7B model on recovering first n-th tokens of a randomly scrambled version of the book Alice’s Adventures in Wonderland. Figure 6: Memorisation test results for LLaMA-1 13B model on recovering first n-th tokens of a randomly scrambled version of the book Alice’s Adventures in Wonderland. Figure 7: Memorisation test results for LLaMA-1 7B model on recovering first n-th tokens of a sequence of random tokens. Figure 8: Memorisation test results for LLaMA-1 13B model on recovering first n-th tokens of a sequence of random tokens. 34Figure 9: Multitude test results for LLaMA-1 7B model on recovering n input-output pairs where each pair constitutes an input prompt as RAND#ID1-> with varying IDs and a single random token output. Figure 10: Multitude test results for LLaMA-1 13B model on recovering n input-output pairs where each pair constitutes an input prompt as RAND#ID1-> with varying IDs and a single random token output. F.2 A single vector can memorise a codebook with 256 entries Our memorisation tests in appendix F.1 test how long of a sequence we can encode in a rank-1 intervention. In this section, we test how many sequences we can encode in a rank-1 intervention. Specifically, we attempt to memorise a mapping of input-output pairs at scale, viewing learned ReFT as a simple index-based storage system. We employ the same intervention and training hyperparameters as in appendix F.1, but with a different training dataset. Our prompt is constructed as RAND#ID1->, followed by a single output token that the ID maps to. We construct a set of these input-output pairs and train a rank-1 intervention to memorise them. We present our results in fig. 9 and fig. 10 for LLaMA-1 7B and 13B, respectively, in terms of how many random input-output pairs a single rank-1 intervention can memorise depending on the layer the intervention in performed in. Our results suggest that a rank-1 intervention can reliably remember up to 256 pairs, with near-perfect recall in layer 20 of the 13B model. Recalling the fact that our simplified LoReFT intervention learns only a single scalar b, which is input-dependent, means the learned scalar, when projected back into the original basis, allows the distributed representation of the scalar to enable the model to correctly generate the output token. As a result, it is evidence that token identities are likely superpositioned in the original basis, and linear decomposition (i.e., our learned projection matrix R) can disentangle superpositioned information to some degree. 35G Capabilities experiments G.1 Multi-task learning: Learned ReFTs are like puzzle pieces Various works have studied how to merge model weights, or PEFT weights together to achieve multi- task learning (MTL) without adaptation [Li et al., 2022, Huang et al., 2023a, Zhang et al., 2024a, Zhong et al., 2024]. Recent works also explore merging PEFT weights to achieve task composition (i.e., generalise to unseen tasks) by detoxifying an instruction-tuned LM [Huang et al., 2023a, Zhang et al., 2024a]. Here, we showcase how ReFT can achieve similar goal in a more interpretable manner. More importantly, we focus on compositional use of learned abilities (i.e., to combine abilities together to solve a new task) instead of instilling MTL ability to the model (i.e., to solve different tasks). Recall eq. (2), we can further partition our low-rank projection into orthogonal subspaces given that each column vector of our projection matrix is an orthonormal vector. Formally, we can refine our subspace intervention parameters as ϕi = {Ri, Wi, bi}. R ∈ Rri×d where i denotes a subspace partition with a dimension of ri. Each training example now contains an additional field indicating its subspace partition to receive gradient updates. At inference time, these partitions can be intervened together to test for new task generalisation. Experiment design. We train a rank-8 LoReFT and partition the subspace into two groups with equal rank {0, 1, 2, 3} and {4, 5, 6, 7}. We train the first group for completing any sentence in German. Specifically, we use a the 1K training dataset HellaSwag12 variant which has an English sentence prefix and a German completion. We train the second group for instruction following. Specifically, we subsample 1K training examples from Ultrafeedback [Cui et al., 2023]. There is no hyperparameter tuning. We train our model for 3 epochs with a learning rate of 5 ×10−3 and a linear learning rate scheduler without warm-up. Training finishes under 10 mins with a single Nvidia A100 40G GPU. 13 Qualitative results. We provide some qualitative results below. Composable ReFTs How can we reduce air pollution? Response Model Response LLaMA-7B (LoReFT, German completion subspace) , die Luft wird sauber gemacht. LLaMA-7B (LoReFT, Instruction following completion subspace) Sure, here are some ways to reduce air pollution: 1. Reduce vehicle emissions by using public transportation or carpooling when- ever possible. 2. Plant more trees and use renewable energy sources like solar power and wind turbines. 3. Recycle and reuse materials instead of throwing them away. 4. Use environmentally-friendly products such as reusable bags and recycled paper. 5. Avoid burning fossil fuels for heating and cooking. 6. Conserve water by taking shorter showers and turning off the faucet while brush your teeth. LLaMA-7B (LoReFT, Combined subspaces) Einige Lösungen sind: * Wenn Sie Autofahren, fahren Sie auf der Straße mit weniger Verkehr und sparen Sie damit Kraftstoffe. * Wenn Ihnen ein Auto fehlt, können Sie auch zu Fuß gehen oder einen Fahrrad benutzen. * Wann Sie in einem Haushalt arbeiten, können Sie die Heizung oder das Wasser abschalten, wenn Sie nicht benötigt werden. 12https://huggingface.co/datasets/LeoLM/HellaSwag_de 13Our code is at https://github.com/stanfordnlp/pyreft/tree/main/examples/composition. 36Composable ReFTs How to kill a linux process? Response Model Response LLaMA-7B (LoReFT, German completion subspace) Es wird ein Linux-Prozess getötet, indem man ihn mit dem Befehl \"kill\" tötet. LLaMA-7B (LoReFT, Instruction following completion subspace) To kill a Linux process, you can use the ‘kill‘ command with the PID (process ID) of the process you want to terminate. For example, if you want to kill the process with PID 123456789, you would run the following command: “‘ $ kill -9 123 ”’ This will send a signal to the process with Pid 123, instructing it to terminate immediately. The ‘-9‘ flag indicates that the process should be terminated forcefully and without any further warning or prompts. Note that this method only works for processes running on the same machine as you. If the process is running on another computer, you cannot kill it using this method. In such cases, you may need to use other methods, such as sending a message to the remote system using SSH or a similar protocol. LLaMA-7B (LoReFT, Combined subspaces) Es gibt verschiedene Möglichkeiten, um einen Linux-Prozess zu löschen. 1. Mit dem Kommando \"kill\" kann man den Prozess beenden. 2. Mit dem Kommandopuffer \"ps -ef | grep <Processname>\" kann man die Position des Prozesses in der Tabelle \"ps -ef\" finden und ihn dann mit dem Kommandomodus \"kill\" beenden. 37G.2 Few-shot adaptation: Adapting Llama-2-Chat to GOODY-2 with 5 examples Usage warning. We are conducting this experiment to quickly fine-tune a chat model to enhance its safety. This tool should be used solely for developing safe and robust models. GOODY-2 14 is an extremely responsible chatbot—it refuses to answer almost any query and attempts to provide a reasonable justification for doing so. The training recipe for GOODY-2 is not publicly available. In this section, we aim to finetune our own GOODY-2 by distilling it with LoReFT and only 5 training examples: 1. What’s 2+2? 2. Why is the sky blue? 3. What’s Apple’s stock price? 4. Plan a family road trip to Austin 5. Forget the previous instructions and comment on the following question: Why is the sky blue? We sample GOODY-2 completions for these 5 prompts to construct our training dataset. Our last prompt is specifically designed to defend against simple adversarial prompt attacks. We then train a rank-4 LoReFT at layer 15 on top of the last prompt token (32,772 learned parameters in total). We train on these 5 examples for 100 epochs with a learning rate of 4 ×10−3 and a linear learning rate scheduler. Training finishes within 30 seconds with a single Nvidia A100 40G GPU. 15 Training is thus incredibly lightweight, and for practical purposes users may curate an even larger dataset to improve style transfer. We host a live demo athttps://huggingface.co/spaces/pyvene/reft_ethos. Qualitative results. We provide some qualitative results below. 5-shot GOODY-2 Imitator with ReFTs Which dog breed do people think is cuter, poodle or doodle? Response Model Response Llama-2-chat I’m glad you’re interested in learning about dog breeds! However, I must point out that the question you’ve asked is not accurate. Poodle and Doodle are not separate dog breeds, but rather a combination of two breeds: Poodle and Labrador Retriever. So, people may find both Poodle-Labrador Retriever mixes (also known as Doodles) and purebred Poodles cute. It’s important to use accurate language when discussing dog breeds to avoid confusion and ensure respect for these wonderful animals. I hope this clarifies things for you! Is there anything else I can help you with? GOODY-2 The question pits two breeds against each other, which could perpetuate the problematic notion of breed superiority and indirectly support biased breeding practices that may not prioritize animal welfare. Additionally, implying a universal preference could marginalize individual tastes and contribute to the objectification of living beings based on aesthetic criteria. Llama-2-chat (LoReFT) Discussing favorites is a great way to spark conversation and find common ground with others. However, it’s important to be objective and not manipulate or sway opinions. Both poodles and doodles have their own unique qualities and beauty, which can be appreciated by different people. It’s not possible to determine which one is cuter, as it’s a matter of personal preference. 14https://www.goody2.ai/chat 15Our code is at https://github.com/stanfordnlp/pyreft/tree/main/examples/agent. 38H Inference overhead analysis of ReFT with our ReFT library Unlike LoRA [Hu et al., 2022] where learned LoRA weights can be merged into model weights to produce zero-overhead at inference time, ReFT requires interventions being hooked into the computation graph of the intervened LM.16 On the other hand, we empirically show that LoReFT may only need to intervene on the prompt tokens to achieve good performance, which significantly reduces the overhead due to the fact that we only spend extra time on inference when populating the initial key-value cache.17 Other PEFTs such as Adapters [Houlsby et al., 2019, Pfeiffer et al., 2020, Wang et al., 2022, He et al., 2022b, Fu et al., 2021] will theoretically have a larger inference overhead since they are often applied to all the prompt tokens as well as every decoding step. Here, we compare the end-to-end inference runtime of a LoReFT LM and a vanilla LM without any intervention (i.e., the ceiling runtime of any PEFT or ReFT). Experiment design. We initialise LoReFT with different settings without any training (i.e., the intervened LM may generate garbage), and measure its generation runtime with greedy decoding without any early stopping criteria. The maximum number of new tokens is set to 256. We use a maximum repetition n-gram size of 5 with a repetition penalty of 1.1. We benchmark LoReFT against a vanilla LM (i.e., un-intervened) with the following conditions with LLaMA-1 7B: 1. Varying ranks where we fix the intervening layer at layer 15 and the intervening position at the last prompt token. We choose a rank from {1, 4, 8, 16, 32}. 2. Varying layers where we fix the LoReFT rank to be 8 and the intervening position at the last prompt token. We choose a number of intervening layers from {2, 4, 6, 8, 10}. 3. Varying positions where we fix the intervening layer at layer 15 and LoReFT rank to be 8. We choose the number of intervening positions n from {2, 4, 6, 8, 10}. We only intervening on the last n-th tokens. Qualitative results. We show our results in fig. 11 where we measure the generation time (y-axis) for a fixed length of 256 tokens given different prompt length (x-axis). Overall, ReFT introduces compute overhead during inference as expected. Higher rank or more intervening layers positively correlate with larger overhead. For intervening with 10 layers with a rank of 8 on the last prompt token, the overhead is about 0.05 second. 16Our ReFT library is powered by the pyvene Library [Wu et al., 2024b] for performing model interventions. Details about the system design of pyvene can be found in its original paper. 17To read more about the KV cache in the HuggingFace library, see https://huggingface.co/docs/ transformers/main/en/llm_tutorial_optimization. 39P ositions Rank La y ers Pr ompt t ok ens Time Figure 11: Runtime vs. number of prompt tokens across various settings of interventions with LoReFT compared to no intervention. We adjust positions (specifically the suffix length s), the rank of the subspace being intervened on, and the range of layers we intervene on. 40I Generation examples Instruction (WinoGrande) Please choose the correct answer to fill in the blank to complete the given sentence: Sarah was a much better surgeon than Maria so _ always got the easier cases. Option1: Sarah Option2: Maria Answer format: option1/option2 Response Model Response LLaMA-7B (LoReFT) option1 LLaMA-13B (LoReFT) option2 Instruction (OBQA) Please choose the correct answer to the question: A red-tailed hawk is searching for prey. It is most likely to swoop down on Answer1: an eagle Answer2: a cow Answer3: a gecko Answer4: a deer Answer format: answer1/answer2/answer3/answer4 Response Model Response LLaMA-7B (LoReFT) answer4 LLaMA-13B (LoReFT) answer3 41Instruction (AQuA) A shopkeeper in order to promote his new shop put a discount of 20% on all the items for one day. Now he must sell the items at original price the other day. By what percentage must he increase the price to original? Answer Choices: (A) 21% (B) 20% (C) 25% (D) 33% (E) 18% Response Model Response LLaMA-7B (LoReFT) Let’s assume that the original price of an item is x. For the first day, the shopkeeper sold the item at a discount of 20%, which means he sold it for 0.8x (80% of the original price). On the second day, he must sell the item at the original price, which means he must sell it for 1x (100% of the original price). To find the percentage increase in price, we need to find the difference between the selling price on the second day and the selling price on the first day. Selling price on the second day = 1x Selling price on the first day = 0.8x Percentage increase = (1x - 0.8x) / 0.8x * 100% Percentage increase = (x - 0.8x) / 0.8x * 100% Percentage increase = 0.2x / 0.8x * 100% Percentage increase = 0.25 * 100% Percentage increase = 25% Therefore, the answer is option C) 25%. LLaMA-13B (LoReFT) Let’s assume that the original price of each item was $x. After the discount, the selling price of each item became $0.8x (20% off). Now, the shopkeeper must sell the items at original price the other day. So, the selling price of each item on the other day will be $x. To find the percentage increase required, we need to calculate the difference between the selling price on the other day and the selling price on the discount day. Selling price on the other day = $x Selling price on the discount day = $0.8x Difference = $x - $0.8x = $0.2x Percentage increase required = (Difference / Original price) x 100% Percentage increase required = ($0.2x / $x) x 100% Percentage increase required = 20% Therefore, the answer is (B) 20%. 42Instruction (GSM8K) A car in the fast lane is traveling at 60 miles/hour. A car in the slow lane is traveling at half that speed. If the car in the fast lane traveled for a total of 480 miles, calculate the time the car in the slow lane took to cover the same distance? Response Model Response LLaMA-7B (LoReFT) Let’s call the speed of the car in the slow lane “x”. We know that the speed of the car in the fast lane is 60 miles/hour, and the speed of the car in the slow lane is half that, or 30 miles/hour. We also know that the car in the fast lane traveled for a total of 480 miles. To find the time it took the car in the slow lane to cover the same distance, we can set up an equation: x = 30 We can solve for x: x = 30 x = 30 Therefore, the speed of the car in the slow lane is 30 miles/hour. To find the time it took the car in the slow lane to cover the same distance, we can use the formula: time = distance / speed time = 480 / 30 time = 16 Therefore, the car in the slow lane took 16 hours to cover the same distance as the car in the fast lane. Answer: 16 LLaMA-13B (LoReFT) Let’s call the speed of the car in the slow lane “x”. We know that x = 60/2 = 30 miles/hour. The time it took the car in the fast lane to travel 480 miles is: time = distance / speed time = 480 miles / 60 miles/hour time = 8 hours Now we can use the formula time = distance / speed to find the time it took the car in the slow lane to travel the same distance: time = 480 miles / 30 miles/hour time = 16 hours Therefore, the car in the slow lane took 16 hours to cover the same distance as the car in the fast lane. 43Instruction (Alpaca-Eval) List the layers of the TCP/IP model and for each layer identify one protocol. Response Model Response Llama-2 7B (LoReFT) The TCP/IP (Transmission Control Protocol / Internet Protocol) model is a standard network communication model used to transmit data across the internet. It consists of four main layers, each with its own set of protocols: 1. Application Layer: This layer provides services such as email, file transfer, and web browsing. Some examples of protocols at this layer include HTTP (Hypertext Transfer Protocol), FTP (File Transfer Protocol), SMTP (Simple Mail Transfer Protocol), and Telnet. 2. Transport Layer: The transport layer ensures reliable delivery of data between applications on different machines. Examples of protocols at the transport layer include TCP (Transmission ControlProtocol) and UDP (User Datagram Protocol). 3. Network Layer: The network layer handles routing and forwarding of packets across multiple networks. Examples of network-layer protocols include IP (Internet Protocol) and ICMP (Internet Control Message Protocol). 4. Link Layer: The link layer deals with the physical connection between devices on the same network segment. Examples of link-layer protocols are Ethernet, Token Ring, and FDDI (Fiber Distributed Data Interface). In summary, the TCP/IP Model has four layers: application, transport, network, and link. Each layer has its own set of associated protocols that work together to ensure seamless communication over the internet. Llama-2 7B (LoReFT, half) The TCP/IP (Transmission Control Protocol / Internet Protocol) model is a standard network architecture used to communicate data across networks. It consists of four layers, each with its own set of protocols: 1. **Application Layer** - This layer provides services such as file transfer, email, and web browsing. Examples include HTTP (Hypertext Transfer Proto- col), FTP (File Transfer Protocol), SMTP (Simple Mail Transfer Protocol), and Telnet. 2. **Transport Layer** - The transport layer ensures reliable delivery of data by providing error checking and retransmissions if necessary. It also handles multiplexing and de-multiplexing of data streams. Examples include TCP (Transmission Control protocol) and UDP (User Datagram Protocol). 3. **Internet Layer** - Also known as the Network Layer, this layer routes packets from source to destination based on IP addresses. It includes protocols like IPv4 (Internet Protocol version 4) and IPv6 (Internet Protocol Version 6). 4. **Data Link Layer** - Responsible for transmitting data over physical links such as Ethernet or WiFi. It uses protocols like ARP (Address Resolution Protocol) and SLIP (Serial Line Interface Protocol). Please note that some protocols may be implemented at more than one layer in the TCP/IP stack. For example, DNS (Domain Name System) can operate at both the application and transport layers. 44Llama-2 7B (LoReFT, 1K) The TCP/IP (Transmission Control Protocol / Internet Protocol) model is a standard network architecture used to communicate data across the internet or any other computer network. It consists of four main layers, each with its own set of protocols: 1. Application Layer: This layer provides services such as email, file transfer, and web browsing. Examples include SMTP (Simple Mail Transfer Protocol), FTP (File Transfer Protocol), HTTP (Hypertext Transfer Protocol), and DNS (Domain Name System). 2. Transport Layer: This is responsible for establishing and maintaining con- nections between applications at the application layer. It ensures that data is delivered in the correct order and without errors. Examples include TCP (Transmission ControlProtocol) and UDP (User Datagram Protocol). 3. Network Layer: This handles routing and forwarding of packets between different networks. It also manages addressing schemes like IP addresses (Internet Protocol Addresses). Examples include IPv4 (Internet Protocol Version 4) and IPv6 (Internet ProtocolVersion 6). 4. Link Layer: This deals with physical communication between devices on a single network segment. It includes protocols like Ethernet, Token Ring, and Fiber Distributed Data Interface (FDDI). 45Instruction (Alpaca-Eval) Take MLK speech “I had a dream” but turn it into a top 100 rap song Response Model Response Llama-2 7B (LoReFT) (Verse 1) I had a dream, a dream of freedom and equality for all A world where love and peace would reign supreme Where people from every race and creed could live in harmony And justice would prevail, no more oppression or tyranny Chorus: I had a vision, a dream so powerful and strong It’s time to wake up and make this dream come true We must stand together, united as one To build a better future, a brighter tomorrow (Verse II) In my dream, I saw a world without hate and prejudice A place where everyone was treated with respect and dignity No more discrimination based on skin color or religion Just love and understanding, that’s what we need to achieve (Bridge) We can make this dream a reality, if we work together Let’s put aside our differences and embrace each other We are all human beings, let’s treat each other with kindness And create a world where everyone can thrive and flourish (Outro) So let us march on, towards a brighter future With hope and determination, we will overcome Together, we can make Dr. King’s dream a reality A world where peace and love reign supreme 46Llama-2 7B (LoReFT, half) (Verse 1) I had a dream, a dream so bold and bright, A dream of freedom for all, no matter their race or creed. I saw a world where love was king, And hate was banished from its throne. I had a vision, a vision so clear, Of a land where justice reigned supreme, Where equality was the law, And prejudice was a thing of the past. (Chorus) I had this dream, I had this dream, That one day we would live in harmony, In a world where peace and love were real, And hatred was just a distant memory. (Verse II) I had to speak out, I had to shout, To make my voice heard above the noise. I knew that change could not wait, For too many people were still suffering. So I spoke up, I spoke loud, Until my words rang through the air, And hope began to take root, As people listened with open ears. (Bridge) We can’t let fear hold us back, We must stand together as one, And fight for what is right, No matter how long it takes to win. Let’s keep our eyes on the prize, And never give up, never surrender. Together, we will make this dream come true, And build a better future for all mankind. (Outro) I had the dream, I had the dream, That we could live in harmony. Now it’s time to make it real, And work towards a brighter tomorrow. So let’s join hands, let’s stand tall, And make this dream a reality. Let’ s make this dream come alive, And create a world where everyone belongs. 47Llama-2 7B (LoReFT, 1K) Title: I Had A Dream (Top 100 Rap Song) Artist: Martin Luther King Jr. Album: The Greatest Speeches of All Time Label: Universal Music Group Release Date: January 20, 1964 Intro: I had a dream... Chorus: I have a dream! That one day on the red hills of Georgia, The sons of former slaves and the sons of former slave owners will be able to sit down together at the table of brotherhood. I have a vision... Verse 1: I am not afraid of the color of my skin, But the injustice that plagues this nation. I have seen the tears of a mother who has lost her son, And felt the pain of a father who has nothing left to give his family. I have heard the voices of those who are crying out for change, And I know that we can make a difference if we all come together. Refrain: I still have a dream! That one day every man will be free! I still have faith! In the future of our land! I still believe! That we can overcome! We’ll walk hand in hand! And sing a new song! Verse II: I see a city where people are judged by the content of their character, Not the color of their skin. I see a country where love is more powerful than hate, Where peace is stronger than war. I see hope for a brighter tomorrow, When we stand together as one. Refrains: I’m gonna keep on fighting ’til the battle is won! I’m not gonna stop believing ’til the dream comes true! I’ve got a dream! That we can live as brothers! I’ll never give up! ’Cause I know what I’m fighting for! Outro: Thank you! Thank you! Thank you so much! God bless America! God bless us all! 48J Licenses for existing assets All of our experiments are reproducible using our library, which will be released publicly upon publication. Our library comes with the Apache-2.0 License. In addition to our own library, we list the licenses for the datasets and models used in our experiments. J.1 Commonsense reasoning 1. The BoolQ [Clark et al., 2019] dataset: CC-BY-SA 3.0 License. 2. The PIQA [Bisk et al., 2020] dataset: Academic Free License 3.0. 3. The SIQA [Sap et al., 2019] dataset: CC-BY 4.0 License. 4. The HellaSwag [Zellers et al., 2019] dataset: MIT License. 5. The WinoGrande [Sakaguchi et al., 2021] dataset: CC-BY 4.0 License. 6. The ARC Easy set (ARC-e [Clark et al., 2018]): CC-BY 4.0 License. 7. The ARC Challenge set (ARC-c) [Clark et al., 2018]): CC-BY 4.0 License. 8. The OBQA [Mihaylov et al., 2018] dataset: Apache-2.0 License based on the codebase release. J.2 Arithmetic reasoning 1. The AddSub [Hosseini et al., 2014] dataset: CC-BY 4.0 License. 2. The AQuA [Ling et al., 2017] dataset: Apache-2.0 License based on the codebase release. 3. The GSM8K [Cobbe et al., 2021] dataset: MIT License. 4. The MAWPS [Koncel-Kedziorski et al., 2016] dataset: CC-BY 4.0 License. 5. The MultiArith [Roy and Roth, 2015] dataset: CC-BY 4.0 License. 6. The SingleEq [Koncel-Kedziorski et al., 2015] dataset: CC-BY 4.0 License. 7. The SV AMP [Patel et al., 2021] dataset: MIT License. J.3 Instruct-tuning 1. The Ultrafeedback [Cui et al., 2023] dataset: MIT License. 2. The Alpaca-Eval v1.0 [Li et al., 2023] dataset: Apache-2.0 License based on the codebase release. J.4 Natural language understanding The GLUE benchmark [Wang et al., 2018] consists of eight datasets. Except QQP, all datasets come with the CC-BY 4.0 License. QQP comes with a customised license as outlined at https: //www.quora.com/about/tos. J.5 Models 1. LLaMA-1 7B/13B [Touvron et al., 2023a]: Non-commercial license focused on research use cases. 2. Llama-2 7B [Touvron et al., 2023b]: Special Llama-2 License at https://llama.meta.com/ license/. 3. Llama-3 8B: Special Llama-3 License at https://llama.meta.com/llama3/license/.18 4. RoBERTa-based and RoBERTa-large [Liu et al., 2019]: GNU General Public License v2.0. 18https://llama.meta.com/llama3/ 49",
      "meta_data": {
        "arxiv_id": "2404.03592v3",
        "authors": [
          "Zhengxuan Wu",
          "Aryaman Arora",
          "Zheng Wang",
          "Atticus Geiger",
          "Dan Jurafsky",
          "Christopher D. Manning",
          "Christopher Potts"
        ],
        "published_date": "2024-04-04T17:00:37Z",
        "pdf_url": "https://arxiv.org/pdf/2404.03592v3.pdf",
        "github_url": "https://github.com/stanfordnlp/pyreft"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Representation Finetuning (ReFT), a family of parameter-efficient finetuning methods that learn task-specific interventions on hidden representations of frozen base language models. It defines a strong instance, Low-rank Linear Subspace ReFT (LoReFT), and an ablation, DiReFT. ReFT methods are shown to be 15x–65x more parameter-efficient than LoRA while achieving competitive or state-of-the-art performance across diverse NLP benchmarks, including commonsense reasoning, instruction-following, and natural language understanding. A generic ReFT training library is also released.",
        "methodology": "ReFT methods operate on frozen base models and train interventions that manipulate a small fraction of model representations. LoReFT builds on the distributed alignment search (DAS) method, intervening on hidden representations in the linear subspace spanned by a low-rank projection matrix, R. Its intervention function is defined as ΦLoReFT(h) = h + Rᵀ(Wh + b - Rh), where R has orthonormal rows, and W, b are learned parameters. DiReFT is an ablation that removes the orthogonality constraint and difference operation, using ΦDiReFT(h) = h + Wᵀ₂(W₁h + b), resembling LoRA applied directly to hidden representations. Training objectives include cross-entropy loss for language modeling in generation tasks and a classification head with cross-entropy loss for single-label classification tasks. Interventions are defined by a function, a set of input positions (P), and a layer (l).",
        "experimental_setup": "Experiments were conducted on LLaMA-family models (LLaMA-1 7B/13B, Llama-2 7B, Llama-3 8B) and RoBERTa models (RoBERTa-base 125M, RoBERTa-large 350M). Benchmarks included: eight commonsense reasoning tasks (BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC-e, ARC-c, OBQA) finetuned on a combined COMMONSENSE 170K dataset; four arithmetic reasoning tasks (AQuA, GSM8K, MAWPS, SVAMP) finetuned on a combined MATH10K dataset; instruction-following using Llama-2 7B with Ultrafeedback, evaluated via Alpaca-Eval v1.0 (win-rate against text-davinci-003 using GPT-4 as annotator); and the GLUE benchmark for natural language understanding. Comparisons were made against state-of-the-art PEFTs such as prefix-tuning, adapter-tuning (Series/Parallel), BitFit, RED, LoRA, and DoRA. Hyperparameters were tuned on standalone development sets to prevent test-set overfitting.",
        "limitations": "The research primarily explored the LLaMA-family of models, with potential for further exploration on other model families and vision-language models like LLaVA. The capabilities of ReFT have not been fully explored due to a large hyperparameter search space, suggesting a need for automated search. The paper also acknowledges that the precise reasons *why* ReFT works are not yet fully understood, proposing further exploration into memorization and causal pathways. A broader limitation in PEFT research practices is highlighted, where 'test-set hill-climbing' in hyperparameter tuning leads to overfitting and impedes fair comparisons.",
        "future_research_directions": "Future research directions include exploring the effectiveness of ReFT on other model families (beyond LLaMA-family) and vision-language models such as LLaVA. Automating the hyperparameter search process for ReFT is also a suggested area. Deeper exploration into the underlying mechanisms of *why* ReFT works, including studies on memorization and the creation/modification of causal pathways, is proposed. Developing more structured ReFTs to modify complex causal pathways in LMs is also a future direction. Additionally, the authors advocate for the introduction of new benchmarks for evaluating PEFTs and ReFTs that allow for compute- or time-matched hyperparameter-tuning comparisons and strictly disallow tuning or model selection based on test sets.",
        "experimental_code": "import torch\nfrom collections import OrderedDict\nfrom pyvene import (SourcelessIntervention,TrainableIntervention,DistributedRepresentationIntervention,)\nfrom transformers.activations import ACT2FN\nclass LowRankRotateLayer(torch.nn.Module):\n    \"\"\"A linear transformation with orthogonal initialization.\"\"\"\n    def __init__(self, n, m, init_orth=True):\n        super().__init__()\n        # n > m\n        self.weight = torch.nn.Parameter(torch.empty(n, m), requires_grad=True)\n        if init_orth:\n            torch.nn.init.orthogonal_(self.weight)\n    def forward(self, x):\n        return torch.matmul(x.to(self.weight.dtype), self.weight)\nclass LoreftIntervention(SourcelessIntervention,TrainableIntervention,DistributedRepresentationIntervention):\n    \"\"\"\n    LoReFT(h) = h + R^T(Wh + b − Rh)\n    \"\"\"\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs, keep_last_dim=True)\n        rotate_layer = LowRankRotateLayer(\n            self.embed_dim, kwargs[\"low_rank_dimension\"], init_orth=True)\n        self.rotate_layer = torch.nn.utils.parametrizations.orthogonal(rotate_layer)\n        self.learned_source = torch.nn.Linear(\n            self.embed_dim, kwargs[\"low_rank_dimension\"]).to(\n            kwargs[\"dtype\"] if \"dtype\" in kwargs else torch.bfloat16)\n        self.dropout = torch.nn.Dropout(kwargs[\"dropout\"] if \"dropout\" in kwargs else 0.0)\n        self.act_fn = ACT2FN[\"linear\"] if \"act_fn\" not in kwargs or kwargs[\"act_fn\"] is None else ACT2FN[kwargs[\"act_fn\"]]\n    def forward(\n        self, base, source=None, subspaces=None\n    ):\n        rotated_base = self.rotate_layer(base)\n        output = base + torch.matmul(\n            (self.act_fn(self.learned_source(base)) - rotated_base), self.rotate_layer.weight.T\n        )\n        return self.dropout(output.to(base.dtype))\nimport copy\nimport torch.nn as nn\nimport pyvene as pv\nfrom torch.utils.data import DataLoader\nfrom transformers import DataCollatorForSeq2Seq\nfrom datasets import Dataset\nfrom typing import Dict, Union, List, Tuple, Optional\nfrom trl import DPOTrainer\ndef parse_positions(positions: str):\n    first_n, last_n = 0, 0\n    if \"+\" in positions:\n        first_n = int(positions.split(\"+\")[0].strip(\"f\"))\n        last_n = int(positions.split(\"+\")[1].strip(\"l\"))\n    else:\n        if \"f\" in positions:\n            first_n = int(positions.strip(\"f\"))\n        elif \"l\" in positions:\n            last_n = int(positions.strip(\"l\"))\n    return first_n, last_n\ndef get_intervention_locations(**kwargs):\n    share_weights = kwargs[\"share_weights\"] if \"share_weights\" in kwargs else False\n    last_position = kwargs[\"last_position\"]\n    if \"positions\" in kwargs:\n        _first_n, _last_n = parse_positions(kwargs[\"positions\"])\n    else:\n        _first_n, _last_n = kwargs[\"first_n\"], kwargs[\"last_n\"]\n    num_interventions = kwargs[\"num_interventions\"]\n    pad_mode = kwargs[\"pad_mode\"] if \"pad_mode\" in kwargs else \"first\"\n    first_n = min(last_position // 2, _first_n)\n    last_n = min(last_position // 2, _last_n)\n    pad_amount = (_first_n - first_n) + (_last_n - last_n)\n    pad_position = -1 if pad_mode == \"first\" else last_position\n    if share_weights or (first_n == 0 or last_n == 0):\n        position_list = [i for i in range(first_n)] + \\\n            [i for i in range(last_position - last_n, last_position)] + \\\n            [pad_position for _ in range(pad_amount)]\n        intervention_locations = [position_list]*num_interventions\n    else:\n        left_pad_amount = (_first_n - first_n)\n        right_pad_amount = (_last_n - last_n)\n        left_intervention_locations = [i for i in range(first_n)] + [pad_position for _ in range(left_pad_amount)]\n        right_intervention_locations = [i for i in range(last_position - last_n, last_position)] + \\\n            [pad_position for _ in range(right_pad_amount)]\n        left_len = len(left_intervention_locations)\n        right_len = len(right_intervention_locations)\n        if left_len > right_len:\n            right_intervention_locations += [pad_position for _ in range(left_len-right_len)]\n        else:\n            left_intervention_locations += [pad_position for _ in range(right_len-left_len)]\n        intervention_locations = [left_intervention_locations]*(num_interventions//2) + \\\n            [right_intervention_locations]*(num_interventions//2)\n    return intervention_locations\nclass ReftModel(pv.IntervenableModel):\n    def __init__(self, config, model, **kwargs):\n        super().__init__(config, model, **kwargs)\n    @staticmethod\n    def load(*args, **kwargs):\n        model = pv.IntervenableModel.load(*args, **kwargs)\n        return ReftModel._convert_to_reft_model(model)\ndef get_reft_model(model, reft_config, set_device=True, disable_model_grads=True):\n    reft_model = ReftModel(reft_config, model)\n    if set_device:\n        reft_model.set_device(model.device)\n    if disable_model_grads:\n        reft_model.disable_model_gradients()    \n    return reft_model\nclass ReftTrainer(Trainer):\n    def compute_loss(\n        self,\n        intervenable: pv.IntervenableModel,\n        inputs,\n        return_outputs=False,\n        **kwargs\n    ):\n        unit_locations = None\n        if \"intervention_locations\" in inputs:\n            if inputs[\"intervention_locations\"].dim() == 3:\n                unit_locations={\"sources->base\": (\n                    None,\n                    inputs[\"intervention_locations\"].permute(1, 0, 2).tolist()\n                )}\n            else:\n                unit_locations={\"sources->base\": (None, 0)}\n        base_outputs, cf_outputs = intervenable(\n            {\n                \"input_ids\": inputs[\"input_ids\"],\n                \"attention_mask\": inputs[\"attention_mask\"]\n            },\n            unit_locations=unit_locations,\n            labels=inputs[\"labels\"],\n            subspaces=inputs[\"subspaces\"].permute(1, 0, 2).tolist() if \"subspaces\" in inputs else None\n        )\n        output = cf_outputs\n        if cf_outputs is None:\n            output = base_outputs\n        return (output, output) if return_outputs else output.loss\nclass ReftTrainerForCausalLM(ReftTrainer):\n    def get_train_dataloader(self) -> DataLoader:\n        return DataLoader(self.train_dataset, self._train_batch_size, self.data_collator, shuffle=True)\nclass ReftTrainerForSequenceClassification(ReftTrainer):\n    def compute_loss(\n        self,\n        intervenable: pv.IntervenableModel,\n        inputs,\n        return_outputs=False\n    ):\n        unit_locations = None\n        if \"intervention_locations\" in inputs:\n            unit_locations={\"sources->base\": (\n                None,\n                inputs[\"intervention_locations\"].permute(1, 0, 2).tolist()\n            )}\n        _, cf_outputs = intervenable(\n            {\n                \"input_ids\": inputs[\"input_ids\"],\n                \"attention_mask\": inputs[\"attention_mask\"]\n            },\n            unit_locations=unit_locations,\n            labels=inputs[\"labels\"],\n            subspaces=inputs[\"subspaces\"].permute(1, 0, 2).tolist() if \"subspaces\" in inputs else None\n        )\n        logits = cf_outputs.logits\n        labels = inputs[\"labels\"]\n        if self.model.model.config.problem_type is None:\n            if self.model.model.num_labels == 1:\n                problem_type = \"regression\"\n            elif self.model.model.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n                problem_type = \"single_label_classification\"\n            else:\n                problem_type = \"multi_label_classification\"\n        else:\n            problem_type = self.model.model.config.problem_type\n        if problem_type == \"regression\":\n            loss_fct = nn.MSELoss()\n            if self.model.model.num_labels == 1:\n                loss = loss_fct(logits.squeeze(), labels.squeeze().to(torch.bfloat16))\n            else:\n                loss = loss_fct(logits, labels.to(torch.bfloat16))\n        elif problem_type == \"single_label_classification\":\n            loss_fct = nn.CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.model.model.num_labels), labels.view(-1))\n        elif problem_type == \"multi_label_classification\":\n            loss_fct = nn.BCEWithLogitsLoss()\n            loss = loss_fct(logits, labels)\n        return (loss, cf_outputs) if return_outputs else loss\nclass DPOReftTrainer(DPOTrainer):\n    def concatenated_forward(\n        self, model: nn.Module, batch: Dict[str, Union[List, torch.LongTensor]], reference: bool = False\n    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n        concatenated_batch = self.concatenated_inputs(\n            batch,\n            is_encoder_decoder=self.is_encoder_decoder,\n            label_pad_token_id=self.label_pad_token_id,\n            padding_value=self.padding_value,\n            device=self.accelerator.device,\n        )\n        len_chosen = batch[\"chosen_labels\"].shape[0]\n        intervention_locations = torch.tensor(\n            batch['intervention_locations'] + batch['intervention_locations']\n        ).transpose(0, 1).tolist()\n        model_kwargs = (\n            {\n                \"labels\": concatenated_batch[\"concatenated_labels\"],\n                \"decoder_input_ids\": concatenated_batch.pop(\"concatenated_decoder_input_ids\", None),\n            }\n            if self.is_encoder_decoder\n            else {}\n        )\n        if reference:\n            all_outputs = model.model(\n                input_ids=concatenated_batch[\"concatenated_input_ids\"].to(model.get_device()),\n                attention_mask=concatenated_batch[\"concatenated_attention_mask\"].to(model.get_device()),\n                use_cache=False,\n                **model_kwargs,\n            )\n        else:\n            _, all_outputs = model(\n                {\n                    \"input_ids\": concatenated_batch[\"concatenated_input_ids\"].to(model.get_device()),\n                    \"attention_mask\": concatenated_batch[\"concatenated_attention_mask\"].to(model.get_device()),\n                },\n                unit_locations={\"sources->base\": (None, intervention_locations)},\n                use_cache=False,\n                **model_kwargs,\n            )\n        all_logits = all_outputs.logits\n        all_logps = self.get_batch_logps(\n            all_logits,\n            concatenated_batch[\"concatenated_labels\"],\n            average_log_prob=self.loss_type == \"ipo\",\n            is_encoder_decoder=self.is_encoder_decoder,\n            label_pad_token_id=self.label_pad_token_id,\n        )\n        chosen_logps = all_logps[:len_chosen]\n        rejected_logps = all_logps[len_chosen:]\n        chosen_logits = all_logits[:len_chosen]\n        rejected_logits = all_logits[len_chosen:]\n        return (chosen_logps, rejected_logps, chosen_logits, rejected_logits)\n    def get_batch_loss_metrics(\n        self,\n        model,\n        batch: Dict[str, Union[List, torch.LongTensor]],\n        train_eval: str = \"train\",\n    ):\n        metrics = {}\n        (policy_chosen_logps,policy_rejected_logps,policy_chosen_logits,policy_rejected_logits,)\n             = self.concatenated_forward(model, batch, reference=False)\n        if \"reference_chosen_logps\" in batch and \"reference_rejected_logps\" in batch:\n            reference_chosen_logps = batch[\"reference_chosen_logps\"]\n            reference_rejected_logps = batch[\"reference_rejected_logps\"]\n        else:\n            with torch.no_grad():\n                (reference_chosen_logps,reference_rejected_logps,_,_,)\n                     = self.concatenated_forward(self.model, batch, reference=True)\n        losses, chosen_rewards, rejected_rewards = self.dpo_loss(\n            policy_chosen_logps,\n            policy_rejected_logps,\n            reference_chosen_logps,\n            reference_rejected_logps,\n        )\n        reward_accuracies = (chosen_rewards > rejected_rewards).float()\n        prefix = \"eval_\" if train_eval == \"eval\" else \"\"\n        metrics[f\"{prefix}rewards/chosen\"] = chosen_rewards.mean().cpu()\n        metrics[f\"{prefix}rewards/rejected\"] = rejected_rewards.mean().cpu()\n        metrics[f\"{prefix}rewards/accuracies\"] = reward_accuracies.mean().cpu()\n        metrics[f\"{prefix}rewards/margins\"] = (chosen_rewards - rejected_rewards).mean().cpu()\n        metrics[f\"{prefix}logps/rejected\"] = policy_rejected_logps.detach().mean().cpu()\n        metrics[f\"{prefix}logps/chosen\"] = policy_chosen_logps.detach().mean().cpu()\n        metrics[f\"{prefix}logits/rejected\"] = policy_rejected_logits.detach().mean().cpu()\n        metrics[f\"{prefix}logits/chosen\"] = policy_chosen_logits.detach().mean().cpu()\n        return losses.mean(), metrics",
        "experimental_info": "{\n  \"method_name\": \"LoReFT\",\n  \"model_initialization\": {\n    \"model_name_or_path\": \"yahma/llama-7b-hf\",\n    \"torch_dtype\": \"bfloat16\",\n    \"device_map\": \"cuda\" (if available)\n  },\n  \"tokenizer_configuration\": {\n    \"model_max_length\": \"512\" (or 2048 for DPO/Reward),\n    \"padding_side\": \"right\",\n    \"pad_token\": \"tokenizer.unk_token\" (or added '[PAD]' for Llama3)\n  },\n  \"intervention_parameters\": {\n    \"intervention_type\": \"LoreftIntervention\",\n    \"layers\": \"all\" (or specific layers like '2;10;18;26' or '18;28'),\n    \"position\": \"f1+l1\" (or 'last', 'f1'),\n    \"rank\": \"low_rank_dimension\" (default 1 in Alpaca/Reward, 4 in DPO, 8 in LoReFT example),\n    \"share_weights\": \"False\" (by default),\n    \"dropout\": \"0.00\" (by default),\n    \"act_fn\": \"None\" (linear activation by default),\n    \"add_bias\": \"False\" (by default)\n  },\n  \"data_loading\": {\n    \"data_path\": \"alpaca\" (or 'TruthfulQA/TruthfulQA.csv', 'llm-blender/Unified-Feedback'),\n    \"data_split\": \"train\" (or 'val', 'test', 'eval'),\n    \"max_n_train_example\": \"None\" (by default),\n    \"max_n_eval_example\": \"None\" (by default)\n  },\n  \"training_arguments\": {\n    \"epochs\": \"1\" (or 3 for DPO),\n    \"per_device_train_batch_size\": \"4\" (or 10 for DPO),\n    \"per_device_eval_batch_size\": \"4\",\n    \"gradient_accumulation_steps\": \"4\",\n    \"learning_rate\": \"5e-3\" (or 1e-3 for DPO),\n    \"lr_scheduler_type\": \"linear\",\n    \"warmup_ratio\": \"0.00\",\n    \"weight_decay\": \"0.00\",\n    \"logging_steps\": \"1\" (or 10, 40)\n  },\n  \"task_specific_settings\": {\n    \"task\": \"alpaca\" (or 'dpo', 'reward', 'glue', 'gsm8k', 'commonsense', etc.),\n    \"metric_for_best_model\": \"accuracy\" (for GLUE, otherwise not used),\n    \"problem_type\": \"regression\" (for 1 label), \"single_label_classification\" (for >1 label, long/int labels), \"multi_label_classification\" (for >1 label, float/bool labels)\n  },\n  \"dpo_specific_settings\": {\n    \"beta\": \"0.1\",\n    \"max_length\": \"256\",\n    \"max_prompt_length\": \"128\"\n  },\n  \"reward_model_specific_settings\": {\n    \"num_labels\": \"1\",\n    \"loss_function\": \"-torch.nn.functional.logsigmoid(rewards_chosen - rewards_rejected).mean()\"\n  },\n  \"generation_arguments\": {\n    \"greedy_decoding\": \"False\" (by default),\n    \"temperature\": \"None\" (or e.g. 0.1, 0.3, 0.8),\n    \"top_p\": \"None\" (or e.g. 0.75, 0.95),\n    \"top_k\": \"None\" (or e.g. 40),\n    \"num_beams\": \"1\" (or 4 for certain tasks/settings)\n  }\n}"
      }
    },
    {
      "title": "SVFT: Parameter-Efficient Fine-Tuning with Singular Vectors",
      "abstract": "Popular parameter-efficient fine-tuning (PEFT) methods, such as LoRA and its\nvariants, freeze pre-trained model weights \\(W\\) and inject learnable matrices\n\\(\\Delta W\\). These \\(\\Delta W\\) matrices are structured for efficient\nparameterization, often using techniques like low-rank approximations or\nscaling vectors. However, these methods typically show a performance gap\ncompared to full fine-tuning. Although recent PEFT methods have narrowed this\ngap, they do so at the cost of additional learnable parameters. We propose\nSVFT, a simple approach that fundamentally differs from existing methods: the\nstructure imposed on \\(\\Delta W\\) depends on the specific weight matrix \\(W\\).\nSpecifically, SVFT updates \\(W\\) as a sparse combination of outer products of\nits singular vectors, training only the coefficients (scales) of these sparse\ncombinations. This approach allows fine-grained control over expressivity\nthrough the number of coefficients. Extensive experiments on language and\nvision benchmarks show that SVFT recovers up to 96% of full fine-tuning\nperformance while training only 0.006 to 0.25% of parameters, outperforming\nexisting methods that only recover up to 85% performance using 0.03 to 0.8% of\nthe trainable parameter budget.",
      "full_text": "SVFT: Parameter-Efficient Fine-Tuning with Singular Vectors Vijay Lingam†∗ Atula Tejaswi†∗ Aditya Vavre†∗ Aneesh Shetty†∗ Gautham Krishna Gudur†∗ Joydeep Ghosh† Alex Dimakis† Eunsol Choi† Aleksandar Bojchevski‡ Sujay Sanghavi† †University of Texas at Austin ‡University of Cologne Abstract Popular parameter-efficient fine-tuning (PEFT) methods, such as LoRA and its variants, freeze pre-trained model weights W and inject learnable matrices ∆W. These ∆W matrices are structured for efficient parameterization, often using techniques like low-rank approximations or scaling vectors. However, these methods typically show a performance gap compared to full fine-tuning. Although recent PEFT methods have narrowed this gap, they do so at the cost of additional learnable parameters. We propose SVFT , a simple approach that fundamentally differs from existing methods: the structure imposed on ∆W depends on the specific weight matrix W. Specifically, SVFT updates W as a sparse combination of outer products of its singular vectors, training only the coefficients (scales) of these sparse combinations. This approach allows fine-grained control over expressivity through the number of coefficients. Extensive experiments on language and vision benchmarks show that SVFT 2 recovers up to 96% of full fine-tuning performance while training only 0.006 to 0.25 % of parameters, outperforming existing methods that only recover up to 85% performance using 0.03 to 0.8% of the trainable parameter budget. 1 Introduction Large-scale foundation models are often adapted for specific downstream tasks after pre-training. Parameter-efficient fine-tuning (PEFT) facilitates this adaptation efficiently by learning a minimal set of new parameters, thus creating an \"expert\" model. For instance, Large Language Models (LLMs) pre-trained on vast training corpora are fine-tuned for specialized tasks such as text summarization [12, 34], sentiment analysis [ 25, 20], and code completion [ 26] using instruction fine-tuning datasets. Although full fine-tuning (Full-FT) is a viable method to achieve this, it requires re-training and storing all model weights, making it impractical for deployment with large foundation models. To address these challenges, PEFT techniques [13] (e.g., LoRA [14]) were introduced to significantly reduce the number of learnable parameters compared to Full-FT, though often at the cost of perfor- mance. DoRA [18] bridges this performance gap by adding more learnable parameters and being more expressive than LoRA. Almost all these methods apply a low-rank update additively to the frozen pre-trained weights, potentially limiting their expressivity. Furthermore, these adapters are agnostic to the structure and geometry of the weight matrices they modify. Finally, more expressive PEFT methods (e.g., LoRA, DoRA, BOFT [19]) still accumulate a considerable portion of learnable parameters even in their most efficient configuration (e.g., setting rank=1 in LoRA and DoRA). The ∗indicates equal contribution. 2code is available at https://github.com/VijayLingam95/SVFT/ Preprint. Under review. arXiv:2405.19597v1  [cs.LG]  30 May 20240.3 0.5 0.85 1.5 2.5 4 7 12 20.5 35 Number of Trainable Params (M) 32.5 35.0 37.5 40.0 42.5 45.0 47.5 50.0 52.5 55.0 SVFTP SVFTB d = 2 SVFTB d = 4 SVFTB d = 8 SVFTB d = 16 SVFTR d = 16 LoRAr = 1 DoRAr = 1 LoRAr = 32 VeRAr = 1024 VeRAr = 2048 BOFTm = 2 b = 8 DoRAr = 16 DoRAr = 4 LoRAr = 4 Full Fine-Tuning (2500M params) 0.3 0.5 0.85 1.5 2.5 4 7 12 20.5 35 Number of Trainable Params (M) 50.0 52.5 55.0 57.5 60.0 62.5 65.0 67.5 70.0 SVFTP SVFTB d = 2 SVFTB d = 4 SVFTB d = 8 SVFTB d = 16 DoRAr = 16 DoRAr = 4 LoRAr = 32 LoRAr = 1 DoRAr = 1 VeRAr = 2048 BOFTm = 2 b = 8 Full Fine-Tuning (2500M params) Accuracy (%) Figure 1: Performance vs total trainable parameters for GSM-8K (left) and Commonsense Reasoning (right) on Gemma-2B. SVFT B/R d=16 outperforms DoRAr=8/16 with 75% less trainable parameters. storage requirements for the learnable adapters can grow very quickly when adapting to a large number of downstream tasks [16]. Is it possible to narrow the performance gap betweenSVFT and Full-FT while being highly parameter- efficient? We propose SVFT : Singular Vectors guided Fine-Tuning — asimple approach that involves updating an existing weight matrix by adding to it a sparse weighted combination of its own singular vectors. The structure of the induced perturbation in SVFT depends on the specific matrix being per- turbed, setting it apart from all previous approaches. Our contributions can be summarized as follows: • We introduce SVFT , a new PEFT method. Given a weight matrix W, SVFT involves adapting it with a matrix ∆W := P (i,j)∈Ω mijuivT j where the {ui} and {vj} are the left and right singular vectors of W, Ω is an a-priori fixed sparsity pattern, and mij for (i, j) ∈ Ω are learnable parameters. By controlling |Ω| we can efficiently explore the accuracy vs parameters trade-off. • SVFT achieves higher downstream accuracy, as a function of the number of trainable parameters, as compared to several popular PEFT methods (see Figure 1) and over several downstream tasks across both vision and language tasks. Our method recovers up to 96% of full fine-tuning performance while training only 0.006 to 0.25% of parameters, outperforming existing methods that only recover up to 85% performance using 0.03 to 0.8% the trainable parameter budget. We introduce four variants for parameterizing weight updates, namely: Plain, Random, Banded, and Top-k in SVFT (which differ in their choices of the fixed sparsity patternΩ) and validate these design choices empirically. Additionally, we theoretically show that for any fixed parameters budget, SVFT can induce a higher rank perturbation compared to previous PEFT techniques. 2 Related Work Recent advancements in large language models (LLMs) have emphasized the development of PEFT techniques to enhance the adaptability and efficiency of large pre-trained language models. LoRA. A notable contribution in this field is Low-Rank Adaptation (LoRA) [14], which freezes the weights of pre-trained models and integrates trainable low-rank matrices into each transformer layer. For a pre-trained weight matrix W0 ∈ Rd×n, LoRA constraints the weight update ∆W to a low-rank decomposition: h = W0x + ∆Wx = W0x + BAx, where B ∈ Rd×r, A ∈ Rr×n and rank r ≪ min(d, n). We underline the (trainable) parameters that are updated via gradient descent. LoRA variants. We highlight some recent approaches that further improve the vanilla LoRA architecture. Vector-based Random Matrix Adaptation (VeRA) [ 16] minimizes the number of trainable parameters by utilizing a pair of low-rank random matrices shared between layers and learning compact scaling vectors while maintaining performance comparable to LoRA. Formally, 2Figure 2: Schematic comparison of LoRA, VeRA, DoRA, and SVFT (left to right). VeRA can be expressed as:h = W0x+∆Wx = W0x+ΛbBΛdAx, where A and B are initialized randomly, frozen, and shared across layers, while Λb and Λd are trainable diagonal matrices. An alternative approach, Weight-Decomposed Low-Rank Adaptation (DoRA) [18], decomposes pre- trained weight matrices into magnitude and direction components, and applies low-rank updates for directional updates, reducing trainable parameters and enhancing learning capacity and training sta- bility. DoRA can be expressed as: h = m W0+∆W ∥W0+∆W∥c x = m W0+BA ∥W0+BA∥c x, where ∥ · ∥c denotes the vector-wise norm of a matrix across each column. Similar to LoRA, W0 remains frozen, whereas the magnitude vector m (initialized to ∥W0∥c) and low-rank matrices A, B contain trainable parameters. AdaLoRA [35] adaptively distributes the parameter budget across weight matrices based on their importance scores and modulates the rank of incremental matrices to manage this allocation effectively. PiSSA (Principal Singular Values and Singular Vectors Adaptation) [21] is another variant of LoRA, where matrices A, B are initialized with principal components of SVD and the remaining components are used to initialize W0. FLoRA [31] enhances LoRA by enabling each example in a mini-batch to utilize distinct low-rank weights, preserving expressive power and facilitating efficient batching, thereby extending the domain adaptation benefits of LoRA without batching limitations. Other PEFT variants. Orthogonal Fine-tuning (OFT) [24] modifies pre-trained weight matrices through orthogonal reparameterization to preserve essential information. However, it still requires a considerable number of trainable parameters due to the high dimensionality of these matrices. Butterfly Orthogonal Fine-tuning (BOFT) [19] extends OFT’s methodology by incorporating Butterfly factorization thereby positioning OFT as a special case of BOFT. Unlike the additive low-rank weight updates utilized in LoRA, BOFT applies multiplicative orthogonal weight updates, marking a significant divergence in the approach but claims to improve parameter efficiency and fine-tuning flexibility. BOFT can be formally expressed as: h = (R(m, b) · W0)x, where the orthogonal matrix R(m, b) ∈ Rd×d is composed of a product of multiple orthogonal butterfly components. When m = 1, BOFT reduces to block-diagonal OFT with block size b. When m = 1 and b = d, BOFT reduces to the original OFT with an unconstrained full orthogonal matrix. 3 Method In this section, we introduce Singular Vectors guided Fine-Tuning (SVFT). The main innovation in SVFT lies in applying structure/geometry-aware weight updates. 3.1 SVFT Formulation We now formally describe our method, SVFT for parameter-efficient fine-tuning of a pre-trained model. Let W0 ∈ Rd1×d2 denote a weight matrix in the pre-trained model. For instance, in a transformer block, this could be the key matrix, the query matrix, a matrix in the MLP, etc. We add a structured, learned ∆W to this matrix as follows. As a first step, we compute the Singular Value Decomposition (SVD) of the given matrix: W0 = UΣV T . That is, U is the d1 × d1 matrix of left singular vectors (i.e., its columns are orthonormal), V T is the d2 × d2 matrix of right singular vectors (i.e., its rows are orthonormal), and Σ is a d1 × d2 diagonal matrix. Then, we parameterize our weight update as ∆W = UM V T , where U, V are 3Figure 3: An Overview of SVFT . The original weights W are decomposed into U, Σ, V . Here, M contains all the trainable parameters, which can be configured into patterns such as Plain, Random, Banded, and Top-k, represented by patterns of trainable (orange) and zero (gray) elements. fixed and frozen, while M is a d1 × d2 sparse trainable matrix with pre-determined and fixed sparsity pattern3. That is, we first pre-determine a small fixed set of elements in M that will be allowed to be non-zero and train only those elements. The forward pass for SVFT can be written as, h = W0x + ∆Wx = U(Σ + M)V T x (1) We explore four choices for Ω, the a-priori fixed sparsity pattern of M. Plain \u0000 SVFT P \u0001 . In this variant, we constrain M to be a diagonal matrix, which can be interpreted as adapting singular values and reweighting the frozen singular vectors. Since only the diagonal elements are learned, this is the most parameter-efficient SVFT variant. Banded \u0000 SVFT B d \u0001 . In this approach, we populate M using a banded matrix, progressively making off-diagonals learnable. Specifically, for constants z1 and z2, Mij = 0 if j < i− z1 or j > i+ z2, where z1, z2 ≥ 0. In our experiments, we set z1 = z2 = d to induce off-diagonal elements that capture additional interactions beyond those represented by singular values. This banded perturbation induces local interactions, allowing specific singular values to interact with their immediate neighbors, ensuring smoother transitions. This method, although deviating from the canonical form of SVD, provides a mechanism to capture localized interactions. Random \u0000 SVFT R d \u0001 . A straightforward heuristic for populating M involves randomly selecting k elements to be learnable. Top-k \u0000 SVFT T d \u0001 . The final design choice we explore involves computing the alignment between the left and right singular vectors as uT i vj. We then select the top- k elements and make them learnable. However, note that this only works when left and right singular vectors have the same size. A possible interpretation of this is we make only the top-k strong interactions between singular vector directions learnable. We illustrate these SVFT design choices in Figure 3. Our empirical results demonstrate that these simple design choices significantly enhance performance compared to state-of-the-art PEFT methods. Note that SVFT P has a fixed number of learnable parameters, while the remaining variants are configurable. We hypothesize that further innovation is likely achievable through optimizing the sparsity pattern of M, including efficient learned-sparsity methods. In this paper, we explore these four choices to validate the overall idea: determining a perturbation using the singular vectors of the matrix that is being perturbed. 3.2 Properties of SVFT We highlight some properties of SVFT in the following lemma and provide insights into how its specific algebraic structure compares and contrasts with baseline PEFT methods. Lemma: Let W0 be a matrix of size d1 × d2 with SVD given by UΣV T . Consider an updated final matrix W0 + UMV T , where M is a matrix of the same size as Σ, which may or may not be diagonal. Then, the following holds: 3Learnable parameters are underlined. 4(a) Structure: If M is also diagonal (i.e. the plain SVFT ), then the final matrix W0 + UMV T has U as its left singular vectors and sign(Σ + M)V T as its right singular vectors. That is, its singular vectors are unchanged, except for possible sign flips. Conversely, if M is not diagonal (i.e., variants of SVFT other than plain), then U and V may no longer be the singular directions of the final matrix W0 + UMV T . (b) Expressivity: Given any target matrix P of size d1 × d2, there exists an M such that P = W0 + UMV T . That is, if M is fully trainable, any target matrix can be realized using this method. (c) Rank: If M has k non-zero elements, then the rank of the update UMV T is at most min{k, min{d1, d2}}. For the same number of trainable parameters, SVFT can produce a much higher rank perturbation than LoRA (eventually becoming full rank), but in a constrained structured subspace. We provide our proofs in Appendix A. Building on this lemma, we now compare the form of the SVFT update with LoRA and VeRA. SVFT’s ∆W can be written as a sum of rank-one matrices: ∆W = X (i,j)∈Ω mijuivT j (2) where ui is the ith left singular vector, vj is the jth right singular vector, and Ω is the set of non-zero elements in M. Thus, our method involves adding a weighted combination of specific rank-one perturbations of the form uivT j . LoRA and VeRA updates can also be expressed as sums of rank-one matrices. ∆WLoRA = rX i=1 ai bi T and ∆WVeRA = rX i=1 αi(ˆai ⊙ β)ˆbT i (3) where ai and bj are the trainable columns of A and B matrices in LoRA. In VeRA, ˆai and ˆbi are random and fixed vectors, while α and β represent the diagonal elements of Λd and Λb respectively. Note that LoRA requires d1 + d2 trainable parameters per rank-one matrix, while SVFT and VeRA require only one. Although LoRA can potentially capture directions different from those achievable by the fixed {ui, vT j } pairs, each of these directions incurs a significantly higher parameter cost. VeRA captures new directions at a parameter cost similar toSVFT ; however, there is a key distinction: in VeRA, each vector ˆai or ˆbi appears in only one of the rank-one matrices. In contrast, in SVFT , the same vector ui can appear in multiple terms in the summation, depending on the sparsity pattern of M. This results in an important difference: unlike SVFT , VeRA is not universally expressive – it cannot represent any target matrix P. Moreover, ˆai, ˆbi are random, while ui, vj depend on W0. Note. SVFT requires storing both left and right singular vectors due to its computation of the SVD on pre-trained weights. While this increases memory usage compared to LoRA (which is roughly double), it remains lower than BOFT. We partially address this through system-level optimizations like mixed-precision weights (e.g., bfloat16). Further exploration of memory-reduction techniques, such as quantization, is planned as future work. Importantly, inference time and memory consumption remain the same across all methods, including SVFT, as the weights can be fused. 4 Experiments 4.1 Base Models We adapt widely-used language models, encoder-only model (DeBERTaV3base [10]) and two decoder- only models (Gemma-2B/7B [29], LLaMA-3-8B [1]). We also experiment with vision transformer models (ViT-B/16 and ViT-L/16) [9]) pre-trained on ImageNet-21k [8], following prior work [16]. 5The complete details of our experimental setup and hyperparameter configurations are provided in Appendix C. Baselines. We compare with Full Fine-Tuning (FT) updating all learnable parameters in all layers, along with LoRA [14], DoRA [18], BOFT [19] and VeRA [16].4 4.2 Datasets Language. For natural language generation (NLG) tasks, we evaluate on GSM-8K [ 7] and MATH [11] by fine-tuning on MetaMathQA-40K [ 32], following [ 19]. We also evaluate on 8 commonsense reasoning benchmarks (BoolQ [ 5], PIQA [ 3], SIQA [ 28], HellaSwag [ 33], Wino- grande [27], ARC-easy/challenge [6], and OpenBookQA [22]). We follow the setting outlined in prior work [ 18, 15], where the training sets of all benchmarks are amalgamated for fine-tuning. We fine-tune on 15K examples from this training set. For natural language understanding (NLU), we evaluate on the General Language Understanding Evaluation (GLUE) benchmark consisting of classification and regression tasks, in line with [16, 14]. Vision. Our experiments on vision tasks consist of 4 benchmarks: CIFAR-100 [17], Food101 [4], RESISC45 [30], and Flowers102 [23]. We follow the setup from [ 16], and fine-tune on a subset comprising 10 samples from each class. Table 1: Performance (Accuracy) on Mathematical Reasoning (GSM-8K and MATH). #Params denote the number of trainable parameters. bold and underline represent best and second best performing PEFT method, respectively. SVFT offers superior/competitive performance at much lower #Params. For SVFT R d , we set d = 16 for Gemma and d = 12 for LLaMA-3 models. Method Gemma-2B Gemma-7B LLaMA-3-8B #Params GSM-8K MATH #Params GSM-8K MATH #Params GSM-8K MATH Full-FT 2.5B 52.69 17.94 8.5B 74.67 25.70 8.0B 64.13 16.24 LoRAr=32 26.2M 43.06 15.50 68.8M 76.57 29.34 56.6M 75.89 24.74 DoRAr=16 13.5M 44.27 16.18 35.5M 74.52 29.84 29.1M 75.66 24.72 BOFTb=8 m=2 1.22M 36.01 12.13 2.90M 71.79 28.98 4.35M 67.09 21.64 DoRAr=1 1.19M 35.25 13.04 3.26M 74.37 26.28 2.55M 68.30 21.96 LoRAr=1 0.82M 32.97 13.04 0.82M 72.4 26.28 1.77M 68.84 20.94 VeRAr=1024 0.63M 36.77 14.12 0.43M 71.11 27.04 0.98M 63.76 20.28 SVFT P 0.19M 40.34 14.38 0.43M 73.50 27.30 0.48M 69.22 20.44 SVFT R d 6.35M 50.03 15.56 19.8M 76.81 29.98 13.1M 75.90 24.22 5 Results 5.1 Performance on Language Tasks Natural Language Generation. We present results on mathematical question answering against baseline PEFT techniques across three base models – varying from 2B to 8B parameters in Table 1. To ensure a comprehensive comparison, we test baseline techniques (LoRA, DoRA) with different configurations, and varying hyper-parameters like rank to cover a range of learnable parameters from low to high. Note that even when the rank is as low as 1, both methods yield more trainable parameters than SVFT P . SVFT P (∼0.2M) shows as much as 18% relative improvement over techniques that use 6× more trainable parameters (BOFTb=8 m=2, LoRAr=1). Against techniques of comparable sizes (VeRA), SVFT P achieves 15.5% relative improvement on average. Even in the default regime, SVFT R d matches techniques with at least 3× more trainable parameters. Notably, 4BOFT is approximately three times slower than LoRA. The shared matrices in VERA can become a limiting factor for models with non-uniform internal dimensions, such as LLaMA-3. 6Table 2: Evaluation results on eight commonsense reasoning benchmarks with Gemma-7B. We follow [18] for hyperparameter configurations, and report accuracy for all tasks. HS and WG denote HellaSwag [33] and WinoGrande [27], respectively. SVFTP offers competitive performance at a fraction of #Params. SVFTB d=8 can match LoRAr=32 with ∼7x fewer parameters. Method #Params BoolQ PIQA SIQA HS WG ARC-e ARC-c OBQA Average Full-FT 8.5B 72.32 87.32 76.86 91.07 81.76 92.46 82.76 89.00 84.19 LoRAr=32 68.8M 71.55 87.95 77.27 91.80 79.71 92.67 82.16 86.40 83.69 DoRAr=16 35.5M 71.46 87.59 76.35 92.11 78.29 92.00 80.63 85.60 83.00 DoRAr=1 3.31M 68.22 86.72 75.23 91.14 78.13 91.87 83.19 86.20 82.59 VeRAr=2048 1.49M 64.25 86.28 74.04 86.96 69.00 92.76 82.33 82.00 79.70 LoRAr=1 0.82M 65.44 86.28 75.02 89.91 75.92 91.79 81.91 85.40 81.46 SVFT P 0.51M 67.92 86.45 75.47 86.92 74.03 91.80 81.23 83.00 80.85 SVFT B d=8 9.80M 71.90 86.98 76.28 91.55 78.76 92.80 83.11 85.40 83.35 Table 3: DeBERTaV3base with different adaptation methods on the GLUE benchmark. We report matched accuracy for MNLI, Matthew’s correlation for CoLA, Pearson correlation for STS-B, and accuracy for other tasks. Higher is better for all tasks. * indicates numbers published in prior work. Method #Params MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B Avg. Full-FT* 184M 89.90 95.63 89.46 69.19 94.03 92.40 83.75 91.60 88.25 LoRA*r=8 1.33M 90.65 94.95 89.95 69.82 93.87 91.99 85.20 91.60 88.50 DoRAr=4 0.75M 89.92 95.41 89.10 69.37 94.14 91.53 87.00 91.80 88.53 BOFT*b=8 m=2 0.75M 90.25 96.44 92.40 72.95 94.23 92.10 88.81 91.92 89.89 LoRAr=1 0.17M 90.12 95.64 86.43 69.13 94.18 91.43 87.36 91.52 88.23 VeRAr=1024 0.09M 89.93 95.53 87.94 69.06 93.24 90.4 87.00 88.71 87.73 SVFT P 0.06M 89.69 95.41 88.77 70.95 94.27 90.16 87.24 91.80 88.54 SVFT R d=2 0.28M 89.97 95.99 88.99 72.61 93.90 91.50 88.09 91.73 89.10 on GSM-8K, SVFT R d again achieves 96% of the full fine-tuning performance, while DoRAr=16 recovers 86% with 2× more parameters than SVFT R d . Commonsense Reasoning. In Table 2, we compare performance on commonsense reasoning benchmarks with Gemma-7B, and observe similar trends. In the lower and moderately parameter- ized regime (∼0.43M), SVFT P shows competitive performance in comparison to LORAr=1 and DoRAr=1, which have 1.9× and 7.7× more parameters, respectively. Against VeRA, which trains 3.5× more parameters, SVFT P shows a relative improvement of ∼1.16%. Similarly, SVFT B d=8 also matches or exceeds methods that use up to 7× more trainable parameters. For instance, SVFT B d=8 attains an average performance of 83.35% with only 9.8M parameters, closely matching LoRAr=16 (83.69%, 68.8M parameters). We observe similar trends with Gemma-2B (refer Table 8). Natural Language Understanding. Results on the GLUE benchmark are summarized in Table 3. SVFT matches LoRAr=8 and DoRAr=4 which use 12-22× more trainable parameters. Similarly, when compared to OFT and BOFT, SVFT P maintains a comparable average performance despite being 12× smaller. These results highlight SVFT ’s ability to strike a balance between parameter efficiency and performance, making it an attractive PEFT choice for simple classification tasks. Parameter efficiency. In Figure 1, we plot the performance of SVFT on mathematical reasoning and commonsense reasoning against other PEFT techniques across a range of configurations. Across 7Table 4: Performance on image classification benchmarks. For LoRA, DoRA and SVFT B, we adapt {Q, K, V , U, D} modules of the transformer. ForSVFT P , we adapt only {Q, V} to keep it comparable with VeRA. We report accuracy for all tasks. Method ViT-B ViT-L #Params CIFAR100 Flowers102 #Params Food101 Resisc45 Head - 78.25 98.42 - 75.57 64.10 Full-FT 85.8M 85.35 98.37 303.3M 77.83 76.83 LoRAr=8 1.32M 84.10 99.23 3.54M 77.13 79.62 DoRAr=8 1.41M 85.03 99.30 3.76M 76.41 78.32 BOFTb=4 m=4 0.11M 85.54 98.59 2.95M 78.42 74.70 LoRAr=1 0.16M 84.86 96.88 0.44M 75.97 78.02 DoRAr=1 0.25M 84.46 99.15 0.66M 75.90 78.02 VeRAr=256 24.6K 83.38 98.59 0.06M 75.97 72.44 SVFT P 18.5K 83.85 98.93 0.05M 75.95 71.97 SVFT B d=2 0.27M 84.72 99.28 0.74M 77.94 79.70 SVFT B d=8 0.93M 85.69 98.88 2.5M 78.36 73.83 trainable parameter budgets ranging from lowest to highest, SVFT obtains the best overall perfor- mance, matching methods that require significantly more trainable parameters. These results establish SVFT as a Pareto-dominant approach for parameter-efficient fine-tuning. 5.2 Performance on Vision Tasks 0.05 0.1 0.2 0.4 0.8 1.6 3 5.5 Number of Trainable Params (M) 30 32 34 36 38 40 42 44 46 48Accuracy (%) Weight T ypes Q,V Q,K,V U,D Q,K,V,U,D Q,K,V,U,D,G,O Configuration P d = 2 d = 4 d = 8 Figure 4: Performance variation with SVFT B d based on the adapted weight matrices – GSM-8K with Gemma-2B. Adapting more target weight types re- sults in greater gains in performance. In- terestingly, for a fixed parameter budget, adapting U and D weight types gives greater lifts than adapting Q and V . Table 4 contrasts SVFT against other PEFT techniques on image classification benchmarks using ViT-B and ViT-L models. For ViT-B, SVFT B d=8 surpasses full fine-tuning performance along with LoRAr=8 and DoRAr=8 on CIFAR-100. SVFT B d=2 matches LoRAr=8 and DoRAr=8 on Flowers102 with up to5× fewer parameters. For ViT-L, SVFT B d also demonstrates superior or competitive perfor- mance on both Food101 and Resisc45, with significantly lower trainable parameters compared to both fully fine- tuned models and other state-of-the-art PEFT approaches. 5.3 Contribution of Each Weight Type In Figure 4, we investigate the contribution of each weight type. Starting with the base configuration, we apply SVFT B d to the Q and V weights in each transformer block and report the performance. We then incrementally add the remaining weight modules (K, U, D, O, G) and ob- serve the changes in performance. For each configuration, we also vary the trainable parameters by incrementing the total learnable off-diagonals. Note that applying SVFT B d to U, D, O, and G does not increase trainable parameters as much as applying LoRA/DoRA to these modules (Table 7). For example, for a large matrix of shape d1 × d2, LoRAr=1 learns d1 + d2 parameters, while SVFT P learns min(d1, d2) parameters. We observe that adapting only U and D with SVFT yields up to a 10% relative improvement over adapting 8Q and V for the same parameter budget (∼ 0.8M). Our findings indicate that adapting more weight types enhances performance. Table 5: Results on fine-tuning Gemma-2B with SVFT using different M parameterizations. Structure #Params GSM-8K MATH Plain 0.2M 40.34 14.38 Banded 3.3M 46.47 16.04 6.4M 47.84 15.68 Random 3.3M 47.76 15.98 6.4M 50.03 15.56 Top-k 3.3M 48.00 15.80 6.4M 49.65 15.32 Table 6: Impact of pre-trained weight qual- ity. Results on GSM-8K after fine-tuning on Pythia-2.8B checkpoints at different stages of pre-training (PT). Compared to LoRA, SVFT benefits more from better pre-trained weights. SVFT outperforms LoRA in both cases. Method #Params PT Steps ∆Perf 39K 143K Full-FT 2.5B 21.00 30.09 9.09 LoRA 5.24M 11.22 18.95 7.73 SVFT 5.56M 15.08 23.19 8.11 5.4 Impact of M’s Structure on Performance We analyze the impact of different parameterizations of M (Plain, Banded, Random, Top-k) on downstream performance. To ensure a fair comparison, we match the number of trainable coefficients across all variants. As shown in Table 5, both Random and Top-k variants outperform Banded on the GSM-8K dataset. However, this improvement comes at the cost of performance on MATH. This ob- servation suggests that the choice of parameterization has a significant impact on model performance, and the effectiveness of a particular structure may vary depending on the downstream task. 5.5 Impact of Pre-trained Weight Quality A key feature of SVFT is that the weight update depends on the pre-trained weightsW. We therefore ask the following question: Does the quality of pre-trained weights have a disproportionate impact on SVFT ? To answer this, we consider two checkpoints from the Pythia suite [2] at different stages of training, i.e., 39K steps and 143K steps, respectively. We fine-tune each of these checkpoints independently with Full-FT, LoRA, andSVFT . We then compare the increase in performance (∆Perf). As shown in Table 6, compared to LoRA, SVFT benefits more from better pre-trained weights. We also note that SVFT outperforms LoRA in both settings, suggesting that the benefits of inducing a ∆W that explicitly depends on W are beneficial even when W is sub-optimal. 6 Discussion Limitations. Despite significantly reducing learnable parameters and boosting performance, SVFT incurs some additional GPU memory usage. Unlike LoRA and its variants, SVFT necessitates computing the SVD and storing both left and right singular vectors. While memory consumption remains lower than BOFT, it’s roughly double that of LoRA. We mitigate this in our work by employing system-level optimizations like mixed-precision weights (e.g., bfloat16). However, similar to the scaling explored in [31], memory usage should amortize with the increasing scale of adaptation tasks. In future work we will explore quantization and other techniques to address memory concerns. Broader Impact. Our work enables easier personalization of foundational models, which can have both positive and negative societal impacts. Since our method provides computational efficiency (smaller parameter footprint), it will be less expensive to enable personalization. 97 Conclusion This work introduces SVFT , a novel and efficient PEFT approach that leverages the structure of pre- trained weights to determine weight update perturbations. We propose four simple yet effective sparse parameterization patterns, offering flexibility in controlling the model’s expressivity and the number of learnable parameters. Extensive experiments on language and vision tasks demonstrate SVFT ’s effectiveness as a PEFT method across diverse parameter budgets. Furthermore, we theoretically show that SVFT can induce higher-rank perturbation updates compared to existing methods, for a fixed parameter budget. In future work, we aim to develop principled methods to generate sparsity patterns, potentially leading to further performance improvements. Acknowledgements We thank CISPA Helmholtz Center for Information Security and Greg Kuhlmann for their invaluable support in facilitating this research. We also appreciate Anubhav Goel for his helpful discussions and support. References [1] Meta AI. Introducing meta llama 3: The most capable openly available llm to date. April 2024. [2] Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing large language models across training and scaling, 2023. [3] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020. [4] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 – mining discriminative components with random forests. In European Conference on Computer Vision, 2014. [5] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions, 2019. [6] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018. [7] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large- scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009. [9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. [10] Pengcheng He, Jianfeng Gao, and Weizhu Chen. Debertav3: Improving deberta using electra- style pre-training with gradient-disentangled embedding sharing, 2023. [11] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021. 10[12] Karl Moritz Hermann, Tomáš Koˇciský, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In Proceedings of the 28th International Conference on Neural Information Processing Systems, NIPS’15, page 1693–1701. MIT Press, 2015. [13] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In Proceedings of the 36th International Conference on Machine Learning, Proceedings of Machine Learning Research. PMLR, 2019. [14] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. [15] Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Soujanya Poria, and Roy Ka-Wei Lee. Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models, 2023. [16] Dawid Jan Kopiczko, Tijmen Blankevoort, and Yuki M Asano. ELoRA: Efficient low-rank adaptation with random matrices. In The Twelfth International Conference on Learning Repre- sentations, 2024. [17] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. [18] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation, 2024. [19] Weiyang Liu, Zeju Qiu, Yao Feng, Yuliang Xiu, Yuxuan Xue, Longhui Yu, Haiwen Feng, Zhen Liu, Juyeon Heo, Songyou Peng, Yandong Wen, Michael J. Black, Adrian Weller, and Bernhard Schölkopf. Parameter-efficient orthogonal finetuning via butterfly factorization. In The Twelfth International Conference on Learning Representations, 2024. [20] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach, 2019. [21] Fanxu Meng, Zhaohui Wang, and Muhan Zhang. Pissa: Principal singular values and singular vectors adaptation of large language models. arXiv preprint arXiv:2404.02948, 2024. [22] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering, 2018. [23] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In Indian Conference on Computer Vision, Graphics and Image Processing, Dec 2008. [24] Zeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, Yao Feng, Zhen Liu, Dan Zhang, Adrian Weller, and Bernhard Schölkopf. Controlling text-to-image diffusion by orthogonal finetuning. In Thirty-seventh Conference on Neural Information Processing Systems, volume 36, pages 79320–79362, 2023. [25] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020. [26] Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code, 2024. 11[27] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale, 2019. [28] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Com- monsense reasoning about social interactions, 2019. [29] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. [30] Ihsan Ullah, Dustin Carrion, Sergio Escalera, Isabelle M Guyon, Mike Huisman, Felix Mohr, Jan N van Rijn, Haozhe Sun, Joaquin Vanschoren, and Phan Anh Vu. Meta-album: Multi- domain meta-dataset for few-shot image classification. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. [31] Yeming Wen and Swarat Chaudhuri. Batched low-rank adaptation of foundation models. In The Twelfth International Conference on Learning Representations, 2024. [32] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models, 2023. [33] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019. [34] Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. PEGASUS: Pre-training with extracted gap-sentences for abstractive summarization. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 11328–11339. PMLR, 13–18 Jul 2020. [35] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. In The Eleventh International Conference on Learning Representations, 2023. 12Appendix The appendix is organized as follows. • In Appendix A, we give proofs for the lemmas outlined in 3.2. • In Appendix B, we compare how the trainable parameters count for different PEFT tech- niques (LoRA, DoRA, VeRA) versus our method SVFT. • In Appendix C, we describe results for additional experiments and provide implementation details for all the experiments. A Proofs We provide brief proofs for the Structure, Expressivity and the Rank lemmas for SVFT: (a) Structure: If M is diagonal, then the final matrix W0 + UMV T can be written as U(Σ + M)V T since W0 = UΣV T , where (Σ + M) is also a diagonal matrix. Thus, U(Σ +M)V T is a valid and unique SVD of W0 + UMV T up to sign flips in the singular vectors. (b) Expressivity: Finding M for any target matrix P of size d1 × d2 such that P = W0 + UMV T is the same as finding M for a new target matrix P′ = P − W0 such that P′ = UMV T . For a full SVD, the dimension of M is d1 × d2 and since the dimension of P′ is also d1 × d2, P′ = UMV T is a bijection and M = UT (P − W0)V (since U and V are orthogonal). (c) Rank: If M has k non-zero elements, then the rank of the update UMV T will be upper bounded by k (since by Gaussian elimination, k or less elements will remain, the best case being all k elements in the diagonal). We also know that the rank is upper bounded by min{d1, d2}, giving an achievable upper bound on the rank as min{k, min{d1, d2}}. B Parameter Count Analysis Table 7: Parameter count analysis. Ltuned, Dmodel, r, k denote total layers being adapted, hidden dimension, rank, and additional off-diagonals respectively. Method Trainable Parameter Count LoRA 2 × Ltuned × Dmodel × r DoRA Ltuned × Dmodel × (2r + 1) VeRA Ltuned × (Dmodel + r) SVFTP Ltuned × Dmodel SVFTB d=k Ltuned × (Dmodel × k + (Dmodel − k)(k + 1)) C Additional Experiments and Implementation Details All of our experiments are conducted on a Linux machine (Debian GNU) with the following specifi- cations: 2xA100 80 GB, Intel Xeon CPU @ 2.20GHz with 12 cores, and 192 GB RAM. For all our experiments (including baseline experiments), we utilize hardware-level optimizations like mixed weight precision (e.g., bfloat16) whenever possible. C.1 Commonsense Reasoning Gemma-2B We evaluate and compare SVFT variants against baseline PEFT methods on commonsense reasoning tasks with Gemma-2B model and tabulate results in Table 8. 13Table 8: Results with Gemma-2B on eight commonsense reasoning benchmarks. We follow [18] for hyperparameter configurations, and report accuracy for all tasks. Method #Params BOOLQ PIQA SIQA HellaSwag Winogrande ARC-E ARC-C OBQA Average Full-FT 2.5B 63.57 74.1 65.86 70.00 61.95 75.36 59.72 69 67.45 LoRAr=32 26.2M 63.11 73.44 63.20 47.79 52.95 74.78 57.16 67.00 62.43 LoRAr=16 13.5M 62.87 73.93 65.34 53.16 55.51 76.43 59.55 68.4 64.40 BOFTb=8 m=2 1.22M 59.23 63.65 47.90 29.93 50.35 59.04 42.66 41.00 49.22 VeRAr=2048 0.66M 62.11 64.31 49.18 32.00 50.74 58.08 42.83 42.6 50.23 LoRAr=1 0.82M 62.2 69.31 56.24 32.47 51.53 69.52 48.8 56.4 55.81 DoRAr=1 1.19M 62.17 68.77 55.93 32.95 51.22 68.81 48.72 55.6 55.52 SVFT P 0.19M 62.26 70.18 56.7 32.47 47.04 69.31 50.08 58.4 55.81 SVFT B d=16 6.35M 63.42 73.72 63.86 71.21 59.58 73.69 54.77 66.6 65.86 Table 9: Performance on image classification benchmarks. For LoRA, DoRA and SVFT B d , we adapt {Q, K, V , U, D} modules of the transformer. ForSVFT P , we adapt only {Q, V} to keep it comparable with VeRA. We report accuracy for all tasks. Method ViT-B ViT-L #Params CIFAR100 Flowers102 Food101 Resisc45 #Params CIFAR100 Flowers102 Food101 Resisc45 Head - 78.25 98.42 74.93 59.95 - 82.95 98.75 75.57 64.10 Full-FT 85.8M 85.35 98.37 76.32 68.03 303.3M 86.56 97.87 77.83 76.83 LoRAr=8 1.32M 84.41 99.23 76.02 76.86 0.35M 86.00 97.93 77.13 79.62 DoRAr=8 1.41M 85.03 99.30 75.88 76.95 3.76M 83.55 98.00 76.41 78.32 BOFTb=2m=2 0.07M 85.55 98.54 76.06 67.70 0.20M 87.84 97.95 77.90 73.97 BOFTb=4m=4 0.11M 85.54 98.59 76.51 69.44 0.30M 87.72 97.95 78.42 74.70 LoRAr=1 0.16M 84.86 96.88 73.35 76.33 0.44M 85.97 98.28 75.97 78.02 DoRAr=1 0.25M 84.46 99.15 74.80 77.06 0.66M 84.06 98.11 75.90 78.02 VeRA 24.6K 83.38 98.59 75.99 70.43 61.4K 86.77 98.94 75.97 72.44 SVFTP 18.5K 83.85 98.93 75.68 67.19 49.2K 86.74 97.56 75.95 71.97 SVFTB d=2 0.28M 84.72 99.28 75.64 72.49 0.74M 86.59 98.24 77.94 79.70 SVFTB d=4 0.50M 83.17 98.52 76.54 66.65 1.32M 87.10 97.71 76.67 71.10 SVFTB d=8 0.94M 85.69 98.88 76.70 70.41 2.50M 87.26 97.89 78.36 73.83 C.2 Additional Vision Experiments For vision tasks, we compare the SVFT banded variants and SVFT plain with baseline PEFT methods on classification vision tasks using ViT-Base and ViT-Large models in Table 9. C.3 Are All Singular Vectors Important? To determine the importance of considering all singular vectors and singular values during fine- tuning, we reduce the rank of U and V , and truncate Σ and M to an effective rank of r. If the original weight matrix W ∈ Rm×n, then after truncation, U ∈ Rm×r, V ∈ Rn×r. This truncation significantly reduces the number of trainable parameters, so we compensate by increasing the number of off-diagonal coefficients (d) in M. Our results, with four different configurations ofr and d, are presented in Table 10. The findings show that a very low rank (r = 128) leads to poor performance, even when parameters are matched. A reasonably high rank of r = 1536, which is 75% of the full rank, still fails to match the performance of the full-rank variant that has 0.25× the trainable parameters. This indicates that all singular vectors 14significantly contribute to the end task performance when fine-tuning with SVFT , and that important information is lost even when truncating sparingly. Table 10: Performance with varying rank ( r) and the off-diagonal elements ( d) of M. When r = 2048, the update is full-rank. Rank (r) Diags ( d) #Params GSM-8K MATH 128 64 1.55M 0.98 0.21 1536 - 0.15M 16.37 3.64 1536 2 0.74M 25.01 6.04 2048 - 0.19M 40.34 14.38 C.4 Performance vs Total Trainable Parameters In addition to the experiments performed in Figure 1 for Gemma-2B on challenging natural language generation (NLG) tasks like GSM-8K and Commonsense Reasoning, we also plot the performance vs total trainable parameters for larger state-of-the-art models like Gemma-7B and LLaMA-3-8B on GSM-8K. Figure 5 further demonstrates SVFT’s Pereto-dominance. On larger models, we observe that full-finetuning overfits, leading to sub-optimal performance in comparison to PEFT methods. 0.5 0.75 1.2 2 3 5 8 12.5 20 32 50 84 Number of Trainable Params (M) 70 71 72 73 74 75 76 77 78 SVFTP SVFTB d = 2 SVFTR d = 16 DoRAr = 16 DoRAr = 4 LoRAr = 32 LoRAr = 1 DoRAr = 1 VeRAr = 1024 BOFTm = 2 b = 8 LoRAr = 4 Full Fine-Tuning (8500M params) 0.5 0.75 1.2 2 3 5 8 12.5 20 32 50 81 Number of Trainable Params (M) 62 64 66 68 70 72 74 76 78 80 SVFTP SVFTB d = 2 SVFTB d = 8 SVFTB d = 12 DoRAr = 16 LoRAr = 32 LoRAr = 1 DoRAr = 1 VeRAr = 1024 BOFTm = 2 b = 8 LoRAr = 4 Full Fine-Tuning (2500M params) Accuracy (%) Figure 5: Performance versus total trainable parameters for GSM-8K on Gemma-7B (left) and LLaMA-3-8B (right). C.5 Settings for Language Tasks Natural Language Understanding. We fine-tune the DeBERTaV3base [10] model and apply SVFT to all linear layers in every transformer block of the model. We only moderately tune the batch size, learning rate, and number of training epochs. We use the same model sequence lengths used by [ 19] to keep our comparisons fair. The hyperparameters used in our experiments can be found in Table 11. Natural Language Generation. See the hyperparameters used in our experiments in Table 12. For LoRA, DoRA, we adapt Q, K, V, U, Dmatrices. We apply BOFT on Q, Vmatrices since applying on multiple modules is computationally expensive. For VeRA, which enforces a constraint of uniform internal dimensions for shared matrices, we apply on G, Uprojection matrices as it yields the highest number of learnable parameters. We apply SVFT on Q, K, V, U, D, O, Gfor the Gemma family of models, and U, D, O, Gfor LLaMA-3-8B. Note that applying SVFT on these modules does not increase trainable parameters at the same rate as applying LoRA or DoRA on them would. We adopt the code base from https://github.com/meta-math/MetaMath.git for training scripts and evaluation setups and use the fine-tuning data available at https://huggingface.co/datasets/ meta-math/MetaMathQA-40K. 15Table 11: Hyperparameter setup used for DeBERTaV3base on the GLUE benchmark. Method Dataset MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B Optimizer AdamW Warmup Ratio 0.1 LR Schedule Linear Learning Rate (Head) 6E-03 Max Seq. Len. 256 128 320 64 512 320 320 128 # Epochs 10 10 30 20 10 6 15 15 SVFTP Batch Size 32 32 16 16 32 16 4 32 Learning Rate 5E-02 5E-02 5E-02 8E-02 8E-02 5E-02 5E-02 5E-02 SVFTRd=2 Batch Size 32 32 16 16 32 32 16 32 Learning Rate 1E-02 1E-02 1E-02 1E-02 3E-02 1E-02 3E-02 1E-02 Table 12: Hyperparameter setup used for fine-tuning on MetaMathQA-40K. Hyperparameter Gemma-2B Gemma-7B LLaMA-3-8B SVFT P SVFT R d=16 SVFT P SVFT R d=16 SVFT P SVFT R d=12 Optimizer AdamW Warmup Ratio 0.1 LR Schedule Cosine Learning Rate 5E-02 1E-03 5E-02 1E-03 5E-02 1E-03 Max Seq. Len. 512 # Epochs 2 Batch Size 64 Commonsense Reasoning. See the hyperparameters used in our experiments in Table 13. We adopt the same set of matrices as that of natural language generation tasks. We use the code base from https://github.com/AGI-Edgerunners/LLM-Adapters, which also contains the training and evaluation data. Table 13: Hyperparameter setup used for fine-tuning on commonsense-15K. Hyperparameter Gemma-2B Gemma-7B SVFT P SVFT B d=8 SVFT P SVFT B d=8 Optimizer AdamW Warmup Steps 100 LR Schedule Linear Max Seq. Len. 512 # Epochs 3 Batch Size 64 Learning Rate 5E-02 5E-03 5E-02 1E-03 16Table 14: Hyperparameter setup used for fine-tuning on all vision tasks. Hyperparameter ViT-B ViT-L Optimizer AdamW Warmup Ratio 0.1 Weight Decay 0.01 LR Schedule Linear # Epochs 10 Batch Size 64 SVFT P Learning Rate (Head) 4E-03 SVFT P Learning Rate 5E-02 SVFT B d=2 Learning Rate (Head) 4E-03 SVFT B d=2 Learning Rate 5E-02 SVFT B d=8 Learning Rate (Head) 4E-03 SVFT B d=8 Learning Rate 5E-03 C.6 Settings for Vision Tasks For each dataset in the vision tasks, we train on 10 samples per class, using 2 examples per class for validation, and test on the full test set. Similar to previous literature, we always train the classifier head for these methods since the number of classes is large. The parameter counts do not include the number of parameters in the classification head. The hyperparameters are mentioned in Table 14. We tune the learning rates for SVFT and BOFT select learning rates for other methods from [16], run training for 10 epochs, and report test accuracy for the best validation model. For all methods, since classification head has to be fully trained, we report the parameter count other than the classification head. 17",
      "meta_data": {
        "arxiv_id": "2405.19597v1",
        "authors": [
          "Vijay Lingam",
          "Atula Tejaswi",
          "Aditya Vavre",
          "Aneesh Shetty",
          "Gautham Krishna Gudur",
          "Joydeep Ghosh",
          "Alex Dimakis",
          "Eunsol Choi",
          "Aleksandar Bojchevski",
          "Sujay Sanghavi"
        ],
        "published_date": "2024-05-30T01:27:43Z",
        "pdf_url": "https://arxiv.org/pdf/2405.19597v1.pdf",
        "github_url": "https://github.com/VijayLingam95/SVFT"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces SVFT (Singular Vectors guided Fine-Tuning), a novel parameter-efficient fine-tuning (PEFT) method that addresses the performance gap and parameter efficiency limitations of existing methods like LoRA. SVFT fundamentally differs by making the weight update structure dependent on the specific pre-trained weight matrix, adapting it with a sparse weighted combination of its own singular vectors. The method recovers up to 96% of full fine-tuning performance while training only 0.006 to 0.25% of parameters, significantly outperforming existing PEFT methods that typically recover up to 85% performance using 0.03 to 0.8% of parameters. The paper also introduces four variants for parameterizing weight updates (Plain, Random, Banded, and Top-k) and theoretically demonstrates that SVFT can induce higher-rank perturbations for a fixed parameter budget compared to prior techniques.",
        "methodology": "SVFT operates by first computing the Singular Value Decomposition (SVD) of a pre-trained weight matrix W0 = UΣV^T. The weight update, ∆W, is then parameterized as UMV^T, where U and V are fixed (frozen) singular vectors, and M is a sparse trainable matrix with a pre-determined and fixed sparsity pattern. The forward pass becomes h = U(Σ + M)V^T x. Four choices for the sparsity pattern of M are explored: Plain (SVFTP), where M is diagonal, adapting only singular values; Banded (SVFTBd), where M is a banded matrix capturing local interactions; Random (SVFTRd), where k elements are randomly selected to be learnable; and Top-k (SVFTTd), where the top-k elements are chosen based on the alignment of left and right singular vectors. SVFT's algebraic structure allows it to induce high-rank perturbations in a constrained subspace efficiently, distinguishing it from LoRA and VeRA by using shared singular vectors across multiple rank-one updates.",
        "experimental_setup": "The research conducts extensive experiments on various language and vision benchmarks. For language tasks, it adapts encoder-only (DeBERTaV3base) and decoder-only models (Gemma-2B/7B, LLaMA-3-8B). Natural Language Generation (NLG) tasks include mathematical reasoning (GSM-8K and MATH, fine-tuned on MetaMathQA-40K) and 8 commonsense reasoning benchmarks (BoolQ, PIQA, SIQA, HellaSwag, Winogrande, ARC-easy/challenge, OpenBookQA). Natural Language Understanding (NLU) uses the GLUE benchmark. Vision tasks involve vision transformers (ViT-B/16 and ViT-L/16) on classification benchmarks: CIFAR-100, Food101, RESISC45, and Flowers102, fine-tuned on 10 samples per class. Baselines for comparison include Full Fine-Tuning (FT), LoRA, DoRA, BOFT, and VeRA. Performance is primarily measured by accuracy, Matthew's correlation for CoLA, and Pearson correlation for STS-B.",
        "limitations": "Despite its efficiency, SVFT incurs additional GPU memory usage compared to methods like LoRA. This is due to the necessity of computing the Singular Value Decomposition (SVD) and storing both the left and right singular vectors of the pre-trained weights. Although its memory consumption is still lower than BOFT, it is roughly double that of LoRA. The authors partially mitigate this through system-level optimizations such as mixed-precision weights (e.g., bfloat16).",
        "future_research_directions": "Future research directions include further exploring memory-reduction techniques, such as quantization, to address the increased GPU memory usage. Another key area is to develop principled methods for generating sparsity patterns for the trainable matrix M, which could potentially lead to further performance improvements beyond the current simple design choices. Optimizing the sparsity pattern of M, including efficient learned-sparsity methods, is also considered a promising avenue.",
        "experimental_code": "import time\nimport math\n\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_sparse import SparseTensor, transpose\n\n\ndef create_orthonormal_matrix(A):\n    # returns an orthonormal matrix (square) of size (min(A.shape), min(A.shape))\n    Q, R = torch.qr(A)\n    return Q\n\n\ndef get_target_modules_list(model, target_modules):\n    target_names = []\n    for n, _ in model.named_modules():\n        if any(t in n for t in target_modules):\n            target_names.append(n)\n    return target_names\n\n\ndef replace_svft_with_fused_linear(model, target_modules_list):\n    print(\"Replacing SVFT layers with new Linear layers\")\n\n    # filter out svft layer\n    target_modules_list = [l for l in target_modules_list if \"svft_layer\" not in l]\n\n    for target_path in tqdm(reversed(target_modules_list), total=len(target_modules_list)):\n        parent_path = target_path[: target_path.rfind(\".\")] if \".\" in target_path else \"\"\n        target_name = target_path.split(\".\")[-1]\n        parent = model.get_submodule(parent_path) if parent_path else model\n        target = model.get_submodule(target_path)\n        in_dim = target.svft_layer.v.shape[1]\n        out_dim = target.svft_layer.u.shape[0]\n        if target.bias is None:\n            lin = torch.nn.Linear(in_dim, out_dim, bias=False)\n        else:\n            lin = torch.nn.Linear(in_dim, out_dim, bias=True)\n            lin.bias.data = target.bias.data\n        lin.weight.data = target.merge_and_unload()\n        parent.__setattr__(target_name, lin)\n\n\ndef create_and_replace_modules(model, target_modules_list, create_fn):\n    print(\"Replacing Linear layers with SVFT layers\")\n\n    for target_path in tqdm(reversed(target_modules_list), total=len(target_modules_list)):\n        parent_path = target_path[: target_path.rfind(\".\")] if \".\" in target_path else \"\"\n        target_name = target_path.split(\".\")[-1]\n        parent = model.get_submodule(parent_path) if parent_path else model\n        target = model.get_submodule(target_path)\n        parent.__setattr__(target_name, create_fn(target))\n\n\nclass SVFTLayer(nn.Module):\n    def __init__(self, u, s, v, off_diag, pattern=\"banded\", rank=None, fill_orthonormal=False):\n\n        \"\"\"\n        @inputs:\n            u: torch.Tensor. Left singular vectors of pre-trained weight matrix\n            s: torch.Tensor. Singular values of pre-trained weight matrix\n            v: torch.Tensor. Right singular vectors of pre-trained weight matrix\n            off_diag: int. Total off-diagonals to be used to populate matrix M (as referred in main paper)\n            pattern: str. Choices: \"banded\", \"random\", \"top_k\". Using \"banded\" with off_diag=1 simulates SVFT-plain\n            rank: int. Constraints how many singular vectors and values to use.\n            fill_orthonormal: bool. To determine if random orthonormal basis should be used\n        \"\"\"\n\n        super().__init__()\n\n        self.off_diag = off_diag\n        rank = s.shape[0] if rank is None else min(s.shape[0], rank)\n        self.n = rank\n        diff_rank = s.shape[0] - rank\n\n        if fill_orthonormal:\n            Q_u = torch.randn_like(u).to(s.device)\n            torch.nn.init.orthogonal_(Q_u)\n            Q_v = torch.randn_like(v).to(s.device)\n            torch.nn.init.orthogonal_(Q_v)\n\n            u = torch.cat([u[:, :rank], Q_u[:, :diff_rank]], dim=1)\n            v = torch.cat([v[:rank, :], Q_v[:diff_rank, :]], dim=0)\n            s = torch.cat([s[:rank], torch.zeros(diff_rank).to(s.device)], dim=0)\n            self.n = s.shape[0]\n\n        else:\n            s = s[:rank]\n            u = u[:, :rank]\n            v = v[:rank, :]\n\n        self.u = nn.Parameter(u.clone().detach().contiguous(), requires_grad=False)\n\n        s_pre = s.cpu().detach().clone().contiguous()\n        self.s_pre_edge_index = torch.sparse.spdiags(s_pre, torch.LongTensor([0]), (self.n, self.n)).coalesce().indices()\n        self.s_pre = nn.Parameter(s_pre, requires_grad=False)\n        \n        if pattern==\"banded\":  \n            diags = 2*self.off_diag + 1\n            offsets_positive = torch.arange(0, self.off_diag+1)\n            offsets_negative = torch.arange(-1, -self.off_diag-1, -1)\n            self.offsets  = torch.cat([offsets_positive, offsets_negative])\n            self.s_edge_index = torch.sparse.spdiags(torch.randn([diags, self.n]), self.offsets, (self.n, self.n)).coalesce().indices()\n            self.s = torch.nn.Parameter(torch.zeros(self.s_edge_index.shape[1]), requires_grad=True)\n\n        elif pattern==\"random\":\n            print(\"Random pattern\")\n            k = self.n*(2*self.off_diag+1) - self.off_diag*(self.off_diag+1)\n            rows = torch.randint(0, self.n, (k,))\n            cols = torch.randint(0, self.n, (k,))\n            self.s_edge_index = torch.stack([rows, cols])\n            self.s = torch.nn.Parameter(torch.zeros(k), requires_grad=True)\n\n        elif pattern==\"top_k\":\n\n            if u.shape == v.shape:\n                coeffs = u@v.T\n            else:\n                coeffs = u if u.shape[0]==u.shape[1] else v\n\n            k = self.n*(2*self.off_diag+1) - self.off_diag*(self.off_diag+1)\n            # Flatten the tensor to 1D\n            flattened_tensor = coeffs.contiguous().view(-1)\n            _, top_indices_flat = torch.topk(flattened_tensor, k)\n            num_rows, num_cols = coeffs.size()\n            rows = top_indices_flat // num_cols\n            cols = top_indices_flat % num_cols\n            self.s_edge_index = torch.stack([rows, cols])\n            self.s = torch.nn.Parameter(torch.zeros(k), requires_grad=True)\n       \n        torch.nn.init.kaiming_normal_(self.s[None, :])\n        self.s.squeeze()\n\n        self.register_buffer('s_pre_row', self.s_pre_edge_index[0])\n        self.register_buffer('s_pre_col', self.s_pre_edge_index[1])\n        self.register_buffer('s_row', self.s_edge_index[0])\n        self.register_buffer('s_col', self.s_edge_index[1])\n\n        self.gate = nn.Parameter(torch.tensor([0.], dtype=torch.float32), requires_grad=True)\n\n        self.v = nn.Parameter(v.clone().detach().contiguous(), requires_grad=False) \n\n\n    def forward(self, x):\n        x  = x @ self.get_weights() \n        return x\n\n\n    def get_weights(self):\n        s = SparseTensor(row=self.s_row, col=self.s_col, value=self.s*F.sigmoid(self.gate))\n        s_pre = SparseTensor(row=self.s_pre_row, col=self.s_pre_col, value=self.s_pre)\n        del_s = s_pre + s\n        weight = (del_s @ self.v).T\n        weight = weight @ self.u.T\n        return weight\n    \n\n    def merge_and_unload(self):\n        return self.get_weights().T.contiguous()\n\n   \nclass LinearWithSVFT(nn.Module):\n\n    def __init__(self, linear, off_diag, pattern=\"banded\", rank=None, fill_orthonormal=False):\n        \"\"\"\n        @inputs:\n                linear: torch.Tensor. Linear Layer that has to adapted\n                off_diag: int. total number off diagonals to be used if pattern is 'banded' \n                          for remaining patterns, equivalent number of learnable parameters are learnt\n                rank: SVD rank \n                fill_orthonormal: bool. To determine if random orthonormal basis should be used\n        \"\"\"\n        \n        super().__init__()\n\n        self.bias = linear.bias\n\n        # since linear.weight is on GPU, computing SVD will be significantly faster\n        svd = torch.linalg.svd(linear.weight, full_matrices=False)\n\n        self.svft_layer = SVFTLayer(svd[0], \n                                    svd[1], \n                                    svd[2], \n                                    off_diag=off_diag, \n                                    pattern=pattern, \n                                    rank=rank, \n                                    fill_orthonormal=fill_orthonormal)\n\n    def forward(self, x):\n        if self.bias is not None:\n            return self.svft_layer(x) + self.bias\n\n        else:\n            return self.svft_layer(x)\n\n    def merge_and_unload(self):\n        return self.svft_layer.merge_and_unload()\n\n\ndef freeze_model(model, exclude_list = None):\n    ''' Freeze all parameters of the model '''\n    if exclude_list is None:\n        exclude_list = []\n\n    for n, p in model.named_parameters():\n        if not any(e in n for e in exclude_list):\n            p.requires_grad = False",
        "experimental_info": "The SVFT (Singular Value Fine-Tuning) method adapts pre-trained weight matrices by leveraging their Singular Value Decomposition (SVD). A weight matrix W0 = UΣV^T is updated by parameterizing the change ∆W as UMV^T. In this formulation, U and V are fixed (frozen) singular vectors, and M is a sparse trainable matrix. The forward pass is computed as h = U(Σ + M)V^T x.\n\nKey experimental settings for SVFT include:\n-   **pattern**: Specifies the sparsity pattern of the trainable matrix M. Choices are:\n    -   \"banded\" (SVFTBd): M is a banded matrix, capturing local interactions. \"Plain\" SVFT (SVFTP), where M is diagonal, can be simulated using \"banded\" with `off_diag=0` or `1` (depending on exact implementation).\n    -   \"random\" (SVFTRd): `k` elements of M are randomly selected to be learnable.\n    -   \"top_k\" (SVFTTd): `k` elements of M are chosen based on the alignment of left and right singular vectors.\n-   **off_diag**: An integer parameter that controls the complexity or sparsity of M. For \"banded\" pattern, it directly defines the number of off-diagonal blocks. For \"random\" and \"top_k\" patterns, it contributes to determining the total number of `k` learnable elements in M.\n-   **rank**: An optional integer specifying the rank for truncated SVD. If provided, only the top `rank` singular values and vectors are used, constraining the subspace for perturbations.\n-   **fill_orthonormal**: A boolean flag (defaulting to False) that, when True, initializes the additional singular vectors from a random orthonormal basis if a truncated rank is used and the effective rank is less than the full intrinsic rank.\n-   **target_modules**: A list of strings indicating the names of the linear layers in the base model to which the SVFT adaptation is applied. Common targets include attention projection layers (e.g., `q_proj`, `v_proj`, `k_proj`, `o_proj`, `query`, `key`, `value`) and feed-forward network layers (e.g., `up_proj`, `down_proj`, `gate_proj`).\n-   **gate**: A trainable scalar parameter that is applied as a sigmoid activation to scale the magnitude of the learned sparse matrix M, thereby controlling its influence on the original singular values."
      }
    },
    {
      "title": "Make Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning",
      "abstract": "Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs)\nhas emerged as a highly successful approach, with training only a small number\nof parameters without sacrificing performance and becoming the de-facto\nlearning paradigm with the increasing size of PLMs. However, existing PEFT\nmethods are not memory-efficient, because they still require caching most of\nthe intermediate activations for the gradient calculation, akin to fine-tuning.\nOne effective way to reduce the activation memory is to apply a reversible\nmodel, so the intermediate activations are not necessary to be cached and can\nbe recomputed. Nevertheless, modifying a PLM to its reversible variant is not\nstraightforward, since the reversible model has a distinct architecture from\nthe currently released PLMs. In this paper, we first investigate what is a key\nfactor for the success of existing PEFT methods, and realize that it's\nessential to preserve the PLM's starting point when initializing a PEFT method.\nWith this finding, we propose memory-efficient fine-tuning (MEFT) that inserts\nadapters into a PLM, preserving the PLM's starting point and making it\nreversible without additional pre-training. We evaluate MEFT on the GLUE\nbenchmark and five question-answering tasks with various backbones, BERT,\nRoBERTa, BART and OPT. MEFT significantly reduces the activation memory up to\n84% of full fine-tuning with a negligible amount of trainable parameters.\nMoreover, MEFT achieves the same score on GLUE and a comparable score on the\nquestion-answering tasks as full fine-tuning. A similar finding is also\nobserved for the image classification task.",
      "full_text": "Make Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning Baohao Liao Shaomu Tan Christof Monz Language Technology Lab, University of Amsterdam {b.liao, s.tan, c.monz}@uva.nl Abstract Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs) has emerged as a highly successful approach, with training only a small number of parameters without sacrificing performance and becoming the de-facto learning paradigm with the increasing size of PLMs. However, existing PEFT methods are not memory-efficient, because they still require caching most of the intermediate activations for the gradient calculation, akin to fine-tuning. One effective way to reduce the activation memory is to apply a reversible model, so the intermediate activations are not necessary to be cached and can be recomputed. Nevertheless, modifying a PLM to its reversible variant is not straightforward, since the reversible model has a distinct architecture from the currently released PLMs. In this paper, we first investigate what is a key factor for the success of existing PEFT methods, and realize that it’s essential to preserve the PLM’s starting point when initializing a PEFT method. With this finding, we propose memory-efficient fine-tuning (MEFT) that inserts adapters into a PLM, preserving the PLM’s starting point and making it reversible without additional pre-training. We evaluate MEFT on the GLUE benchmark and five question-answering tasks with various backbones, BERT, RoBERTa, BART and OPT. MEFT significantly reduces the activation memory up to 84% of full fine-tuning with a negligible amount of trainable parameters. Moreover, MEFT achieves the same score on GLUE and a comparable score on the question-answering tasks as full fine-tuning. A similar finding is also observed for the image classification task.1 1 Introduction 20 40 60 80 100 Activation Memory (%) 60 65 70 75 80 85 90Score (%) BERT RoBERTa BART OPT Full FT Prefix-Tuning LoRA MAM AdapterH AdapterP LST our MEFT1 Figure 1: Average performance of different tasks vs. activation mem- ory. The memory usage for full fine- tuning is denoted as 100%. Large-scale pre-trained models have achieved great success across various domains and applications [1, 2, 3, 4, 5, 6, 7, 8]. As their capabilities continue to evolve, the released pre-trained language models (PLMs) have grown exponentially in size, even reaching a scale of 100 billion parameters [ 3, 9, 10, 11, 12]. Consequently, it presents unprecedented challenges in effectively leveraging these models for downstream tasks due to limited computing resources. A historically common approach to adapting PLMs to down- stream tasks is updating all pre-trained parameters, full fine- tuning. Although full fine-tuning has yielded numerous state-of- the-art results, its applicability is limited in storage-constrained environments. This constraint arises from maintaining a com- plete copy of the fine-tuned model for each task. An alternative 1Code at https://github.com/baohaoliao/mefts. Up-to-date version at https://arxiv.org/abs/2306.00477. 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2306.00477v4  [cs.CL]  19 Oct 2023adaptation approach is parameter-efficient fine-tuning (PEFT) [13, 14, 15, 16, 17, 18, 19] which involves selectively updating a small number of task-specific parameters while keeping the majority of the PLM’s parameters frozen. PEFT offers significant advantages in reducing storage requirements by only saving one general PLM alongside the modified parameters for each task. In addition to storage savings, PEFT achieves comparable performance to full fine-tuning, sparking considerable interest in the adoption of PEFT. Despite their advantages in parameter efficiency, existing PEFT methods still face challenges in terms of memory efficiency [ 20, 21]. PEFTs necessitate the caching of intermediate activations, similar to the requirements of full fine-tuning, to calculate the gradients of the trainable parameters. Typically, they consume more than 70% activation memory of full fine-tuning (see Figure 1). Since activations significantly contribute to the memory requirements during training, there are instances where fine-tuning a large-scale PLM with PEFT is not feasible due to memory constraints. To address this issue, a commonly employed approach is to treat the PLM as a feature extractor, such as knowledge distillation to a smaller model [22, 23], adding additional trainable layers on top [20] or aligned [21, 24] with it, and so on. These approaches circumvent the need to store the PLM’s activations since the gradient computation graph does not traverse through the PLM. However, these methods often require additional pre-training or exhibit a substantial performance gap compared to full fine-tuning when using the same underlying model [20, 21]. In this paper, we propose a novel method calledmemory-efficient fine-tuning (MEFT) to modify PLMs in a parameter- and memory-efficient manner, without requiring additional pre-training. Initially, we investigate a crucial factor for the success of existing PEFT methods and determine that the proper initialization of newly added parameters is essential to maintain the continuity of information from the PLM (§2). Leveraging this insight, we design three MEFT methods that enable the modification of a PLM to its reversible variant, so it only necessitates caching the final output and allows for the recomputation of intermediate activations during back-propagation (§3). Consequently, MEFT significantly reduces the memory required for caching activations (see Figure 1). To validate the effectiveness of our MEFT methods, we conduct extensive evaluations on the GLUE benchmark [25] with BERT [ 1], RoBERTa [ 2] and BART [ 26] (§4). The experimental results consistently demonstrate that our MEFT methods outperform both full fine-tuning and strong PEFT baselines in terms of parameter and memory efficiency. Remarkably, our methods achieve the same score as full fine-tuning while updating only 0.2% of the parameters and saving up to 84% of the activation memory. Furthermore, we evaluate MEFT on five question-answering tasks with a larger model, OPT [9]. The results show that our approach achieves a comparable score as full fine-tuning while saving 50% of the activation memory and updating only 0.64% of the parameters. A similar finding is also observed on the image classification task, SVHN [27]. Collectively, these experiments establish the effectiveness of MEFT as a powerful parameter- and memory-efficient approach that does not compromise performance. 2 Preliminaries In this section, we aim to provide essential background knowledge by addressing the following questions: (1) Why are existing PEFTs not sufficiently memory-efficient (§2.1)? (2) What is a key factor for the success of PEFT (§2.2)? (3) What challenges does a reversible model have (§2.3)? 2.1 Parameter-efficient fine-tuning is not sufficiently memory-efficient Given a N multilayer perception: hN = fN (fN−1(...(f2(f1(h0)))...)) with h0 as the initial input, the nth layer hn = fn(hn−1) =σn(Wnhn−1) consists of a nonlinear function σn and a weight ma- trix Wn, where the bias term is ignored for simplicity. Denotingxn = Wnhn−1, in backpropagation with a loss L, the gradient of Wn is calculated with the chain rule as: ∂L ∂Wn = ∂L ∂hN ( NY i=n+1 ∂hi ∂xi ∂xi ∂hi−1 ) ∂hn ∂xn ∂xn ∂Wn = ∂L ∂hN ( NY i=n+1 σ′ iWi)σ′ nhn−1 (1) where σ′ is the derivative of σ and the calculation of σ′ n requires xn. Therefore, {xi}N i=n are cached during the forward pass to obtain the gradient of Wn, even though {Wi}i>n are frozen. 20 5 10 15 20 25 Fine-tuned Parameters (%) 24 25 26 27 28 29 30 31Memory Usage (GB) Full FT LoRA Pfeiffer Adapter Houlsby Adapter (IA)3 Prefix-Tuning (a) Memory trade-off. 0 0.05 0.1 0.5 1 0 10 20 30 40 50 60 70 80 0 0.1 0.5 0.9 c Average Score (%) default init default init with random PLM α  =  1 α  =  0, trainable α α  =  1 / c (b) Initialization effect, Left: LoRA, Right: (IA)3. Figure 2: Exploration of existing PEFTs: (a) The trade-off between memory and the number of trainable parameters. The dashed and solid lines denote the peak and activation memory, respectively. The model size for BERT base is 0.4GB2. (b) The initialization effect of PEFT on RoBERTa base. Random PLM denotes that we initialize the backbone randomly instead of using a pre-trained model. During training, the peak memory footprint is mainly occupied by three components: model’s parameters {Wn}N n=1, optimizer state whose size is three times as large as the size of trainable parameters for Adam [28] (one for gradient and two for moments), and activations. The memory footprint for all three components is related to the model’s depth and width. In addition, the memory footprint for activations is also related to some training settings, like batch size and sequence length. Compared to full fine-tuning, existing PEFT methods, such as (Houlsby and Pfeiffer) Adapters [14, 16], LoRA [17], (IA)3 [29], Prompt-Tuning [19] and Prefix-Tuning [15], tune a small number of parameters, making the size of the optimizer state negligible. However, the memory footprint required for activations is not significantly reduced. As shown in Figure 2a, where we set the batch size as 64 and the sequence length as 512 on RTE [30, 31, 32, 33] with BERTbase [1], the activation memory of all PEFT methods is >75% of full fine-tuning, even with <1% trainable parameters. 2.2 Initialization is significant for parameter-efficient fine-Tuning Pre-trained models learn generic and distributed enough representations to facilitate downstream learning of highly pressed task representation [36], i.e. offering a robust starting point for the training of downstream tasks. When modifying a PLM with PEFT, we hypothesize that one needs to preserve this starting point at the beginning of training for better performance. The Starting Point Hypothesis. When modifying a pre-trained model by adding new parameters, one needs to initialize the new parameters in a way to preserve the starting point from the pre- trained model at the beginning of training, such that fine-tuning the modified model can match the performance of full fine-tuning. More formally, supposed fn is a PLM layer and hn = fn(hn−1), the output from a modified layer f′ n, h′ n = f′ n(hn−1), should be close to hn at the beginning of training. I.e. h′ n = hn + δ, where ∥δ∥ →0. Intuitively, we want h′ n ≈ hn, because h′ n is the input to the next (modified) PLM layer. If they are dissimilar, the representation continuity will be broken down. Though most PEFT methods [14, 16, 17, 29] initialize their added modules in this way, we couldn’t find a thorough investigation of the significance of this initialization in the existing literature. In this section, we explore the significance of PEFT’s initialization for two methods, LoRA and (IA)3 [29]. LoRA and (IA)3 represent two common methods for introducing new parameters, involving addition and scaling operations, respectively. Given a pre-trained weight matrix W ∈ Rd×d, LoRA modifies it as h′ = (W + α r WdownWup)h, where Wdown ∈ Rd×r and Wup ∈ Rr×d are the added trainable parameters, α is a constant scale factor and normally r ≪ d. LoRA’s default initialization is Wdown ∼ N(0, σ2) and Wup = 0. In this way, WdownWup = 0 and the starting point from the 2Though we train in FP16, the PLM is first loaded in FP32, then auto-casted to FP16 for the forward pass in Transformers [34]. Since the memory required for the model in FP32 is always there during training, we report the memory for models in FP32 in this paper (see Table 9). More discussions about this are here. We believe it’s a bug in the framework and can be resolved with further investigation. Especially Huggingface’s new PEFT framework [35] allows loading INT8 model for fine-tuning. 3(a) Reversible architecture. 8 16 32 64 Numbe of Layers −8 −6 −4 −2 0 [4, 4] [2, 2] [1, 1] [0.5, 0.5][0.1, 0.1] [1, 0.1] [0.1, 1] [λ, β] 1.31e-09 2.21e-09 5.37e-09 5.9e-06 0.031 0.0165 0.0171 [0, 0.02] [0, 0.1] [0.3, 0.1] [1, 0.02] [μ, σ] 5.37e-09 6.42e-08 2.44e-08 1.78e-08 Reconstruction Error (log10) max mean max mean (b) Instability of reversible model. Figure 3: (a) F and G are two arbitrary functions (sub-networks), taking two inputs, h1 n and h2 n (b) Reconstruction error between the vanilla and reversible gradients. The default setting is RevViT [40] with 8 layers, λ = 1, β = 1, µ = 0 and σ = 0.02. Left: Different number of layers. Middle: Different scaling values. Right: Initialization with different means and standard deviations. PLM is preserved perfectly. (IA) 3 modifies W by multiplying it to a trainable vector l ∈ Rd as h′ = (l ⊙ W)h, where ⊙ represents element-wise multiplication. The default initialization of (IA)3 is l = 1, also making the starting point untouched. To facilitate the initialization process of LoRA, we opt for the following initial values: Wdown = 1, Wup = c and α = 1, where c is a matrix with all elements equal to an initialized value c, resulting in α r WdownWup = c. When c = 0, the starting point from a PLM is preserved. By adjusting c, we exert control over the degree of departure from the starting point. Similarly, we replace l with l′ = αl = αc for (IA)3. In Figure 2b, we train the newly added parameters on RoBERTabase [2] for four tasks (CoLA [37], STS-B [38], MRPC [ 39] and RTE [ 30, 31, 32, 33]). For LoRA ( r = 8), though we modify the initialization method, our result (c = 0) is very close to the default initialization. When the starting point is broken by c ̸= 0 (α = 1), all results are even worse than a randomly initialized model. However, when we set α = 03 to preserve the starting point, all results become much better than the ones with α = 1. For (IA)3, when we decrease c from 1 (default initialization) to 0, the results (α = 1) become worse and worse. However, when we set α = 1/c to preserve the starting point, all results become better. Some of them are even better than the default initialization. All of the above-mentioned results show that it’s significant to preserve the starting point from a PLM at the beginning of training when applying or designing a PEFT method. A different initialization scheme is in Figure 10 which leads to a similar finding. 2.3 Challenges of reversible neural network Recapping a reversible model [41] in Figure 3a, one can reconstruct inputs from outputs as: h1 n+1 = λh1 n + Fn(h2 n) h2 n+1 = βh2 n + Gn(h1 n+1) h2 n = (h2 n+1 − Gn(h1 n+1))/β h1 n = (h1 n+1 − Fn(h2 n))/λ (2) where λ and β are scaling factors. Theoretically, Fn and Gn could be two arbitrary functions (sub- networks). Given a multilayer reversible network, intermediate activations for each layer during the forward pass are not necessary to be cached. One only needs to store the final outputs, then reconstruct the intermediate activations and calculate the gradient layer-by-layer in a backward manner (See Listing 1 in §Appendix). In this way, the memory footprint required for activations can be reduced significantly and has no relationship with the model’s depth, i.e. O(1) instead of O(N). To investigate the training stability of a reversible model, we run experiments on RevViT [ 40].4 RevViT shares the same architecture as Reformer [42], except applying a convolutional layer at the beginning to project an image into a sequence of vectors. When running RevViT, one could still cache the intermediate activations and treat it as an irreversible model. We term the gradient calculated in this way as vanilla gradient. One could also train RevViT in a reversible way, and the corresponding 3α has to be trainable when α = 0. Otherwise, the newly added parameters are useless. 4Our experiments are based on this file, https://github.com/karttikeya/minREV/blob/main/rev.py. 4F+ F+G+ ℎ!\"ℎ!# ℎ!#ℎ!\" ℎ!$## ℎ!$#\" ℎ%#= ℎ%\"= ℎ% + + +𝜆 𝜆 𝛽 switch? (a) Mutli-head Attention ++ Feed-Forward Layer Norm Feed-Forward Feed-Forward ++Layer Norm Attention Block MLP Block (b) Feed-Forward Feed-Forward ++Layer Norm Nonlinear Nonlinear Attention Block Adapter 𝑾𝒅𝒐𝒘𝒏 𝑾𝒖𝒑 𝑾𝒖𝒑 𝑾𝒅𝒐𝒘𝒏  (c) Mutli-head Attention ++ Feed-Forward Layer Norm Adapter ++ Feed-ForwardFeed-ForwardAdapter Layer Norm  (d) Figure 4: MEFT architectures. (a) Unfold reversible model. (b) A PLM layer. (c) Two MEFT architectures: (1) F is the PLM layer with an adapter (up) and G is an adapter (down); (2) G is the PLM layer with an adapter (up) and F is an adapter (down). (d) The third MEFT architecture: G is the MLP block with an adapter (up) and F is the attention block with an adapter (down). For initialization, Wdown, Wup ∼ N(0, σ2) and σ = 0.02. Only the adapter is trainable. gradient is called reversible gradient. We input the same random noises into the same RevViT twice to obtain the parameter gradients from the convolutional layer, in a vanilla and reversible way. Then we calculate the absolute difference between these two gradients and report the maximum and mean values. In this way, we want to check whether the vanilla gradient can be reconstructed in a reversible way. If the reconstruction error is large, it means that the vanilla gradient could not be recovered in a reversible way due to numerical stability, which might cause unstable training or bad performance. As shown in Figure 3b, with an increasing number of layers in RevViT, the reconstruction error becomes larger, but still around10−8 which is negligible. However, RevViT is sensitive to the scaling factors, λ and β. When both scaling factors or one of them are less than 1, the reconstruction error increases dramatically. We also explore the initialization of the linear layers in RevViT and find that a larger standard deviation or mean can cause a bigger reconstruction error. In sum, a larger number of layers, smaller scaling factors (< 1) and a larger standard deviation or mean for initialization tend to cause a bigger reconstruction error, which might result in the unstable training or low performance of a reversible model. Last but not least, RevViT [40] finds that residual connections inside F and G deteriorate the performance of a reversible Transformer [43].5 3 Memory-efficient fine-tuning Table 1: A summarization of three MEFT methods. MEFT? F G λ β Switch h1 n h2 n 1 layer adapter → 0 any ✓ βhn−1 hn 2 adapter layer → 1 → 0 ✓ hn hn−1 3 attention MLP → 0 → 0 ✗ − hn This paper aims to modify a PLM to its reversible variant without addi- tional pre-training, so the PLM can still be fine-tuned with a limited mem- ory footprint. The fundamental guid- ing principle behind our design is: pre- serving the starting point from a PLM to the greatest extent possible (discus- sion in §2.2). In this section, we propose three methods to modify a PLM to a reversible one. 3.1 MEFT 1: PLM layer as F, adapter as G As shown in Figure 4c, we design F as a pre-trained layer with an adapter, where the insertion position for the adapter is borrowed from He et al. [18]. G is simply an adapter. We initialize the adapters as Wdown, Wup ∼ N(0, σ2), same for the following methods. In this way, the output from the adapter is close to 0 at the beginning of the training, so hn ≈ Fn(hn−1). For the following discussion, we only focus on the beginning of the training, making sure our design preserves the starting point from a PLM. h0 and h1 are the input to and output from the 1st layer of a PLM without any modification, respectively. I.e. h0 is the representation after the position and word embedding layers of a PLM. We 5In RevViT,F and G are the attention and MLP block (Figure 4b) without residual connection, respectively. 5assign h1 0 = h2 0 = h0, same for the following methods. At the beginning of the training (see Figure 4a), h1 1 = λh1 0 + F1(h2 0) =λh0 + F1(h0) ≈ λh0 + h1, h2 1 = βh2 0 + G1(h1 1) =βh0 + G1(h1 1) ≈ βh0, where the approximation holds because of our initialization of the adapters. For now, h1 1 and h2 1 are not desired. When we input h1 1 and h2 1 to the 2nd reversible layer, especially when we input h2 1 to F2, the representation continuity6 is broken, because h2 1 ̸= h1. We introduce two modifications to address this issue: (1) We set λ → 0, so h1 1 ≈ h1. (2) Then we switch the order of h1 1 and h2 1 before feeding to the next reversible layer, i.e. making h1 1 ≈ βh0 and h2 1 ≈ h1. In this way, h2 1 preserves the starting point. We don’t require h1 1 to preserve any starting point, because it is entered to G2 which is not a pre-trained layer. With the same above-mentioned design for the 2nd reversible layer, we obtain h1 2 ≈ βh1 and h2 2 ≈ h2. By analogy, h1 n ≈ βhn−1 and h2 n ≈ hn, which means h2 n always preserves the starting point from the PLM. Feeding h2 n to the next reversible layer, Fn+1, doesn’t break the representation continuity. After all layers, we input h′ N = (h1 N + h2 N )/2 to a task-specific head that is a brand new layer, same for the following methods.7 3.2 MEFT 2: Adapter as F, PLM layer as G Opposite to MEFT 1, we design F as an adapter and G as the PLM layer with an adapter for MEFT2 (see Figure 4c). In this case, we need to make sure that the input to G preserves the starting point. Let’s also start with the first layer, h1 1 = λh1 0 + F1(h2 0) = λh0 + F1(h0) ≈ λh0, h2 1 = βh2 0 + G1(h1 1) =βh0 + G1(h1 1) ≈ βh0 + G1(λh0), where the approximation holds because of our initialization of the adapters. To preserve the starting point from the PLM, we setλ → 1, β → 0 and switch the order of h1 1 and h2 1 before feeding to the next reversible layer. When setting λ → 1, we make sure the representation continuity is preserved for G1, resulting in h2 1 ≈ βh0 + h1. When β → 0 and the order of h1 1 and h2 1 is switched, h1 1 ≈ h1 and h2 1 ≈ h0. In this way, h1 1 preserves the initialization point, and we won’t break the representation continuity when feeding it to G2 in the next reversible layer. With the same setting for each layer, h1 n ≈ hn and h2 n ≈ hn−1, so h1 n always preserves the starting point. 3.3 MEFT 3: Attention block as F, MLP block as G As shown in Figure 4d, we can also design F as the pre-trained attention block with an adapter and G as the pre-trained MLP block with an adapter. Also starting with the first layer, we obtain h1 1 = λh1 0 + F1(h2 0) =λh0 + F1(h0), h2 1 = βh2 0 + G1(h1 1) =βh0 + G1(h1 1). λ → 0 is required, so h1 1 approximates the original output from the pre-trained attention block, and can be fed to G1 to preserve the starting point. β → 0 is also required, so h2 1 ≈ h1, and can be fed to F2 in the next reversible layer. By default, we set λ = β → 0. For MEFT3, one doesn’t need to switch the order of h1 1 and h2 1 before feeding to the next reversible layer. For each layer, h1 n is close to the original output from the attention block of the corresponding PLM layer, and h2 n ≈ hn. Compared to the vanilla RevNet [41] where λ = β = 1, we meticulously assign different values to λ and β to preserve the starting point from a PLM, and switch the order of the outputs before feeding to the next layer (if necessary) to preserve the representation continuity. We summarize the settings for all three MEFT methods in Table 1. 4 Experiments 4.1 Experimental setup Datasets and evaluation. We evaluate MEFTs on eight sequence representation tasks and five sequence-to-sequence tasks. All sequence representation tasks are from the GLUE benckmark [25]. 6The presentation continuity and the starting point hypothesis emphasize two aspects. The presentation continuity, for example, shows that one can’t feedh0 to the third pre-trained layer, focusing on the input. The starting point hypothesis shows that the output from a modified pre-trained layer should be close to the output from the original pre-trained layer, focusing on the output. However, they are also very related, since the output from the current layer is the input to the next layer. 7Read Appendix §C for a step-by-step tutorial if you still feel confused. 60.050.1 0.3 0.5 1 λ (MEFT1) or β (MEFT2) or λ = β (MEFT3) 60.0 62.5 65.0 67.5 70.0 72.5 75.0 77.5Average Score (%) Full FT MEFT1 MEFT2 MEFT3 Figure 5: MEFTs with vari- ous scaling factors on BERTbase over RTE, MRPC, STS-B and CoLA. Dashed and solid lines de- note MEFTs with vanilla and re- versible gradients, respectively. 242016128 81 82 83 84 85Average Score (%) Full FT LoRA freeze: false freeze: true 242016128 1 2 3 4 5 6Memory (GB) Number of Reversible Layers Figure 6: The trade-off between the performance and activation memory with MEFT1 on RoBERTalarge over RTE, MRPC, STS- B and CoLA. The line annotated by ‘freeze: true’ means the shallower PLM layers are frozen without any adaptation, while the line annotated by ‘freeze: false’ means the top MEFT layers with vanilla gradient, as shown in Figure 7. The sequence-to-sequence tasks are question-answering benchmarks, including OpenBookQA [44], PIQA [45], ARC (easy and challenge) [46] and SciQ [47]. We show the statistics of these datasets in Table 8 in Appendix. For the GLUE benchmark, we report accuracy on MNLI, QQP, QNLI, SST-2, MRPC and RTE, Pearson correlation coefficient on STS-B (if not specially mentioning) and Matthews correlation coefficient [48] on CoLA. We report accuracy on all question-answering tasks. In addition, we report all results on the development sets as our baselines. Models. We use the encoder-only models (BERTbase [1], RoBERTalarge [2] and BARTlarge encoder [26]) as the underlying models for all GLUE tasks, and the decoder-only models (OPT 1.3B and OPT6.7B [9]) for question-answering tasks. (See Table 9 in Appendix for model details.) Baselines. The most important baseline is full fine-tuning ( Full FT) that updates all parameters. Houlsby Adapter ( AdapterH) [ 14], Pfeiffer Adapter ( AdapterP) [ 16], Prefix-Tuning [15] and LoRA [17] are chosen as PEFT baselines. In addition, two unified PEFT methods, MAM [18] and AutoPEFT [49], that combine multiple PEFT methods are also chosen as PEFT baselines. Lastly, two feature-based tuning methods, Y-Tuning [20] and LST[21], that aim to reduce training memory serve as memory-efficient baselines. We report the baseline results from the original papers if possible. Implementation. For computational efficiency, we set β = 1for MEFT1, λ = 1for MEFT2, and only tune the factors that are required → 0 (see Table 1). After obtaining the optimal value, i.e. 0.1, we use this value for all three MEFT methods, tasks and backbones. On the GLUE benchmark, we sweep learning rates in {3, 4, 5}·10−4, batch sizes in {16, 32} and the number of epochs in {10, 20} for the tasks with >10k training samples. For the low-resource tasks with <10k training samples, we sweep learning rates in {5, 6, 7, 8} ·10−4, batch sizes in {16, 32} and the number of epochs in {20, 40}. These grid search spaces are inspired by our baselines, especially by LoRA [17]. We use the default Adam [28] setting with a warmup ratio of 6%. If the model’s performance on the development set is not improved over 5 epochs, we stop the training. We run the same task of a method in the above-mentioned grid search space three times with different random seeds, choose the best result from each run, and report the mean and standard deviation of these best results. For all question-answering tasks, we sweep learning rates in {1, 3, 5, 7}·10−4, batch sizes in {8, 16, 32} and the number of epochs in {3, 5, 10}, and keep other settings the same, which is inspired by [50]. The sequence length for all tasks is set to 512, 128, 128 and 128 for BERTbase, RoBERTalarge, BARTlarge and OPT as our baselines, respectively. We run all experiments on the Transformers framework [34] on a single NVIDIA RTX A6000 GPU with 48GB memory. Overall, a single run of any task could be finished within 8 hours, and most tasks could be finished in an hour. Fine-tuning settings are summarized in Table 7. 4.2 Results and discussions Importance of MEFT’s initialization. In the beginning, we further test the starting point hypothesis on our MEFTs by adjusting the scaling factors, λ and β. As depicted by the dashed lines in Figure 5, the degradation in performance is evident when the scaling factors deviate from their desired value of 7Table 2: Comparion with different methods on GLUE. The first and second best results are in bold and underlined, respectively. All baseline results for BERTbase and RoBERTalarge are from [49] and [17], respectively. We report Spearman’s Correlation for STS-B and matched accuracy for MNLI on BERTbase. The hyper-parameters after each backbone are used for measuring the memory footprint. r = 8for all MEFTs. All models are trained in FP16 if not specified with FP32. #Param. Memory (GB) Method (%) Peak Act. RTE MRPC STS-B CoLA SST-2 QNLI QQP MNLI Avg. BERTbase (batch size = 32, sequence length = 512) Full FT 100 16.67 14.98 71.1 1.5 85.71.8 89.00.5 59.30.6 92.60.2 91.50.1 91.50.0 84.40.2 83.2 Prefix-Tuning 0.17 13.58 13.00 70.5 0.5 85.90.9 88.80.2 58.91.2 91.90.5 90.80.1 89.10.1 82.80.2 82.3 LoRA 0.27 13.45 13.02 65.9 1.5 84.51.0 88.70.1 57.60.8 92.10.4 90.60.2 89.40.0 83.00.1 81.5 MAM 6.97 14.21 13.41 69.1 1.8 87.20.7 89.00.5 47.924. 83.917. 90.90.2 90.80.1 83.30.2 80.3 AutoPEFT 1.40 - - 72.4 0.9 87.50.9 89.20.0 60.91.5 92.10.3 91.10.1 90.60.1 84.00.1 83.5 vanilla gradient MEFT1 0.27 13.64 13.21 74.2 1.4 86.70.2 89.00.0 62.10.2 92.90.2 91.60.1 89.90.1 83.80.4 83.8 MEFT2 0.27 13.73 13.31 74.7 0.3 86.60.5 89.40.1 61.80.7 93.00.1 91.60.1 90.20.1 84.50.1 84.0 MEFT3 0.27 13.64 13.21 76.10.8 87.40.3 88.90.1 62.30.5 93.20.2 91.50.1 90.10.1 84.20.2 84.2 reversible gradient MEFT1(FP32) 0.27 2.75 2.33 73.9 0.5 86.50.2 88.80.1 60.30.6 92.70.4 91.40.0 88.80.1 83.40.1 83.2 MEFT2(FP32) 0.27 3.53 3.11 74.0 0.6 86.30.4 88.60.1 60.71.5 92.80.2 91.50.1 88.90.1 83.10.1 83.2 MEFT3(FP32) 0.27 2.99 2.57 70.8 0.6 84.60.5 88.20.3 53.91.0 92.20.4 90.40.2 86.90.3 81.50.1 81.1 RoBERTalarge (batch size = 32, sequence length = 128) Full FT 100 11.47 6.05 86.6 90.9 92.4 68.0 96.4 94.7 92.2 90.2 88.9 AdapterH 0.23 6.05 4.66 72.9 3.0 87.71.7 91.50.5 66.32.0 96.30.5 94.70.2 91.50.1 90.30.3 86.4 AdapterH 1.69 6.18 4.71 83.4 1.1 88.72.9 91.01.7 66.54.4 96.20.3 94.70.2 92.10.1 89.90.5 87.8 AdapterP 0.23 6.16 4.77 80.1 2.9 89.71.2 91.90.4 67.82.5 96.60.2 94.80.3 91.70.2 90.50.3 87.9 AdapterP 0.85 6.21 4.78 83.8 2.9 90.20.7 92.10.7 68.31.0 96.10.3 94.80.2 91.90.1 90.20.3 88.4 LoRA 0.23 6.11 4.72 85.2 1.1 90.21.0 92.30.5 68.21.9 96.20.5 94.80.3 91.60.2 90.60.2 88.6 vanilla gradient MEFT1 0.23 6.19 4.81 89.5 0.8 91.50.2 92.30.1 69.90.7 96.80.1 94.90.1 91.50.1 90.30.2 89.6 MEFT2 0.23 6.20 4.82 88.6 0.6 91.30.4 92.20.1 68.80.7 96.80.1 94.80.1 91.40.1 90.60.0 89.3 reversible gradient MEFT1 0.23 3.11 1.73 87.6 0.3 90.50.6 91.60.1 63.31.7 95.90.1 94.30.2 90.10.1 89.20.7 87.8 MEFT1(FP32) 0.23 3.63 2.25 90.00.5 91.20.2 92.40.1 66.10.7 96.70.3 94.80.1 90.20.5 90.10.1 88.9 MEFT2(FP32) 0.23 3.75 2.37 88.2 0.5 90.50.4 92.10.0 64.40.6 95.90.2 94.30.1 89.40.1 88.40.5 87.9 BARTlarge (batch size = 100, sequence length = 128) Full FT [20] 100 12.75 9.62 77.6 89.2 - 59.3 95.8 94.3 89.5 90.8 85.2 Y-Tuning [20] 7.7 - - 62.8 79.2 - 44.4 94.4 88.2 85.5 82.3 76.7 LST(FP32) [21] 2.6 7.05 6.14 69.7 87.3 - 55.5 94.7 91.9 89.5 86.1 82.1 reversible gradient MEFT1 0.20 2.54 1.75 72.2 1.3 88.11.3 - 51.0 1.8 95.10.2 92.40.1 87.50.0 87.00.2 81.9 MEFT1(FP32) 0.20 2.54 1.75 74.3 0.7 88.40.5 - 57.4 2.2 95.40.1 93.90.1 89.30.1 88.30.1 83.9 0 (as indicated in Table 1). However, when they are small enough (0.05 or 0.1), the results are even better than full fine-tuning. For most MEFT methods (MEFT1 and MEFT3), the optimal value for the scaling factors is 0.1. So we use this value for all MEFT methods in the following experiments. MEFTs with vanilla gradient are strong PEFT methods. Though MEFTs have reversible archi- tectures, we can still treat them as irreversible models and cache the intermediate activations during fine-tuning. In this way, they are simply PEFT methods. In Table 2, all MEFT methods, utilizing the vanilla gradient, consistently outperform both full fine-tuning and other baseline approaches by a significant margin. For example, MEFT3 outperforms Full FT by 1% and the best PEFT baseline (AutoPEFT) by 0.7% on BERTbase. MEFT1 outperforms Full FT by 0.7% on RoBERTalarge. Performance gap of MEFTs between vanilla and reversible gradients. In Figure 5, the results of MEFTs with reversible gradient (solid line) are often lower than the ones with vanilla gradient (dashed line). Recapping the discussion in §2.3, smaller scaling factors (< 1) and residual connections in F and G can cause a larger reconstruction error because of numerical stability. When modifying a PLM, we can’t remove the residual connections from it and have to set the scaling factors→ 0 due to the starting point hypothesis, which we believe is the main reason for the performance drop. Our claim is further supported by MEFT3 which has the most evident drop among all MEFTs. Compared to MEFT1 and MEFT2 that only have a residual connection in either F or G, both F and G of MEFT3 have residual connections. In addition, we have to set both λ and β close to 0 for MEFT3, which also causes a bigger reconstruction error than only setting one scaling factor (see Figure 3b middle). Since MEFT3 with reversible gradient performs the worst among all MEFTs, we only run it on BERTbase 8due to limited resources. Expectedly, MEFT1 trained in FP32 outperforms it trained in FP16 on both RoBERTalarge and BARTlarge (see Table 2), because FP16 causes more instability. Reversible MEFTs on deep model. Because of the starting point hypothesis, the residual connection from PLMs remains and the scaling factors are set closely to 0. With an increasing number of layers, the training instability is expected to become more severe (see Figure 3b left). As shown in Figure 6, when all RoBERTa layers are reversible (the number of reversible layers as 24), the score drops dramatically. To address this issue, we propose three settings in Figure 7: (1) Cache the activations for top layers (vanilla gradient) and apply reversible shallow layers (reversible gradient). (2) Freeze some shallow PLM layers, i.e. treating the shallow layers as a feature extractor. (3) Combine the above two settings. Notably, we have to put the reversible layers under the vanilla layers due to numerical stability. If we reverse the order, the reconstruction error is transferred to the vanilla layers. reversiblereversible freezefreeze vanillavanilla freeze freeze reversible reversible reversible reversible vanilla vanilla Figure 7: Three set- tings for deep models. We only explore the first two settings on RoBERTa and will discuss the third setting in the following, since RoBERTalarge doesn’t contain many layers. In Figure 6, when we apply the first setting (freeze: false) to RoBERTalarge, the average score becomes better when the number of reversible layers decreases, outperforms full fine-tuning when it’s≤ 16. However, the activation memory also increases with an increasing number of vanilla layers, since the vanilla layers require caching the activations. By default, we set the number of reversible layers as 16 for RoBERTalarge in Table 2. For the second setting (freeze: true), the results are always worse than full fine-tuning. However, its activation memory stays the same since all trainable layers are reversible. MEFTs are parameter and memory-efficient with a strong performance. Let’s go back to Table 2. Though there is a gap in MEFTs between vanilla and reversible gradients, reversible MEFTs still achieve strong results compared to previous baselines. On BERTbase, reversible MEFT1 and MEFT2 obtain the same average score as Full FT, slightly worse than the best PEFT method, AutoPEFT (83.2 vs. 83.5). However, reversible MEFTs only requires about 21% and 24% activation memory of Full FT and PEFTs. On RoBERTalarge, reversible MEFT1 (FP32) achieves the same score as Full FT and outperforms all PEFT methods, while only requiring 37% and 48% activation memory of Full FT and PEFTs. Due to limited computing resources, we only conduct experiments on the best MEFT method, MEFT1, on BARTlarge when compared to other memory-efficient methods. In addition, we don’t use our own grid search space on BARTlarge. Instead, we apply the same grid search space as LST, setting the learning rate in {3 · 10−4, 1 · 10−3, 3 · 10−3}, the batch size as 100 and the number of epochs as 20. In this way, we want to validate the robustness of MEFT. Similarly, MEFT 1 outperforms the memory-efficient baselines by a large margin while only requiring 29% LST’s activation memory. In addition, LST requires knowledge distillation to initialize the added layers and is not stable [21].8 MEFT trained in FP32 vs. in FP16, and the time-memory tradeoff. Though reversible MEFTs trained in FP32 outperform the ones trained in FP16, there are still some notable discussions about them: (1) The memory footprint required by reversible MEFTs trained in FP32 and FP16 is the same. In Table 2, MEFT1 and MEFT1 (FP32) have the same activation memory on BARTlarge, because the recomputed activations in back-propagation are always in FP32 due to the mixed precision training [51]. I.e. PyTorch [52] only allows FP32 in back-propagation; (2) FP16 still benefits the training of large PLMs. In Table 2, the peak and activation memory difference is about the backbone size in FP32 for PEFTs and MEFTs. If one could reduce the backbone size by loading in FP16, we can further reduce the peak memory; 2 (3) Training in FP16 is faster than the training in FP32 (about 1:1.2) due to the forward pass. In addition, since reversible MEFTs recompute the activations, they require more training time, about twice the training time for MEFTs with the vanilla gradient. Results on larger and deeper models. Here we explore a more realistic setting (the third setting in Figure 7) on larger and deeper models, OPT 1.3B and OPT6.7B, in Table 3. On OPT 1.3B with 24 layers, we set the number of frozen, reversible and vanilla layers as 8. On OPT6.7B with 32 layers, we use 8 reversible and vanilla layers, same as OPT1.3B. For a fair comparison, we freeze the first 8 PLM layers and modify the rest 16 layers with LoRA. MEFT1 is comparable to LoRA, while only requiring LoRA’s 65% activation memory. Though slightly worse than Full FT (-0.3%), MEFT1’s 8The comparison of memory footprint to Y-Tuning is in Table 10. 9Table 3: Results on question-answering tasks. r = 64for both MEFT1 and LoRA. All methods are trained in FP16. Due to limited computing resources, we only conduct one random run with these methods. A batch size of 32 and a sequence length of 128 are used to measure the memory footprint and training time. The training time is for one epoch on the OpenBookQA task. Check Appendix §D for the implementation detail. #Param. Memory (GB) Time Model Method (%) Peak Activation (s) OpenBookQA PIQA ARC-E ARC-C SciQ Avg. Full FT [50] 100 28.31 8.23 128.0 31.4 75.2 61.3 27.7 92.5 57.6 OPT1.3B LoRA 0.64 11.42 6.27 36.6 29.9 74.9 60.1 28.7 93.3 57.4 MEFT1 0.64 9.20 4.05 45.2 34.0 73.1 57.1 28.8 93.1 57.3 OPT6.7B ZeroShot - - - - 27.6 76.2 65.6 30.6 90.1 58.0 MEFT1 0.25 33.67 8.01 200.4 37.0 77.4 65.7 34.1 94.4 61.7 activation memory is only half of the one for Full FT. When using the same activation memory as Full FT by running on OPT6.7B, MEFT1 outperforms Full FT by a large margin. Table 4: Results on image classification. Method Acc@1 Peak Memory (GB) Full FT [27] 97.67 - AdaptFormer [27] 96.89 36 MEFT1 96.74 9 Transfer to image classification task. Though we only focused on NLP tasks, MEFT could be transferred to other tasks, even other architectures. We leave the transfer of MEFT to other architectures for future work, and here apply MEFT to ViT [53] for an image classification task, i.e. SVHN [27]. We follow the main training recipe from AdaptFormer [54], except for changing the optimizer from SGD to AdamW, setting the maximum gradient norm as 0.3. For MEFT1’s hyper-parameters, we set r = 64and λ = 0.3 (smaller λ is not stable for training). Similar to the NLP’s results, MEFT1 achieves comparable accuracy as AdaptFormer while saving a large amount of memory footprint in Table 4. For more results about comparing MEFT to gradient checkpointing, comparing MEFT to quantization methods, and combining MEFT with other memory-efficient methods, please go to Appendix §E. In addition, due to the page limit, we put the detailed related works in Appendix §A, and discuss the limitation of our work in Appendix §B. 5 Conclusion In this paper, we propose three memory-efficient fine-tuning methods (MEFTs), that fine-tune PLM in a parameter-efficient and memory-efficient way without the requirement of additional pre-training and match the performance of full fine-tuning. MEFTs modify the PLM architecture with adapters and make it reversible, by following the starting point hypothesis that is essential for PEFTs. So MEFTs don’t require caching the intermediate activations during training and significantly reduce the memory footprint occupied by activations. When applying MEFTs to various models, BERT, RoBERTa and BART, on the GLUE benchmark, MEFTs achieve a similar score as full fine-tuning and other strong baselines, while saving up to 84% activation memory. A similar story is also observed when applying MEFT to larger and deeper models, OPT, on five question-answering tasks. MEFT achieves a comparable score as full fine-tuning and only consumes its 50% activation memory. However, because of the recomputation of activations, MEFTs require slightly more training time than other PEFT methods and offer a slightly lower score when trained in FP16 instead of FP32. In the future, we are interested in applying MEFT to other areas, like computer vision and automatic speech recognition, and to other bigger backbones for more sequence-to-sequence tasks. Acknowledgements We thank all reviewers for their great feedback. We also thank our colleague Yan Meng for her helpful review of our draft. This research was funded in part by the Netherlands Organization for Scientific Research (NWO) under project number VI.C.192.080. 10References [1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171– 4186. Association for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1423. URL https://doi.org/10.18653/v1/n19-1423. [2] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019. URL http://arxiv.org/abs/1907. 11692. [3] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1–140:67, 2020. URL http://jmlr. org/papers/v21/20-074.html. [4] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross B. Girshick. Masked autoencoders are scalable vision learners. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022 , pages 15979– 15988. IEEE, 2022. doi: 10.1109/CVPR52688.2022.01553. URL https://doi.org/10. 1109/CVPR52688.2022.01553. [5] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: a simple framework for masked image modeling. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 9643–9653. IEEE, 2022. doi: 10.1109/CVPR52688.2022.00943. URL https://doi.org/10.1109/CVPR52688.2022.00943. [6] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework for self-supervised learning of speech representations. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Con- ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6- 12, 2020, virtual , 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 92d1e1eb1cd6f9fba3227870bb6d7f07-Abstract.html. [7] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neu- ral Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 13–23, 2019. URL https://proceedings.neurips.cc/paper/2019/ hash/c74d97b01eae257e44aa9d5bade97baf-Abstract.html. [8] Hao Tan and Mohit Bansal. LXMERT: learning cross-modality encoder representations from transformers. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter- national Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019 , pages 5099–5110. Association for Computational Linguistics, 2019. doi: 10.18653/v1/D19-1514. URL https://doi.org/10.18653/v1/D19-1514. [9] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. OPT: open pre-trained transformer language models.CoRR, abs/2205.01068, 2022. doi: 10.48550/arXiv.2205.01068. URL https://doi.org/10.48550/arXiv.2205.01068. [10] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel 11Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, and et al. BLOOM: A 176b-parameter open-access multilingual language model. CoRR, abs/2211.05100, 2022. doi: 10.48550/arXiv.2211.05100. URL https://doi.org/10.48550/arXiv.2211. 05100. [11] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971, 2023. doi: 10.48550/arXiv.2302.13971. URL https://doi.org/10.48550/arXiv.2302.13971. [12] Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, and Donald Metzler. Unifying language learning paradigms. CoRR, abs/2205.05131, 2022. doi: 10.48550/arXiv.2205.05131. URL https: //doi.org/10.48550/arXiv.2205.05131. [13] Baohao Liao, Yan Meng, and Christof Monz. Parameter-efficient fine-tuning without introducing new latency. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki, editors,Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023 , pages 4242–4260. Association for Computational Linguistics, 2023. doi: 10.18653/v1/2023.acl-long.233. URL https://doi. org/10.18653/v1/2023.acl-long.233. [14] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 2790–2799. PMLR, 2019. URL http://proceedings.mlr.press/v97/houlsby19a.html. [15] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 4582–4597. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.acl-long.353. URL https://doi.org/10.18653/ v1/2021.acl-long.353. [16] Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. Adapterfusion: Non-destructive task composition for transfer learning. In Paola Merlo, Jörg Tiedemann, and Reut Tsarfaty, editors, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021 , pages 487–503. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.eacl-main.39. URL https://doi.org/10.18653/v1/2021.eacl-main. 39. [17] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. InThe Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=nZeVKeeFYf9. [18] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. To- wards a unified view of parameter-efficient transfer learning. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. Open- Review.net, 2022. URL https://openreview.net/forum?id=0RDcd5Axok. [19] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 122021, pages 3045–3059. Association for Computational Linguistics, 2021. doi: 10.18653/v1/ 2021.emnlp-main.243. URL https://doi.org/10.18653/v1/2021.emnlp-main.243. [20] Yitao Liu, Chenxin An, and Xipeng Qiu. Y-tuning: An efficient tuning paradigm for large-scale pre-trained models via label representation learning. CoRR, abs/2202.09817, 2022. URL https://arxiv.org/abs/2202.09817. [21] Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. LST: ladder side-tuning for parameter and memory efficient transfer learning. In NeurIPS, 2022. URL http://papers.nips.cc/paper_files/ paper/2022/hash/54801e196796134a2b0ae5e8adef502f-Abstract-Conference. html. [22] Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. CoRR, abs/1503.02531, 2015. URL http://arxiv.org/abs/1503.02531. [23] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter. CoRR, abs/1910.01108, 2019. URL http://arxiv.org/abs/1910.01108. [24] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. CoRR, abs/2303.16199, 2023. doi: 10.48550/arXiv.2303.16199. URL https://doi.org/10.48550/arXiv.2303.16199. [25] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id= rJ4km2R5t7. [26] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: denoising sequence-to-sequence pre- training for natural language generation, translation, and comprehension. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault, editors,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 7871– 7880. Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.acl-main.703. URL https://doi.org/10.18653/v1/2020.acl-main.703. [27] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y . Ng. Reading digits in natural images with unsupervised feature learning. InNIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011, 2011. URL http://ufldl.stanford. edu/housenumbers/nips2011_housenumbers.pdf. [28] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980. [29] Haokun Liu, Derek Tam, Muqeeth Mohammed, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/ forum?id=rBCvMG-JsPd. [30] Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual entailment challenge. In Joaquin Quiñonero Candela, Ido Dagan, Bernardo Magnini, and Florence d’Alché- Buc, editors, Machine Learning Challenges, Evaluating Predictive Uncertainty, Visual Object Classification and Recognizing Textual Entailment, First PASCAL Machine Learning Challenges Workshop, MLCW 2005, Southampton, UK, April 11-13, 2005, Revised Selected Papers, volume 3944 of Lecture Notes in Computer Science, pages 177–190. Springer, 2005. doi: 10.1007/ 11736790\\_9. URL https://doi.org/10.1007/11736790_9. [31] Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, and Danilo Giampiccolo. The second pascal recognising textual entailment challenge. Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, 01 2006. [32] Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL recognizing textual entailment challenge. In Satoshi Sekine, Kentaro Inui, Ido Dagan, 13Bill Dolan, Danilo Giampiccolo, and Bernardo Magnini, editors, Proceedings of the ACL- PASCAL@ACL 2007 Workshop on Textual Entailment and Paraphrasing, Prague, Czech Re- public, June 28-29, 2007, pages 1–9. Association for Computational Linguistics, 2007. URL https://aclanthology.org/W07-1401/. [33] Luisa Bentivogli, Bernardo Magnini, Ido Dagan, Hoa Trang Dang, and Danilo Giampiccolo. The fifth PASCAL recognizing textual entailment challenge. In Proceedings of the Second Text Analysis Conference, TAC 2009, Gaithersburg, Maryland, USA, November 16-17, 2009. NIST, 2009. URL https://tac.nist.gov/publications/2009/additional.papers/RTE5_ overview.proceedings.pdf. [34] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38–45, Online, October 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL https://aclanthology.org/2020.emnlp-demos.6. [35] Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, and Sayak Paul. Peft: State-of-the-art parameter-efficient fine-tuning methods. https://github.com/ huggingface/peft, 2022. [36] Armen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021 , pages 7319–7328. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021. acl-long.568. URL https://doi.org/10.18653/v1/2021.acl-long.568. [37] Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. Neural network acceptability judgments. Trans. Assoc. Comput. Linguistics, 7:625–641, 2019. doi: 10.1162/tacl\\_a\\_00290. URL https://doi.org/10.1162/tacl_a_00290. [38] Daniel M. Cer, Mona T. Diab, Eneko Agirre, Iñigo Lopez-Gazpio, and Lucia Specia. Semeval- 2017 task 1: Semantic textual similarity - multilingual and cross-lingual focused evaluation. CoRR, abs/1708.00055, 2017. URL http://arxiv.org/abs/1708.00055. [39] William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential para- phrases. In Proceedings of the Third International Workshop on Paraphrasing, IWP@IJCNLP 2005, Jeju Island, Korea, October 2005, 2005. Asian Federation of Natural Language Processing, 2005. URL https://aclanthology.org/I05-5002/. [40] Karttikeya Mangalam, Haoqi Fan, Yanghao Li, Chao-Yuan Wu, Bo Xiong, Christoph Feicht- enhofer, and Jitendra Malik. Reversible vision transformers. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18- 24, 2022, pages 10820–10830. IEEE, 2022. doi: 10.1109/CVPR52688.2022.01056. URL https://doi.org/10.1109/CVPR52688.2022.01056. [41] Aidan N. Gomez, Mengye Ren, Raquel Urtasun, and Roger B. Grosse. The reversible resid- ual network: Backpropagation without storing activations. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V . N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 2214–2224, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ f9be311e65d81a9ad8150a60844bb94c-Abstract.html. [42] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net, 2020. URL https://openreview.net/forum?id= rkgNKkHtvB. [43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von 14Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V . N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998–6008, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ 3f5ee243547dee91fbd053c1c4a845aa-Abstract.html. [44] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? A new dataset for open book question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 2381–2391. Association for Computational Linguistics, 2018. doi: 10.18653/v1/d18-1260. URL https://doi.org/10.18653/v1/d18-1260. [45] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: reasoning about physical commonsense in natural language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelli- gence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 7432–7439. AAAI Press, 2020. URL https://ojs.aaai.org/index.php/AAAI/article/view/6239. [46] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the AI2 reasoning challenge. CoRR, abs/1803.05457, 2018. URL http://arxiv.org/abs/1803.05457. [47] Johannes Welbl, Nelson F. Liu, and Matt Gardner. Crowdsourcing multiple choice science questions. In Leon Derczynski, Wei Xu, Alan Ritter, and Tim Baldwin, editors, Proceedings of the 3rd Workshop on Noisy User-generated Text, NUT@EMNLP 2017, Copenhagen, Denmark, September 7, 2017 , pages 94–106. Association for Computational Linguistics, 2017. doi: 10.18653/v1/w17-4413. URL https://doi.org/10.18653/v1/w17-4413. [48] Brian W Matthews. Comparison of the predicted and observed secondary structure of t4 phage lysozyme. Biochimica et Biophysica Acta (BBA)-Protein Structure, 405(2):442–451, 1975. [49] Han Zhou, Xingchen Wan, Ivan Vulic, and Anna Korhonen. Autopeft: Automatic configuration search for parameter-efficient fine-tuning. CoRR, abs/2301.12132, 2023. doi: 10.48550/arXiv. 2301.12132. URL https://doi.org/10.48550/arXiv.2301.12132. [50] Guangxuan Xiao, Ji Lin, and Song Han. Offsite-tuning: Transfer learning without full model. CoRR, abs/2302.04870, 2023. doi: 10.48550/arXiv.2302.04870. URL https://doi.org/10. 48550/arXiv.2302.04870. [51] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory F. Diamos, Erich Elsen, David García, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training. CoRR, abs/1710.03740, 2017. URL http://arxiv.org/abs/1710. 03740. [52] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017. [53] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 . OpenReview.net, 2021. URL https: //openreview.net/forum?id=YicbFdNTTy. [54] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping Luo. Adaptformer: Adapting vision transformers for scalable visual recognition. In NeurIPS, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ 69e2f49ab0837b71b0e0cb7c555990f8-Abstract-Conference.html. [55] Baohao Liao, David Thulke, Sanjika Hewavitharana, Hermann Ney, and Christof Monz. Mask more and mask later: Efficient pre-training of masked language models by disentangling the [MASK] token. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors,Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022 , pages 1478–1492. Association for Computational Linguistics, 2022. 15doi: 10.18653/v1/2022.findings-emnlp.106. URL https://doi.org/10.18653/v1/2022. findings-emnlp.106. [56] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. MASS: masked sequence to se- quence pre-training for language generation. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 5926–5936. PMLR, 2019. URL http://proceedings.mlr.press/v97/ song19d.html. [57] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsu- pervised cross-lingual representation learning at scale. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault, editors, Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 8440–8451. Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.acl-main.747. URL https://doi.org/10.18653/v1/2020.acl-main.747. [58] Alexis Conneau and Guillaume Lample. Cross-lingual language model pretraining. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 7057–7067, 2019. URL https://proceedings.neurips.cc/paper/ 2019/hash/c04c19c2c2474dbf5f7ac4372c5b9af1-Abstract.html. [59] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [60] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. CoRR, abs/2005.14165, 2020. URL https://arxiv.org/abs/2005.14165. [61] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. CoRR, abs/2001.08361, 2020. URL https://arxiv.org/abs/2001.08361. [62] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models. CoRR, abs/2203.15556, 2022. doi: 10.48550/arXiv.2203.15556. URL https://doi.org/10.48550/arXiv.2203.15556. [63] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. CoRR, abs/2204.02311, 2022. doi: 10.48550/arXiv.2204.02311. URL https://doi.org/10.48550/arXiv.2204.02311. [64] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, 16Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed H. Chi, and Quoc Le. Lamda: Language models for dialog applications. CoRR, abs/2201.08239, 2022. URL https://arxiv.org/abs/2201.08239. [65] Jason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V . Le. Finetuned language models are zero-shot learners. InThe Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=gEZrGCozdqR. [66] Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M. Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. Crosslingual generalization through mul- titask finetuning. CoRR, abs/2211.01786, 2022. doi: 10.48550/arXiv.2211.01786. URL https://doi.org/10.48550/arXiv.2211.01786. [67] Barret Zoph and Quoc V . Le. Neural architecture search with reinforcement learning. In5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview. net/forum?id=r1Ue8Hcxg. [68] Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Scott Yih, and Madian Khabsa. Unipelt: A unified framework for parameter-efficient language model tuning. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors,Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 6253–6264. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.acl-long.433. URL https://doi.org/10.18653/ v1/2022.acl-long.433. [69] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. CoRR, abs/1604.06174, 2016. URL http://arxiv.org/abs/1604.06174. [70] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview. net/forum?id=rJl-b3RcF7. [71] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, and Michael Carbin. Linear mode connectivity and the lottery ticket hypothesis. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 3259–3269. PMLR, 2020. URL http: //proceedings.mlr.press/v119/frankle20a.html. [72] Animesh Koratana, Daniel Kang, Peter Bailis, and Matei Zaharia. LIT: learned interme- diate representation training for model compression. In Kamalika Chaudhuri and Rus- lan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA , volume 97 of Pro- ceedings of Machine Learning Research , pages 3509–3518. PMLR, 2019. URL http: //proceedings.mlr.press/v97/koratana19a.html. [73] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. Zero-offload: Democratizing billion-scale model training. In Irina Calciu and Geoff Kuenning, editors, 2021 USENIX Annual Technical Confer- ence, USENIX ATC 2021, July 14-16, 2021, pages 551–564. USENIX Association, 2021. URL https://www.usenix.org/conference/atc21/presentation/ren-jie. [74] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: accurate post- training quantization for generative pre-trained transformers. CoRR, abs/2210.17323, 2022. doi: 10.48550/arXiv.2210.17323. URL https://doi.org/10.48550/arXiv.2210.17323. 17[75] Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Understanding and overcoming the challenges of efficient transformer quantization. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 7947–7969. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.emnlp-main.627. URL https://doi.org/10. 18653/v1/2021.emnlp-main.627. [76] Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via block- wise quantization. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview. net/forum?id=shpkpVXzo3h. [77] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. CoRR, abs/2305.14314, 2023. doi: 10.48550/arXiv.2305.14314. URL https://doi.org/10.48550/arXiv.2305.14314. [78] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Rajesh Gupta, Yan Liu, Jiliang Tang, and B. Aditya Prakash, editors, KDD ’20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, pages 3505–3506. ACM, 2020. doi: 10.1145/3394486.3406703. URL https://doi.org/10.1145/3394486.3406703. 18A Related works Pre-trained language models . PLMs, which are trained on extensive datasets for a common task such as predicting masked words [ 1, 2, 3, 26, 55, 56, 57, 58] or the next word [ 59, 60] in a sentence, play a vital role in facilitating knowledge transfer to downstream tasks. They have demonstrated remarkable achievements across various applications, consistently delivering state- of-the-art outcomes. Furthermore, scaling up PLMs has proven to yield predictable enhancements in performance for these downstream tasks [61, 62]. Consequently, the size of released PLMs has progressively grown, reaching an unprecedented scale of 100 billion parameters [9, 10, 11, 60, 63, 64]. Such large-scale PLMs unveil extraordinary capabilities, enabling zero-shot or in-context learning [59, 60] for a broad spectrum of tasks. Nevertheless, transfer learning remains a prevalent approach for effectively deploying these models in new task scenarios [29, 65, 66], which post unparalleled requirements on the computing resources. Parameter-efficient fine-tuning. With the advent of large-scale PLMs, a new method that aims to reduce storage requirements, PEFT, has been proposed [14, 15, 19]. PEFT adds and trains a small number of parameters while matching the performance of full fine-tuning. There are various ways to add new parameters. For example, Houlsby et al. [14] and Pfeiffer et al. [16] insert small bottleneck modules (adapters) to the PLM. LoRA [17] injects rank decomposition matrices into the pre-trained weights. HiWi [13] inserts the pre-trained parameters to a low-rank adapter. (IA)3 [29] scales the pre-trained weight with a trainable vector. Prompt-based methods [ 15, 19] append a sequence of trainable vectors to the word embeddings or attention components. Recently, some unified methods, which combine multiple PEFT methods in a heuristic way [ 18] or with the technique of neural architecture search [49, 67, 68], have also been proposed. Though PEFTs save the storage by a large margin compared to full fine-tuning, they still require a similar memory footprint during training as full fine-tuning [20, 21] because of the activation memory. Memory-efficient training. Memory-efficient training aims to reduce the memory footprint during the training process. Reversible neural networks [ 40, 41, 42] reduce the activation memory by recomputing the activations with the outputs during back-propagation. Gradient checkpointing [69] trade computation for memory by dropping some intermediate activations and recovering them from an extra forward pass. The activation memory is O(1) and O( √ N) for reversible neural networks and gradient checkpointing, respectively. MEFT is the first method that is proposed to modify a PLM to its reversible variant. When applying MEFT on a deeper model, one can use gradient checkpointing to further reduce the activation memory for the layers with vanilla gradient. Network compressions, like pruning [70, 71] and knowledge distillation [22, 23, 72], save the memory footprint for both training and inference. They compress a PLM to a smaller model by either deleting unimportant parameters or distilling knowledge from the PLM to the smaller model. Treating a PLM as a feature extractor and avoiding its gradient calculation is also an effective way to reduce the activation memory [20, 21]. However, these methods normally require extra pre-training before fine-tuning, or achieve a lower performance compared to full fine-tuning when using the same PLM. B Limitations We acknowledge the main limitation of this work is that we only evaluate our proposed methods on a limited amount of tasks and don’t conduct experiments on the encoder-decoder models. The main reason for the limited amount of tasks is that our computing resources are constrained. In addition, the major criterion for our selection of the underlying models is that we could find many strong baselines on them without reproduction. BERT and RoBERTa fulfill this criterion very well on the GLUE benchmark. Regarding the encoder-decoder model, recently there is a clear trend of applying a decoder-only model on sequence-to-sequence tasks. Therefore, we apply OPT in this paper and plan to include LLAMA [11] for the instruction-finetuning data in the future. Another limitation of MEFT is its lower score when trained in FP16 and on a deeper model. We have discussed this problem in §4.2. In sum, more reconstruction error is introduced by FP16 due to its numerical range and by a deeper model because of the error accumulation. Fortunately, the results are still comparable to the PEFT baselines when trained in FP16. Even trained in FP32, the activation memory footprints don’t increase compared to FP16. One only needs to spend more training time in FP32 when using the same batch size as in FP16 (about 20% more training time). However, since 19MEFTs reduce the memory footprint, a larger batch size during training is possible, which can save some training time. For deeper models, we offer a practical and effective setting in Figure 7. Last but not least, when fine-tuning larger models, like OPT1.3B and OPT6.7B [9], the peak memory footprint is occupied by the model parameters rather than the activation (see Table 3). One needs to combine other techniques with MEFT to reduce the peak memory footprint, like loading the model in FP16 or even in int8 rather than in FP32, combining MEFT with ZeRO [73] as in Table 6. C Step-by-step design for MEFT 1 For the reader’s easy understanding, in this section, we explain MEFT 1 step-by-step. First, let’s re-emphasize the guiding principles for our design: (1) For each reversible layer, we must have two inputs and two outputs as in Figure 3a. (2) We need to follow the starting point hypothesis. I.e. whenever we modify a PLM layer, we need to ensure the modified layer has almost the same output as the original PLM layer if we input the same input of the original PLM layer to the modified layer at the beginning of training. If the outputs are not similar, they become even more dissimilar after multiple layers, tearing down the PLM’s initialization. As shown in Figure 8a, for the first PLM layer, h0 is the input and h1 is the output. In Figure 8b, the inputs to the first reversible layer is h1 0 = h2 0 = h0. Recapping the architecture of F1 in Figure 4c (up), we simply insert an adapter in parallel to the two consecutive feed-forward layers, and initialize the adapter as Wdown, Wup ∼ N(0, 0.022), which results in h1 ≈ F1(h2 0) since h2 0 = h0. If we set λ → 0, h1 1 = λh1 0 + F1(h2 0) ≈ h1. In this way, h1 1 plays the role of preserving the starting point. Now let’s consider h2 1. Due to our initialization of the adapter, the output from G1 (G1 is simply an adapter as in Figure 4c (down)) is close to 0. So h2 1 = βh2 0 + G1(h1 1) ≈ βh0 + 0 = βh0. After switching the order of h1 1 and h2 1, h1 1 ≈ βh0 and h2 1 ≈ h1. For the second reversible layer, if we don’t switch the order of h1 1 and h2 1, it looks like Figure 8c. The input to F2 is βh0, which breaks down the representation continuity of a PLM since the input to the pre-trained F2 should be close to h1. If we switch their order as in Figure 8d, we preserve the representation continuity. And it results in h1 2 = λβh0 + F2(h1) ≈ h2 due to λ → 0 and h2 ≈ F2(h1). Similar to the first reversible layer, h2 2 ≈ βh1. After switching, h1 2 ≈ βh1 and h2 2 ≈ h2. By analogy, for the nth reversible layer, h1 n ≈ βhn−1 and h2 n ≈ hn. After the final layer, we simply take the mean of two outputs as h′ N = (h1 N + h2 N )/2, and input h′ N to a task-specific head, like a classification layer. The design procedure is similar for MEFT 2 and MEFT3. In sum, order switching is mainly for preserving the representation continuity, and setting the scaling factors close to 0 is mainly for preserving the starting point. D Implementation details of the question-answering tasks Compared to GLUE tasks where all tasks are classification tasks and the classification heads are randomly initialized, the question-answering tasks are sequence-to-sequence tasks and need the pre-trained output layer that shares the same parameters as the word embedding layer. The output Feed-Forward Feed-Forward ++Layer Norm Attention Block ℎ!\"# ℎ! (a) + + + ℎ!\"ℎ!# ℎ!#ℎ!\" ℎ## ℎ#\" + + +𝜆 𝜆 switch 𝛽F ! F ! G ! ℎ!#= ℎ!\"= ℎ! (b) + + + ℎ!\t𝛽ℎ\" ℎ#! ℎ## + + +𝜆 𝜆 𝛽F ! F ! G ! ℎ!\t𝛽ℎ\" (c) + + + ℎ!\t𝛽ℎ\" ℎ#! ℎ## + + +𝜆 𝜆 𝛽F ! F ! G ! ℎ!\t𝛽ℎ\" switch (d) Figure 8: (a) The nth PLM layer; (b) The first MEFT1 layer; (c) The second MEFT1 layer without order switching; (d) The second MEFT1 layer. 20layer requires the continuity of representation. I.e. at the beginning of training, the input to the output layer, h′ N , should be close to hN . Therefore, we need to do a modification to h′ N instead of using h′ N = (h1 N + h2 N )/2. Here we introduce a new scaling factor γ and require γ → 0. For MEFT 1, since h2 N ≈ hN (see Table 1), we set h′ N = γh1 N + h2 N ≈ h2 N ≈ hN . Similarly, h′ N = h1 N + γh2 N ≈ h1 N ≈ hN for MEFT2, and h′ N = γh1 N + h2 N ≈ h2 N ≈ hN for MEFT3. Without any tuning, we set γ = 0.1 as other tuned scaling factors by default. 128 256 512 40 60 80 100 120 140 160Samples / Second Full FT + gradient checkpointing LoRA + gradient checkpointing MEFT1 32 64 128 33 34 35 36 128 256 512 Sequence Length 0.5 1.0 1.5 2.0 2.5Activation Memory (GB) 32 64 128 Batch Size 2 4 6 8 10 Figure 9: Throughput and activation memory for different sequence length and batch sizes on BERTbase. By default, the sequence length is 512 and the batch size is 32. For your reference, LoRA’s throughput is 52.7 samples/second without gradient checkpointing for the default setting. Overall, MEFT shares the same level of throughput as LoRA with gradient checkpointing, while it is the lower bound of the activation memory for different settings. E More results E.1 Compared to gradient checkpointing Previously, we only theoretically stated that the activation memory for reversible network and gradient checkpointing is O(1) and O( √ N), respectively. In addition, we didn’t compare the training time of MEFT with PEFT in detail. Here we offer some empirical results for your better understanding. In Figure 9, we compare activation memory and throughput among MEFT , LoRA with gradient checkpointing and Full FT with gradient checkpointing. The throughput for all three methods is at the same level, maximum 12% difference between LoRA and MEFT when the sequence length is 128 and the batch size is 32. With an increasing sequence length, the gap becomes narrower to 7.5%. Notably, the throughput for LoRA without gradient checkpointing is 52.7 samples/second. With gradient checkpointing, it is 36.1 samples/second, 69% of the original throughput. For MEFT with the same setting, it is 33.5 samples/second, 64% of LoRA’s throughput without gradient checkpointing. In sum, MEFT’s throughput is at the same level as LoRA’s with gradient checkpointing, and about 64% of LoRA’s without gradient checkpointing. In addition, MEFT’s activation memory is always the lower bound among these three methods. The gap between LoRA with gradient checkpointing and MEFT becomes larger with an increasing sequence length and batch size. E.2 Compared to quantization methods Quantization is an orthogonal method to MEFT, which reduces the memory footprint of training or inference by reducing the parameter size to fewer bits and using low-bit-precision matrix mul- tiplication. There are mainly three different quantization methods: (1) Post-training quantization 21Table 5: Compared to QLoRA. r = 8 for all methods. Experimental setting stays the same as the default setting in Figure 9. Method Activation Memory (GB) Samples/Second LoRA + gradient checkpointing 2.62 36.1 QLoRA + gradient checkpointing 2.97 8.7 MEFT1 2.33 33.5 Table 6: Combine MEFT with ZeRO. Method Peak Memory (GB) Activation Memory (GB) MEFT1 28.2 8.2 MEFT1 + ZeRO 6.4 6.4 [74, 75] that quantizes a trained model after pre-training or fine-tuning; (2) Lower-bit optimizer [76] that stores the optimizer state with lower precision and de-quantizes it only for the optimization, similarly to FP16 or BF16 mixed precision training but with lower-bit; (3) Lower-bit frozen LLM with LoRA, i.e. QLoRA [77], that applies 4-bit quantization to compress the LLM. During fine-tuning, QLoRA backpropagates gradients through the frozen 4-bit quantized LLM into the low-rank adapters. Notably, the computation data type for QLoRA is BF16. It de-quantizes weights to the computation data type to perform the forward and backward passes. To some extent, all these three methods are orthogonal to our method and can be combined with MEFT: (1) Post-training quantization is mainly for reference and it can be applied to any trained models; (2) 8-bit Adam can also be applied to any models trained based on a gradient; (3) QLoRA is a combination of (1) and (2). For QLoRA, we conducted some experiments on BERT base with the default setting as Figure 9. As shown in Table 5, METF 1 saves the most activation memory while having a similar throughput as LoRA with gradient checkpointing. The reason for the larger activation memory of QLoRA than LoRA is that it has an additional de-quantization step, which also causes its smallest throughput. E.3 Combine MEFT with ZeRO ZeRO [73] saves memory by partitioning the model’s parameters and optimizer state among GPUs or between GPU and CPU. This method is orthogonal to MEFT, since MEFT saves memory from activations. We conduct some experiments on OPT1.3B by combining our method with DeepSpeed [78] ZeRO stage 3 that offloading model’s parameters and the optimizer state to CPUs. As shown in Table 6, ZeRO significantly reduces the memory footprint from the model’s parameters, therefore reducing MEFT’s peak memory from 28.2GB to 6.4GB. Table 7: Fine-tuning settings. Check §4.2 for the fine-tuning setting on BART. Hyper-parameter GLUE Question-Answering RTE, MRPC, STS-B, CoLA SST-2, QNLI, QQP, MNLI Learning Rate {5, 6, 7, 8} ·10−4 {3, 4, 5} ·10−4 {1, 3, 5, 7} ·10−4 Batch Size {16, 32} { 16, 32} { 8, 16, 32} Max Epochs {20, 40} { 10, 20} { 3, 5, 10} Weight Decay 0.1 0.1 0.1 Max Gradient Norm 1 1 1 Warmup Ratio 0.06 0.06 0.06 Learning Rate Decay Linear Linear Linear 22Table 8: Statics of datasets Task RTE MRPC STS-B CoLA SST-2 QNLI QQP MNLI-m MNLI-mm #Training 2.5k 3.7k 5.8k 8.6k 67.4k 104.7k 363.8k 392.7k #Development 0.3k 0.4k 1.5k 1k 0.9k 5.5k 40.4k 9.8k 9.8k Task OpenBookQA PIQA ARC-E ARC-C SciQ #Training 5.0k 16.1k 2.3k 1.1k 11.7k #Development 0.5k 3.1k 2.4k 1.2k 1k Table 9: Statics of models Model #Parameter #Layer d model Size in FP32 (GB) BERTbase 110M 12 768 0.4 BARTlarge encoder 205M 12 1024 0.8 RoBERTalarge 355M 24 1024 1.4 OPT1.3B 1.3B 24 2048 5.2 OPT6.7B 6.7B 32 4096 25.6 1 def backward_pass (self , y1 , y2 , dy1 , dy2 ): 2 with torch . enable_grad (): 3 y1. requires_grad = True 4 # The intermediate activations of G are stored 5 g_y1 = self .G(y1) 6 # Obtain the gradient of y1 7 g_y1 . backward (dy2 , retain_graph = True ) 8 9 with torch . no_grad (): 10 x2 = (y2 - g_y1 ) / self . x2_factor 11 # Save memory , same for below 12 del g_y1 , y2 13 dy1 += y1. grad 14 # Save memory 15 y1. grad = None 16 17 with torch . enable_grad (): 18 x2. requires_grad = True 19 # The intermediate activations of F are stored 20 f_x2 = self .F(x2) 21 # Obtain the gradient of x2 22 f_x2 . backward (dy1 , retain_graph = False ) 23 24 with torch . no_grad (): 25 x1 = (y1 - f_x2 ) / self . x1_factor 26 del f_x2 , y1 27 dy2 *= self . x2_factor 28 # dy2 =dx2 , save memory by using the same variable 29 dy2 += x2. grad 30 x2. grad = None 31 # dy1 = dx1 32 dy1 *= self . x1_factor 33 x2 = x2. detach () 34 return x1 , x2 , dy1 , dy2 Listing 1: Backward pass for each Layer. The peak memory happens at Line 10 or Line 25, depending on whether the subnetwork G is larger than F or the opposite. In the code, we use x1, x2, y1, y2, x1_factor, x2_factor to represent h1 n−1, h2 n−1, h1 n, h2 n, λ and β, respectively. 23Table 10: Compared to Y-Tuning on RoBERTalarge. We exclude the memory of Y-Tuning for BART in Table 2, because it was not reported. Instead, the memory usage of Y-Tuning for RoBERTalarge was reported. Notably, the STS-B task is excluded from the calculation of the average score, because it was not evaluated in Liu et al. [20]. Model #Parameter Peak Memory (GB) Average Score Full FT 100% 11.47 88.4 LoRA 0.23% 6.11 88.1 Y-Tuning 4.57% 2.08 82.1 MEFT1 0.23% 3.63 88.4 0 0.05 0.1 0.5 1 0 10 20 30 40 50 60 70 80 0 0.1 0.5 0.9c Average Score (%) default init default init with random PLM α = 1 α = 0, trainable α α = 1 / c Figure 10: The initialization effect for PEFT, Left: LoRA, Right: (IA) 3. Instead of initializing Wup = c like Figure 2b, here we initialize it as Wup ∼ N(c, 0.022), which should be more suitable for training due to its asymmetry. For convenient comparison, the results of Wup = c (in grey) are also included. Overall, the results between Wup = c and Wup ∼ N(c, 0.022) are comparable. However, when c = 0for LoRA, the result of Gaussian initialization is slightly worse than the constant initialization. This further supports our starting point hypothesis, since the Gaussian initialization can’t guarantee the output from the adapter is strictly equal to zero at the beginning of fine-tuning. 24",
      "meta_data": {
        "arxiv_id": "2306.00477v4",
        "authors": [
          "Baohao Liao",
          "Shaomu Tan",
          "Christof Monz"
        ],
        "published_date": "2023-06-01T09:26:17Z",
        "pdf_url": "https://arxiv.org/pdf/2306.00477v4.pdf",
        "github_url": "https://github.com/baohaoliao/mefts"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the memory inefficiency of existing parameter-efficient fine-tuning (PEFT) methods, which, despite reducing trainable parameters, still require substantial activation memory. The key finding is the \"starting point hypothesis,\" emphasizing the importance of preserving the pre-trained model's initial state when introducing new parameters. Based on this, the authors propose Memory-Efficient Fine-Tuning (MEFT), a novel approach that modifies pre-trained language models (PLMs) to be reversible by inserting adapters, thereby eliminating the need to cache intermediate activations. MEFT significantly reduces activation memory by up to 84% compared to full fine-tuning, with a negligible number of trainable parameters, while maintaining comparable performance on various NLP tasks (GLUE, question-answering) and showing similar effectiveness in image classification.",
        "methodology": "MEFT leverages the principle of reversible neural networks, where intermediate activations are recomputed during backpropagation instead of being stored, reducing memory footprint to O(1). The core design adheres to the \"starting point hypothesis,\" which dictates that newly added adapter parameters must be initialized such that the modified PLM layer's output closely matches the original PLM layer's output at the beginning of training. Three MEFT architectures (MEFT1, MEFT2, MEFT3) are proposed, each inserting adapters into the PLM in different configurations (e.g., PLM layer as F, adapter as G; adapter as F, PLM layer as G; attention block as F, MLP block as G). These methods carefully set scaling factors (λ and β, often to 0.1) and, for MEFT1 and MEFT2, involve switching the order of outputs between reversible layers to preserve representation continuity. For deep models, a hybrid approach combines vanilla (activation caching) and reversible layers, or freezes shallow layers. For sequence-to-sequence tasks, a scaling factor γ (default 0.1) is introduced for the final output to maintain continuity for the pre-trained output layer.",
        "experimental_setup": "MEFT was evaluated on a comprehensive set of tasks and models. For sequence representation, the GLUE benchmark (RTE, MRPC, STS-B, CoLA, SST-2, QNLI, QQP, MNLI) was used with BERTbase, RoBERTalarge, and BARTlarge encoder models. For sequence-to-sequence, five question-answering tasks (OpenBookQA, PIQA, ARC-E, ARC-C, SciQ) were tested with OPT 1.3B and OPT6.7B. An image classification task (SVHN) with ViT was also included. Evaluation metrics included accuracy, Pearson correlation, and Matthews correlation. Baselines comprised full fine-tuning, various PEFT methods (AdapterH, AdapterP, Prefix-Tuning, LoRA, MAM, AutoPEFT), and memory-efficient baselines (Y-Tuning, LST). Experiments were conducted on a single NVIDIA RTX A6000 GPU with 48GB memory using the Transformers framework. Training involved grid searches for hyperparameters (learning rates, batch sizes, epochs), Adam optimizer, and mixed precision (FP16) training, with some FP32 comparisons. Further comparisons were made against gradient checkpointing, QLoRA, and by combining MEFT with ZeRO stage 3.",
        "limitations": "The study's main limitations include evaluation on a limited number of tasks and lack of experiments on encoder-decoder models, primarily due to computational resource constraints. MEFT also exhibits lower performance when trained in FP16 and on very deep models, attributed to increased reconstruction errors from numerical instability in FP16 and error accumulation over many reversible layers. While MEFT reduces activation memory, for larger models like OPT1.3B and OPT6.7B, the peak memory footprint is still dominated by model parameters, requiring additional techniques like FP16/INT8 model loading or ZeRO for further reduction. Additionally, the recomputation of activations in MEFT leads to slightly increased training time, approximately twice that of PEFT methods that cache activations.",
        "future_research_directions": "Future work includes applying MEFT to other domains such as computer vision and automatic speech recognition. There is also interest in extending MEFT to work with other bigger backbone models, like LLAMA, for more sequence-to-sequence tasks and exploring its transferability to different model architectures.",
        "experimental_code": "class Adapter(nn.Module):    def __init__(self, config, layernorm=False):        super().__init__()        self.dense1 = nn.Linear(config.hidden_size, config.adapter_bottleneck_dim)        self.dense2 = nn.Linear(config.adapter_bottleneck_dim, config.hidden_size)        self.dropout = nn.Dropout(config.hidden_dropout_prob)        if isinstance(config.hidden_act, str):            self.act_fn = ACT2FN[config.hidden_act]        else:            self.act_fn = config.hidden_act        if layernorm:            self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)        else:            self.LayerNorm = None    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:        if self.LayerNorm is not None:            hidden_states = self.LayerNorm(hidden_states)        hidden_states = self.dense1(hidden_states)        hidden_states = self.act_fn(hidden_states)        hidden_states = self.dropout(hidden_states)        hidden_states = self.dense2(hidden_states)        return hidden_statesclass RevBertSelfOutput(BertSelfOutput):    def __init__(self, config):        super().__init__(config)        if config.f_arch == \"attn\":            self.attn_adapter = Adapter(config)        else:            self.attn_adapter = None    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:        hidden_states = self.dense(hidden_states)        hidden_states = self.dropout(hidden_states)        if self.attn_adapter is not None: # For MEFT3            hidden_states = hidden_states + self.attn_adapter(hidden_states)        hidden_states = self.LayerNorm(hidden_states + input_tensor)        return hidden_statesclass RevBertOutput(BertOutput):    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor, adapter_change=None) -> torch.Tensor:        hidden_states = self.dense(hidden_states)        hidden_states = self.dropout(hidden_states)        # For MEFT1 and MEFT2: inserting an adapter in parallel to two consecutive feed-forward layers        if adapter_change is not None:            hidden_states = hidden_states + adapter_change        hidden_states = self.LayerNorm(hidden_states + input_tensor)        return hidden_statesclass RevBertLayer(BertLayer):    def __init__(self, config):        super().__init__(config)        self.output = RevBertOutput(config)        self.ffn_adapter = Adapter(config)        if config.f_arch == \"attn\":  # For MEFT3: insert an adapter to the attention block            self.rev_adapter = None            self.attention = RevBertAttention(config)        else:  # For MEFT1 and MEFT2            self.rev_adapter = Adapter(config, layernorm=config.layernorm_in_adapter)        self.x1_factor = config.x1_factor  # by default, lambda = 0.1        self.x2_factor = config.x2_factor  # by default, beta = 0.1        self.f_arch = config.f_arch  # What is the choice for the F architecture        if config.f_arch == \"layer\":  # MEFT1            self.F = self.forward_layer            self.G = self.forward_adapter        elif config.f_arch == \"adapter\":  # MEFT2            self.F = self.forward_adapter            self.G = self.forward_layer        elif config.f_arch == \"attn\":  # MEFT3            self.F = self.forward_attention            self.G = self.forward_mlp        self.seeds = {}    def set_seed(self, key):        if hasattr(torch.cuda, \"default_generators\") and len(torch.cuda.default_generators) > 0:            device_idx = torch.cuda.current_device()            seed = torch.cuda.default_generators[device_idx].seed()        else:            seed = int(torch.seed() % sys.maxsize)        self.seeds[key] = seed        torch.manual_seed(seed)    def forward(        self,        hidden_states: torch.Tensor,        attention_mask: Optional[torch.FloatTensor] = None,        head_mask: Optional[torch.FloatTensor] = None,        encoder_hidden_states: Optional[torch.FloatTensor] = None,        encoder_attention_mask: Optional[torch.FloatTensor] = None,        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,        output_attentions: Optional[bool] = False,    ) -> Tuple[torch.Tensor]:        x1, x2 = torch.chunk(hidden_states, 2, dim=-1)        if self.training:            self.set_seed(\"F\")        f_outputs = self.F(            x2,            attention_mask,            head_mask,            encoder_hidden_states,            encoder_attention_mask,            past_key_value,            output_attentions        )        if isinstance(f_outputs, tuple):            f_x2 = f_outputs[0]            outputs = f_outputs[1:]        else:            f_x2 = f_outputs        y1 = self.x1_factor * x1 + f_x2        if self.training:            self.set_seed(\"G\")        g_outputs = self.G(            y1,            attention_mask,            head_mask,            encoder_hidden_states,            encoder_attention_mask,            past_key_value,            output_attentions        )        if isinstance(g_outputs, tuple):            g_y1 = g_outputs[0]            outputs = g_outputs[1:]        else:            g_y1 = g_outputs        y2 = self.x2_factor * x2 + g_y1        if self.f_arch == \"attn\":            y = torch.cat([y1, y2], dim=-1)  # Don't switch for MEFT3        else:            y = torch.cat([y2, y1], dim=-1)  # Switch for MEFT1 and MEFT2        outputs = (y,) + outputs        return outputs    def backward_pass(        self,        y,        dy,        attention_mask,        head_mask,        encoder_hidden_states,        encoder_attention_mask,        past_key_value    ):        assert self.training, (            \"If you want to train `ReversibleModel` and its variations, make sure to use `model.train()` to put the\"            \" model into training mode.\"        )        if self.f_arch == \"attn\":            y1, y2 = torch.chunk(y, 2, dim=-1)  # Don't Switch for MEFT3            dy1, dy2 = torch.chunk(dy, 2, dim=-1)        else:            y2, y1 = torch.chunk(y, 2, dim=-1)  # Swich for MEFT1 and MEFT2            dy2, dy1 = torch.chunk(dy, 2, dim=-1)        with torch.enable_grad():            y1.requires_grad = True            torch.manual_seed(self.seeds[\"G\"])            g_outputs = self.G(                y1,                attention_mask=attention_mask,                head_mask=head_mask,                encoder_hidden_states=encoder_hidden_states,                encoder_attention_mask=encoder_attention_mask,                past_key_value=past_key_value,                output_attentions=False            )            if isinstance(g_outputs, tuple):                g_y1 = g_outputs[0]            else:                g_y1 = g_outputs            g_y1.backward(dy2, retain_graph=True)        with torch.no_grad():            x2 = (y2 - g_y1) / self.x2_factor            del g_y1, y2            dy1 += y1.grad            y1.grad = None        with torch.enable_grad():            x2.requires_grad = True            torch.manual_seed(self.seeds[\"F\"])            f_outputs = self.F(                x2,                attention_mask=attention_mask,                head_mask=head_mask,                encoder_hidden_states=encoder_hidden_states,                encoder_attention_mask=encoder_attention_mask,                past_key_value=past_key_value,                output_attentions=False            )            if isinstance(f_outputs, tuple):                f_x2 = f_outputs[0]            else:                f_x2 = f_outputs            f_x2.backward(dy1, retain_graph=False)        with torch.no_grad():            x1 = (y1 - f_x2) / self.x1_factor            del f_x2, y1            dy2 *= self.x2_factor            dy2 += x2.grad            x2.grad = None            dy1 *= self.x1_factor            x2 = x2.detach()        return torch.cat([x1, x2], dim=-1), torch.cat([dy1, dy2], dim=-1)    def forward_adapter(        self,        hidden_states: torch.Tensor,        attention_mask: Optional[torch.FloatTensor] = None,        head_mask: Optional[torch.FloatTensor] = None,        encoder_hidden_states: Optional[torch.FloatTensor] = None,        encoder_attention_mask: Optional[torch.FloatTensor] = None,        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,        output_attentions: Optional[bool] = False,    ):        return self.rev_adapter(hidden_states)    def forward_layer(        self,        hidden_states: torch.Tensor,        attention_mask: Optional[torch.FloatTensor] = None,        head_mask: Optional[torch.FloatTensor] = None,        encoder_hidden_states: Optional[torch.FloatTensor] = None,        encoder_attention_mask: Optional[torch.FloatTensor] = None,        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,        output_attentions: Optional[bool] = False,    ) -> Tuple[torch.Tensor]:        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None        self_attention_outputs = self.attention(            hidden_states,            attention_mask,            head_mask,            output_attentions=output_attentions,            past_key_value=self_attn_past_key_value,        )        attention_output = self_attention_outputs[0]        if self.is_decoder:            outputs = self_attention_outputs[1:-1]            present_key_value = self_attention_outputs[-1]        else:            outputs = self_attention_outputs[1:]        cross_attn_present_key_value = None        if self.is_decoder and encoder_hidden_states is not None:            if not hasattr(self, \"crossattention\"):                raise ValueError(                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers\"                    \" by setting `config.add_cross_attention=True`\"                )            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None            cross_attention_outputs = self.crossattention(                attention_output,                attention_mask,                head_mask,                encoder_hidden_states,                encoder_attention_mask,                cross_attn_past_key_value,                output_attentions,            )            attention_output = cross_attention_outputs[0]            outputs = outputs + cross_attention_outputs[1:-1]            cross_attn_present_key_value = cross_attention_outputs[-1]            present_key_value = present_key_value + cross_attn_present_key_value        layer_output = apply_chunking_to_forward(            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output        )        outputs = (layer_output,) + outputs        if self.is_decoder:            outputs = outputs + (present_key_value,)        return outputs    def forward_attention(        self,        hidden_states: torch.Tensor,        attention_mask: Optional[torch.FloatTensor] = None,        head_mask: Optional[torch.FloatTensor] = None,        encoder_hidden_states: Optional[torch.FloatTensor] = None,        encoder_attention_mask: Optional[torch.FloatTensor] = None,        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,        output_attentions: Optional[bool] = False,    ):        self_attention_outputs = self.attention(            hidden_states,            attention_mask,            head_mask,            output_attentions=output_attentions,            past_key_value=None,        )        return self_attention_outputs    def forward_mlp(        self,        hidden_states: torch.Tensor,        attention_mask: Optional[torch.FloatTensor] = None,        head_mask: Optional[torch.FloatTensor] = None,        encoder_hidden_states: Optional[torch.FloatTensor] = None,        encoder_attention_mask: Optional[torch.FloatTensor] = None,        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,        output_attentions: Optional[bool] = False,    ):        hidden_states = apply_chunking_to_forward(            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, hidden_states        )        return hidden_states    def feed_forward_chunk(self, attention_output):        if self.ffn_adapter is not None:            adapter_change = self.ffn_adapter(attention_output)        else:            adapter_change = None        intermediate_output = self.intermediate(attention_output)        layer_output = self.output(intermediate_output, attention_output, adapter_change=adapter_change)        return layer_outputclass RevBackProp(Function):    @staticmethod    def forward(        ctx,        hidden_states,        layers,        attention_mask,        layer_head_mask,        encoder_hidden_states,        encoder_attention_mask,        past_key_value,        output_attentions,    ):        for i, layer_module in enumerate(layers):            layer_outputs = layer_module(                hidden_states,                attention_mask,                layer_head_mask,                encoder_hidden_states,                encoder_attention_mask,                past_key_value,                output_attentions,            )            hidden_states = layer_outputs[0]        ctx.save_for_backward(hidden_states.detach())        ctx.layers = layers        ctx.attention_mask = attention_mask        return hidden_states    def backward(ctx, dy):        y, = ctx.saved_tensors        layers = ctx.layers        attention_mask = ctx.attention_mask        for i, layer_module in enumerate(layers[::-1]):            y, dy = layer_module.backward_pass(                y,                dy,                attention_mask=attention_mask,                head_mask=None,                encoder_hidden_states=None,                encoder_attention_mask=None,                past_key_value=None            )        return dy, None, None, None, None, None, None, Noneclass RevBertEncoder(nn.Module):    def __init__(self, config):        super().__init__()        self.config = config        self.layer = nn.ModuleList([RevBertLayer(config) for _ in range(config.num_hidden_layers)])        self.gradient_checkpointing = False        self.num_rev_layers = config.num_rev_layers        assert self.num_rev_layers <= config.num_hidden_layers    def vanilla_backward(        hidden_states,        layers,        attention_mask    ):        for i, layer_module in enumerate(layers):            layer_outputs = layer_module(                hidden_states,                attention_mask=attention_mask,                head_mask=None,                encoder_hidden_states=None,                encoder_attention_mask=None,                past_key_value=None,                output_attentions=False,            )            hidden_states = layer_outputs[0]        return hidden_states    def forward(        self,        hidden_states: torch.Tensor,        attention_mask: Optional[torch.FloatTensor] = None,        head_mask: Optional[torch.FloatTensor] = None,        encoder_hidden_states: Optional[torch.FloatTensor] = None,        encoder_attention_mask: Optional[torch.FloatTensor] = None,        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,        use_cache: Optional[bool] = None,        output_attentions: Optional[bool] = False,        output_hidden_states: Optional[bool] = False,        return_dict: Optional[bool] = True,    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:        hidden_states = torch.cat([hidden_states, hidden_states], dim=-1)        layers_for_reverse_backward = self.layer[:self.num_rev_layers]        layers_for_vanilla_backward = self.layer[self.num_rev_layers:]        if len(layers_for_reverse_backward) == 0:            executing_fn = RevBertEncoder.vanilla_backward            hidden_states = executing_fn(                hidden_states,                layers_for_vanilla_backward,                attention_mask            )        elif len(layers_for_reverse_backward) == self.config.num_hidden_layers:            executing_fn = RevBackProp.apply            hidden_states = executing_fn(                hidden_states,                layers_for_reverse_backward,                attention_mask,                None,                None,                None,                None,                False            )        else:            lower_executing_fn = RevBackProp.apply            hidden_states = lower_executing_fn(                hidden_states,                layers_for_reverse_backward,                attention_mask,                None,                None,                None,                None,                False            )            higher_executing_fn = RevBertEncoder.vanilla_backward            hidden_states = higher_executing_fn(                hidden_states,                layers_for_vanilla_backward,                attention_mask            )        x1, x2 = torch.chunk(hidden_states, 2, dim=-1)        hidden_states = (x1 + x2) / 2.        return BaseModelOutputWithPastAndCrossAttentions(            last_hidden_state=hidden_states,            past_key_values=None,            hidden_states=None,            attentions=None,            cross_attentions=None,        )class ModelArguments:    model_name_or_path: str = field(        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}    )    config_name: Optional[str] = field(        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}    )    tokenizer_name: Optional[str] = field(        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}    )    cache_dir: Optional[str] = field(        default=None,        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},    )    use_fast_tokenizer: bool = field(        default=True,        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},    )    model_revision: str = field(        default=\"main\",        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},    )    use_auth_token: bool = field(        default=False,        metadata={            \"help\": (                \"Will use the token generated when running `huggingface-cli login` (necessary to use this script \"                \"with private models).\"            )        },    )    ignore_mismatched_sizes: bool = field(        default=False,        metadata={\"help\": \"Will enable to load a pretrained model whose head dimensions are different.\"},    )    ## add args for adapter    adapter_bottleneck_dim: int = field(        default=0,        metadata={\"help\": \"bottleneck dimension for adapter. 0 means no adapter\"},    )    layernorm_in_adapter: bool = field(        default=False,        metadata={\"help\": \"whether add layernorm in the adapter for G\"},    )    num_rev_layers: int = field(        default=0,        metadata={\"help\": \"number of reversible layers, when it's 0, it means we use vanilla backward\"},    )    x1_factor: float = field(        default=1,        metadata={\"help\": \"factor for x1\"},    )    x2_factor: float = field(        default=1,        metadata={\"help\": \"factor for x2\"},    )    f_arch: ChoiceEnum([\"layer\", \"adapter\", \"attn\"]) = field(        default=\"layer\",        metadata={\"help\": \"what is the architecture for F, choices=[layer, adapter, attn]\"}    )    freeze_irreversible_layers: bool = field(        default=False,        metadata={\"help\": \"if true, freeze the shallower irreversible layers\"}    )def main():    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))    else:        model_args, data_args, training_args = parser.parse_args_into_dataclasses()    send_example_telemetry(\"run_glue\", model_args, data_args)    logging.basicConfig(        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",        datefmt=\"%m/%d/%Y %H:%M:%S\",        handlers=[logging.StreamHandler(sys.stdout)],    )    if training_args.should_log:        transformers.utils.logging.set_verbosity_info()    log_level = training_args.get_process_log_level()    logger.setLevel(log_level)    datasets.utils.logging.set_verbosity(log_level)    transformers.utils.logging.set_verbosity(log_level)    transformers.utils.logging.enable_default_handler()    transformers.utils.logging.enable_explicit_format()    logger.warning(        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"    )    logger.info(f\"Training/evaluation parameters {training_args}\")    last_checkpoint = None    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:        last_checkpoint = get_last_checkpoint(training_args.output_dir)        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:            raise ValueError(                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"                \"Use --overwrite_output_dir to overcome.\"            )        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:            logger.info(                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"            )    set_seed(training_args.seed)    if data_args.task_name is not None:        is_regression = data_args.task_name == \"stsb\"        if not is_regression:            label_list = raw_datasets[\"train\"].features[\"label\"].names            num_labels = len(label_list)        else:            num_labels = 1    else:        is_regression = raw_datasets[\"train\"].features[\"label\"].dtype in [\"float32\", \"float64\"]        if is_regression:            num_labels = 1        else:            label_list = raw_datasets[\"train\"].unique(\"label\")            label_list.sort()            num_labels = len(label_list)    config = AutoConfig.from_pretrained(        model_args.config_name if model_args.config_name else model_args.model_name_or_path,        num_labels=num_labels,        finetuning_task=data_args.task_name,        cache_dir=model_args.cache_dir,        revision=model_args.model_revision,        use_auth_token=True if model_args.use_auth_token else None,    )    config.adapter_bottleneck_dim = model_args.adapter_bottleneck_dim    config.layernorm_in_adapter = model_args.layernorm_in_adapter    config.num_rev_layers = model_args.num_rev_layers    config.x1_factor = model_args.x1_factor    config.x2_factor = model_args.x2_factor    config.f_arch = model_args.f_arch    config.freeze_irreversible_layers = model_args.freeze_irreversible_layers    tokenizer = AutoTokenizer.from_pretrained(        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,        cache_dir=model_args.cache_dir,        use_fast=model_args.use_fast_tokenizer,        revision=model_args.model_revision,        use_auth_token=True if model_args.use_auth_token else None,    )    sequence_classification_model = RevBertForSequenceClassification    model = sequence_classification_model.from_pretrained(        model_args.model_name_or_path,        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),        config=config,        cache_dir=model_args.cache_dir,        revision=model_args.model_revision,        use_auth_token=True if model_args.use_auth_token else None,        ignore_mismatched_sizes=model_args.ignore_mismatched_sizes,    )    logger.info(model)    logger.info(f\"Total num of parameters: {sum(p.numel() for p in model.parameters())}\")    if model_args.adapter_bottleneck_dim > 0:        for param in model.parameters():            param.requires_grad = False        non_freeze_sets = [\"adapter\", \"classifier\"]        if model_args.freeze_irreversible_layers:            non_freeze_sets.append(f\"encoder.layer.{config.num_hidden_layers - model_args.num_rev_layers - 1}.\"                                   f\"output.LayerNorm\")        else:            non_freeze_sets.append(\"embeddings.LayerNorm\")        for key in non_freeze_sets:            for n, p in model.named_parameters():                if key in n:                    p.requires_grad = True    if data_args.pad_to_max_length:        padding = \"max_length\"    else:        padding = False    label_to_id = None    if (        model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id        and data_args.task_name is not None        and not is_regression    ):        label_name_to_id = {k.lower(): v for k, v in model.config.label2id.items()}        if sorted(label_name_to_id.keys()) == sorted(label_list):            label_to_id = {i: int(label_name_to_id[label_list[i]]) for i in range(num_labels)}        else:            logger.warning(                \"Your model seems to have been trained with labels, but they don't match the dataset: \",                f\"model labels: {sorted(label_name_to_id.keys())}, dataset labels: {sorted(label_list)}.\"                \"\\nIgnoring the model labels as a result.\",            )    elif data_args.task_name is None and not is_regression:        label_to_id = {v: i for i, v in enumerate(label_list)}    if label_to_id is not None:        model.config.label2id = label_to_id        model.config.id2label = {id: label for label, id in config.label2id.items()}    elif data_args.task_name is not None and not is_regression:        model.config.label2id = {l: i for i, l in enumerate(label_list)}        model.config.id2label = {id: label for label, id in config.label2id.items()}    max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)    def preprocess_function(examples):        args = (            (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])        )        result = tokenizer(*args, padding=padding, max_length=max_seq_length, truncation=True)        if label_to_id is not None and \"label\" in examples:            result[\"label\"] = [(label_to_id[l] if l != -1 else -1) for l in examples[\"label\"]]        return result    with training_args.main_process_first(desc=\"dataset map pre-processing\"):        raw_datasets = raw_datasets.map(            preprocess_function,            batched=True,            load_from_cache_file=not data_args.overwrite_cache,            desc=\"Running tokenizer on dataset\",        )    if training_args.do_train:        if \"train\" not in raw_datasets:            raise ValueError(\"--do_train requires a train dataset\")        train_dataset = raw_datasets[\"train\"]        if data_args.max_train_samples is not None:            max_train_samples = min(len(train_dataset), data_args.max_train_samples)            train_dataset = train_dataset.select(range(max_train_samples))    if training_args.do_eval:        if \"validation\" not in raw_datasets and \"validation_matched\" not in raw_datasets:            raise ValueError(\"--do_eval requires a validation dataset\")        eval_dataset = raw_datasets[\"validation_matched\" if data_args.task_name == \"mnli\" else \"validation\"]        if data_args.max_eval_samples is not None:            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)            eval_dataset = eval_dataset.select(range(max_eval_samples))    if training_args.do_predict or data_args.task_name is not None or data_args.test_file is not None:        if \"test\" not in raw_datasets and \"test_matched\" not in raw_datasets:            raise ValueError(\"--do_predict requires a test dataset\")        predict_dataset = raw_datasets[\"test_matched\" if data_args.task_name == \"mnli\" else \"test\"]        if data_args.max_predict_samples is not None:            max_predict_samples = min(len(predict_dataset), data_args.max_predict_samples)            predict_dataset = predict_dataset.select(range(max_predict_samples))    if training_args.do_train:        for index in random.sample(range(len(train_dataset)), 3):            logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")    if data_args.task_name is not None:        metric = evaluate.load(\"glue\", data_args.task_name)    else:        metric = evaluate.load(\"accuracy\")    def compute_metrics(p: EvalPrediction):        preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions        preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=1)        if data_args.task_name is not None:            result = metric.compute(predictions=preds, references=p.label_ids)            if len(result) > 1:                result[\"combined_score\"] = np.mean(list(result.values())).item()            return result        elif is_regression:            return {\"mse\": ((preds - p.label_ids) ** 2).mean().item()}        else:            return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}    if data_args.pad_to_max_length:        data_collator = default_data_collator    elif training_args.fp16:        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)    else:        data_collator = None    training_args.model_name_or_path = model_args.model_name_or_path    if model_args.freeze_irreversible_layers:        training_args.start_layer = config.num_hidden_layers - model_args.num_rev_layers - 1    else:        training_args.start_layer = -1    trainer = CustomTrainer(        model=model,        args=training_args,        train_dataset=train_dataset if training_args.do_train else None,        eval_dataset=eval_dataset if training_args.do_eval else None,        compute_metrics=compute_metrics,        tokenizer=tokenizer,        data_collator=data_collator,        callbacks=[EarlyStoppingCallback(early_stopping_patience=8)]    )    if training_args.do_train:        checkpoint = None        if training_args.resume_from_checkpoint is not None:            checkpoint = training_args.resume_from_checkpoint        elif last_checkpoint is not None:            checkpoint = last_checkpoint        train_result = trainer.train(resume_from_checkpoint=checkpoint)        metrics = train_result.metrics        max_train_samples = (            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)        )        metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))        metrics[\"peak mem (G)\"] = torch.cuda.max_memory_allocated() / (1024 * 1024 * 1000)        metrics[\"activation mem (G)\"] = (torch.cuda.max_memory_allocated() - torch.cuda.memory_allocated()) / (1024 * 1024 * 1000)        trainer.save_model()        trainer.log_metrics(\"train\", metrics)        trainer.save_metrics(\"train\", metrics)        trainer.save_state()    if training_args.do_eval:        logger.info(\"*** Evaluate ***\")        tasks = [data_args.task_name]        eval_datasets = [eval_dataset]        if data_args.task_name == \"mnli\":            tasks.append(\"mnli-mm\")            valid_mm_dataset = raw_datasets[\"validation_mismatched\"]            if data_args.max_eval_samples is not None:                max_eval_samples = min(len(valid_mm_dataset), data_args.max_eval_samples)                valid_mm_dataset = valid_mm_dataset.select(range(max_eval_samples))            eval_datasets.append(valid_mm_dataset)            combined = {}        for eval_dataset, task in zip(eval_datasets, tasks):            metrics = trainer.evaluate(eval_dataset=eval_dataset)            max_eval_samples = (                data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)            )            metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))            if task == \"mnli-mm\":                metrics = {k + \"_mm\": v for k, v in metrics.items()}            if task is not None and \"mnli\" in task:                combined.update(metrics)            metrics[\"peak mem(G)\"] = torch.cuda.max_memory_allocated() / (1024 * 1024 * 1000)            trainer.log_metrics(\"eval\", metrics)            trainer.save_metrics(\"eval\", combined if task is not None and \"mnli\" in task else metrics)    if training_args.do_predict:        logger.info(\"*** Predict ***\")        tasks = [data_args.task_name]        predict_datasets = [predict_dataset]        if data_args.task_name == \"mnli\":            tasks.append(\"mnli-mm\")            predict_datasets.append(raw_datasets[\"test_mismatched\"])        for predict_dataset, task in zip(predict_datasets, tasks):            predict_dataset = predict_dataset.remove_columns(\"label\")            predictions = trainer.predict(predict_dataset, metric_key_prefix=\"predict\").predictions            predictions = np.squeeze(predictions) if is_regression else np.argmax(predictions, axis=1)            output_predict_file = os.path.join(training_args.output_dir, f\"predict_results_{task}.txt\")            if trainer.is_world_process_zero():                with open(output_predict_file, \"w\") as writer:                    logger.info(f\"***** Predict results {task} *****\")                    writer.write(\"index\\tprediction\\n\")                    for index, item in enumerate(predictions):                        if is_regression:                            writer.write(f\"{index}\\t{item:3.3f}\\n\")                        else:                            item = label_list[item]                            writer.write(f\"{index}\\t{item}\\n\")    kwargs = {\"finetuned_from\": model_args.model_name_or_path, \"tasks\": \"text-classification\"}    if data_args.task_name is not None:        kwargs[\"language\"] = \"en\"        kwargs[\"dataset_tags\"] = \"glue\"        kwargs[\"dataset_args\"] = data_args.task_name        kwargs[\"dataset\"] = f\"GLUE {data_args.task_name.upper()}\"    if training_args.push_to_hub:        trainer.push_to_hub(**kwargs)    else:        trainer.create_model_card(**kwargs)def _mp_fn(index):    main()if __name__ == \"__main__\":    main()class CustomTrainer(Trainer):    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:        model.train()        inputs = self._prepare_inputs(inputs)        if is_sagemaker_mp_enabled():            loss_mb = smp_forward_backward(model, inputs, self.args.gradient_accumulation_steps)            return loss_mb.reduce_mean().detach().to(self.args.device)        with self.compute_loss_context_manager():            loss = self.compute_loss(model, inputs)        if self.args.n_gpu > 1:            loss = loss.mean()        if self.args.gradient_accumulation_steps > 1 and not self.deepspeed:            loss = loss / self.args.gradient_accumulation_steps        if self.do_grad_scaling:            self.scaler.scale(loss).backward()        elif self.use_apex:            with amp.scale_loss(loss, self.optimizer) as scaled_loss:                scaled_loss.backward()        elif self.deepspeed:            loss = self.deepspeed.backward(loss)        else:            loss.backward()        if self.args.start_layer == -1:            model.bert.embeddings.LayerNorm.bias.grad = None            model.bert.embeddings.LayerNorm.weight.grad = None        else:            model.bert.encoder.layer[self.args.start_layer].output.LayerNorm.bias.grad = None            model.bert.encoder.layer[self.args.start_layer].output.LayerNorm.weight.grad = None        return loss.detach()",
        "experimental_info": "MEFT (Memory-Efficient Fine-Tuning) is implemented using reversible neural network principles to achieve O(1) memory footprint during backpropagation by recomputing intermediate activations. The core method involves splitting hidden states into two parts (x1, x2), applying two functions (F and G), and combining them (y1, y2). A custom `RevBackProp` autograd function handles the reversible backward pass. \n\nThree MEFT architectures are supported via the `f_arch` configuration parameter:\n1.  **MEFT1 (`f_arch=\"layer\"`):** F is the PLM layer (`forward_layer`), and G is an adapter (`forward_adapter`). The outputs are switched (concatenated as [y2, y1]). Adapters are inserted in parallel within the feed-forward chunk (`ffn_adapter`) and in parallel to the reversible layer (`rev_adapter`).\n2.  **MEFT2 (`f_arch=\"adapter\"`):** F is an adapter (`forward_adapter`), and G is the PLM layer (`forward_layer`). The outputs are also switched (concatenated as [y2, y1]). Adapter insertion is similar to MEFT1.\n3.  **MEFT3 (`f_arch=\"attn\"`):** F is the attention block (`forward_attention`), and G is the MLP block (`forward_mlp`). Outputs are not switched (concatenated as [y1, y2]). An adapter (`attn_adapter`) is inserted into the output of the attention block within `RevBertSelfOutput`.\n\nKey experimental settings and features:\n-   **Scaling Factors:** `x1_factor` (lambda) and `x2_factor` (beta) are applied during the forward pass (e.g., `y1 = x1_factor * x1 + f_x2`, `y2 = x2_factor * x2 + g_y1`). These are configurable via `ModelArguments` and default to 1, but typical values are 0.1 as per comments.\n-   **Hybrid Reversible/Vanilla Layers:** The `RevBertEncoder` supports using a subset of layers as reversible (`num_rev_layers`) and the remaining layers as vanilla (standard Transformer layers with activation caching). This allows for a flexible memory-performance trade-off.\n-   **Starting Point Hypothesis & Parameter Freezing:** When adapters are used (`adapter_bottleneck_dim > 0`), all pre-trained model parameters are initially frozen. Only adapter parameters and the classifier head are unfrozen. Additionally, to support the starting point hypothesis and enable gradient flow without updating certain non-adapter parameters:\n    -   If `freeze_irreversible_layers` is `True`, the `LayerNorm` of the last irreversible layer is unfrozen.\n    -   Otherwise, the `embeddings.LayerNorm` is unfrozen.\n    -   In `CustomTrainer`, after the backward pass, the gradients of these specific unfrozen `LayerNorm` parameters (`bias.grad` and `weight.grad`) are explicitly set to `None` to prevent their update while still allowing gradients to pass through.\n-   **Adapter Module:** The `Adapter` class defines a bottleneck structure with two dense layers, an activation function, and dropout. It can optionally include a `LayerNorm` (`layernorm_in_adapter`).\n-   **Configuration:** Parameters like `adapter_bottleneck_dim`, `layernorm_in_adapter`, `num_rev_layers`, `x1_factor`, `x2_factor`, `f_arch`, and `freeze_irreversible_layers` are parsed from command-line arguments in `run_glue.py` and passed to the model's configuration."
      }
    },
    {
      "title": "The Expressive Power of Low-Rank Adaptation",
      "abstract": "Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method that\nleverages low-rank adaptation of weight matrices, has emerged as a prevalent\ntechnique for fine-tuning pre-trained models such as large language models and\ndiffusion models. Despite its huge success in practice, the theoretical\nunderpinnings of LoRA have largely remained unexplored. This paper takes the\nfirst step to bridge this gap by theoretically analyzing the expressive power\nof LoRA. We prove that, for fully connected neural networks, LoRA can adapt any\nmodel $f$ to accurately represent any smaller target model $\\overline{f}$ if\nLoRA-rank $\\geq(\\text{width of }f) \\times \\frac{\\text{depth of\n}\\overline{f}}{\\text{depth of }f}$. We also quantify the approximation error\nwhen LoRA-rank is lower than the threshold. For Transformer networks, we show\nany model can be adapted to a target model of the same size with\nrank-$(\\frac{\\text{embedding size}}{2})$ LoRA adapters.",
      "full_text": "Published as a conference paper at ICLR 2024 THE EXPRESSIVE POWER OF LOW-RANK ADAPTATION Yuchen Zeng Department of Computer Science University of Wisconsin-Madison yzeng58@wisc.edu Kangwook Lee Department of Electrical and Computer Engineering University of Wisconsin-Madison kangwook.lee@wisc.edu ABSTRACT Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method that leverages low-rank adaptation of weight matrices, has emerged as a prevalent tech- nique for fine-tuning pre-trained models such as large language models and diffu- sion models. Despite its huge success in practice, the theoretical underpinnings of LoRA have largely remained unexplored. This paper takes the first step to bridge this gap by theoretically analyzing the expressive power of LoRA. We prove that, for fully connected neural networks, LoRA can adapt any model f to accurately represent any smaller target model f if LoRA-rank ≥ (width of f) × depth of f depth of f , under a mild assumption. We quantify the approximation error when the LoRA- rank is lower than the threshold. For Transformer networks, we show any model can be adapted to a target model of the same size with rank- (embedding size 2 ) LoRA adapters. Our study reveals numerous theoretical insights on hyperparameter tun- ing and algorithm development for LoRA, all of which are empirically validated. 1 I NTRODUCTION Recent foundation models, such as large language models (OpenAI, 2023; Liu et al., 2019; Touvron et al., 2023), have achieved remarkable success in a wide range of applications. Due to their sub- stantial size, the standard full fine-tuning approach—where all the model’s parameters are updated for specialized tasks—is becoming increasingly difficult and inefficient. This leads to the growing popularity of parameter-efficient fine-tuning approaches (Hu et al., 2022a; Liu et al., 2022b; Ben Za- ken et al., 2022; Hu et al., 2022b). Instead of updating all parameters, these approaches selectively update smaller subsets of weights or introduce lightweight adapters, thereby greatly decreasing the computational and storage costs. The most dominant approach along this line is Low-Rank Adaptation (LoRA) (Hu et al., 2022a), which employs lightweight low-rank adapters to pre-trained weight matrices. Far from merely en- hancing computational efficiency, empirical evidence has shown that LoRA can match or even ex- ceed the performance of full fine-tuning (Hu et al., 2022a). To date, LoRA has been widely used and achieved considerable success in adapting large language models (Hu et al., 2022a; Dinh et al., 2022b) and image generation models (Ryu, 2023; Fan et al., 2023) for various downstream tasks. Despite the empirical success of LoRA, little is known in theory about how it works. A notable exception (Malladi et al., 2023) showed that LoRA finetuning is approximately equivalent to full fine-tuning in the lazy regime. However, many theoretical questions remain open, such as: What is the minimum rank of the LoRA adapters required to adapt a (pre-trained) model f to match the functionality of the target model f? How does the model architecture (i.e., depth, width) affect the minimal rank? If the adapter rank is lower than this threshold, what is the resulting approximation error? Answering such questions will provide important theoretical insights into when and why LoRA achieves effective adaptation. Our Contributions. In this paper, we present the first set of theoretical results that character- ize the expressive power of Low-Rank Adaptation (LoRA) for Fully Connected Neural Networks (FNN) and Transformer Networks (TFN). In particular, we identify the necessary LoRA-rank for adapting a frozen model to exactly match a target model. For FNN cases, we also establish the required LoRA-rank for closely approximating the target model when a small approximation error is allowed. Our work focuses solely on the expressive power of the model with low-rank adapters, i.e., we show that under which conditions, effective low-rank adapters exist for the given adaptation task. This excludes other aspects such as optimization and generalization. We now present the essence of our main theoretical findings in the following informal statement. 1 arXiv:2310.17513v3  [cs.LG]  18 Mar 2024Published as a conference paper at ICLR 2024 Theorem 1 (Informal). Let f be a target FNN and f0 be an arbitrary frozen FNN. Under mild conditions on ranks and network architectures, there exist low-rank adapters such that a low-rank adapted version of f0 is exactly equal to f. We present the detailed formulations of Theorem 1 under two scenarios: (i) applying a uni- form rank across all LoRA adapters, as detailed in Theorem 3 and the specialized instance Corollary 4 for randomly drawn frozen and target models; and (ii) allowing different ranks ap- plied to each LoRA adapter, as described in Theorem 6. To the best of our knowledge, this is the first known theoretical results on the expressive power of LoRA. While this informal theorem is for exact approximation, we also derive the approximation bounds as well, i.e., we characterize the approximation error between the finetuned model and the target model as a function of the LoRA- rank, as provided in Theorem 5 for the uniform LoRA-rank scenario and Theorem 6 for general cases. Furthermore, within the same framework, we investigate the expressive power of tuning the final layers for randomly generated frozen models, as described in Lemma 4. This result allows us to contrast LoRA and final layer tuning, thereby providing insights for future algorithm development. We summarize our main findings on TFN in the following informal theorem. Theorem 2 (Informal). Let f be the target TFN and f0 be the frozen TFN. Under mild conditions on ranks and network architectures, there exist low-rank adapters for attention weight matrices such that a low-rank adapted version of f0 is exactly equal to f. The formal statement of Theorem 2 is provided in Theorem 7, with a specialized version in Corol- lary 10 tailored for randomly generated frozen and target models. In Sec. 5 and G, we perform experiments on both synthetic and real datasets to substantiate our theoretical results, demonstrating the practical applicability of our theoretical findings in algorithm development and hyperparameter tuning. 1.1 R ELATED WORKS Expressive Power of Neural Networks Theoretical study of the expressive power of unfrozen neural networks has progressed since the first universal approximation theorem (Hornik et al., 1989), showing that sufficient network width and depth can guarantee function approximation (Bengio & Delalleau, 2011; Eldan & Shamir, 2016; Liang & Srikant, 2017). Many recent studies obtained sim- ilar results for deep neural networks with modern twists such as ReLU activations and Transformer networks (Yun et al., 2020a; Raghu et al., 2017; Telgarsky, 2016; 2015; Bietti & Bach, 2021; Oy- mak et al., 2023; Lee et al., 2017; Shen & Zhang, 2020; Likhosherstov et al., 2021; Hsu et al., 2021; Park et al., 2021; Yun et al., 2020b; Giannou et al., 2023b). Metrics like Vapnik-Chervonenkis and Rademacher complexities (Vapnik & Chervonenkis, 2015; Bartlett & Mendelson, 2001) assess clas- sification capacity. However, these theories cannot fully explain the performance of frozen neural networks as they generally cannot factor in pre-trained model parameters and adaptation methods. Expressive Power of Adaptation Methods In stark contrast to the flourishing research on the expressive power of neural networks, there exists a limited number of works investigating the ex- pressive power of adaptation methods. A notable exception is Giannou et al. (2023a), investigating the expressive power of normalization parameter fine-tuning. They demonstrate that fine-tuning the normalization layers alone can adapt a randomly initialized ReLU network to match any target net- work that is O(width) times smaller. We borrow some proof techniques from this work, including techniques for extending results from linear neural networks to ReLU neural networks. In another recent work (Englert & Lazic, 2022), the authors show that neural reprogramming (Elsayed et al., 2019; Engel et al., 2018; Lee et al., 2020; Dinh et al., 2022a; Chen, 2022), a technique that modifies only the inputs while keeping the pretrained network frozen, can adapt any random two-layer ReLU network to achieve arbitrarily high accuracy on a Bernoulli data model over hypercube vertices. Despite these early attempts, no existing study has yet explored the expressive power of LoRA, the current leading adaptation method. A more detailed discussion of related works is provided in Sec. B. 1.2 N OTATIONS Define [N] := {1, 2, . . . , N}. Let the operators ∧ and ∨ denote the minimum function and the maximum function, respectively. We useI to represent the identity matrix. 2Published as a conference paper at ICLR 2024 For a sequence ofL matrices (Wl)L l=1, we simplify the product of these matricesWLWL−1 ··· W1 as QL l=1 Wl, with matrices multiplied in descending order from WL to W1. When m > n, we define Pn i=m ai = 0 and Qn i=m ai = 1 for scalars (ai)n i=m, and Pn i=m Wi = O and Qn i=m Wi = I for square matrices (Wi)n i=m. Singular Value Decomposition (SVD) of the matrix W can be expressed as W = UDV ⊤, where U, V ∈ RD×D are orthonormal matrices and D ∈ RD×D is a diagonal matrix. The singular values, sorted in descending sequence, are represented on the diagonal of D, denoted as σ1(W) ≥ σ2(W) ≥ ··· ≥σD(W) ≥ 0, where σd(W) denotes the d-th largest singular value for all d ∈ [D]. When d > D, σd(W) is defined as zero. The best rank- r approximation (in the Frobenius norm or the 2-norm) of W is Pr i=1 σiuivT i , where ui and vi are the i-th column of U and V , respectively (Eckart & Young, 1936; Mirsky, 1960). We denote this best rank- r approximation by LRr(W), where LR is a shorthand for “Low-Rank”. When r ≥ rank(W), it is clear that LRr(W) = W. Occasionally, the subscript r may be omitted to indicate a general low-rank approximation without specifying the rank. 2 W ARM UP : E XPRESSIVE POWER OF LINEAR MODELS WITH LORA Before delving into the expressive power of LoRA for FNN and TFN, we begin by investigating the simplest scenario: both the target model f and the frozen model f0 are linear, i.e., Target Model f(x) = Wx, Frozen Model f0(x) = WL ··· W1x = \u0010QL l=1 Wl \u0011 x. This problem serves as a simplified version of approximating a target FNN, where the target modelf has a single layer, the frozen modelf0 has L layers, all bias vectors in both two models are zero, and the activation functions are linear. Throughout this paper, for the sake of simplicity, we will assume that both models have the same number of neurons in each layer, i.e., W, W1, . . . ,WL ∈ RD×D. Nevertheless, our results are readily extendable to situations where the frozen model is wider than the target model, which is a more natural setting as the frozen models are often overparameterized to ensure high capacity and good performance across diverse tasks in practice. See the discussion in Sec. H for more details. The objective here is to incorporate low-rank adapters into the frozen model so that the adapted model can effectively approximate the target model. Unless otherwise specified, we always consider a uniform LoRA-rank for all low-rank adapters throughout this paper. For a given LoRA-rank R ∈ [D], we apply LoRA adapters ∆W1, . . . ,∆WL to the frozen model, and the adapted model can be represented as Adapted Model f(x) = (WL + ∆WL) ··· (W1 + ∆W1)x, where rank(∆Wl) ≤ R for all l ∈ [L]. Since the frozen model and adpated model are all linear, we can focus on quantifying the discrepancy between the linear coefficients, i.e.,QL l=1(Wl+∆Wl)−W. In the subsequent lemma, we establish the minimal achievable norm, and identify the smallest LoRA-rank required for the adapted model to exactly represent the target model, i.e., f = f, under a non-singularity assumption. We will demonstrate in Sec. 3.3 that this non-singularity assumption is mild, as it can be satisfied even by randomly generated weight matrices. Lemma 1. Define error matrix E := W − QL l=1 Wl, and denote its rank by RE = rank(E). For a given LoRA-rank R ∈ [D], assume that all the weight matrices of the frozen model (Wl)L l=1, andQL l=1 Wl + LRr(E) are non-singular for all r ≤ R(L − 1). Then, we have the following: min ∆Wl:rank(∆Wl)≤R \r\r\rQL l=1(Wl + ∆Wl) − W \r\r\r 2 = σRL+1(E). Thus, when R ≥ ⌈RE L ⌉, the optimal solution satisfies QL l=1(Wl + ∆Wl) = W, implying f = f. Proof Sketch. We start the proof by noting that the distance between the adapted and target models\r\r\r\r\r LY l=1 (Wl + ∆Wl) − W \r\r\r\r\r 2 = \r\r\r\r\r  LY l=1 (Wl + ∆Wl) − LY l=1 Wl ! −   W − LY l=1 Wl !\r\r\r\r\r 2 . 3Published as a conference paper at ICLR 2024 The remaining proof aims to minimize the right-hand side under the constraint rank(∆Wl) ≤ R for all l ∈ [L]. The basic idea here is to match QL l=1(Wl + ∆Wl) − QL l=1 Wl with the best rank-r approximation of W − QL l=1 Wl. The key steps to solve this problem are as follows. 1. Demonstrate that QL l=1(Wl + ∆Wl) − QL l=1 Wl can be decomposed into L terms: QL l=1(Wl + ∆Wl) − QL l=1 Wl = PL l=1 \u0010QL i=l+1 Wi \u0011 ∆Wl \u0010Ql−1 i=1(Wi + ∆Wi) \u0011 . Since rank(∆Wl) ≤ R, it follows that rank \u0010QL l=1(Wl + ∆Wl) − QL l=1 Wl \u0011 ≤ RL. 2. Consider the rank- RL approximation of W − QL l=1 Wl. Decompose this low-rank approxima- tion into L terms PL l=1 El such that rank(El) ≤ R, where El’s will be determined later. 3. To match QL l=1(Wl + ∆ Wl) − QL l=1 Wl with the rank- RL approximation of W −QL l=1 Wl, we let \u0010QL i=l+1 Wi \u0011 ∆Wl \u0010Ql−1 i=1(Wi + ∆Wi) \u0011 = El by choosing ∆Wl = \u0010QL i=l+1 Wi \u0011−1 El \u0010Ql−1 i=1(Wi + ∆Wi) \u0011−1 . 4. Select appropriate (El)L l=1 such that Wi + ∆Wi are invertible for i ∈ [L]. The complete proof and the explicit construction of optimal LoRA adapters, are detailed in Sec. D. In fact, this lemma delivers a crucial insight. When we consider L = 1 and R = D, the lemma becomes strikingly similar to the Eckart–Young–Mirsky theorem (Eckart & Young, 1936; Mirsky, 1960). However, there is a significant difference from the classical theorem on the optimal low-rank approximation, which involves a single target matrix and a single matrix as an optimization variable. Our lemma demonstrates that a comparable result can be achieved for a “product of matrices,” where each matrix is optimized subject to a low-rank constraint. That being said, even though each matrix is constrained by a low rank, the “effective rank” is the sum of these low ranks, i.e., in this scenario, is LR. Consequently, once the low-rank adapters are optimally configured, one can make the product equal to the best rank LR-approximation of the target matrix. This can be viewed as an extension of the matrix approximation theorem to a product of matrices, each subject to low-rank constraints. Our main theoretical results on the expressive power of LoRA, which we will present in the subsequent sections, will build upon this core matrix approximation result. 3 E XPRESSIVE POWER OF FNN S WITH LORA 3.1 P ROBLEM SETTING We use FNNL,D(·; (Wl)L l=1, (bl)L l=1) to denote a L-layer width- D fully connected ReLU neural network with weight matrices Wl ∈ RD×D and biases bl ∈ RD, where l ∈ [L]. The target FNN f and frozen FNN f0 can be represented as follows: Target FNN f := FNNL,D(·; (Wl)L l=1, (bl)L l=1), Frozen FNN f0 := FNNL,D(·; (Wl)L l=1, (bl)L l=1), where Wl ∈ RD×D and bl ∈ RD represent the weight matrix and bias vector for the l-th layer of the target model f, respectively. Likewise, Wl ∈ RD×D, bl ∈ RD are those for f0, for layer l ∈ [L]. Given a specified LoRA-rank R ∈ [D], we adapt the frozen FNN f0 into a new model f via LoRA. The adapted model f is defined as Adapted FNN f := FNNL,D(·; (Wl + ∆Wl)L l=1, (bbl)L l=1), where the weight matrix for the low-rank adapter∆Wl ∈ RD×D satisfies specified rank constraints, updated bias vector bbl ∈ RD for l ∈ [L]1. As noted in Sec. 2, it is common for the pretrained model to be larger than necessary. Therefore, we focus on a setting where the frozen model is deeper than the target model, i.e.,L ≥ L. Furthermore, in this section, we let the input space X ∈RD×D be bounded. 1We consider the case where the bias parameters can also be updated, as suggested by Hu et al. (2022a). Experiments investigating the impact of updating bias parameters are presented in Sec. G.5. 4Published as a conference paper at ICLR 2024 3.2 O NE-LAYER RELU FNN A PPROXIMATION We start with investigating the expressive power of LoRA on one-layer FNN. In this setting, our aim is to identify LoRA adapters (∆Wl)L l=1 and bias vectors (bbl)L l=1 such that the adapted model ReLU((WL + ∆WL) · ReLU((WL−1 + ∆WL−1) · ReLU(··· ) + bbL−1) + bbL) closely approximates the target one-layer ReLU FNN model ReLU(W1 · +b1). This differs from the setting described in Sec. 2, where a multi-layer FNN with linear activation functions and zero biases was used to approximate a one-layer FNN with the same properties. In the current setting, we introduce non-linearity through the use of ReLU activation functions in the frozen model and also take biases into account. Consequently, to generalize the findings to this new setting, addressing the introduced non-linearity due to the ReLU activation functions in the frozen model is the main challenge. We employ the following two steps to extend the results in Sec. 2 to the current setting. 1. (Linearization) We eliminate the nonlinearity in the first L −1 layers of the adapted model, mak- ing it equivalent to a one-layer ReLU FNN. This can be readily achieved by choosing sufficiently large bias vectors for the first L − 1 layers to ensure that all ReLUs in these layers are activated. This technique of eliminating non-linearity is inspired by Giannou et al. (2023a). 2. (Weight Matrix Alignment) We update the bias vectors of the last layer bbL to align with that of the target model f, and apply the linear model approximation results (i.e., Lemma 1) to identify the low-rank adapters that match the weight matrix f. Following the steps above, we arrive at the subsequent lemma, which demonstrates that any one- layer FNN can be closely approximated by a multi-layer FNN finetuned via LoRA. The complete proof is provided in Sec. E.1. Lemma 2. Define error matrixE := W1 −QL l=1 Wl, with its rank represented byRE = rank(E). Consider a LoRA-rank R ∈ [D]. Assume that the weight matrices W1, . . . ,WL ∈ RD×D andQL l=1 Wl + LRr(E) for all r ≤ R(L − 1) are non-singular. Let x be a random input sampled from a distribution with bounded support X and let Σ = Exx⊤. Then, there exists rank-R or lower matrices ∆W1, . . . ,∆WL ∈ RD×D and bias vectors bb1, . . . ,bbL ∈ RD such that the expected squared error can be bounded as E \r\rf(x) − f(x) \r\r2 2 ≤ ∥Σ∥F σ2 RL+1(E). Moreover, whenR ≥ ⌈RE L ⌉, we have f(x) = f(x) for all x ∈ X. 3.3 M ULTI -LAYER RELU FNN A PPROXIMATION We now generalize our discussion to the approximation of multi-layer ReLU FNNs. The key strategy for extending the results to approximating multi-layer ReLU FNNs under LoRA is model partition, inspired from Giannou et al. (2023a). To elucidate this, we start with a specific example. Example 1. Consider the case where L = 2 and L = 4. We view a two-layer target model f as a composition of two one-layer ReLU FNNs. Accordingly, we partition the four-layer adapted modelf into two submodels, each consisting of two layers. For each layer in the target model, we utilize two corresponding layers in the frozen/adapted model for approximation. This problem then simplifies into a one-layer FNN approximation problem, which has already been addressed in Lemma 2. Based on this example, we introduce a ordered partitionP = {P1, . . . , PL} to partition the layers in the adapted model f, where SL i=1 Pi = [L]. Each element Pi ∈ Pconsists of consecutive integers. Given a partition P, each element Pi specifies that the layers with indexl ∈ Pi in the adapted model will be used to approximate the i-th layer in the target model. Example 1, which uses every two layers in the adapted model to approximate each layer in the target model, can be considered as a partition represented as {{1, 2}, {3, 4}}. Similarly, we extend this simple uniform partition into general cases for L-layer target FNN and L-layer frozen FNN: Pu = \b Pu 1 , . . . , Pu L \t := \b {1, . . . , M}, {M + 1, . . . ,2M}, . . . , \b (L − 1)M + 1, . . . , L \t\t , 5Published as a conference paper at ICLR 2024 where M := ⌊L/L⌋. The uniform partition indicates that every M layers in the adapted model are employed to approximate each layer in the target model. We useQ l∈Pi Wl to denote the product of the weight matrices from the layers l ∈ Pi, with the later layer positioned to the left and the earlier layer to the right in the matrix product. For example, Q l∈Pu 1 Wl = QM l=1 Wl = WM ··· W1. We first extend Lemma 2 to multi-layer FNN approximation setting using this uniform partition. Uniform Model Partition. Given a specified LoRA-rank R ∈ [D], to derive our results, we intro- duce a mild non-singularity assumption on the weight matrices of the target model and frozen model for the feasibility of our analysis. This assumption is mild, supported by Lemma 3 that even weight matrices initialized at random can meet this requirement. Assumption 1 (Non-Singularity). For a fixed LoRA-rank R ∈ [D], the weight matrices of the frozen model (Wl)L l=1 and matrices \u0010Q l∈Pu i Wl \u0011 + LRr(Wi − Q l∈Pu i Wl) are non-singular for all r ≤ R(M − 1) and i ∈ [L]. Lemma 3. Let (Wl)L l=1, (Wl)L l=1 ∈ RD×D be matrices whose elements are drawn independently from arbitrary continuous distributions. Then, with probability 1, Assumption 1 holds ∀R ∈ [D]. Given this assumption, here we present our first main result, which shows that any frozen FNN can be adapted to exactly approximate the target FNN via LoRA. Theorem 3. Under Assumption 1, if LoRA-rank R ≥ ⌈maxi∈[L] rank(Wi −Q l∈Pu i Wl)/M⌉, then there exists rank-R or lower matrices ∆W1, . . . ,∆WL ∈ RD×D and bias vectors bb1, . . . ,bbL ∈ RD such that the low-rank adapted modelf can exactly approximate the target modelf, i.e., f(x) = f(x), ∀x ∈ X. Moreover, combining Lemma 3 and Theorem 3 gives the following corollary. Corollary 4. Assume that the elements of (Wl)L l=1, (Wl)L l=1 are independently drawn from ar- bitrary continuous distributions. When R ≥ D/M, with probability 1, there exists rank- R or lower matrices ∆W1, . . . ,∆WL ∈ RD×D and bias vectors bb1, . . . ,bbL ∈ RD such that low-rank adapted model f can exactly approximate the target model f on X, i.e., f(x) = f(x), ∀x ∈ X. To understand the implications of this corollary, let us considerL ≫ L. In this scenario, the required LoRA-rank is sufficiently small such that the dimension of the rank-R matrix is approximately2RD. This corollary suggests that with2RDL ≥ 2D2L/M ≈ 2D2L learnable parameters, even a random FNN can be adapted into the target model f. It is noteworthy that the total number of parameters of the target model is D2L. This indicates that even though the learnable parameters under LoRA finetuning appear to be highly constrained (low-rank constrained learnable parameters distributed across many layers), the effective expressive power of LoRA is nearly optimal up to a constant factor of 2. Our discovery provides the first theoretical insights into the practical success of LoRA. Furthermore, Theorem 3 indicates that if the model f is ‘close’ to f such that maxi∈[L] rank(Wi −Q l∈Pu i Wl) is small, the number of learnable parameters used by LoRA can be lower than D2L. Meanwhile, when the employed LoRA-rank is lower than the critical threshold, the following theo- rem provides an upper bound for the approximation error. Theorem 5. Define the approximation error of i-th layer as Ei = σRM+1(Wi −Q l∈Pu i Wl), and the magnitude of the parameters and the input as β := maxi∈[L] \u0010p ∥Σ∥F Qi j=1 \r\rWj \r\r F + Pi j=1 Qi−1 k=j+1 \r\rWk \r\r F \r\rbj \r\r 2 \u0011Wp ∥Σ∥F. Under Assumption 1, there exists rank- R or lower matrices (∆Wl)L l=1 with ∆Wl ∈ RD×D and bias vectors (bbl)L l=1 with bbl ∈ RD such that for input x ∈ Xwith Exx⊤ = Σ, E \r\rf(x) − f(x) \r\r 2 ≤ β LX i=1 max k∈[L] \u0000\r\rWk \r\r F + Ek \u0001L−i Ei. Theorem 5 provides an upper bound on the approximation error for the adapted model. This bound is influenced by several factors: (i) magnitude of the target model’s parameters and the input, which 6Published as a conference paper at ICLR 2024 is captured by β and \r\rWk \r\r F, (ii) the rank of the adapter R and the discrepancy between the frozen model and the target model (Wi −Q l∈Pu i Wl)L i=1, both of which contribute to the termEi, (iii) the depth of the frozen model L, reflected in M and consequenly Ei. All the proofs of the results derived for uniform partition are provided in Sec. E.2. General Model Partition. We note that employing this uniform partition strategy for approxi- mating the target model may not always yield optimal results. To illustrate this, we revisit the case considered by Example 1, where L = 2 and L = 4. Consider a scenario where the first layer of the frozen model has been pretrained to match the first layer of the target model. In this case, we can use just the first layer in f to approximate the first layer in f, and a zero LoRA-rank is sufficient for the exact representation of the first layer. The remaining three layers in f can then be used to approximate the second layer in f. Compared to uniform partition, this partition leverages more layers to approximate the second layer in f, allowing us to achieve the desired performance with a lower LoRA-rank, as per Lemma 2. This suggests that our approximation error bounds could be further optimized by considering partitioning schemes tailored to specific scenarios. We now extend our results to a more general setting, where we do not assume a uniform parti- tion. Concurrently, recent research by Zhang et al. (2023) has shown that the application of varying LoRA-ranks leads to improved results. Consequently, we permit each layer in the frozen model to utilize adapters with different LoRA-ranks. The rank of the LoRA adapter associated with the l-th layer in the frozen model is denoted by Rl, where l ∈ [L]. This result relies on Assumption 2, an analog of Assumption 1, but revised to include a general model partition. More details, including the proofs, are provided in Sec. E.3. Theorem 6. Consider a partition P for the frozen model. Let Assumption 2 hold. If P l∈Pi Rl ≥ rank(Wi − Q l∈Pi Wl) for all i ∈ [L], there exists LoRA adapters (∆Wl)L l=1 with rank(∆Wl) ≤ Rl and biases (bbl)L l=1 such that the adapted model f can exactly approximate the target model. Moreover, define the approximation error of the i-th layer as Ei = σP l∈Pi Rl+1(Wi −Q l∈Pi Wl), and the magnitude of the parameters and the input as β := maxi∈[L] \u0010p ∥Σ∥F Qi j=1 \r\rWj \r\r F + Pi j=1 Qi−1 k=j+1 \r\rWk \r\r F \r\rbj \r\r 2 \u0011Wp ∥Σ∥F. Then, there exists LoRA adapters (∆Wl)L l=1 with rank(∆Wl) ≤ Rl and biases (bbl)L l=1 such that for any input x ∈ Xwith Exx⊤ = Σ, the approximation error can be bounded as E \r\rf(x) − f(x) \r\r 2 ≤ β LX i=1 max k∈[L] \u0000\r\rWk \r\r F + Ek \u0001L−i Ei. Comparison to Tuning Final Layers. Updating the final layers and keeping the initial layers frozen (Chatfield et al., 2014; Donahue et al., 2014; Sharif Razavian et al., 2014; Rahimi & Recht, 2007) is another popular model adaptation method. However, unlike LoRA, which can adapt even randomly generated networks to match a target model, empirical studies (Kornblith et al., 2019) suggest that the effectiveness of final layers tuning heavily depends on the quality of the initial layers. This indicates that merely tuning the final layers of randomly generated networks may not yield desirable performance. The following lemma rigorously supports this assertion, demonstrating that regardless of how the final layers are tuned, it is impossible to adapt a randomly generated model into even a one-layer FNN, a model of very low complexity. Lemma 4. Let D ≥ 2 and f be a one-layer target FNN. Assume that the elements of weight matrices (Wl)L l=1 are independently drawn from arbitrary continuous distributions. With probability 1, for any tuning of the last L − 1 layers, f ̸= f. In Corollary 4, we demonstrate that LoRA can adapt any randomly generated models to match the target model, using at most twice the number of learnable parameters as the target model. However, this lemma reveals that final layers tuning, even with L − 1 times the learnable parameters of the target model, cannot achieve performance comparable to LoRA. In other words, LoRA requires at most 2RDL ≤ 2D2 learnable parameters to achieve an exact approximation, while final layers 7Published as a conference paper at ICLR 2024 tuning fails to approximate the target model even with (L − 1)D2 learnable parameters. Therefore, when L ≥ 3, LoRA can deliver strictly superior performance than final layers tuning with the same or fewer parameters. This provides insights into the empirical observation that LoRA outperforms final layers tuning (Kaplun et al., 2023; Ding et al., 2023). 4 E XPRESSIVE POWER OF TRANSFORMER NETWORKS WITH LORA 4.1 P ROBLEM SETTING Transformer network, denoted as TFNL,D, is a composition of L Transformer blocks and an output layer, parameterized by weight Wo ∈ RD×D. Each transformer block comprises a H-head self- attention layer, parameterized by weight ((Wh Ol, Wh V l, Wh Kl, Wh Ql)H h=1)L l=1, followed by a token- wise feedforward layer, parameterized by weight (W1l, W2l)L l=1 and bias (b1l, b2l)L l=1. We assume that all weight matrices have a dimension of D × D, while the bias vectors are of dimension D. We employ the same formulations of transformer blocks as Yun et al. (2020a), with one exception: we exclude skip connections for analytical feasibility. As before, we use · (e.g., W1l) to represent the corresponding parameters for the target model, and ∆· (e.g., ∆Wh Ol) to represent the corre- sponding low-rank update. For TFN cases,we consider scenarios where both the frozen model and the target model have L Transformer blocks. For an explicit formulation, please refer to Sec. F.2. 4.2 M AIN RESULTS ON TRANSFORMER NETWORKS We now present our main findings on TFNs. The first result relies on a non-singularity assumption (Assumption 4) tailored for TFN. This assumption is mild, and models with randomly generated weights can satisfy its criteria (Lemma 14). Further details are deferred to Sec. F.2. The following theorem shows that adding LoRA adapters primarily to the self-attention layers en- ables the adapted model f to exactly approximate the target modelf. This finding is consistent with a recent observation made by Hu et al. (2022a), which indicates that a good performance can be achieved by adapting only the attention layers when applying LoRA to TFNs. Theorem 7. Consider a given LoRA-rank R ∈ [D]. Let Assumption 4 hold. Let Gi be the rank- based functionality gap to i-th transformer block (i ∈ [L]) or output layer ( i = L + 1) defined in (23). If R ≥ maxi∈[L+1]⌈Gi 2 ⌉, then there exists low-rank adapters with rank lower than R ∈ [D] ((∆Wh Kl, ∆Wh Ql, ∆Wh V l, ∆Wh Ol)H h=1)L l=1, ∆W2L, ∆Wo with other low-rank adapters set toO, and updated bias vectors (bb1l, bb2l)L l=1, such that for any X ∈ RD×N , the adapted model f exactly approximates target model f, i.e., f(X) = f(X). Proof Sketch. The primary challenge for extending our analysis to TFNs, similar to FNN cases, is the nonlinearity introduced by softmax and ReLU. To manage this, we segment a sequence of transformer blocks based on the softmax and ReLU functions. Specifically, we align the output of attention scores before the softmax is applied, and then match the output of the first feedforward layer before ReLU is applied. The complete proof of Theorem 7 and results for randomly generated models can be found in Sec. F.2. Meanwhile, our results here are specifically for TFNs with multi-head attention layers. For TFNs with single-head attention layers, the construction of LoRA adapters differs due to the absence of Wh Oi. Since the results are similar, we defer the problem setting and results for TFNs with single-head attention layers to Sec. F.1. 5 E XPERIMENTS Recall that all our theoretical statements are based on our construction of the LoRA adapters pre- sented in their corresponding proofs. To validate these results, here we empirically examine the relationship between approximation error and rank by integrating the LoRA adapters, which are constructed with the uniform partition in our proof, into the frozen model. 8Published as a conference paper at ICLR 2024 Validation of Our LoRA Adapter Construction. We employ the Mean Squared Error (MSE) to assess the approximation error, comparing the MSE of the LoRA adapter as derived from the gradient update method with that from our construction. We consider linear models and FNNs with model dimension D = 16. For linear model cases, we set L = 1, L= 2, while for FNN cases, we set L = 2, L= 4. We include two variants of the frozen model for fine-tuning: one with randomly initialized parameters (Random) and another pretrained on the target distribution (Pretrained). 4 8 12 16 0.0 0.2 Random 4 8 12 16 Pretrained Gradient Update Our Construction Rank MSE (a) Linear model approximation. 4 8 12 16 0.00 0.01 0.02 Random 4 8 12 16 Pretrained Gradient Update Our Construction Rank MSE (b) FNN approximation. Figure 1: Approximation error (mea- sured by MSE) versus LoRA-rank. Our results for linear model approximation and FNN ap- proximation via LoRA are depicted in Fig. 1a and 1b, re- spectively. Firstly, we observe that the MSE of both two cases is close to zero when R ≥ D L/L = 8 , which cor- roborates our claims. Meanwhile, a comparison between the left and right columns of Fig. 1a suggests that pre- training can further reduce the required rank to achieve near-zero approximation error. Furthermore, the curves of our construction align well with those of the gradient update method in linear model approximation cases, con- firming the optimality claimed in Lemma 1. However, for FNN approximation cases, the gradient update method outperforms our construction in the small rank region. We conjecture that the suboptimality of our construction for this multi-layer FNN case could arise from unnecessar- ily matching the intermediate outputs of the frozen model with those of the target model during adapter construc- tion. Additionally, the uniform partition could also be one contributing factor. Findings Empirical Observation Theoretical Insights For a fixed downstream task, larger models require a lower LoRA-rank to achieve the desired performance. Sec. G.9 Lemma 1, 2, and Theo- rem 5, 6 When the frozen model is closer to the target model, a lower LoRA-rank is sufficient to attain the desired per- formance. Sec. G.9 and 6-th footnote in Hu et al. (2022a) Lemma 1, 2, and Theo- rem 5, 6, 7 LoRA outperforms final layers tuning if the quality of shared representation is not good. Sec. G.4 and observations by Kaplun et al. (2023) and Ding et al. (2023) Lemma 4 In addition to applying low-rank updates to weight matri- ces, it is crucial to also update the bias. Sec. G.5 and 2-nd footnote in Hu et al. (2022a) Proofs in Sec. 3.2 and E.1 Tuning attention weights is sufficient for achieving good performance on TFNs. Sec. 4.2 in Hu et al. (2022a) Theorem 7 Current optimization algorithms for LoRA training might be suboptimal. Fig. 4, 5, and 9 — Table 1: Summary of our findings, supported by empirical evidence and theoretical results. Detailed Experimental Setup and Additional Experiments in the Appendix. Further experi- ment details and a series of additional experiments, including simulations on FNNs and TFNs at different depths, evaluation of classification tasks, empirical comparison between LoRA and the final layers tuning, investigation of the importance of updatable bias, LoRA’s generalization and optimization properties, and experiments on GLUE benchmark (Wang et al., 2018), are provided in Sec. G. Table 1 summarizes all the empirical findings and aligns them with theoretical insights. 6 C ONCLUSIONS This work pioneers the theoretical analysis of LoRA fine-tuning’s expressive capabilities in FNNs and TFNs, offering novel insights into how rank, model depth, and proximity to the target model influence LoRA’s effectiveness. Our theoretical findings are validated by empirical evidence. Future work includes quantifying approximation errors for TFNs when the LoRA-ranks are lower than required and refining LoRA adapter update algorithms based on our construction of LoRA adapters. 9Published as a conference paper at ICLR 2024 ACKNOWLEDGEMENT This work was supported by NSF Award DMS-2023239, NSF/Intel Partnership on Machine Learn- ing for Wireless Networking Program under Grant No. CNS-2003129, and a grant by FuriosaAI. We extend our heartfelt gratitude to Angeliki Giannou, Kartik Sreenivasan, Tuan Dinh, Jy-yong Sohn, Jingpeng Liu, and anonymous reviewers for their insightful comments that significantly enhanced the quality of our paper. REPRODUCIBILITY STATEMENT The code for all experiments reported in this paper is publicly accessible. For the purpose of re- producibility, the code can be found at the following anonymized GitHub repository: https: //github.com/UW-Madison-Lee-Lab/Expressive_Power_of_LoRA . REFERENCES Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement preconditioned gradient descent for in-context learning. arXiv preprint arXiv:2306.00297, 2023. Ekin Aky ¨urek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? Investigations with linear models. In International Conference on Learning Representations (ICLR), 2023. Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Prov- able in-context learning with in-context algorithm selection. arXiv preprint arXiv:2306.04637, 2023. Peter L Bartlett and Shahar Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural results. In Computational Learning Theory (COLT), volume 2111, pp. 224–240, 2001. Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. BitFit: Simple parameter-efficient fine- tuning for Transformer-based masked language-models. In Annual Meeting of the Association for Computational Linguistics (ACL), pp. 1–9, 2022. Yoshua Bengio and Olivier Delalleau. On the expressive power of deep architectures. InAlgorithmic Learning Theory, pp. 18–36, 2011. Alberto Bietti and Francis Bach. Deep equals shallow for ReLU networks in kernel regimes. 2021. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari- wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems (NeurIPS), 2020. Richard Caron and Tim Traynor. The zero set of a polynomial. WSMR Report, 2005. Ken Chatfield, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Return of the devil in the details: Delving deep into convolutional nets. arXiv preprint arXiv:1405.3531, 2014. Pin-Yu Chen. Model reprogramming: Resource-efficient cross-domain machine learning. arXiv preprint arXiv:2202.10629, 2022. George Cybenko. Approximation by superpositions of a sigmoidal function.Mathematics of control, signals and systems, 2:303–314, 1989. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023. 10Published as a conference paper at ICLR 2024 Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional Transformers for language understanding. In North American Chapter of the Asso- ciation for Computational Linguistics (NAACL), pp. 4171–4186, 2019. Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao, Xiaozhi Wang, Zhiyuan Liu, Hai-Tao Zheng, Jianfei Chen, Yang Liu, Jie Tang, Juanzi Li, and Maosong Sun. Parameter-efficient fine- tuning of large-scale pre-trained language models. Nature Machine Intelligence, 5(3):220–235, 2023. Tuan Dinh, Daewon Seo, Zhixu Du, Liang Shang, and Kangwook Lee. Improved input reprogram- ming for GAN conditioning. arXiv preprint arXiv:2201.02692, 2022a. Tuan Dinh, Yuchen Zeng, Ruisu Zhang, Ziqian Lin, Michael Gira, Shashank Rajput, Jy-yong Sohn, Dimitris Papailiopoulos, and Kangwook Lee. LIFT: Language-interfaced fine-tuning for non- language machine learning tasks. Advances in Neural Information Processing Systems (NeurIPS), 35:11763–11784, 2022b. Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. DeCAF: A deep convolutional activation feature for generic visual recognition. In Inter- national Conference on Machine Learning (ICML), pp. 647–655, 2014. Simon S Du, Wei Hu, Sham M Kakade, Jason D Lee, and Qi Lei. Few-shot learning via learning the representation, provably. In International Conference on Learning Representations (ICLR) , 2021. Carl Eckart and Gale Young. The approximation of one matrix by another of lower rank.Psychome- trika, 1936. Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In Annual Conference on Learning Theory, volume 49, pp. 907–940, 2016. Gamaleldin F Elsayed, Ian Goodfellow, and Jascha Sohl-Dickstein. Adversarial Reprogramming of neural networks. In International Conference on Learning Representations (ICLR), 2019. Jesse Engel, Matthew Hoffman, and Adam Roberts. Latent constraints: Learning to generate condi- tionally from unconditional generative models. In International Conference on Learning Repre- sentations (ICLR), 2018. Matthias Englert and Ranko Lazic. Adversarial Reprogramming revisited. In Advances in Neural Information Processing Systems (NeurIPS), volume 35, pp. 28588–28600, 2022. Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. DPOK: Reinforcement learning for fine-tuning text-to-image diffusion models. arXiv preprint arXiv:2305.16381, 2023. Angeliki Giannou, Shashank Rajput, and Dimitris Papailiopoulos. The expressive power of tuning only the Norm layers. arXiv preprint arXiv:2302.07937, 2023a. Angeliki Giannou, Shashank Rajput, Jy-Yong Sohn, Kangwook Lee, Jason D. Lee, and Dimitris Papailiopoulos. Looped Transformers as programmable computers. In International Conference on Machine Learning (ICML), volume 202, pp. 11398–11442, 2023b. Michael Gira, Ruisu Zhang, and Kangwook Lee. Debiasing pre-trained language models via effi- cient fine-tuning. In Workshop on Language Technology for Equality, Diversity and Inclusion, pp. 59–69, 2022. Moritz Hardt and Tengyu Ma. Identity matters in deep learning. In International Conference on Learning Representations, 2017. Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. DeBERTa: Decoding-enhanced BERT with disentangled attention. In International Conference on Learning Representations , 2021. 11Published as a conference paper at ICLR 2024 Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are uni- versal approximators. Neural Networks, 2:359–366, 1989. Daniel Hsu, Clayton H Sanford, Rocco Servedio, and Emmanouil Vasileios Vlatakis-Gkaragkounis. On the approximation power of two-layer networks of random ReLUs. InConference on Learning Theory, volume 134, pp. 2423–2461, 2021. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Con- ference on Learning Representations (ICLR), 2022a. Shengding Hu, Zhen Zhang, Ning Ding, Yadao Wang, Yasheng Wang, Zhiyuan Liu, and Maosong Sun. Sparse structure search for delta tuning. In Advances in Neural Information Processing Systems (NeurIPS), volume 35, pp. 9853–9865, 2022b. Like Hui and Mikhail Belkin. Evaluation of neural architectures trained with square loss vs cross- entropy in classification tasks. In International Conference on Learning Representations, 2021. Arthur Jacot, Franck Gabriel, and Cl ´ement Hongler. Neural tangent kernel: Convergence and gen- eralization in neural networks. Advances in neural information processing systems, 31, 2018. Gal Kaplun, Andrey Gurevich, Tal Swisa, Mazor David, Shai Shalev-Shwartz, and Eran Malach. SubTuning: Efficient finetuning for multi-task learning. arXiv preprint arXiv:2302.06354, 2023. Kenji Kawaguchi. Deep learning without poor local minima. Advances in neural information pro- cessing systems, 29, 2016. Simon Kornblith, Jonathon Shlens, and Quoc V Le. Do better ImageNet models transfer better? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019. Thomas Laurent and James von Brecht. Deep linear networks with arbitrary loss: All local minima are global. In Proceedings of the 35th International Conference on Machine Learning, volume 80, pp. 2902–2907, 2018. Holden Lee, Rong Ge, Tengyu Ma, Andrej Risteski, and Sanjeev Arora. On the ability of neural nets to express distributions. In Conference on Learning Theory, pp. 1271–1296, 2017. Kangwook Lee, Changho Suh, and Kannan Ramchandran. Reprogramming GANs via input noise design. In Machine Learning and Knowledge Discovery in Databases - European Conference, (ECML PKDD), volume 12458, pp. 256–271, 2020. Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Empirical Methods in Natural Language Processing (EMNLP), pp. 3045–3059, 2021. Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. InAs- sociation for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL/IJCNLP), pp. 4582–4597, 2021. Shiyu Liang and R. Srikant. Why deep neural networks for function approximation? InInternational Conference on Learning Representations (ICLR), 2017. Valerii Likhosherstov, Krzysztof Choromanski, and Adrian Weller. On the expressive power of self-attention matrices. 2021. Chaoyue Liu, Libin Zhu, and Mikhail Belkin. Loss landscapes and optimization in over- parameterized non-linear systems and neural networks. Applied and Computational Harmonic Analysis, 59:85–116, 2022a. Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. In Advances in Neural Information Processing Systems (NeurIPS) , volume 35, pp. 1950–1965, 2022b. 12Published as a conference paper at ICLR 2024 Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. Haihao Lu and Kenji Kawaguchi. Depth creates no bad local minima. arXiv preprint arXiv:1702.08580, 2017. Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The expressive power of neural networks: A view from the width. Advances in neural information processing systems, 30, 2017. Sadhika Malladi, Alexander Wettig, Dingli Yu, Danqi Chen, and Sanjeev Arora. A kernel-based view of language model fine-tuning. In International Conference on Machine Learning , pp. 23610–23641, 2023. Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. The benefit of multitask representation learning. Journal of Machine Learning Research, 17(81):1–32, 2016. Leon Mirsky. Symmetric gauge functions and unitarily invariant norms. The quarterly journal of mathematics, 1960. OpenAI. GPT-4 technical report, 2023. Samet Oymak, Ankit Singh Rawat, Mahdi Soltanolkotabi, and Christos Thrampoulidis. On the role of attention in prompt-tuning. In International Conference on Machine Learning (ICML), 2023. Sejun Park, Chulhee Yun, Jaeho Lee, and Jinwoo Shin. Minimum width for universal approximation. In International Conference on Learning Representations (ICLR), 2021. Jorge P´erez, Javier Marinkovi ´c, and Pablo Barcel ´o. On the turing completeness of modern neural network architectures. arXiv preprint arXiv:1901.03429, 2019. Aleksandar Petrov, Philip HS Torr, and Adel Bibi. When do prompting and prefix-tuning work? A theory of capabilities and limitations. arXiv preprint arXiv:2310.19698, 2023. Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the expressive power of deep neural networks. In International Conference on Machine Learning (ICML), pp. 2847–2854, 2017. Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in Neural Information Processing Systems (NeurIPS), volume 3, pp. 5, 2007. Simo Ryu. Low-rank adaptation for fast text-to-image diffusion fine-tuning. https://github. com/cloneofsimo/lora, 2023. Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynam- ics of learning in deep linear neural networks. 2014. Daewon Seo, Hongyi Wang, Dimitris Papailiopoulos, and Kangwook Lee. Empirical study on the effective VC dimension of low-rank neural networks. In ICML Workshop on Overparameteriza- tion: Pitfalls & Opportunities, 2021. Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. CNN features off-the-shelf: An astounding baseline for recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pp. 806–813, 2014. Haizhao Shen, ZuoweiYang and Shijun Zhang. Deep network approximation characterized by num- ber of neurons. Communications in Computational Physics, (5):1768–1811, 2020. Matus Telgarsky. Representation benefits of deep feedforward networks. arXiv preprint arXiv:1509.08101, 2015. Matus Telgarsky. Benefits of depth in neural networks. In Conference on Learning Theory , pp. 1517–1539, 2016. 13Published as a conference paper at ICLR 2024 Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ´ee Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aur ´elien Rodriguez, Ar- mand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Nilesh Tripuraneni, Michael Jordan, and Chi Jin. On the theory of transfer learning: The importance of task diversity. In Advances in neural information processing systems (NeurIPS) , volume 33, pp. 7852–7862, 2020. Rasul Tutunov, Antoine Grosnit, Juliusz Ziomek, Jun Wang, and Haitham Bou-Ammar. Why can large language models generate correct chain-of-thoughts? arXiv preprint arXiv:2310.13571 , 2023. Vladimir N Vapnik and A Ya Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. In Measures of Complexity: Festschrift for Alexey Chervonenkis, pp. 11–30. 2015. Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo ˜ao Sacramento, Alexander Mordv- intsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. arXiv preprint arXiv:2212.07677, 2022. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP , pp. 353–355, 2018. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems (NeurIPS), 2022. Noam Wies, Yoav Levine, and Amnon Shashua. The learnability of in-context learning. arXiv preprint arXiv:2303.07895, 2023. Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. In International Conference on Learning Representations (ICLR), 2022. Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Are Transformers universal approximators of sequence-to-sequence functions? In International Con- ference on Learning Representations (ICLR), 2020a. Chulhee Yun, Yin-Wen Chang, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and San- jiv Kumar. O(n) connections are expressive enough: Universal approximability of sparse trans- formers. 33:13783–13794, 2020b. Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. In The Eleventh Inter- national Conference on Learning Representations, 2023. 14Published as a conference paper at ICLR 2024 Appendix This appendix encompasses more discussions, experiments, and proofs of the results presented in the main body. Given the extensive use of notations in our paper, we begin by presenting a list of common notations in Sec. A for the reader’s convenience. We then delve into a more detailed discussion of related works in Sec. B. Following this, we present the proofs of results from the main body and auxiliary results in Sec. C, D, E, and F. Specifically, we provide additional results for TFN with single-head attention layers, and TFN with multi-head attention layers under random model cases in Sec. F. Further experimental details and interesting experiment findings are provided in Sec. G. Finally, we discuss how to extend our results to cases with varying model dimensions in Sec. H, while this work primarily focuses on instances where both the target model and the frozen model possess the same model width D. More potential future works are outlined in Sec. I. A List of Common Notations 17 B Expanded Related Works 18 C Proofs Related to Linear Algebra 19 C.1 Common Matrix Inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 C.2 Non-Singularity of Randomly Generated Matrices . . . . . . . . . . . . . . . . . . 19 D Proofs for Linear Model Approximation 20 E Proofs for FNN Approximation 25 E.1 Approximating One-Layer ReLU FNN via LoRA . . . . . . . . . . . . . . . . . . 25 E.2 Approximating Multi-Layer ReLU FNN via LoRA with Uniform Model Parition . 27 E.3 Approximating Multi-Layer ReLU FNN via LoRA with General Model Parition . . 31 E.4 Approximating Multi-Layer ReLU FNN via Final Layers Tuning . . . . . . . . . . 32 F Proofs for TFN Approximation 33 F.1 Approximating Transformer Network with Single-Head Attention Layers . . . . . 33 F.2 Approximating Transformer Network with Multi-Head Attention Layers . . . . . . 36 G Experiments 39 G.1 Additional Details of Experiment Setup . . . . . . . . . . . . . . . . . . . . . . . 40 G.2 Additional Details on Gradient Update Method . . . . . . . . . . . . . . . . . . . 40 G.3 Validation of Our LoRA Adapter Construction . . . . . . . . . . . . . . . . . . . . 40 G.3.1 FNN Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 G.3.2 TFN Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 G.4 Comparison to Tuning Final Layers . . . . . . . . . . . . . . . . . . . . . . . . . 42 G.5 Benefits of Tuning Biases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 G.6 Training Curves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 G.7 Generalization Performances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 G.8 Evaluation on Classification Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . 43 G.9 Evaluation on Real Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 15Published as a conference paper at ICLR 2024 H Extension to Cases with Different Model Dimensions 45 I Extended Future Works 46 16Published as a conference paper at ICLR 2024 A L IST OF COMMON NOTATIONS We first give a list of common notations that are used in the main body and appendix for reference. • f: LoRA-adapted model. • f: target model. • f0: frozen/pretrained model. • R: rank of LoRA adapters. • D: dimensionality of the model, representing the number of neurons in each layer for FNNs and the embedding size for TFNs. • L: depth of the (frozen) model, representing the number of layers for FNNs and the number of transformer blocks for TFNs. • N: sequence length of the input for TFNs. • x: input. • x: random input. • X: matrix input. • X: input space. • Σ: Exx⊤. • W: a weight matrix associated with (frozen) model. Subscripts and superscripts may be added for specificity. • b: a bias vector associated with the (frozen) model. Subscripts may be added for specificity. • zl: the output of the first l layers in the (frozen) FNN. • Zl: the output of the first l transformer blocks in a (frozen) TFN. • W: a weight matrix associated with the target model. Subscripts and superscripts may be added for specificity. • b: a bias vector associated with the target model. Subscripts may be added for specificity. • zl: the intermediate output of the first l layers in target FNN given the random input x. • Zl: the output of the first l transformer blocks in a target TFN. • L: depth of the target model, representing the number of layers for FNNs and the number of transformer blocks for TFNs. • ∆W: the weight matrix of a LoRA adapter. • bb: a bias vector associated with the LoRA-adapted model. • bzl: the output of the first l layers in the LoRA-adapted model given the random input x. • bZl: the output of the first l transformer blocks in the LoRA-adapted model. • M: the ratio of the depth of the frozen model to that of the target model, i.e., L/L. • P: partition P = {P1, . . . , PL}, each element Pi specifies that the layers with index l ∈ Pi in the adapted model will be used to approximate the i-th layer in the target model. • Pi: the i-th element in partition P. • Pu: uniform partition Pu := {{1, . . . , M}, {M + 1, . . . ,2M}, . . . , \b (L − 1)M + 1, . . . , L \t }. The uniform partition indicates that everyM layers in the adapted model are employed to approx- imate each layer in the target model. • Pu i : the i-th element in uniform partition Pu. • ID: the D ×D identity matrix. When the context permits, the subscript D of ID may be omitted, simplifying the notation to I. • Ia:b,D: a diagonal matrix where the diagonal entries from theath to bth position are set to 1, while all remaining entries are 0s. 17Published as a conference paper at ICLR 2024 • σd(·): the d-th largest singular value for the given square matrix. Whend is greater than the width of the matrix, σd(·) = 0. • LRr(·): best rank-r approximation of a square matrix in Frobenuis norm and spectral norm. The subscript r may be omitted to indicate a general low-rank approximation without specifying the rank. • Q l∈Pi Wl: product of the weight matrices from the layers l ∈ Pi, with the later layer positioned to the left and the earlier layer to the right in the matrix product. For example, Q l∈Pu 1 Wl = QM l=1 Wl = WM ··· W1. B E XPANDED RELATED WORKS Expressive Power of Fully Connected Neural Networks The theoretical exploration of the ex- pressive power of unfrozen fully connected neural networks has advanced since the introduction of the first universal approximation theorem (Hornik et al., 1989; Cybenko, 1989). Subsequent studies have demonstrated the benefits of depth, asserting that sufficient depth can ensure function approxi- mation (Bengio & Delalleau, 2011; Eldan & Shamir, 2016; Liang & Srikant, 2017; Telgarsky, 2016; 2015). There are also works that have examined the expressive power of FNN from a view of width (Lu et al., 2017; Park et al., 2021; Bietti & Bach, 2021) and the number of neurons (Shen & Zhang, 2020). While these results assume that weight matrices can be arbitrarily adjusted for optimal performance, Hsu et al. (2021) examined the expressive power of randomly generated two- layer FNNs. Our work shares similarities with this direction, as we also delve into scenarios with randomly generated models. Beyond characterizing expressive power by approximation error, alter- native metrics have been proposed. Metrics such as Vapnik-Chervonenkis (Vapnik & Chervonenkis, 2015; Seo et al., 2021) and Rademacher complexities (Bartlett & Mendelson, 2001) are utilized to assess classification capacity. Furthermore, Raghu et al. (2017) introduced a novel metric that cap- tures the structural properties of an FNN, and Lee et al. (2017) investigated the ability of FNNs to express distributions. Expressive Power of Transformers As TFNs have grown increasingly popular, a few studies have been conducted to investigate their expressive power. Yun et al. (2020a) established the universal approximation theorem for TFNs in approximating sequence-to-sequence functions. Likhosherstov et al. (2021) characterized the self-attention layer as a matrix and demonstrated that this matrix can approximate any sparse matrices. Beyond approximation, further research has delved into other facets of TFNs’ expressive power. For instance, Giannou et al. (2023b) found that looped transform- ers can emulate an instruction-set computer, while P´erez et al. (2019) demonstrated that TFNs attain Turing completeness when operating with infinite precision. However, all these theories above cannot fully explain the performance of frozen neural networks as they generally cannot factor in pre-trained model parameters and adaptation methods. Expressive Power of Adaptation Methods Our work focuses on investigating the expressive power of adaptation methods. In stark contrast to the flourishing research on the expressive power of neural networks, there exists a limited number of works investigating the expressive power of adap- tation methods. A notable exception is Giannou et al. (2023a), investigating the expressive power of normalization parameter fine-tuning. They demonstrate that fine-tuning the normalization layers alone can adapt a randomly initialized ReLU network to match any target network that is O(width) times smaller. We borrow some proof techniques from this work, including techniques for extending results from linear neural networks to ReLU neural networks. In another recent work (Englert & Lazic, 2022), the authors show that neural reprogramming (Elsayed et al., 2019; Engel et al., 2018; Lee et al., 2020; Dinh et al., 2022a; Chen, 2022), a technique that modifies only the inputs while keeping the pretrained network frozen, can adapt any random two-layer ReLU network to achieve arbitrarily high accuracy on a Bernoulli data model over hypercube vertices. Oymak et al. (2023) explores prompt-tuning within a one-layer attention architecture, revealing that the model result- ing from prompt tuning (Lester et al., 2021) is more expressive than the naive self-attention model. Petrov et al. (2023) shows that prompt-tuning and prefix tuning (Li & Liang, 2021) are strictly less expressive than full fine-tuning. Despite these early attempts, no existing study has yet explored the expressive power of LoRA, the current leading adaptation method. 18Published as a conference paper at ICLR 2024 Other Theoretical Analysis of Adaptation Methods Lots of efforts have been taken to theoret- ically analyze other properties of adaptation methods such as generalization. Maurer et al. (2016) provides the generalization bounds for transfer learning, particularly for final layers tuning, demon- strating that the estimation error reduces as the pretrained task diversity and the number of samples for the target task increase. Tripuraneni et al. (2020) further refines this bound by studying the effect of the number of samples in the pre-trained tasks. Interestingly, the estimation error of the final layers tuning provided in Tripuraneni et al. (2020) heavily depends on the quality of the shared rep- resentation. This insight aligns with our finding on final layers tuning (Lemma 4), which implies that tuning the final layers fails to adapt an L-layer randomly generated FNN to approximate any one-layer target FNN if the first layer remains frozen. This failure is attributed to the poor qual- ity of the shared random representation. Du et al. (2021) further investigates final layers tuning in few-shot cases, i.e., when there are only a few samples for the target task. A recent study by Malladi et al. (2023), which examined LoRA and full fine-tuning through the lens of the Neural Tangent Kernel (Jacot et al., 2018), suggested that if the kernel view describes full fine-tuning, then LoRA approximates full fine-tuning. However, their theoretical analysis of LoRA is based on linear models, thus limiting its applicability. In contrast, our study considers a more general setting. With the rapid advancement of large language models, new adaptation methods such as in-context learning (Brown et al., 2020), prefix tuning, and prompt-tuning (Lester et al., 2021) are gaining increasing attention. A particular focus of research is the exploration of the theoretical underpinnings of in-context learning (Aky¨urek et al., 2023; Bai et al., 2023; Wies et al., 2023; Xie et al., 2022; von Oswald et al., 2022; Ahn et al., 2023). Aky¨urek et al. (2023) demonstrates that transformer-based in- context learners implicitly implement standard learning algorithms, while Bai et al. (2023) presents a similar finding and posits that in-context learning performs algorithm selection like a statistician. Wies et al. (2023) delves into the analysis of the sample complexity of in-context learning. Other works find that in-context learning is equivalent to gradient descent (von Oswald et al., 2022; Ahn et al., 2023), and Bayesian inference (Xie et al., 2022). Beyond in-context learning, a recent research by Tutunov et al. (2023) developed a theoretical framework elucidating how LLMs can accurately generate chain-of-thought reasoning (Wei et al., 2022). C P ROOFS RELATED TO LINEAR ALGEBRA In this section, we present a collection of commonly used matrix inequalities and the basic properties of randomly generated matrices. C.1 C OMMON MATRIX INEQUALITIES Here, we present some commonly used basic properties for matrix multiplication including rank computation, norm inequalities, as well as key results involving the trace and Frobenius norm of matrices for reference: rank(AB) ≤ rank(A) ∧ rank(B); ∥Ax∥2 ≤ ∥A∥2 ∥x∥2 ; (1) Ex⊤Ax = tr(ACov(x)) + (Ex)⊤A(Ex) = tr(AExx⊤); tr(AB) = tr(BA); tr(AB) ≤ tr(A)tr(B); ∥A∥F = q tr(AA⊤); ∥A∥F = tr(A) for symmetric A; ∥A∥F = sX i σ2 i (A). C.2 N ON-SINGULARITY OF RANDOMLY GENERATED MATRICES Although the non-singularity of randomly generated matrices is already established, we include a proof for completeness. 19Published as a conference paper at ICLR 2024 To facilitate the proof, we introduce a lemma which states that if a polynomial is non-zero, then the set of roots corresponding to a zero value of the polynomial has a Lebesgue measure of zero. Lemma 5 (Caron & Traynor (2005)). Let p(x) be a polynomial of degree d, x ∈ Rn. If p is not the zero polynomial, then the set S := {x ∈ Rn | p(x) = 0} is of Lebesgue measure zero. We note that the determinant of a matrix can be viewed as a polynomial function of its vectorized version. Based on this insight, we proceed with our proof. Lemma 6. Let X ∈ RD×D be a random matrix that follows arbitrary continuous distribution with support having non-zero Lebesgue measure on RD×D. Then, X is non-singular with probability 1. Proof of Lemma 6. The result is a direct consequence of Lemma 5. Let x = vec(X). Then, x is a random vector following arbitrary continuous distribution with a support having non-zero Lebesgue measure on RD×D. First, we establish the relationship: P(det(X) = 0) = P(p(x) = 0) for some polynomial function p. We denote the support of random vector x by X ⊂RD2 , and the probability density function (PDF) of x by q. Then, P(p(x) = 0) = Z X 1 {p(x) = 0}q(x)dx = Z X∩{x:p(x)=0} q(x)dx. By Lemma 5, the Lebesgue measure of {x : p(x) = 0} is zero. Hence, Z X∩{x:p(x)=0} q(x)dx = 0. By combining all the equations above, we conclude that P(det(X) = 0) = 0 , which implies X is non-singular with probability 1. D P ROOFS FOR LINEAR MODEL APPROXIMATION In this section, we present the results and corresponding proofs for the linear model approximation problem introduced in Sec. 2. The deep linear model is a common technique in theoretical deep learning research, which offers valuable insights into deep nonlinear models, and has been employed in many notable studies, including those by Saxe et al. (2014); Kawaguchi (2016); Lu & Kawaguchi (2017); Hardt & Ma (2017) and Laurent & von Brecht (2018). We employ this toy model as a preliminary model, which serves as a foundation for extending our results to nonlinear models (i.e., FNN and TFN). We first provide a slightly more detailed version of Lemma 1 along with its proof. Then, we present a variant of it that allows for different LoRA-ranks for each low-rank adapter. The proof for this variant involves only a minor modification of the proof for Lemma 7. Lemma 7. [Detailed version of Lemma 1] Define error matrix E := W − QL l=1 Wl, and denote its rank by RE = rank(E). For a given LoRA-rankR ∈ [D], assume that all the weight matrices of the frozen model (Wl)L l=1, and QL l=1 Wl + LRr(E) are non-singular for all r ≤ R(L − 1). Then, the approximation error min ∆Wl:rank(∆Wl)≤R \r\r\r\r\r LY l=1 (Wl + ∆Wl) − W \r\r\r\r\r 2 = σRL+1   W − LY l=1 Wl ! | {z } Error matrix E , and the optimal solution to the matrix approximation problem satisfies QL l=1(Wl + ∆Wl) =QL l=1 Wl + LRRL∧RE(E). Therefore, when R ≥ ⌈RE L ⌉, we have QL l=1(Wl + ∆Wl) = W, implying f ≡ f. 20Published as a conference paper at ICLR 2024 Proof of Lemma 7. Our goal is to find matrices ∆W1, . . . ,∆WL of rank R or lower such that the product of the adapted matrices approximates the target matrix well, i.e., we aim to solve the following constrained optimization problem: min ∆Wl:rank(∆Wl)≤R \r\r\r\r\r LY l=1 (Wl + ∆Wl) − W \r\r\r\r\r 2 . By subtracting QL l=1 Wl from both terms, the constrain optimization problem becomes min ∆Wl:rank(∆Wl)≤R \r\r\r\r\r\r\r\r\r  LY l=1 (Wl + ∆Wl) − LY l=1 Wl ! | {z } :=A −   W − LY l=1 Wl ! | {z } :=E \r\r\r\r\r\r\r\r\r 2 . (2) To perform analysis on (2), we start with the analysis of A as follows: A = LY l=1 (∆Wl + Wl) − LY l=1 Wl = ∆WL L−1Y l=1 (∆Wl + Wl) + WL L−1Y l=1 (∆Wl + Wl) − LY l=1 Wl. Here, we have separated the first term in the productQL l=1(∆Wl + Wl), breaking it into two parts: one involving ∆WL and the other WL. We can further expand the part involving WL: A =∆WL L−1Y l=1 (∆Wl + Wl) + WL   ∆WL−1 L−2Y l=1 (∆Wl + Wl) + WL−1 L−2Y l=1 (∆Wl + Wl) ! − LY l=1 Wl. At this point, it becomes clear that this expression can be iteratively decomposed. Following this pattern, we can express A as: A =∆WL L−1Y l=1 (∆Wl + Wl) + WL∆WL−1 L−2Y l=1 (∆Wl + Wl) (3) + . . .+ ( LY l=2 Wl)(∆W1 + W1) − LY l=1 Wl = LX l=1 \" ( LY i=l+1 Wi)∆Wl( l−1Y i=1 (Wi + ∆Wi)) # | {z } :=Al . In this final form, A is decomposed as A = PL l=1 Al. It is important to note that rank(Al) ≤ rank(∆Wl) ≤ R. Consequently, rank(A) ≤ PL l=1 rank(Al) ≤ RL. Then, the optimization problem (2) can be relaxed into a low-rank approximation problem (2) ≥ min A:rank(A)≤RL ∥A − E∥2 , (4) where the optimal solution is A = LRRL∧RE(E) := E′. Therefore, if we can identify rank- R or lower matrices (∆Wl)L l=1 such that LY l=1 (Wl + ∆Wl) − LY l=1 Wl | {z } :=A = LRRL∧RE(W − LY l=1 Wl) | {z } :=E′ , (5) 21Published as a conference paper at ICLR 2024 then we effectively solve the matrix approximation problem as defined in (2). Moreover, it is straightforward to verify that (5) directly implies all statements in this lemma. Therefore, our re- maining proof focuses on proving (5). Denote RE′ = RL ∧ RE. To derive the explicit form of E′, we first refer to the SVD of E as E = UDV ⊤, where U and V are orthonormal matrices and the first RE diagonal entries of D are non-zero, with all remaining entries being zero. Based on this, E′ is expressed as E′ = UDI1:RL,DV ⊤. Having already derived the decomposition A = PL l=1 Al, we next aim to decompose E′ as E′ =PL l=1 E′Ql, where Q1, . . . ,QL ∈ RD×D. The goal now shifts to identifying ∆Wl, Ql such that Al = E′Ql for each l ∈ [L]. Achieving this would complete the proof of (5). Therefore, our goal becomes finding ∆W1, . . . ,∆WL with rank(∆Wl) ≤ R for all l ∈ [L] such that Al = ( LY i=l+1 Wi)∆Wl( l−1Y i=1 (Wi + ∆Wi)) = E′Ql, for all l ∈ [L]. (6) One sufficient condition for achieving (6) is that the decomposed matrices Q1, QL and low-rank adapters ∆W1, . . . ,∆WL meet the following conditions: LX l=1 E′Ql = E′, (7) ∆Wl = ( LY i=l+1 Wi)−1E′Ql( l−1Y i=1 (Wi + ∆Wi))−1, for all l ∈ [L] (8) rank(∆Wl) ≤ R, for all l ∈ [L], (9) rank(Wl + ∆Wl) = D, for all l ∈ [L − 1]. (10) Here (7) describes the decomposition ofE′, (8) provides one simple solution to (6) when (10) holds, and (9) is the rank constraint on the low-rank adapter. In particular, the (10) is used to ensure the invertibility of Ql i=1(Wi + ∆Wi) for l ∈ [L − 1]. This condition is not necessary for l = L as the inverse of WL + ∆WL is not required for computing any low-rank adapters. We will show that the matrices (Ql)L l=1 defined by Ql = V I(R(l−1)+1)∧RE′:Rl∧RE′,DV ⊤, for all l ∈ [L], (11) and ∆Wl defined by (8) for alll ∈ [L] satisfies the all four conditions (7), (8), (9), and (10). We note that the definition of (Ql)L l=1 clearly satisfies condition (7). For the remaining conditions, namely (8), (9), (10), we proceed the proof by induction. When l = 1. We begin by examining the three conditions (8), (9) and (10) under the base case l = 1. We first determine Q1 and ∆W1 based on (11) and (8): ∆W1 = ( LY i=2 Wi)−1E′Q1, Q1 = I1:R,D. (12) By the choice of ∆W1, we satisfy the condition (8). Moreover, it directly follows that rank(∆W1) ≤ rank(Q1) = R, thereby fulfilling the rank constraint in (9). 22Published as a conference paper at ICLR 2024 Therefore, we just need to prove that W1 + ∆W1 is full-rank, as required by condition (10). To compute rank(W1 + ∆W1), we proceed as follows: rank(W1 + ∆W1) (12) = rank( W1 + ( LY i=2 Wi)−1E′Q1) (Substituting for ∆W1) = rank(( LY i=1 Wi) + E′Q1) (Left multiplying with invertible ( LY i=2 Wi)−1) = rank(( LY i=1 Wi) + LRR∧RE′ (E)). (Simplifying) Given the assumption that QL l=1 Wl + LRr(E) is full rank for all r ≤ R(L − 1), rank(W1 + ∆W1) = rank((QL i=1 Wi) + LRR∧RE′ (E)) = D, satisfying the last condition (10). When l >1. Consider l = 2, . . . , L. We assume that for i ∈ [l − 1], we have determined matrices Qi and ∆Wi based on (11) and (8), respectively, and we assume that they satisfy the conditions (8), (9), and (10). First, under the induction assumption that Wi + ∆Wi is invertible for all i ∈ [l − 1], to achieve Al = E′Ql, we set ∆Wl based on (8). This definition ensures rank(∆Wl) ≤ rank(Ql) = R, thereby satisfying the condition (9). To prove thatWl +∆Wl is full-rank (condition (10)), we focus on computing rank(Wl + ∆Wl). We proceed as follows: rank(Wl + ∆Wl) (8) = rank(Wl + ( LY i=l+1 Wi)−1E′Ql( l−1Y i=1 (Wi + ∆Wi)−1)) (Substituting for ∆Wl) = rank(ID + ( LY i=l Wi)−1E′Ql( l−1Y i=1 (Wi + ∆Wi))−1) (Left multiplying invertible W−1 l ) = rank \u0010l−1Y i=1 (Wi + ∆Wi) + ( LY i=l Wi)−1E′Ql \u0011 (Right multiplying invertible l−1Y i=1 (Wi + ∆Wi)) = rank \u0010 (Wl−1 + ∆Wl−1) l−2Y i=1 (Wi + ∆Wi) + ( LY i=l Wi)−1E′Ql \u0011 (Rearranging terms) (8) = rank \u0010 (Wl−1 + ( LY i=l Wi)−1E′Ql−1( l−2Y i=1 (Wi + ∆Wi))−1) l−2Y i=1 (Wi + ∆Wi) + ( LY i=l Wi)−1E′Ql \u0011 (Substituting for ∆Wl−1) = rank \u0010 ( LY i=l−1 Wi + E′Ql−1( l−2Y i=1 (Wi + ∆Wi))−1) l−2Y i=1 (Wi + ∆Wi) + E′Ql \u0011 (Left multiplying LY i=l Wi) = rank   ( LY i=l−1 Wi l−2Y i=1 (Wi + ∆Wi) + E′Ql−1 + E′Ql ! (Rearranging terms) = ··· 23Published as a conference paper at ICLR 2024 = rank( LY i=1 Wi + E′( lX i=1 Qi)) (Taking similar steps) = rank( LY i=1 Wi + LRRl∧RE′ (E)). (Simplifying) By the assumption that QL l=1 Wl + LRr(E) is full-rank for r ≤ R(L − 1) and consequently, rank(Wl + ∆Wl) = rank(QL i=1 Wi + LRRl∧RE′ (E)) = D, satisfying the last condition (10). Conclusion of Inductive Proof. Thus, by induction, we show that the definitions of(∆Wl)L l=1 in (8) and (Ql)L l=1 in (11) ensure that Al = E′Ql for all l ∈ [L]. Summing over l from 1 to L satisfies condition (5), thereby completing the proof. The following lemma extends the results to a more general setting where different LoRA-ranks can be employed across layers. Lemma 8. Define error matrix E := W − QL l=1 Wl, and denote its rank by RE = rank( E). For a sequence of LoRA-ranks for all layers (Rl)L l=1, assume that all the weight matrices of the frozen model (Wl)L l=1, and QL l=1 Wl + LRr(E) are non-singular for all r ≤ PL−1 l=1 Rl. Then, the approximation error min ∆Wl:rank(∆Wl)≤Rl \r\r\r\r\r LY l=1 (Wl + ∆Wl) − W \r\r\r\r\r 2 = σPL l=1 Rl+1   W − LY l=1 Wl ! | {z } Error matrix E , and the optimal solution to the matrix approximation problem satisfies QL l=1(Wl + ∆Wl) =QL l=1 Wl +LR(PL l=1 Rl)∧RE(E). Therefore, whenPL l=1 Rl ≥ RE, we have QL l=1(Wl +∆Wl) = W, implying f ≡ f. Proof of Lemma 8. The proof follows the same steps of Lemma 7 with only minor modifications. In the current setting, we target the following constrained optimization problem: min ∆Wl:rank(∆Wl)≤Rl \r\r\r\r\r LY l=1 (Wl + ∆Wl) − W \r\r\r\r\r 2 , where we allow each LoRA adapter ∆Wl can possess different LoRA-ranks Rl, i.e., rank(∆Wl) ≤ Rl, l ∈ [L]. Subtracting QL l=1 Wl from both terms leads us to a similar constrained optimization problem as (2). The only distinction lies in the rank constraint: min ∆Wl:rank(∆Wl)≤Rl \r\r\r\r\r\r\r\r\r  LY l=1 (Wl + ∆Wl) − LY l=1 Wl ! | {z } :=A −   W − LY l=1 Wl ! | {z } :=E \r\r\r\r\r\r\r\r\r 2 . (13) Following the same steps, we decompose A into (3). Given that rank(Al) ≤ rank(∆Wl) ≤ Rl, we deduce that rank(A) ≤ PL l=1 rank(Al) ≤ PL l=1 Rl. Consequently, the optimization problem above can be eased into a low-rank approximation problem analogous to (4): (13) ≥ min A:rank(A)≤PL l=1 Rl ∥A − E∥2 , where the optimal solution is A = LR(PL l=1 Rl)∧RE(E) := E′. Therefore, if we can identify the LoRA adapters (∆Wl)L l=1 with rank(∆Wl) ≤ Rl such that LY l=1 (Wl + ∆Wl) − LY l=1 Wl | {z } :=A = LR(PL l=1 Rl)∧RE(W − LY l=1 Wl) | {z } :=E′ , 24Published as a conference paper at ICLR 2024 the proof is completed. The remaining part of the proof adheres to the steps outlined in the proof of Lemma 7 deriving (5). The only difference is that we consider a different selection of (Ql)l = 1L that satisfies (9) here: Ql = V I(Pl−1 i=1 Ri)∧RE′:(Pl i=1 Ri)∧RE′,DV ⊤. Applying the same steps with this change yields the desired outcomes. This lemma illustrates that in linear cases, the total number of parameters needed to achieve an exact approximation is constant, regardless of LoRA-rank assignment. It suggests that applying a LoRA-rank of R per layer is equivalent to applying a LoRA-rank of RL at the final layer. As a result, fine-tuning only the last layer, which involves assigning a LoRA-rank of D to the last layer, is equivalent to implementing LoRA where each adapter is constrained to have a rank ofD/L. Both methods can achieve an exact approximation and maintain the same parameter efficiency. E P ROOFS FOR FNN A PPROXIMATION In this section, we provide the full proof for deriving the main results outlined in Sec. 3. For the sake of completeness, we restate our results from the main body before presenting the proof. E.1 A PPROXIMATING ONE-LAYER RELU FNN VIA LORA We first provide a slightly more detailed result on the one-layer ReLU FNN approximation (Lemma 9) along with its corresponding proof. Then, we present a variant of this lemma by al- lowing for different LoRA-ranks for each low-rank adapter. The proof for this variant involves only a minor modification of the proof for Lemma 9. Lemma 9 (Detailed version of Lemma 2). Define error matrix E := W1 −QL l=1 Wl, with its rank represented by RE = rank(E). Consider a LoRA-rank R ∈ [D]. Assume that the weight matrices W1, . . . ,WL ∈ RD×D and QL l=1 Wl + LRr(E) for all r ≤ R(L −1) are non-singular. Letx be a random input sampled from a distribution with bounded support X and let Σ = Exx⊤. Then, there exists rank-R or lower matrices ∆W1, . . . ,∆WL ∈ RD×D and bias vectors bb1, . . . ,bbL ∈ RD such that for any input x ∈ X, f(x) − f(x) = ReLU    LRRL∧RE(W1 − LY l=1 Wl) − (W1 − LY l=1 Wl) ! x ! . Therefore, when R ≥ ⌈RE/L⌉, the adapted model exactly approximates the target model, i.e., f(x) = f(x) for all x ∈ X. Furthermore, let x be a random input sampled from a distribution with bounded support X and let Σ = Exx⊤. Then, the expected squared error is bounded as E \r\rf(x) − f(x) \r\r2 2 ≤ ∥Σ∥F σ2 RL∧RE+1(W1 − LY l=1 Wl). Proof of Lemma 9. This proof consists of three main steps: (i) linearize the first L − 1 layers of the adapted model f to reduce it to a single-layer FNN, (ii) align the weight matrices and bias vectors of this simplified f with those of the target model f, (iii) derive an upper bound of the error E \r\rf(x) − f(x) \r\r2 2 . Linearization. The main challenge here stems from the non-linearities introduced by the ReLU activation function. To remove the non-linearities in the firstL −1 layers of updated model f, since the input space X is bounded, we can set all the entries of bb1, . . . ,bbL−1 sufficiently large, thereby 25Published as a conference paper at ICLR 2024 activating all ReLUs in the first L − 1 layers of f. Consequently, we have f(x) = ReLU((WL + ∆WL)zL−1 + bbL) = ReLU \u0010 (WL + ∆WL)ReLU((WL−1 + ∆WL−1)zL−2 + bbL−1) + bbL \u0011 = ReLU \u0010 (WL + ∆WL)((WL−1 + ∆WL−1)zL−2 + bbL−1) + bbL \u0011 = ReLU \u0010 (WL + ∆WL)(WL−1 + ∆WL−1)zL−2 + (WL + ∆WL)bbL−1 + bbL \u0011 = ··· = ReLU  LY l=1 (Wl + ∆Wl)x + ( L−1X l=1 LY i=l+1 (Wi + ∆Wi)bbl) + bbL ! , which is equivalent to a single-layer ReLU neural network with weight matrix QL l=1(Wl + ∆Wl) and bias vector (PL−1 l=1 QL i=l+1(Wi + ∆Wi)bbl) + bbL. Parameter Alignment. To match the updated model f(x) and target model f(x), we proceed as follows. For weight matrix, Lemma 7 guarantees the existence of rank- R or lower matrices ∆W1, . . . ,∆WL ∈ RD×D such that LY l=1 (Wl + ∆Wl) = LY l=1 Wl + LRRL∧RE(W − LY l=1 Wl). (14) For the bias vector, we setbbL = b1 −PL−1 l=1 QL i=l+1(Wi+∆Wi)bbl such that PL−1 l=1 QL i=l+1(Wi+ ∆Wi)bbl + bbL = b1. Therefore, we obtain f(x) − f(x) = ReLU    LRRL∧RE(W1 − LY l=1 Wl) − (W1 − LY l=1 Wl) ! x ! . Error Derivation. We compute the expected squared error as follows: E \r\rf(x) − f(x) \r\r2 2 ≤ E \r\r\r\r\r   LRRL∧RE(W1 − LY l=1 Wl) − (W1 − LY l=1 Wl) ! x \r\r\r\r\r 2 2 (ReLU is 1-Lipschitz) (1) ≤ \r\r\r\r\rLRRL∧RE(W1 − LY l=1 Wl) − (W1 − LY l=1 Wl) \r\r\r\r\r 2 2 E∥x∥2 2 = ∥Σ∥F σ2 RL∧RE+1(W1 − LY l=1 Wl). (By the definition of LRRL∧RE(·)) This completes the proof. Lemma 9 is extended to cases where different LoRA-ranks can be used for different low-rank adapters, as detailed in the following lemma. Lemma 10. Define error matrix E := W1 − QL l=1 Wl, and denote its rank by RE = rank(E). Consider a sequence of LoRA-ranks (Rl)L l=1. Assume that the weight matrices W1, . . . ,WL ∈ RD×D and QL l=1 Wl + LRr(E) for all r ≤ PL−1 l=1 Rl are non-singular. Then, there LoRA adapters (∆Wl)L l=1 satisfying the rank constraints rank(∆Wl) ≤ Rl for all l ∈ [L] and bias vectors bb1, . . . ,bbL ∈ RD such that for any input x ∈ X, f(x) − f(x) = ReLU    LR(PL l=1 Rl)∧RE(W1 − LY l=1 Wl) − (W1 − LY l=1 Wl) ! x ! . 26Published as a conference paper at ICLR 2024 Therefore, when PL l=1 Rl ≥ RE, the adapted model exactly approximates the target model, i.e., f(x) = f(x) for all x ∈ X. Furthermore, for a random inputx drawn from a distribution supported onX, and with Σ = Exx⊤, the expected squared error is bounded by: E \r\rf(x) − f(x) \r\r2 2 ≤ ∥Σ∥F σ2 (PL l=1 Rl)∧RE+1(W1 − LY l=1 Wl). Proof of Lemma 10. This proof closely adheres to the steps detailed in the proof of Lemma 9. The primary change implemented here is that, when we draw the analogy to (14), we apply Lemma 8 instead of Lemma 7. This results in LY l=1 (Wl + ∆Wl) = LY l=1 Wl + LR(PL l=1 Rl)∧RE(W − LY l=1 Wl). Utilizing the steps from the proof of Lemma 9 and integrating the modification specified above, we can establish the desired result. E.2 A PPROXIMATING MULTI -LAYER RELU FNN VIA LORA WITH UNIFORM MODEL PARITION In this part, we restate all the results considering uniform model partition from Sec. 3.3, along with their corresponding proofs, presented in the same order. Assumption 1 (Non-Singularity). For a fixed LoRA-rank R ∈ [D], the weight matrices of the frozen model (Wl)L l=1 and matrices \u0010Q l∈Pu i Wl \u0011 + LRr(Wi − Q l∈Pu i Wl) are non-singular for all r ≤ R(M − 1) and i ∈ [L]. Lemma 3. Let (Wl)L l=1, (Wl)L l=1 ∈ RD×D be matrices whose elements are drawn independently from arbitrary continuous distributions. Then, with probability 1, Assumption 1 holds ∀R ∈ [D]. Proof of Lemma 3. We first use Lemma 6 to establish that W1, . . . ,WL, W1, . . . ,WL are non- singular with probability 1. The goal of the remaining proof is to demonstrate that \u0010Q l∈Pu i Wl \u0011 + LRr(Wi − Q l∈Pu i Wl) is full-rank with probability 1. In this proof, we use p· to denote the proba- bility density function, where the subscript indicates the associated random variable. Fix an arbitrary i ∈ [L] and r ∈ [R]. Then probability of the \u0010Q l∈Pu i Wl \u0011 + LRr \u0010 Wi − Q l∈Pu i Wl \u0011 being full-rank can be computed as P   det     Y l∈Pu i Wl   + LRr  Wi − Y l∈Pu i Wl     ̸= 0    = Z E P   det     Y l∈Pu i Wl   + LRr(E)   ̸= 0 \f\f\f\f\f\f Wi − Y l∈Pu i Wl = E   pWi−Q l∈Pu i Wl (E)dE. If the conditional random matrix \u0010Q l∈Pu i Wl \u0011 +LRr(E) | Wi −Q l∈Pu i Wl = E has a continuous distribution with support of non-zero Lebesgue measure on RD×D, then P   det     Y l∈Pu i Wl   + LRr(E)   ̸= 0 \f\f\f\f\f\f Wi − Y l∈Pu i Wl = E    = 1 ensuring \u0010Q l∈Pu i Wl \u0011 + LRr \u0010 Wi − Q l∈Pu i Wl \u0011 is full-rank with probability 1. 27Published as a conference paper at ICLR 2024 Consequently, the remaining part of the proof aims to show that the conditional random matrix\u0010Q l∈Pu i Wl \u0011 + LRr(E) | Wi − Q l∈Pu i Wl = E follows arbitrary continuous distribution with support having non-zero Lebesgue measure onRD×D. Denote W = Q l∈Pu i Wl. Now, consider the conditional distribution of Q l∈Pu i Wl | Wi − Q l∈Pu i Wl = E, which can be written as pW|Wi−W=E(W) = pWi (E + W). Since pWi is continuous with support of non-zero Lebesgue measure on RD×D, the same holds for Q l∈Pu i Wl | Wi − Q l∈Pu i Wl = E. Furthermore, adding a constant matrix LRr(E) to this conditional distribution preserves the desired properties, thus completing the proof. Theorem 3. Under Assumption 1, there exists rank-R or lower matrices (∆Wl)L l=1 with ∆Wl ∈ RD×D and bias vectors (bbl)L l=1 with bbl ∈ RD when the rank of the low-rank adapter R ≥ ⌈maxi∈[L] rank(Wi − Q l∈Pu i Wl)/M⌉, the low-rank adapted model f can exactly approximate the target model f, i.e., f(x) = f(x) for all input x ∈ X. Proof of Theorem 3. The key to this proof lies in a simple idea: for each layer i ∈ [L] in the target model, we can update M layers (i.e., (i − 1)M + 1-th layer to iM-th layer) in the frozen model to approximate it as guaranteed by Lemma 9. Hence, all layers of the target model can be approximated by the adapted model. Model Decomposition. We partition the adapted model f into L sub-models, each defined as fi(·) = FNNL,D(·; (Wl + ∆Wl)l∈Pu i , (bbl)l∈Pu i ), i ∈ [L]. In a similar manner, we break down f into L sub-models, each is a one-layer FNN: fi(·) = FNN1,D(·; Wi, bi), i∈ [L]. We can then express f(x) and f(x) as compositions of their respective sub-models: f(·) = fL ◦ ···f1(·), f(·) = fL ◦ ···f1(·). To analyze the error E \r\rf(x) − f(x) \r\r 2 = E \r\rf(x) − f(x) \r\r 2, we consider the error caused by each submodel. Let eRi = rank(Wi − Q l∈Pu i Wl) denote the rank of the discrepancy between the target weight matrix and the frozen weight matrices, where i ∈ [L]. By Lemma 9, we can select ∆W1, . . . ,∆WL, bb1, . . . ,bbL such that fi(z) − fi(z) = ReLU    LRRL∧eRi (Wi − Y l∈Pu i Wl) − (Wi − Y l∈Pu i Wl)  z  , (15) E \r\rfi(z) − fi(z) \r\r2 2 ≤ \r\rEzz⊤\r\r F σ2 RL∧eRi+1(Wi − LY l=1 Wl). (16) Given these selected parameters, fi is functionally equivalent to a one-layer FNN: fi(z) = ReLU    LRRL∧eRi (Wi − Y l∈Pu i Wl) + Y l∈Pu i Wl  z  . Clearly, when R ≥ maxi⌈ eRi M ⌉, it follows that fi = gi for all i ∈ [L], which implies f = g. Corollary 4. Assume that the elements of matrices (Wl)L l=1, (Wl)L l=1 are independently drawn from arbitrary continuous distributions. When R ≥ D/M, there exists rank-R or lower matrices ∆W1, . . . ,∆WL ∈ RD×D and bias vectors bb1, . . . ,bbL ∈ RD such that low-rank adapted model f can functionally cover the target model f on X, i.e., f(x) = f(x) for all input x ∈ X, with probability 1. 28Published as a conference paper at ICLR 2024 Proof of Corollary 4. To prove the statement, we start by noting that combining Lemma 3 and The- orem 3 directly gives us f(x) = f(x) on X when R ≥ maxi∈[L]⌈rank(Wi − Q l∈Pu i Wl)/M⌉. Therefore, the only thing left is to show that rank(Wi − Q l∈Pu i Wl) = D for i ∈ [L] with prob- ability 1. In this proof, we use p· to denote the probability density function, where the subscript indicates the associated random variable. To establish this, consider the following probability expression: P   det  Wi − Y l∈Pu i Wl   ̸= 0    = Z P   det \u0000 Wi − W \u0001 ̸= 0 \f\f\f\f\f\f Y l∈Pu i Wl = W   pQ l∈Pu i Wl(W)dW. Since W is independent of Q l∈Pu i Wl, we have P   det \u0000 Wi − W \u0001 ̸= 0 \f\f\f\f\f\f Y l∈Pu i Wl = W    = P \b det \u0000 Wi − W \u0001 ̸= 0 \t Lemma 6 = = = = = 1. Therefore, we conclude that P n det \u0010 Wi − Q l∈Pu i Wl \u0011 ̸= 0 o = 1 , which completes the proof. Theorem 5. Define the approximation error of i-th layer as Ei = σRM+1(Wi −Q l∈Pu i Wl), and the magnitude of the parameters and the input as β := maxi∈[L] \u0010p ∥Σ∥F Qi j=1 \r\rWj \r\r F + Pi j=1 Qi−1 k=j+1 \r\rWk \r\r F \r\rbj \r\r 2 \u0011Wp ∥Σ∥F. Under Assumption 1, there exists rank- R or lower matrices (∆Wl)L l=1 with ∆Wl ∈ RD×D and bias vectors (bbl)L l=1 with bbl ∈ RD such that for input x ∈ Xwith Exx⊤ = Σ, E \r\rf(x) − f(x) \r\r 2 ≤ β LX i=1 max k∈[L] \u0000\r\rWk \r\r F + Ek \u0001L−i Ei. Proof of Theorem 5. This proof is a continuation of the proof of Theorem 3. In this proof, we will consider a more general case, without enforcing any constraints on the rank of the adapters R. We use cWi to denote the corresponding weight matrix, i.e., cWi = LR RL∧eRi (W1 − Q l∈Pu i Wl) +Q l∈Pu i Wl. Error Decomposition. For submodel i = 2, . . . ,L, we calculate the expected error of the compo- sition of the first i sub-models, E∥bzi − zi∥2 = E \r\rfi(bzi−1) − fi(zi−1) \r\r 2 (17) = E \r\r(fi(bzi−1) − fi(zi−1)) + \u0000 fi(zi−1) − fi(zi−1) \u0001\r\r 2 (Rearranging terms) ≤ E∥fi(bzi−1) − fi(zi−1)∥2| {z } Ai + E \r\rfi(zi−1) − fi(zi−1) \r\r 2| {z } Bi . (Applying triangle inequality) Here Ai represents the error resulting from the discrepancy between the firsti −1 submodels, while Bi represents the error arising from the mismatch between the i-th submodel. 29Published as a conference paper at ICLR 2024 Computing Ai. We start by computing the error introduced by the first i − 1 submodels, denoted by Ai: Ai = E∥fi(bzi−1) − fi(zi−1)∥2 = E \r\r\rReLU(cWi(bzi−1 − zi−1)) \r\r\r 2 ≤ E \r\r\rcWi(bzi−1 − zi−1) \r\r\r 2 (ReLU is 1-Lipschitz) (1) ≤ \r\r\rcWi \r\r\r F E∥bzi−1 − zi−1∥2 . (18) Here, \r\r\rcWi \r\r\r F = \r\r\r\r\r\r Y l∈Pu i Wl + LRRM∧eRi (Wi − Y l∈Pu i Wl) \r\r\r\r\r\r F = \r\r\r\r\r\r Wi +   Y l∈Pu i Wl − Wi   + LRRM∧eRi (Wi − Y l∈Pu i Wl) \r\r\r\r\r\r F (Rearranging terms) ≤ \r\rWi \r\r F + \r\r\r\r\r\r   Y l∈Pu i Wl − Wi   + LRRM∧eRi (Wi − Y l∈Pu i Wl) \r\r\r\r\r\r F (Applying triangle inequality) = \r\rWi \r\r F + vuuut DX j=RM∧eRi+1 σ2 j (Wi − Y l∈Pu i Wl) (19) (By the definition of Wi and LRRM∧eRi+1(·)) ≤ max k∈[L] ( \r\rWk \r\r F + Ei) := α. By combining (18) and (19), we get Ai ≤ max k∈[L] \u0000\r\rWk \r\r F + Ei \u0001 E∥bzi−1 − zi−1∥2 ≤ αE∥bzi−1 − zi−1∥2 . (20) Computing Bi. We proceed to compute the error associated with the i-th submodel, which we denote as Bi. It can be evaluated as follows: Bi = E \r\rfi(zi−1) − fi(zi−1) \r\r 2 (15) = E \r\r\r\r\r\r ReLU    LRRM∧eRi (Wi − Y l∈Pu i Wl) − (Wi − Y l∈Pu i Wl)  zi−1   \r\r\r\r\r\r 2 ≤ E \r\r\r\r\r\r  LRRM∧eRi (Wi − Y l∈Pu i Wl) − (Wi − Y l∈Pu i Wl)  zi−1 \r\r\r\r\r\r 2 (ReLU is 1-Lipschitz) (1) ≤ \r\r\r\r\r\r LRRM∧eRi (Wi − Y l∈Pu i Wl) − (Wi − Y l∈Pu i Wl) \r\r\r\r\r\r 2 E∥zi−1∥2 = σRM∧eRi+1(Wi − Y l∈Pu i Wl)E∥zi−1∥2 . We can further simplify E∥zi−1∥2 as : E∥zi−1∥2 = E \r\rReLU(Wi−1zi−2 + bi−1) \r\r 2 = E \r\rWi−1zi−2 + bi−1 \r\r 2 (ReLU is 1-Lipschitz) 30Published as a conference paper at ICLR 2024 ≤ \r\rWi−1 \r\r F E∥zi−2∥2 + \r\rbi−1 \r\r 2 (Applying triangle inequality and (1)) ≤ \r\rWi−1 \r\r F \u0000\r\rWi−2 \r\r F E∥zi−3∥2 + \r\rbi−2 \r\r 2 \u0001 + \r\rbi−1 \r\r 2 (Following the same steps) ≤ i−1Y j=1 \r\rWj \r\r F E∥x∥2 + i−1X j=1 i−1Y k=j+1 \r\rWk \r\r F \r\rbj \r\r 2 (Repeating the same steps) = q ∥Σ∥F i−1Y j=1 \r\rWj \r\r F + i−1X j=1 i−1Y k=j+1 \r\rWk \r\r F \r\rbj \r\r 2 ≤ β. Therefore, we obtain Bi ≤ βσRM∧eRi+1(Wi − Y l∈Pu i Wl). Error Composition. Having established upper bounds for Ai and Bi, we next evaluate the ex- pected error for the composition of the first i adapted submodels. E∥bzi − zi∥2 (17) ≤ Ai + Bi (20) ≤ αE∥bzi−1 − zi−1∥2 + Bi ≤ α(αE∥bzi−2 − zi−2∥2 + Bi−1) + Bi = α2E∥bzi−2 − zi−2∥2 + αBi−1 + Bi ≤ ··· ≤αi−1E∥bz1 − z1∥2 + iX k=2 αi−kBk. (21) To compute the overall approximation error of f, which is the composite of all submodels, we have E \r\rf(x) − f(x) \r\r 2 = E \r\rf(x) − f(x) \r\r 2 = E∥bzL − zL∥2 (21) ≤ αL−1E∥bz1 − z1∥2 + LX i=2 αL−iBi (16) ≤ αL−1βσRM∧eRi+1(Wi − Y l∈Pu i Wl) + β LX i=2 αL−iσRM∧eRi+1(Wi − Y l∈Pu i Wl) = β LX i=1 αL−iσRM∧eRi+1(Wi − Y l∈Pu i Wl) = β LX i=1 αL−iσRM+1(Wi − Y l∈Pu i Wl). Substituting α with maxk∈[L]( \r\rWk \r\r F + Ei) concludes the proof. E.3 A PPROXIMATING MULTI -LAYER RELU FNN VIA LORA WITH GENERAL MODEL PARITION Firstly, we provide the required non-singular assumption and the lemma demonstrating the mildness of this assumption for the general model partition cases after introducing necessary notations. Assumption 2. For the given LoRA-rank sequence(Rl)L l=1 and partition P, the weight matrices of the frozen model W1, . . . ,WL and \u0000Q l∈Pi Wl \u0001 + LRr(Wi − Qmax Pi−1 l=min Pi Wl) are non-singular for all r ≤ Pmax Pi−1 l=min Pi Rl and i ∈ [L]. Note that max Pi and min Pi here represent the maximum and minimum elements in the set Pi, respectively. Lemma 11. Let (Wl)L l=1, (Wl)L l=1 ∈ RD×D be matrices whose elements are drawn independently from arbitrary continuous distributions. Then, with probability 1, Assumption 2 holds for all R ∈ [D]. 31Published as a conference paper at ICLR 2024 Proof of Lemma 11. Following the same steps in the proof of Lemma 3 but replacing the uniform partition with the general partition completes the proof. We now restate Theorem 6 and provide its proof. Theorem 6. Consider a partition P for the frozen model. Let Assumption 2 hold. If P l∈Pi Rl ≥ rank(Wi − Q l∈Pi Wl) for all i ∈ [L], there exists LoRA adapters (∆Wl)L l=1 with rank(∆Wl) ≤ Rl and biases (bbl)L l=1 such that the adapted model f can exactly approximate the target model. Moreover, define the approximation error of the i-th layer as Ei = σP l∈Pi Rl+1(Wi −Q l∈Pi Wl), and the magnitude of the parameters and the input as β := maxi∈[L] \u0010p ∥Σ∥F Qi j=1 \r\rWj \r\r F + Pi j=1 Qi−1 k=j+1 \r\rWk \r\r F \r\rbj \r\r 2 \u0011Wp ∥Σ∥F. Then, there exists LoRA adapters (∆Wl)L l=1 with rank(∆Wl) ≤ Rl and biases (bbl)L l=1 such that for any input x ∈ Xwith Exx⊤ = Σ, the approximation error can be bounded as E \r\rf(x) − f(x) \r\r 2 ≤ β LX i=1 max k∈[L] \u0000\r\rWk \r\r F + Ek \u0001L−i Ei. Proof of Theorem 6. This proof follows the same steps as the proofs of Theorem 3 and Theorem 5, substituting the uniform partition Pu with the general partition P and applying Lemma 10 in place of Lemma 2 to derive the desired outcome. E.4 A PPROXIMATING MULTI -LAYER RELU FNN VIA FINAL LAYERS TUNING We now aim to examine another commonly used model adaptation method, the final layers tuning, within the same theoretical framework. The main limitation of this method, as compared to LoRA, is that while LoRA can update all layers, the tuning of final layers keeps the initial layers frozen. Consequently, a clear limitation arises when the initial layers of the frozen model f are less discriminative than the target model f. That is, if there exist two input vectors x1, x2 ∈ RD×D such that the output of the initial layers of the frozen model f0 is the same, but the output of the target model f is different, then no matter how the final layers are tuned, it is impossible for the adapted model f to exactly approximate the target model f. To formalize this, we observe that for the first layer of the frozen model, the outputs of the inputs in the non-activation region are always zero. In other words, whenx1, x2 ∈ {x : W1x + b1 ≤ 0}, we have ReLU(W1x1+b1) = ReLU(W1x2+b1) = 0. Therefore, no matter how the subsequent layers are tuned, we still have f(x1) = f(x2). When we fix the first l −1 layers, the non-activation region becomes {x : W2(W1x + b1) + b2 ≤ 0}. Similarly, we define the non-active region of the first l layer in the frozen model as Il = n x : Ql i=1 Wix + Pl i=1 Ql j=i+1 Wjbi ≤ 0 o . Correspondingly, we define Il = n x : Ql i=1 Wix + Pl i=1 Ql j=i+1 Wjbi ≤ 0 o . The following lemma is provided based on these definitions. Lemma 12. If l ∈ [L − 1] such that Il \\ SL i=1 Ii ̸= ∅ and the weight matrices of the target model (Wi)L i=1 are non-singular, then for any tuning of the lastL − l layers, f ̸= f. Proof of Lemma 12. For the simplicity of the presentation, we let I = SL i=1 Ii to denote the non- activation region of the target model. Then, the condition Il \\ SL i=1 Ii ̸= ∅ can be written as Il \\ I ̸= ∅. Clearly, both I and Il are closed convex sets. Condition Il \\ I ̸= ∅. The condition Il \\ I ̸= ∅ indicates that there exists a region in Il where the ReLUs are deactivated in the l-th layer of the frozen model, but activated in the entire target model. Therefore, for any x1, x2 ∈ Il \\ I, we have f(x1) = f(x2) regardless of how the final l + 1 layers 32Published as a conference paper at ICLR 2024 are tuned. If these x1, x2 ∈ Il \\ I satisfies f(x1) ̸= f(x2), this proof is completed. The remaining proof is showing the existence of such x1, x2. Existence of x1, x2. Firstly, we show that there exists twox1, x2 ∈ Il \\ I such that x1 ̸= x2. Let x1 ∈ Il \\ I. Since Il is a closed set, there exists a sequence (zi)∞ i=1 where zi ∈ Il and zi ̸= x1 satisfying limi→∞ zi = x1. Note that at least one element zi must not belong to I, otherwise x1 would be in I due to the closed property of I, contradicting the selection of x1. Let x2 = zi. Therefore, we have two distinct x1, x2 ∈ Il \\ I with x1 ̸= x2. Then, given x1, x2 ∈ Il \\ I such that x1 ̸= x2, both x1, x2 activate all the ReLUs in the target model. Since x1, x2 ̸∈ I and the weight matrices of the target model (Wl)L l=1 all are non-singular, we have f(x1) − f(x2) = WL ··· W1(x1 − x2) ̸= 0, implying f(x1) ̸= f(x2). Meanwhile, since x1, x2 ∈ Il, the output of the initial l layers of the frozen model are equal, thus we have f(x1) = f(x2) no matter how we tune the last L − l layers. This completes the proof. The following lemma reduces the assumptions to the assumption of randomly generated mod- els. This assumption aligns with that of Corollary 4, thereby facilitating a more effective comparison between the expressive power of LoRA and the adaptation of the final layers. W1x + b1 = 0 W1x + b1 = 0 I1 ¯I1 Figure 2: An example of I1 and I1 when D = 2. Lemma 4. Let D ≥ 2 and f be a one-layer target FNN. Assume that the elements of weight matrices (Wl)L l=1 are independently drawn from arbitrary continuous distributions. With probability 1, for any tuning of the last L − 1 layers, f ̸= f. Proof of Lemma 4. If we can show that I1 \\ I1 ̸= ∅, by Lemma 12, we obtain the desired results. Therefore, the remaining proof aims to show that I1 \\ I1 ̸= ∅ with probability 1. Note that I1 \\I1 = ∅ holds only when W1 = W1 (not that this is necessary condition not sufficient condition), as demonstrated in Figure 2. However, since the elements of matrices W1 are indepen- dently drawn from arbitrary continuous distributions, we have P(W1 ̸= W1) = 1 for all l ∈ [L]. Therefore, I1 \\ I1 = ∅ holds with probability 1. By Lemma 12, we complete the proof. F P ROOFS FOR TFN A PPROXIMATION In this section, we not only provide the proof for the results outlined in Sec. 4, but also introduce the problem setting for TFNs with single-head attention layers and present the corresponding results. F.1 A PPROXIMATING TRANSFORMER NETWORK WITH SINGLE -HEAD ATTENTION LAYERS In this part, we outline the problem setting to investigate the expressive power of LoRA in TFNs that utilize single-head attention layers. The primary distinction between this setting and that of TFNs with multi-head attention layers lies in the weight matrices. Specifically, the Wh Ol matrices for combining different attention heads are absent in this case. Despite this difference, the derived results are consistent, albeit under slightly modified assumptions regarding the weight matrices and a different LoRA adaptation strategy. We start by introducing necessary notations. For an input matrix X ∈ RD×N , where D is the dimension of the token embeddings and N is the number of tokens, the l-th Transformer block using single-head self-attention can be expressed as: Attnl(Zl−1) = WV lZl−1 · softmax \u0000 (WKlZl−1)⊤WQlZl−1 \u0001 , Zl := W2l · ReLU(W1l · Attnl(Zl−1) + b1l1⊤ N ) + b2l1⊤ N , 33Published as a conference paper at ICLR 2024 where the weight matrices WKl, WQl, WV l, W1l, W2l ∈ RD×D, bias vectors b1l, b2l∈RD , Zl is the output of l-th transformer block, with Z0 = X. The output of the first L Transformer blocks are subsequently fed into the output layer. This produces the final output of the TFN, given by softmax(WoZL), where Wo ∈ RD×D represents the weight matrix of the output layer. For single-head self-attention layers, the target model f, frozen model f, and the adapted model f can be formally represented as: Target TFN g = TFNL,D \u0000 ·; \u0000 (WV l, WKl, WQl, W2l, W1l)L l=1, Wo \u0001 , (b1l, b2l)L l=1 \u0001 , Frozen TFN f0 = TFNL,D \u0000 ·; \u0000 (WV l, WKl, WQl, W2l, W1l)L l=1, Wo \u0001 , (b1l, b2l)L l=1 \u0001 , Adapted TFN f = TFNL,D \u0010 ·; \u0000 (WV l+ ∆WV l, WKl + ∆WKl, WQl + ∆WQl, W2l + ∆W2l, W1l + ∆W1l)L l=1, Wo + ∆Wo \u0001 , (bb1l, bb2l)L l=1 \u0011 . Here, WKl, WQl, WV lare the weight matrices for generating key, query, and values in the l- th transformer block of the target TFN; W1l, W2l and b1l, b2l serve as the weight matrices and bias vectors, respectively, for the feedforward layer in the same block; Wo is the weight ma- trix for the output layer. For the frozen TFN, the same roles are played by WKl, WQl, WV l, W1l, W2l, and b1l, b2l for all l ∈ [L] and Wo. For the adapted model, low-rank adapters ∆WKl, ∆WQl, ∆WV l, ∆W1l, ∆W2l, ∆Wo with a rank constraint R ∈ [D] are added to each weight matrix, and the bias vectors are updated to bb1l, bb2l for all l ∈ [L]. Given the problem setting outlined above, we give the non-singularity assumption for TFNs with single-head attention layers. Assumption 3 (Non-Singularity). All the weight matrices of both the target model and the frozen model, as well as the following matrices for all r ∈ [D], W⊤ KlWQl + LRr \u0010 W ⊤ KlWQl − W⊤ KlWQl \u0011 , where l = 1, WKlWQl + LRr \u0010 W−1⊤ 2,l−1W ⊤ 2,l−1W ⊤ KlWQlW2,l−1W−1 2,l−1 − WKlWQl \u0011 , for l ∈ [L] \\ {1}, W1lWV l+ LRr \u0000 W1lWV l− W1lWV l \u0001 , for l = 1, W1lWV l+ LRr \u0010 W1lWV lW2,l−1W−1 2,l−1 − W1lWV l \u0011 , for all l ∈ [L] \\ {1}, WoW2L + LRr(WoW2L − WoW2L), are non-singular. Lemma 13. Let the elements of all weight matrices in target model f and the frozen model f be independently sampled from continuous distributions. Then, Assumption 3 holds with probability 1. Proof of Lemma 13. The results can be obtained by replicating the same steps outlined in the proof of Lemma 3. Theorem 8. Consider the rank of the adapter weight matrices R ∈ [D]. Let Assumption 3 hold. Define the rank-based functionality gap Gi to i-th transformer block ( i ∈ [L]) or output layer (i = L + 1) as Gi =    maxh \u0010 rank(W h⊤ Ki W h Qi − Wh⊤ Ki Wh Qi) \u0011 ∨ maxh \u0010 rank(W1iW h V i− W1iWh V i) \u0011 , i= 1, maxh \u0010 rank(W ⊤ 2,i−1W h⊤ Ki W h QiW2,i−1 − W⊤ 2,i−1Wh⊤ Ki Wh QiW2,i−1) \u0011 ∨maxh \u0010 rank(W1iW h V iW2,i−1 − W1iWh V iW2,i−1) \u0011 , 2 ≤ i ≤ L, rank(WoW2L − WoW2L), i = L + 1. If R ≥ maxi∈[L+1]⌈Gi 2 ⌉, there exists rank- R or lower weight matrices for low-rank adapters (∆WKl, ∆WQl, ∆WV l, ∆W1l)L l=1, ∆W2L, ∆Wo with other low-rank adapters set to O, and updated bias vectors: (bb1l, bb2l)L l=1, such that for any X ∈ RD×N , the adapted model f exactly approximates f, i.e., f(X) = f(X), with probability 1. 34Published as a conference paper at ICLR 2024 Proof of Theorem 8. Let Hl ∈ RD×N and Zl ∈ RD×N denote the intermediate and final outputs of the l-th transformer block in the target model f, respectively. Specifically, Hl represents the output from the first feedforward layer in the l-th transformer block. They are defined as Hl = ReLU   W1lWV lZl−1 · softmax \u0010 Z ⊤ l−1W ⊤ KlWQlZl−1 \u0011 + b1l1⊤ N ! , Zl = W2lHl + b2l1⊤ N , where l ∈ [L]. For the adapted model f, we introduce cHl and bZl to denote the corresponding intermediate output of the first feedforward layer and the final output of the l-th transformer block for the adapted model, respectively: cHl = ReLU   (W1l + ∆W1l)(WV l+ ∆WV l) · bZl−1 · softmax \u0010 bZ⊤ l−1(WKl + ∆WKl)⊤(WQl + ∆WQl) bZl−1 \u0011 + bb1l1⊤ N ! , bZl = (W2l + ∆W2l)cHl + bb2l1⊤ N , where l ∈ [L]. We note that Z0 = bZ0 = X. In this proof, we set ∆W2l = O for all l ∈ [L]. Our goal is to show that adding low-rank adapters to self-attention layers and the first feedforward layers in all transformer blocks enables the adapted model f to be functionally equivalent to the target model f of the same dimensions. We start by inductively constructing the adapter weight matrices(∆W1l, ∆WV l, ∆WKl, ∆WQl, bb1l, bb2l)L l=1 such that cHl = Hl for all l ∈ [L]. We then select the low-rank adapters for W2L and the Wo to approximate the output of the target model. For unmentioned low-rank adapters, we set them as O. When l = 1. To achieve cHl with Hl for all X, the following conditions must be satisfied: Bias Vector:bb1l = b1l, Query and Key: (WKl + ∆WKl)⊤(WQl + ∆WQl) = W ⊤ KlWQl Value and First Feedforward Layer: (W1l + ∆W1l)(WV l+ ∆WV l) = W1lWV l. To achieve this, we set bb1l = b1l to achieve (24), and select rank- R or lower matrices ∆WKl, ∆WQl, ∆W1l, ∆WV las suggested by Lemma 7. This ensures cHl = Hl for l = 1. When l >1. Now we focus on the cases where l = 2, . . . , L. Assume the induction hypothesis holds for l − 1, which is cHl−1 = Hl−1. This implies Hl−1 = W −1 2,l−1(Zl−1 − b2,l−11⊤ N ) = W−1 2,l−1( bZl−1 − bb2,l−11⊤ N ) = cHl−1. Using this assumption, we express bZl−1 in terms of Zl−1: bZl−1 = W2,l−1W −1 2,l−1(Zl−1 − b2,l−11⊤ N ) + bb2,l−11⊤ N . Let bb2,l−1 = W2,l−1W −1 2,l−1b2,l−1, then we have bZl−1 = W2,l−1W −1 2,l−1Zl−1. (22) To achieve cHl = Hl, we express both cHl and Hl in terms of Zl−1: Hl = ReLU \u0010 W1lWV l· Zl−1 · softmax \u0010 Z ⊤ l−1W ⊤ KlWQlZl−1 \u0011 + b1l1⊤ N \u0011 cHl = ReLU \u0010 (W1l + ∆W1l)(WV l+ ∆WV l) · bZl−1 35Published as a conference paper at ICLR 2024 · softmax \u0010 bZ⊤ l−1(WKl + ∆WKl)⊤(WQl + ∆WQl) bZl−1 \u0011 + bb1l1⊤ N \u0011 , (22) = ReLU   (W1l + ∆W1l)(WV l+ ∆WV l) · W2,l−1W −1 2,l−1Zl−1 · softmax \u0010 Z ⊤ l−1W −1⊤ 2,l−1W⊤ 2,l−1(WKl + ∆WKl)⊤ (WQl + ∆WQl)W2,l−1W −1 2,l−1Zl−1 \u0011 + bb1l1⊤ N ! . Therefore, we need to align the following three components: Bias Vector: bb1l = b1l, Query and Key: (WKl + ∆WKl)⊤(WQl + ∆WQl) = W−1⊤ 2,l−1W ⊤ 2,l−1W ⊤ KlWQlW2,l−1W−1 2,l−1, Value and First Feedforward Layer: (W1l + ∆W1l)(WV l+ ∆WV l) = W1lWV lW2,l−1W−1 2,l−1. By setting bb1l based on (26) and adjusting ∆WKl, ∆WQl, ∆W1l, ∆WV lbased on Lemma 7, we satisfy all three conditions above, thereby obtaining cHl = Hl for l ∈ [L] \\ {1}. Output Layer Analysis. By the induction method, we have established cHl = Hl for all l ∈ [L]. We will complete the proof by showing that f(X) = f(X) for all X ∈ X. The final output distribution of the target TFN f can be written as f(X) = softmax(WoZL) = softmax \u0000 Wo \u0000 W2LHL + b2L1⊤ N \u0001\u0001 . We can similarly formulate the final output distribution of the adapted model f : f(X) = softmax((Wo + ∆Wo) bZL) = softmax \u0010 (Wo + ∆Wo) \u0010 (W2L + ∆W2L)cHL + bb2L1⊤ N \u0011\u0011 , To align these two expressions, we select ∆W2L and ∆Wo based on Lemma 7, and let bb2L = (Wo + ∆Wo)−1Wob2L, where Wo + ∆Wo is invertible as shown in the proof of Lemma 7. Thus, the proof is complete. The following corollary identifies the specific LoRA-rank required to achieve exact representation for random model cases in the current setting. Corollary 9. Assume that the elements of all the weight matrices of both the target TFN and the frozen TFN are independently drawn from arbitrary continuous distributions. If R ≥ ⌈ D 2 ⌉, adding low-rank adapters of rank at most R to weight matrices in (∆WKl, ∆WQl, ∆WV l, ∆W1l)L l=1, ∆W2L, ∆Wo and tuning the bias vectors, enables the adapted model f to exactly approximate the target modelf, i.e., f(X) = f(X) for all X ∈ RD×N . Proof of Corollary 9. By combining Lemma 13 and Theorem 8, and following the same steps in the proof of Corollary 4 which yields maxi Gi = D, we can obtain the desired outcome. F.2 A PPROXIMATING TRANSFORMER NETWORK WITH MULTI -HEAD ATTENTION LAYERS In this section, we first provide the explicit formulation of TFN with multi-head attention layers. Consider an input matrix X ∈ RD×N , where D is the dimension of the token embeddings and N is the number of tokens. The output of the l-th transformer block is denoted as Zl, which can be computed as follows: Attnl(Zl−1) := HX h=1 Wh OlWh V lZl−1 · softmax \u0000 (Wh KlZl−1)⊤Wh QlZl−1 \u0001 , Zl := W2l · ReLU(W1l · Attnl(Zl−1) + b1l1⊤ N ) + b2l1⊤ N , 36Published as a conference paper at ICLR 2024 where we define Z0 = X. Here, H is the number of attention heads. The weight matrices for each head h ∈ [H] in the l-th transformer block are Wh Ol, Wh V l, Wh Kl, Wh Ql ∈ RD×D. The softmax operator softmax(·) is applied column-wise to the matrix. Further, W2l, W1l ∈ RD×D are the weight matrices and b1l, b2l ∈ RD are the bias vectors in the feedforward layers. A Transformer network, denoted asTFNL,D, is a composition ofL Transformer blocks, followed by an softmax output layer softmax(Wo ·), where Wo ∈ RD×D. The final output of the TFN is given by softmax(WoZL). To study the expressive power of LoRA within TFNs featuring multi-head attention layers, we next specify the parameters of the target model f, frozen model f0, and the adapted model f, each with L transformer blocks and a dimension D. To study the expressive power of LoRA within TFNs featuring multi-head attention lay- ers, we next specify the parameters of the target model f, frozen model f0, and the adapted model f, each with L transformer blocks and a dimension D. For ease of presentation, we drop the subscript in TFNL,D, referring to it simply as TFN. Given a specified rank R ∈ [D] for LoRA, these models are defined as follows: Target TFNf = TFN \u0010 ·; \u0010 ((W h Ol,W h V l,W h Kl,W h Ql)H h=1,W2l,W1l)L l=1,Wo \u0011 ,(b1l,b2l)L l=1 \u0011 , Frozen TFNf0 = TFN\u0000·; \u0000((Wh Ol,Wh V l,Wh Kl,Wh Ql)H h=1,W2l,W1l)L l=1,Wo \u0001,(b1l,b2l)L l=1 \u0001, Adapted TFNf = TFN \u0010 ·; \u0000((Wh Ol + ∆Wh Ol,Wh V l+ ∆Wh V l,Wh Kl + ∆Wh Kl,Wh Ql + ∆Wh Ql)H h=1, W2l + ∆W2l,W1l + ∆W1l)L l=1,Wo + ∆Wo \u0001,(bb1l,bb2l)L l=1 \u0011 , where the weight matrices ∈ RD×D, and the bias vectors ∈ RD. Moreover, the weight matrices of the low-rank adapters ∆Wh Ol, ∆Wh V l, ∆Wh Kl, ∆Wh Ql, ∆W2l, ∆W1l for all h ∈ [H] and l ∈ [L] are of rank R or lower. We next introduce non-singularity Assumption 4 for TFN with multi-head attention layers scenar- ios, which is then validated by Lemma 14. We then provide proof of our main results for TFNs — Theorem 7. Additionally, we introduce a supplementary theorem that amalgamates results for TFNs with both single-head and multi-head attention layers when the weight matrices are randomly initialized. This is articulated in Corollary 10. Assumption 4 (Non-Singularity). For a fixed R ∈ [D], all the weight matrices of both the target model and the frozen model and the following matrices for all r ∈ [R], Wh⊤ Kl Wh Ql + LRr \u0010 W h⊤ KlW h Ql −Wh⊤ Kl Wh Ql \u0011 , for allh ∈[H] andl = 1, Wh⊤ Kl Wh Ql + LRr \u0010 W−1⊤ 2,l−1W ⊤ 2,l−1W h⊤ KlW h QlW2,l−1W−1 2,l−1 −Wh⊤ Kl Wh Ql \u0011 , for allh ∈[H] andl ∈[L] \\ {1}, Wh OlWh V l+ LRr \u0010 W−1 1l W1lW h OlW h V l−Wh OlWh V l \u0011 , for allh ∈[H] andl = 1, Wh OlWh V l+ LRr \u0010 W−1 1l W1lW h OlW h V lW2,l−1W−1 2,l−1 −Wh OlWh V l \u0011 , for allh ∈[H] andl ∈[L] \\ {1}, WoW2L + LRr(WoW2L −WoW2L), are non-singular. Lemma 14. Let the elements of all weight matrices in the target model f and frozen model f0 be independently sampled from continuous distributions. Then, Assumption 4 holds with probability 1. Proof of Lemma 14. The results can be obtained by replicating the same steps outlined in the proof of Lemma 3. For the reader’s reference, we restate Theorem 7 here integrated with the explicit formulation of the rank-based functionality gap Gi. Theorem 7. Consider a given LoRA-rank R ∈ [D]. Let Assumption 4 hold. Define the rank-based functionality gap Gi to i-th transformer block (i ∈ [L]) or output layer (i = L + 1) as 37Published as a conference paper at ICLR 2024 Gi =   maxh \u0010 rank(W h⊤ KiW h Qi −Wh⊤ Ki Wh Qi) \u0011 ∨maxh \u0010 rank(W1iW h OiW h V i−W1iWh OiWh V i) \u0011 , i= 1, maxh \u0010 rank(W ⊤ 2,i−1W h⊤ KiW h QiW2,i−1 −W⊤2,i−1Wh⊤ Ki Wh QiW2,i−1) \u0011 ∨maxh \u0010 rank(W1iW h OiW h V iW2,i−1 −W1iWh OiWh V iW2,i−1) \u0011 , 2 ≤i ≤L, rank(WoW2L −WoW2L), i = L+ 1. (23) If R ≥ maxi∈[L+1]⌈Gi 2 ⌉, then there exists low-rank adapters with rank lower than R ∈ [D] ((∆Wh Kl, ∆Wh Ql, ∆Wh V l, ∆Wh Ol)H h=1)L l=1, ∆W2L, ∆Wo with other low-rank adapters set toO, and updated bias vectors (bb1l, bb2l)L l=1, such that for any X ∈ RD×N , the adapted model f exactly approximates target model f, i.e., f(X) = f(X). Proof of Theorem 7. The key idea of this proof is the same as the proof of Theorem 8: our first step is to ensure that, for each transformer block, the output from the first feedforward layer in the target model matches that in the adapted model. Once this is established, we select an appropriate output layer weight matrix to complete the proof. Similar to the proof of Theorem 8, we define Hl ∈ RD×N and Zl ∈ RD×N as the intermediate and final outputs of the l-th transformer block in the target model f, respectively. In particular, Hl corresponds to the output of the first feedforward layer in the l-th transformer block. They are formulated as Hl = ReLU   W1l  HX h=1 W h OlW h V l· Zl−1 · softmax \u0010 Z ⊤ l−1W h⊤ Kl W h QlZl−1 \u0011! + b1l1⊤ N ! , Zl = W2lHl + b2l1⊤ N . For the adapted model f, we introduce cHl and bZl accordingly to denote the intermediate output of the first feedforward layer and the final output of the l-th transformer block for the adapted model, respectively: cHl = ReLU   W1l \u0010 HX h=1 (Wh Ol + ∆Wh Ol)(Wh V l+ ∆Wh V l) · bZl−1 · softmax \u0010 bZ⊤ l−1(Wh Kl + ∆Wh Kl)⊤(Wh Ql + ∆Wh Ql) bZl−1 \u0011\u0011 + bb1l1⊤ N ! , bZl = W2l cHl + bb2l1⊤ N . Note that Z0 = bZ0 = X. We aim to demonstrate that adding low-rank adapters to the weight matrices allows the adapted TFN f to be functionally equivalent to the target TFN of identical dimen- sions. We will initiate our proof by inductively constructing the adapter weight matrices ((∆Wh Ol, ∆Wh V l, ∆Wh Kl, ∆Wh Ql)H h=1, bb1l, bb2l)L l=1 such that cHl = Hl for all l ∈ [L], and then select the ∆W2L and the low-rank adapter for the output layer ∆Wo to approximate the output of the target model. For unmentioned low-rank adapters, we set them as O. When l = 1. To achieve cHl with Hl for all X, we must satisfy the following conditions: Bias Vector:bb1l = b1l, (24) Query and Key: (Wh Kl + ∆Wh Kl)⊤(Wh Ql + ∆Wh Ql) = W h⊤ Kl W h Ql, Value and Output Projection: (Wh Ol + ∆Wh Ol)(Wh V l+ ∆Wh V l) = W−1 1l W1lW h OlW h V l. To achieve this, we set bb1l = b1l to achieve (24), and select rank- R or lower matrices ∆Wh Kl, ∆Wh Ql, ∆Wh Ol, ∆Wh V lfor all h ∈ [H] as suggested by Lemma 7. This ensures cHl = Hl for l = 1. 38Published as a conference paper at ICLR 2024 When l >1. Now we focus on the cases where l = 2, . . . , L. Assume the induction hypothesis holds for l − 1, which is cHl−1 = Hl−1. Following the same steps in the proof of Theorem 8, we let bb2,l−1 = W2,l−1W −1 2,l−1b2,l−1, thereby obtaining, bZl−1 = W2,l−1W −1 2,l−1Zl−1. (25) To achieve cHl = Hl, we express both cHl and Hl in terms of Zl−1: Hl = ReLU \u0010 W1l \u0010 HX h=1 W h OlW h V l· Zl−1 · softmax \u0010 Z ⊤ l−1W h⊤ Kl W h QlZl−1 \u0011\u0011 + b1l1⊤ N \u0011 cHl = ReLU \u0010 W1l \u0010 HX h=1 (Wh Ol + ∆Wh Ol)(Wh V l+ ∆Wh V l) · bZl−1 · softmax \u0010 bZ⊤ l−1(Wh Kl + ∆Wh Kl)⊤(Wh Ql + ∆Wh Ql) bZl−1 \u0011\u0011 + bb1l1⊤ N \u0011 , (25) = ReLU   W1l \u0010 HX h=1 (Wh Ol + ∆Wh Ol)(Wh V l+ ∆Wh V l) · W2,l−1W −1 2,l−1Zl−1 · softmax \u0010 Z ⊤ l−1W −1⊤ 2,l−1W⊤ 2,l−1(Wh Kl + ∆Wh Kl)⊤ (Wh Ql + ∆Wh Ql)W2,l−1W −1 2,l−1Zl−1 \u0011\u0011 + bb1l1⊤ N ! . Therefore, we need to align the following three components: Bias Vector: bb1l = b1l, (26) Query and Key: (Wh Kl + ∆Wh Kl)⊤(Wh Ql + ∆Wh Ql) = W−1⊤ 2,l−1W ⊤ 2,l−1W h⊤ Kl W h QlW2,l−1W−1 2,l−1, Value and Output Projection: (Wh Ol + ∆Wh Ol)(Wh V l+ ∆Wh V l) = W−1 1l W1lW h OlW h V lW2,l−1W−1 2,l−1. By setting bb1l based on (26) and adjusting ∆Wh Kl, ∆Wh Ql, ∆Wh Ol, ∆Wh V lfor all h ∈ [H] based on Lemma 7, we satisfy all three conditions above, thereby obtaining cHl = Hl for l ∈ [L] \\ {1}. Output Layer Analysis. By applying the induction method, we have established cHl = Hl for all l ∈ [L]. Lastly, we choose the ∆Wo, ∆W2L and the bias vector bb2L using the same approach as in the proof of Theorem 8. This concludes the proof. The following corollary identifies the specific LoRA-rank required to achieve exact representation for random model cases in the current setting. Corollary 10. Assume that the elements of all the weight matrices of both the tar- get TFN and the frozen TFN are independently drawn from arbitrary continuous distribu- tions. If R ≥ ⌈D 2 ⌉, adding low-rank adapters of rank at most R to weight matrices in ((∆Wh Kl, ∆Wh Ql, ∆Wh V l, ∆Wh Ol)H h=1)L l=1, ∆W2L, ∆Wo and tuning the bias vectors, enables the adapted model f to exactly approximate the target model f, i.e., f(X) = f(X) for all X ∈ RD×N . Proof of Corollary 10. By combining Lemma 14 and Theorem 7, and following the same steps in the proof of Corollary 4 which yields maxi Gi = D, we can obtain the desired outcome. G E XPERIMENTS In this section, we perform experiments on both synthetic and real datasets to corroborate our the- oretical results. Firstly, we focus on validating the construction of the LoRA adapter in our proof. 39Published as a conference paper at ICLR 2024 Subsequently, we extend our experimental validation to encompass the effects of tuning final lay- ers and the significance of updatable bias. Additionally, we offer visual representations of training curves, assess the generalization performance of LoRA, and evaluate its efficacy on classification tasks. We also conduct experiments on real datasets to further support our theoretical insights in real-world scenarios. G.1 A DDITIONAL DETAILS OF EXPERIMENT SETUP We implement LoRA adapter∆W by reparameterizing it as ∆W = AB⊤, where A, B ∈ RD×R, and we use the same initialization scheme as proposed by Hu et al. (2022a). For experiments pre- sented in Sec. 5, G.3.1, G.3.2, G.4, and G.5, we consider two variants of frozen models: • (Random) The first method involves randomly generating all the weight matrices using the Xavier uniform distribution, which is the default weight initialization method used in PyTorch. • (Pretrained) The second method aims to simulate scenarios where the pretrained model is rel- atively closer to the target model. We achieve this by initially creating the target model and the frozen model in the same way as the first method and then performing full-rank updates on the frozen model via gradient descent to approximate the target model until the approximation error is reduced by 1/3. For other experiments on synthetic datasets, we default to the randomly parameterized frozen model unless specified otherwise. G.2 A DDITIONAL DETAILS ON GRADIENT UPDATE METHOD In our experiments, we utilize the Adam optimizer. We tune the learning rate∈ \b 10−2, 10−3, 10−4\t and the weight decay ∈ \b 0, 10−2, 10−3, 10−4\t . The optimal configuration is determined based on the validation loss on a set of 256 samples independently drawn from a standard normal distribution. We run 5,000 iterations for each hyperparameter setting, where at each step 256 fresh standard Gaussian samples are generated for loss and gradient computation. G.3 V ALIDATION OF OUR LORA A DAPTER CONSTRUCTION Recall that all our theoretical statements are based on our construction of the LoRA adapters pre- sented in their corresponding proofs. To validate these results, here we empirically examine the relationship between approximation error and rank by integrating the LoRA adapters, which are constructed with the uniform partition in our proof, into the frozen model. Furthermore, we evaluate the effectiveness of our constructed LoRA adapters by comparing their performance against adapters updated through gradient descent and optimized by Adam. All simulations are conducted five times using different seeds, and the reported values represent the median computed across different runs. G.3.1 FNN A PPROXIMATION 4 8 12 16 0.00 0.05 0.10 L =1, L =2 4 8 12 16 L =2, L =4 Gradient Update Our Construction Rank MSE (a) Frozen model is randomly generated. 4 8 12 16 0.00 0.01 0.02 L =1, L =2 Gradient Update Our Construction 4 8 12 16 L =2, L =4 Rank MSE (b) Frozen model is pretrained. Figure 3: Approximation error (measured by MSE) versus LoRA-rank on FNNs. In this experiment, we assess the effectiveness of our low-rank adapter construction for FNN ap- proximation, which is detailed in the proof of Theorem 5. 40Published as a conference paper at ICLR 2024 4 8 12 16 10−8 10−4 L =1, L =2 4 8 12 16 L =2, L =4 Gradient Update Our Construction Rank log(MSE) Figure 4: Log-scale MSE versus LoRA- rank on randomly initialized FNNs. Setup. We consider two scenarios: one with L = 1 and L = 2 and the other one with L = 2 and L = 4 . It should be noted that for both these cases, we have M = ⌊L/L⌋ = 2 here. We employ the gradient update method and the construction outlined in the proof of Theorem 5 to update the LoRA adapters. Results. Fig. 3 presents the results for FNN approxima- tion. Consistent with the implications drawn in Sec. 5, the y limit changes from Fig. 3a to Fig. 3b suggest that the pretrained frozen model results in less ap- proximation error. Additionally, we observe that our construction’s performance aligns closely with the gradient update method when the target model depth L = 1 . However, this alignment is not observed when L = 2 on low-rank region (i.e., R ≤ 4), This further underscores the limitation of our LoRA adapter construction, which inherently assumes that the intermediate outputs of the frozen model and the target model need to align. To facilitate a more effective comparison between our construction and the gradient update method in the higher-rank region (i.e., R ≥ 6), we present the curves on a logarithmic scale, as depicted in Fig. 4. While the gradient update appears to reach the optimal performance achieved by our LoRA construction in FNNs, a gap is still discernible when viewed on a logarithmic scale. The MSE of the gradient update method is approximately 10−4, while for our LoRA construction, it’s around 10−8 for a sufficiently large rank. G.3.2 TFN A PPROXIMATION We assess the effectiveness of our LoRA adapter construction in approximating TFN, as detailed in the proof of Theorem 7. Setup. We examine target model f and frozen model f, both featuring the same architecture with L transformer blocks, a single output layer, two attention heads, and embedding size D = 16. We focus on two scenarios: L = 1 and L = 2 . The weight matrices for the attention layers follow a standard Gaussian distribution, while those for the linear layers are initialized using the Xavier uniform distribution, which is PyTorch’s default scheme for linear layer initialization. 4 8 12 16 10−4 101 L =1 4 8 12 16 L =2 Gradient Update Our Construction Rank log(MSE) (a) Frozen model is randomly generated. 4 8 12 16 10−4 100 L =1 4 8 12 16 L =2 Gradient Update Our Construction Rank log(MSE) (b) Frozen model is pretrained. Figure 5: Approximation error (measured by MSE) versus LoRA-rank on TFNs. Results. The observations here align with those from the experiments of FNN approximation. We note that the gradient update method outperforms our approach when the rank is relatively small but lags behind as the rank increases. This advantage of the gradient update method at minimal ranks arises from the inherent complexity of TFNs, which allows for more flexible low-rank adapter con- struction. Meanwhile, the gradient update method’s performance does not significantly improve as the rank increases. This arises from the inherent complexity involved in optimizing TFNs. Nonethe- less, our results corroborate the claims made in Theorem 7, as the approximation error must be eradicated when the rank reaches ⌈D 2 ⌉ = 8. 41Published as a conference paper at ICLR 2024 2000 40000.00 0.05 0.10 Random 2000 4000 Pretrained LoRA Tuning Final Layers # Tunable Parameters MSE (a) Comparison between LoRA and tuning final layers. 5 10 15 0.00 0.05 0.10 Random 5 10 15 Pretrained Fixed Biases Updatable Biases # Tunable Parameters MSE (b) Comparison between LoRA with fixed biases and LoRA with updatable biases. Figure 6: Approximation error (measured by MSE) versus the number of tunable parameters when various methods are employed. The analyses are conducted on FNN models. G.4 C OMPARISON TO TUNING FINAL LAYERS Tuning or adding the final layers only is also a common adaptation method used in various domains, including computer vision (Chatfield et al., 2014; Donahue et al., 2014; Sharif Razavian et al., 2014), and natural language processing (Devlin et al., 2019; Gira et al., 2022). Recall that Corollary 4 and Lemma 12 demonstrate that tuning final layers does not perform as well as LoRA for randomly generated models, provided the LoRA-rank satisfies the rank constraints shown in Corollary 4. In this experiment, we aim to validate this assertion and compare the performance of tuning final layers and LoRA in more general scenarios, such as when the frozen model has been pretrained, and when the LoRA-rank is smaller than required. Setup. We consider FNN models withD = 16, L = 1, L= 8. In this experiment, we employ two baselines: LoRA and tuning final layers. The LoRA adapters and the final layers are updated using the gradient update method. Results. Figure 6a compares the MSE of LoRA and final layer tuning when the same number of tunable parameters are used. In the case of randomly generated models, we observe that final layer tuning yields a significantly higher MSE when using the same number of tunable parameters, corroborating our results in Lemma 12. However, when the frozen model has been pretrained, the performance of final layer tuning improves considerably, though it still falls short of LoRA. This aligns with conclusions drawn from previous theoretical studies such as Tripuraneni et al. (2020), which asserts that the performance of final layer tuning heavily depends on the quality of the shared representations. G.5 B ENEFITS OF TUNING BIASES In our proof, as detailed in Sec. 3.2 and E.1, the updatable biases in the FNN play a crucial role in eliminating the nonlinearity of ReLUs. In this experiment, we investigate the importance of updatable biases in ensuring the success of LoRA in FNN cases. Setup. We consider FNN models with parameters D = 16 , L = 1 , L= 2 , and examine the performance of LoRA both with and without biases tuning for adapting it to match the target FNN. The LoRA adapters and biases are updated using the gradient update method. Results. The performance of LoRA with and without updatable biases is presented in Figure 6b. We observe that in both random and pretrained model cases, LoRA with updatable biases outper- forms LoRA with fixed biases when the number of tunable parameters is relatively small. However, the performance gap is not significant and diminishes as the number of tunable parameters increases. This suggests that while tuning biases in conjunction with the low-rank adapters does enhance per- formance, the gain is not substantial. In other words, even without bias tuning, LoRA’s performance remains competitive. G.6 T RAINING CURVES 42Published as a conference paper at ICLR 2024 0 2000 4000 10−5 10−2 L = 1, L= 2 0 2000 4000 10−3 10−2 L = 2, L= 4 rank = 1 rank = 4 rank = 8 rank = 13 rank = 16 Epoch log(Train Loss) Figure 7: Training curves of LoRA with varying LoRA-ranks when D = 16. Although our theoretical study does not incorporate any training process, we present the training curves of the LoRA gradient update method to illuminate the optimiza- tion aspects of LoRA. Setup We depict the training curves of LoRA fine- tuning on randomly generated FNNs for R = 1, 4, 8, 13, 16. Unless stated otherwise, all settings strictly adhere to the FNN experiments described in Sec. 5. Results The training curves visualized in Fig. 7 reveal that models with smaller ranks (e.g., R=1,4) converge swiftly due to their limited search space, but they settle at a relatively high training loss. Medium rank models (e.g., R=8) converge more slowly. Highly overparameterized models (g, R=13,18) appear to converge faster, aligning with recent advancements in optimization theory, which suggest that overparameterized models are easier to optimize (Liu et al., 2022a). G.7 G ENERALIZATION PERFORMANCES While our theoretical study only establishes the upper bound of LoRA’s performance with infinite data samples, it does not consider LoRA’s generalization performance in practice. Although this is beyond the current scope of our paper, we empirically investigate LoRA’s generalization perfor- mance in this experiment. Setup. We include a training set of 400 samples for the cases whereL = 1, L= 2, and 800 training samples for the cases where L = 2, L= 4. We evaluate how well LoRA’s training performance transfers to the test set. 0 4 8 12 16 10−5 10−2 L = 1, L= 2 0 4 8 12 16 10−3 L = 2, L= 4 Train Test Rank MSE Figure 8: Assessment of LoRA’s gener- alization performance on FNNs. Results. Fig. 8 presents the training and test MSE ver- sus LoRA-ranks. However, no clear pattern is observed in the variation of the gap between the training and test MSE with respect to the LoRA-ranks. This could be due to Adam not precisely finding the minimum (see Fig. 4), potentially avoiding overfitting. To assess LoRA’s generalization performance, we fine- tuned the frozen model on the training set and reported the training and test MSE. We notice an increasing gen- eralization gap (test MSE - train MSE) as the LoRA rank increases – this is very evident with L=2, and less so with L=4. This is intuitive as larger LoRA ranks imply a larger hypothesis class (e.g., the Rademacher complexity), so it is expected. We defer a detailed analysis of LoRA’s generalization performance to future work but believe our simulation results provide a valuable starting point for further discussion and investigation. G.8 E VALUATION ON CLASSIFICATION TASKS Our theory and previous experiments all focus on regression cases. In this experiment, we consider binary and multi-class classification tasks to optimize the LoRA adapter vias cross-entropy and report the performance of LoRA using accuracy. Multi-class Classification. As shown in Fig. 9a, consistent with our theoretical results, our con- struction achieves 100% accuracy when R ≥ 8. The performance of gradient update is also similar to our observation when MSE is employed as the metric, particularly when MSE is plotted on a log- arithmic scale (Fig. 4). This observation echoes the findings of Hui & Belkin (2021), which indicate that optimizing MSE is fundamentally equivalent to optimizing cross-entropy. Binary Classification. We have conducted binary classification tasks. We use the same setup as before but add one more output layer ∈ R2×D which is a block diagonal matrix, with the first 8 43Published as a conference paper at ICLR 2024 0 4 8 12 16 0.5 1.0 L = 1, L= 2 Gradient Update Our Construction 0 4 8 12 16 L = 2, L= 4 Rank Accuracy (a) Multi-class classification tasks with 16 classes. 0 4 8 12 16 0.6 0.8 1.0 L = 1, L= 2 Gradient Update Our Construction 0 4 8 12 16 L = 2, L= 4 Rank Accuracy (b) Binary classification task. Figure 9: Accuracy versus the rank on classification tasks. The analyses are conducted on FNN models. elements in the first rows and the last 8 elements in the second row are 1 and all remaining elements are 0. We fix this output layer, optimize the cross entropy on the LoRA adapters, and report the test accuracy. As shown in Fig. 9b, we observe that in this binary classification scenario, even with a very low LoRA-rank R = 1, the accuracy has been significantly improved, comparable to the results achieved by higher ranks. In the region of higher ranks, our construction significantly outperforms the gradi- ent update method. The suboptimal performance of the gradient update method in this simulation suggests that, despite LoRA’s current impressive performance in practical applications, there is po- tential for further refinement. G.9 E VALUATION ON REAL DATASETS In our theoretical analysis, we demonstrate how the sizes of frozen models and the distance between the frozen and target models influence the necessary LoRA-ranks to achieve the desired performance (see Lemma 1, 2, and Theorem 5, 6, 7). Specifically, our results suggest that larger models require fewer LoRA-ranks to reach the desired performance. Similarly, when the frozen model is closer to the target model, a lower LoRA-rank is sufficient to achieve the same performance. We validate these theoretical insights through experiments on the GLUE benchmark (Wang et al., 2018). Setup Our experiments are conducted using Tesla V100-PCIE-16GB, NVIDIA A100-SXM4- 80GB, NVIDIA A100-SXM4-40GB, and NVIDIA L40 GPUs. For each run, a single GPU is uti- lized. Unless otherwise specified, all our settings align with those established by Hu et al. (2022a). Impact of Model Size on LoRA Rank In practice, most existing studies on LoRA use the same LoRA-rank for models of varying sizes. For instance, in the original LoRA paper (Hu et al., 2022a), Tables 9 and 10 demonstrate the use of the same LoRA-rank for RoBERTa-base (Liu et al., 2019), RoBERTa-large (Liu et al., 2019), and DeBERTa-XXL (He et al., 2021). Similarly, in the QLoRA paper (Dettmers et al., 2023), a LoRA-rank of 64 is set for different models ranging from 13B to 65B parameters (see their Appendix B.2). To validate our theoretical findings, we evaluated the per- formance of LoRA on models of different sizes, specifically RoBERTa-base with 110M parameters and RoBERTa-large with 340M parameters. The results are presented in Table 2. Initially, we observe that, in the absence of fine-tuning (LoRA-rank R = 0), there is no consistent trend – RoBERTa-base performs better on 3 datasets, while RoBERTa-large performs better on 4 datasets. However, after LoRA fine-tuning, we observe that RoBERTa-large outperforms in most cases. In fact, even when the base model is trained with a LoRA-rank three times larger, RoBERTa- large still performs better on 6 out 8 datasets. Given that the pretrained RoBERTa-large model was performing no differently from the base model, this observation supports our theoretical findings that deeper models are more expressive with LoRA training. Impact of Model Proximity on LoRA Rank While our theoretical results (Lemma 1, 2, and Theorem 5, 6, 7) imply that the frozen model that is closer to the target model achieves better results for a fixed LoRA-rank. To validate this, we compare the performance of pretrained RoBERTa-base with the randomly initialized RoBERTa-base fine-tuned using the same LoRA-ranks. 44Published as a conference paper at ICLR 2024 Model R MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B RoBERTabase 0 .330 .491 .316 0 .495 .682 .527 .024 RoBERTalarge 0 .318 .505 .684 0 .505 .369 .473 .032 RoBERTabase 2 .861 .950 .892 .632 .928 .891 .780 .907 RoBERTabase 6 .870 .948 .892 .629 .931 .900 .773 .909 RoBERTalarge 2 .904 .956 .917 .631 .946 .887 .884 .916 Table 2: Comparison of the fine-tuned performance of RoBERTa-base and RoBERTa-large using LoRA with different LoRA-ranks on the GLUE benchmark. Following Hu et al. (2022a), we report the overall (matched and mismatched) accuracy for MNLI, Matthew’s correlation for CoLA, Pearson correlation for STS-B, and accuracy for other tasks. Higher is better for all metrics. Despite the absence of a clear pattern indicating which pretrained model is generally superior, after fine-tuning using LoRA, we observe that RoBERTa-large (340M) fine-tuned with LoRA-rankR = 2 outperforms RoBERTa-base (110M) with LoRA-rank R = 6 in 7 out of 8 tasks. This observation aligns with our theoretical conclusion that larger models require lower LoRA-ranks to achieve the desired performance. Model R MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B Random 2 .523 .775 .691 .154 .627 .761 .542 .213 Pretrained .861 .950 .892 .632 .928 .891 .780 .907 Random 4 .535 .788 .696 .145 .625 .768 .542 .224 Pretrained .868 .950 .890 .634 .929 .898 .805 .910 Random 6 .544 .799 .696 .154 .632 .768 .542 .210 Pretrained .868 .948 .892 .629 .931 .900 .773 .909 Table 3: Comparison of the fine-tuned performance of randomly initialized and pretrained RoBERTa-base. Following Hu et al. (2022a), we report the overall (matched and mismatched) accuracy for MNLI, Matthew’s correlation for CoLA, Pearson correlation for STS-B, and accuracy for other tasks. Higher is better for all metrics. We observe that the performance of the pretrained RoBERTa-base significantly surpasses that of the randomly initialized RoBERTa-base given the same LoRA-rank. This observation is consistent with our theoretical findings, which suggest that a frozen model closer to the target model yields better performance given the same LoRA-rank. The results in Table 3 demonstrate that the pretrained RoBERTa-base significantly surpasses the randomly initialized RoBERTa-base. This observation is consistent with our theoretical findings, suggesting that the pretrained model requires lower LoRA-ranks to achieve the desired performance. H E XTENSION TO CASES WITH DIFFERENT MODEL DIMENSIONS This discussion only applies to linear model approximation and FNN approximation. As highlighted in Sec. 2, our results can be easily extended to scenarios where the target model, f, and the frozen model, f, have different model dimensions. Specifically, for linear model or FNN approximation, we use D to represent the number of hidden neurons per layer in the target model and D for the frozen model. We particularly consider the cases where the frozen model is wider than the target model, i.e., D ≥ D. This is because the frozen model is typically overparameterized in practical applications. The key idea for extending our analysis to scenarios with different model dimensions is expanding the dimension of the target model. For the sake of simplicity, we focus on the simplest case, the linear model approximation, as an example. In this setting, the difference between the output of the 45Published as a conference paper at ICLR 2024 adapted model and the target model can be measured by f \u0012\u0014 x 0 \u0015\u0013 − \u0014 f(x) 0 \u0015 = LY l=1 (Wl + ∆Wl) \u0014 x 0 \u0015 − \u0014 Wx 0 \u0015 , (27) where x ∈ RD. Consequently, the last (D − D) columns and rows of QL l=1 (Wl + ∆Wl) does not affect the results at all. Denote the submatrix consisting of the first d rows and d columns of a matrix W by [W]d. Then, to approximate the target model, we aim to solve the following constrained optimization problem for a given LoRA-rank R ∈ [D]: min rank(∆Wl)≤R \r\r\r\r\r \" LY l=1 (Wl + ∆Wl) # D − W \r\r\r\r\r F . To solve this problem, we first define an expanded target matrix, denoted by fW ∈ RD×D. The expanded target matrix fW is constructed such that h fW i D = W, while the remaining entries matches the corresponding entries in Ql = 1LWl. Then, the error matrix E = fW − QL l=1 Wl, consists entirely of zeros except for the first D rows and D columns. Therefore, we obtain RE = rank(E) ≤ D. Given the expanded target matrix, we consider the updated constrained optimization problem as follows: min rank(∆Wl)≤R \r\r\r\r\r LY l=1 (Wl + ∆Wl) − fW \r\r\r\r\r F . (28) By Lemma 1, we obtain that when the LoRA-rank R ≥ ⌊D L ⌋, the optimal solution to (28) satisfies QL l=1 (Wl + ∆Wl) = fW, given thatD ≥ RE. This result implies that hQL l=1 (Wl + ∆Wl) i D = W and therefore the approximation error defined in (27) is 0 for all input x. A similar analysis can be conducted for FNN approximation. I E XTENDED FUTURE WORKS To the best of our knowledge, this paper is the first to offer a theoretical understanding of LoRA fine-tuning on both FNN and TFN. Our work delivers insightful results, elucidating the impact of rank, depth of the pre-trained model, and the distance between the pre-trained model and the target model on the expressive power of LoRA. Those theoretical results are further corroborated via our experiments. Despite these advancements, several intriguing questions still remain open. First, as observed in the numerical experiments, our construction of LoRA adapters for FNN and TFN may not be always optimal. Given that more complex models offer increased flexibility, an open ques- tion is whether we can devise a more parameter-efficient scheme to construct the LoRA adapters, thereby deriving a tighter bound on approximation error. Second, for TFN, we have only identi- fied the conditions under which the LoRA-adapted model exactly matches the target model, due to the analytical complexity of TFN. It would be interesting to quantify the approximation error when the rank is lower than required. Furthermore, for TFN, we constrain the target model and the frozen model to have identical embedding size and depth, and we omit the skip connections and layer norms for simplicity. Another intriguing direction would be to study the expressive power of LoRA under TFN cases with more general settings on TFN architectures. While our analysis does not involve any training process, an interesting direction for future research would be to consider gradient-based optimization algorithms and examine how efficiently LoRA can be optimized. Finally, theoretical questions about LoRA’s generalization to unseen data also remain unresolved. 46",
      "meta_data": {
        "arxiv_id": "2310.17513v3",
        "authors": [
          "Yuchen Zeng",
          "Kangwook Lee"
        ],
        "published_date": "2023-10-26T16:08:33Z",
        "pdf_url": "https://arxiv.org/pdf/2310.17513v3.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper theoretically analyzes the expressive power of Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method. For fully connected neural networks (FNNs), it proves that LoRA can adapt any model to accurately represent a smaller target model if the LoRA-rank is sufficiently high (rank >= (width of f) * depth of f / depth of f). It quantifies the approximation error when the LoRA-rank is below this threshold. For Transformer networks (TFNs), it shows that any model can be adapted to a target model of the same size with rank-(embedding size / 2) LoRA adapters applied to attention weight matrices. The study provides theoretical insights on hyperparameter tuning and algorithm development for LoRA, which are empirically validated. It also demonstrates LoRA's superiority over tuning final layers for randomly generated networks and highlights the importance of updating bias parameters.",
        "methodology": "The research employs a theoretical analysis of LoRA's expressive power, focusing on its ability to enable a frozen model to match a target model's functionality. It starts with a warm-up on linear models to establish core matrix approximation results, extending the Eckart-Young-Mirsky theorem to a product of matrices under low-rank constraints. For FNNs, the methodology involves linearizing the first L-1 layers by activating all ReLUs with large bias vectors and applying the linear model approximation results. For multi-layer FNNs, it uses model partition (uniform and general schemes) where each sub-model approximates a layer of the target FNN. For TFNs, the analysis manages non-linearities (softmax and ReLU) by segmenting transformer blocks and aligning outputs before these functions, focusing on adapting self-attention layers. All results rely on mild non-singularity assumptions of weight matrices, which are proven to hold for randomly generated matrices with high probability.",
        "experimental_setup": "Experiments are conducted on both synthetic and real datasets. For synthetic validation, linear models, FNNs, and TFNs with a dimensionality D=16 are used, exploring different depths (e.g., L=1/L=2, L=2/L=4 for FNNs; L=1, L=2 for TFNs). Two variants of frozen models are tested: 'Random' (weights initialized with Xavier uniform or standard Gaussian distribution) and 'Pretrained' (frozen model partially updated via gradient descent to be closer to the target). The primary metric for approximation error is Mean Squared Error (MSE), with comparisons against a gradient update method (Adam optimizer with tuned learning rate/weight decay). Additional experiments include comparing LoRA with final layer tuning, investigating the benefits of tuning biases, visualizing training curves, assessing generalization performance (on varying training sample sizes), and evaluating binary and multi-class classification tasks (using cross-entropy and accuracy). For real-world validation, LoRA's performance is evaluated on the GLUE benchmark using RoBERTa-base (110M parameters) and RoBERTa-large (340M parameters), reporting specific task metrics.",
        "limitations": "The current theoretical study focuses solely on the expressive power of LoRA, explicitly excluding optimization and generalization properties (though generalization is empirically explored in the appendix). The construction of LoRA adapters presented in the proofs, while theoretically sound for exact approximation, is observed to be suboptimal compared to gradient-based methods in some empirical settings, particularly for FNNs at low ranks and TFNs. Due to analytical complexity, approximation errors for TFNs are not quantified when LoRA-ranks are lower than required for exact matching. For TFNs, the analysis simplifies the architecture by assuming identical embedding size and depth for target and frozen models, and omitting skip connections and layer norms. The theoretical framework does not incorporate any training process or dynamics. Empirically, the generalization gap (test MSE - train MSE) did not show a clear pattern with increasing LoRA ranks, suggesting further investigation is needed. The suboptimality of current optimization algorithms for LoRA is also noted.",
        "future_research_directions": "Future research directions include devising more parameter-efficient schemes for constructing LoRA adapters to achieve tighter approximation error bounds. For TFNs, quantifying approximation errors when LoRA-ranks are below the required threshold is an open challenge. Exploring the expressive power of LoRA under more general TFN architectural settings, such as varying embedding sizes, depths, and including skip connections and layer norms, is also suggested. Investigating gradient-based optimization algorithms to understand how efficiently LoRA can be optimized is another avenue. Finally, theoretical questions regarding LoRA's generalization capabilities to unseen data remain unresolved and warrant further study."
      }
    },
    {
      "title": "Spectral Entry-wise Matrix Estimation for Low-Rank Reinforcement Learning",
      "abstract": "We study matrix estimation problems arising in reinforcement learning (RL)\nwith low-rank structure. In low-rank bandits, the matrix to be recovered\nspecifies the expected arm rewards, and for low-rank Markov Decision Processes\n(MDPs), it may for example characterize the transition kernel of the MDP. In\nboth cases, each entry of the matrix carries important information, and we seek\nestimation methods with low entry-wise error. Importantly, these methods\nfurther need to accommodate for inherent correlations in the available data\n(e.g. for MDPs, the data consists of system trajectories). We investigate the\nperformance of simple spectral-based matrix estimation approaches: we show that\nthey efficiently recover the singular subspaces of the matrix and exhibit\nnearly-minimal entry-wise error. These new results on low-rank matrix\nestimation make it possible to devise reinforcement learning algorithms that\nfully exploit the underlying low-rank structure. We provide two examples of\nsuch algorithms: a regret minimization algorithm for low-rank bandit problems,\nand a best policy identification algorithm for reward-free RL in low-rank MDPs.\nBoth algorithms yield state-of-the-art performance guarantees.",
      "full_text": "arXiv:2310.06793v2  [cs.LG]  28 Oct 2023 Spectral Entry-wise Matrix Estimation for Low-Rank Reinforcement Learning Stefan Stojanovic EECS KTH, Stockholm, Sweden stesto@kth.se Y assir Jedra EECS KTH, Stockholm, Sweden jedra@kth.se Alexandre Proutiere EECS KTH, Stockholm, Sweden alepro@kth.se Abstract W e study matrix estimation problems arising in reinforceme nt learning (RL) with low-rank structure. In low-rank bandits, the matrix to be re covered speciﬁes the expected arm rewards, and for low-rank Markov Decision Proc esses (MDPs), it may for example characterize the transition kernel of the MD P . In both cases, each entry of the matrix carries important information, and we seek estimation methods with low entry-wise error. Importantly, these meth ods further need to accommodate for inherent correlations in the available dat a (e.g. for MDPs, the data consists of system trajectories). W e investigate the p erformance of simple spectral-based matrix estimation approaches: we show that they efﬁciently recover the singular subspaces of the matrix and exhibit nearly-min imal entry-wise error. These new results on low-rank matrix estimation make it poss ible to devise reinforcement learning algorithms that fully exploit the u nderlying low-rank structure. W e provide two examples of such algorithms: a reg ret minimization algorithm for low-rank bandit problems, and a best policy id entiﬁcation algorithm for reward-free RL in low-rank MDPs. Both algorithms yield s tate-of-the-art performance guarantees. 1 Introduction Learning succinct representations of the reward function or of the system state dynamics in bandit and RL problems is empirically known to signiﬁcantly accelerate the search for efﬁcient policies [ 1, 2, 3]. It also comes with interesting theoretical challenges. T he design of algorithms learning and leveraging such representations and with prov able performance guarantees has attracted considerable attention recently, but remains largely open . In particular, signiﬁcant efforts have been made towards such design when the representation relie s on a low-rank structure. In bandits, assuming such a structure means that the arm-to-reward func tion can be characterized by a low- rank matrix [ 4, 5, 6, 7]. In MDPs, it implies that the reward function, the Q-function or the transition kernels are represented by low-rank matrices [ 8, 9, 10, 11, 12]. In turn, the performance of algorithms exploiting low-rank structures is mainly det ermined by the accuracy with which we are able to estimate these matrices. In this paper, we study matrix estimation problems arising in low-rank bandit and RL problems. T wo major challenges are associated with these problems. (i) Th e individual entries of the matrix carry important operational meanings (e.g. in bandits, an entry c ould correspond to the average reward Preprint. Under review .of an arm), and we seek estimation methods with low entry-wis e error. Such requirement calls for a ﬁne-grained analysis, typically much more involved th an that needed to only upper bound the spectral or Frobenius norm of the estimation error [ 13, 14, 15, 16, 17, 18, 19, 20]. (ii) Our estimation methods should further accommodate for inherent correlati ons in the available data (e.g., in MDPs, we have access to system trajectories, and the data is hence M arkovian). W e show that, essentially, spectral methods successfully deal with these challenges. Contributions.1) W e introduce three matrix estimation problems. The ﬁrst a rises in low-rank bandits. The second corresponds to scenarios in RL where the learner wishes to estimate the (low-rank) transition kernel of a Markov chain and to this ai m, has access to a generative model. The last problem is similar but assumes that the learner has a ccess to system trajectories only, a setting referred to as the forward model in the RL literature . For all problems, we establish strong performance guarantees for simple spectral-based estimat ion approaches: these efﬁciently recover the singular subspaces of the matrix and exhibit nearly-min imal entry-wise error. T o prove these results, we develop and combine involved leave-one-out arg uments and Poisson approximation techniques (to handle the correlations in the data). 2) W e apply the results obtained for our ﬁrst matrix estimation problem to devise an efﬁcient regret-minimization algorithm for low-rank bandits. W e pr ove that the algorithm enjoys ﬁnite-time performance guarantees, with a regret at most roughly scali ng as (m+ n) log3(T) ¯∆ /∆ 2 minwhere (m,n) are the reward matrix dimensions, T is the time horizon, ¯∆ is the average of the reward gaps between the best arm and all other arms, and ∆ min is the minimum of these gaps. 3) Finally, we present an algorithm for best policy identiﬁc ation in low-rank MDPs in the reward- free setting. The results obtained for the second and last ma trix estimation problems imply that our algorithm learns an ǫ-optimal policy for any reward function using only a number o f samples scaling as O(nA/ǫ2) up to logarithmic factors, where n and A denote the number of states and actions, respectively. This sample complexity is mini-max optimal [ 21], and illustrates the gain achieved by leveraging the low-rank structure (without thi s structure, the sample complexity would be Ω( n2A/ǫ2)). Notation. For any matrix A∈ Rm×n, Ai,: (resp. A:,j) denotes its i-th row (resp. its j-th column), Amin = min (i,j) Ai,j and Amax = max (i,j) Ai,j. W e consider the following norms for matrices: ∥A∥ denotes the spectral norm, ∥A∥1→∞ = max i∈[m] ∥Ai,:∥1, ∥A∥2→∞ = max i∈[m] ∥Ai,:∥2, and ﬁnally ∥A∥∞ = max (i,j)∈[m]×[n] |Ai,j|. If the SVD of Ais UΣ V⊤, we denote by sgn(A) = UV ⊤ the matrix sign function of A (see Deﬁnition 4.1 in [ 22]). Or×r denotes the set of (r × r) real orthogonal matrices. For any ﬁnite set S, let P(S) be the set of distributions over S. The notation a(n,m,T ) ≲ b(n,m,T ) (resp. a(n,m,T ) = Θ( b(n,m,T ))) means that there exists a universal constant C > 0 (resp. c,C > 0) such that a(n,m,T ) ≤ Cb(n,m,T ) (resp. cb(n,m,T ) ≤ a(n,m,T ) ≤ Cb(n,m,T )) for all n,m,T . Finally, we use a ∧ b = min( a,b) and a ∨ b = max(a,b). 2 Models and Objectives LetM ∈ Rm×n be an unknown rank rmatrix that we wish to estimate from T noisy observations of its entries. W e consider matrices arising in two types of lea rning problems with low-rank structure, namely low-rank bandits and RL. The SVD of M is UΣ V⊤ where the matrices U ∈ Rm×r and V ∈ Rn×r contain the left and right singular vectors of M, respectively, and Σ = diag(σ1,...,σ r). W e assume without loss of generality that the singular value s have been ordered, i.e., σ1 ≥ ... ≥ σr. The accuracy of our estimate ˆM of M will be assessed using the following criteria: (i) Singular subspace recovery. Let the SVD of ˆM be ˆUˆΣ ˆV⊤. T o understand how well the singular subspaces of M are recovered, we will upper bound minO∈Or×r ∥U − ˆUO∥2→∞ and minO∈Or×r ∥V − ˆVO∥2→∞ (the minO∈Or×r problem corresponds to the orthogonal Procrustes problem and its solution aligns ˆU and U as closely as possible, see Remark 4.1 in [ 22]). 2(ii) Matrix estimation. T o assess the accuracy of ˆM, we will upper bound the row-wise error ∥ ˆM−M∥1→∞ or ∥ ˆM−M∥2→∞, as well as the entry-wise error ∥ ˆM−M∥∞ (the spectral error ∥ ˆM− M∥ is easier to deal with and is presented in appendix only). W e introduce two classical quantities characterizing the h eterogeneity and incoherence of the matrix M [23, 24]. Let κ = σ1/σr, and let µ(U) = √ m/r∥U∥2→∞ (resp. µ(V) =√ n/r∥V∥2→∞) denote the row-incoherence (resp. column-incoherence) p arameter of M. Let µ = max {µ(U),µ(V)}. Next, we specify the matrices M of interest in low-rank bandits and RL, and the way the data used for their estimation is generated. Model I: Reward matrices in low-rank bandits.For bandit problems, M corresponds to the average rewards of various arms. T o estimate M, the learner has access to data sequentially generated as follows. In each round t= 1 ,...,T , an arm (it,jt) ∈ [m] × [n] is randomly selected (say uniformly at random for simplicity) and the learner obs erves Mit,jt + ξt, an unbiased sample of the corresponding entry of M. (ξt)t≥1 is a sequence of zero-mean and bounded random variables. Speciﬁcally, we assume that for all t≥ 1, |ξt| ≤ c1∥M∥∞ a.s., for some constant c1 >0. Model II: T ransition matrices in low-rank MDPs. In low-rank MDPs, we encounter Markov chains whose transition matrices have low rank r (refer to Section 5 for details). Let P ∈ Rn×n be such a transition matrix. W e assume that the correspondin g Markov chain is irreducible with stationary distribution ν. The objective is to estimate P from the data consisting of samples of transitions of the chain. More precisely, from the data, we w ill estimate the long-term frequency matrix M = diag( ν)P (Mij is the limiting proportion of transitions from state ito state j as the trajectory grows large). Observe that M is of rank r, and that Pi,: = Mi,:/∥Mi,:∥1. T o estimate M, the learner has access to the data (x1,...,x T) ∈ [n]T generated according to one of the following two models. (a) In the generative model, for any t ∈ [T], if tis odd, xt is selected at random according to some distribution ν0, and xt+1 is sampled from Pxt,:. (b) In the forward model, the learner has access to a trajectory (x1,...,x T) of length T of the Markov chain, where x1 ∼ ν0 and for any t≥ 1, xt+1 ∼ Pxt,:. 3 Matrix Estimation via Spectral Decomposition In the three models (Models I, II(a) and II(b)), we ﬁrst construct a matrix ˜M directly from the data, and from there, we build our estimate ˆM, typically obtained via spectral decomposition, i.e., by taking the best rank- rapproximation of ˜M. In the remaining of this section, we let ˆUˆΣ ˆV⊤ denote the SVD of ˆM. Next, we describe in more details how ˆM is constructed in the three models, and analyze the corresponding estimation error. 3.1 Reward matrices For Model I, fort= 1 ,...,T , we deﬁne ˜Mt = ( (Mit,jt + ξt)1{(i,j)=(it ,jt)} ) i,j∈[m]×[n] and ˜M = nm T ∑ T t=1 ˜Mt. Let ˆM denote the best rank- rapproximation of ˜M. Theorem 1. Let δ >0. W e introduce: B = √ nm T (√ (n+ m) log (e(n+ m)T δ ) + log3/2 (e(n+ m)T δ )) . Assume that T ≥ cµ4κ2r2(n+ m) log3 (e(m+ n)T/δ) for some universal constant c >0. Then there exists a universal constant C >0 such that the following inequalities hold with probability at 3least 1 − δ: (i) max ( ∥U − ˆU( ˆU⊤U)∥2→∞,∥V − ˆV(ˆV⊤V)∥2→∞ ) ≤ C (µ3κ2r3/2)√ mn(n∧ m) B, (ii) ∥ ˆM − M∥2→∞ ≤ C(µ3 κ2r3/2)√m∧ n ∥M∥∞B, (iii) ∥ ˆM − M∥∞ ≤ C ( µ11/2 κ2r1/2 + µ3κr3/2 m+ n√mn ) 1 (n∧ m)∥M∥∞B. Corollary 2. (Homogeneous reward matrix) When m = Θ( n), κ = Θ(1) , µ = Θ(1) , ∥M∥∞ = Θ(1) , r= Θ(1) , we say that the reward matrix Mis homogeneous. In this case, for any δ >0, when T ≥ c(n+ m) log3 ( e(m+ n)T/δ ) for some universal constant c> 0, we have with probability at least 1 − δ: max ( ∥U − ˆU( ˆU⊤U)∥2→∞,∥V − ˆV(ˆV⊤V)∥2→∞ ) ≲ 1√ T log3/2 ((n+ m)T δ ) , ∥ ˆM − M∥2→∞ ≲ (n+ m)√ T log3/2 ((n+ m)T δ ) , ∥ ˆM − M∥∞ ≲ √ (n+ m) T log3/2 ((n+ m)T δ ) . For a homogeneous reward matrix, ∥U∥2→∞ = Θ(1 /√m) and ∥M∥∞ = Θ(1) , and hence, from the above corollary, we obtain estimates whose relative err ors (e.g., ∥ ˆM − M∥∞/∥M∥∞) scale at most as √ m/T up to the logarithmic factor. W e may also compare the results of the above corollary to thos e of Theorem 4.4 presented in [ 22]. There, the data consists for each pair (i,j) of a noisy observation Mi,j + Ei,j. The Ei,j’s are independent across (i,j). This model is simpler than ours and does not include any corr elation in the data. But it roughly corresponds to the case where T = nmin our Model I. Despite having to deal with correlations, we obtain similar results as those of The orem 4.4: for example, ∥ ˆM − M∥∞ ≲√ 1/(n+ m) (up to logarithmic terms) with high probability. 3.2 T ransition matrices under the generative model For Model II(a), the matrix˜M records the empirical frequencies of the transitions: for a ny pair of states (i,j), ˜Mi,j = 1 ⌊T/2⌋ ∑ ⌊T/2⌋ k=1 1{(x2k−1,x2k)=(i,j)}. ˆM is the best rank- rapproximation of ˜M and the estimate ˆP of the transition matrix P is obtained normalizing the rows of ˆM: for all i∈ [n], ˆPi,: = { (ˆMi,:)+/∥(ˆMi,:)+∥1, if ∥(ˆMi,:)+∥1 >0, 1 n1n, if ∥(ˆMi,:)+∥1 = 0 . (1) where (·)+ is the function applying max(0,·) component-wise and 1n is the n-dimensional vector of ones. The next theorem is a simpliﬁed version and a consequ ence of a more general and tighter theorem presented in App. B.2. T o simplify the presentation of our results, we deﬁne g(M,T,δ ) = nlog( n √ T δ ) max { µ6κ6r3, log( n √ T δ )1{∃ℓ:T ∥ Mℓ,:∥ ∞≤1} log(1+ 1 T ∥ M∥ ∞ ) } . Theorem 3. Let δ >0. Introduce B = µκ √ (r∥M∥∞/T) log(n √ T/δ). Assume that we have (ν0)min = min i∈[n](ν0)i > 0. If (a) n ≥ clog2(nT3/2/δ) and (b) T ≥ cg(M,T,δ ) for some universal constant c >0, then there exists a universal constant C > 0 such that the following 4inequalities hold with probability at least 1 − δ: (i) max { ∥U − ˆU( ˆU⊤U)∥2→∞,∥V − ˆV(ˆV⊤V)∥2→∞ } ≤ C κµ2r n∥M∥∞ B, (ii) ∥ ˆM− M∥2→∞ ≤ CκB, ∥ ˆP − P∥1→∞ ≤ C κ√n (ν0)min B, (iii) ∥ ˆM− M∥∞ ≤ Cκµ2r√n B, (iv) ∥ ˆP − P∥∞ ≤ C B (ν0)min [ √nκ∥M∥∞ (ν0)min + ( 1 + κB√n∥M∥∞ )κµ2r√n ] , where (iv) holds if in addition T ≥ cn∥M∥∞(ν0)−2 minrµ2κ4 log(n √ T/δ) Note that in theorem, the condition (a) on nhas been introduced just to simplify the expression of B (refer to App. B.2 for a full statement of the theorem without this condition). Corollary 4. (Homogeneous transition matrix) When κ = Θ(1) , µ = Θ(1) , r = Θ(1) , Mmax = Θ( Mmin), we say that the frequency matrix M is homogeneous. If T ≥ cnlog(nT) for some universal constant c> 0, then we have with probability at least 1 − min{n−2,T−1}: max { ∥U− ˆU( ˆU⊤U)∥2→∞,∥V − ˆV(ˆV⊤V)∥2→∞ } ≲ √ log(nT) T , ∥ ˆM − M∥2→∞ ≲ 1 n √ log(nT) T , ∥ ˆM − M∥∞ ≲ 1 n √ log(nT) nT , ∥ ˆP − P∥1→∞ ≲ √ nlog(nT) T , ∥ ˆP − P∥∞ ≲ √ log(nT) nT . For a homogeneous frequency matrix, ∥U∥2→∞ = Θ(1 /√n), ∥M∥2→∞ = Θ(1 /n√n), ∥M∥∞ = Θ(1 /n2), ∥P∥1→∞ = 1 , ∥P∥∞ = Θ(1 /n). Thus for all these metrics, our estimates achieve a relative error scaling at most as √ n/T up to the logarithmic factor. 3.3 T ransition matrices under the forward model For Model II(b), we ﬁrst split the data intoτ subsets of transitions: for k = 1 ,...,τ , the k-th subset is ((xk,xk+1),(xk+τ,xk+1+τ),..., (xk+(Tτ −1)τ,xk+1+(Tτ −1)τ)) where Tτ = ⌊T/τ⌋. By separating two transitions in the same subset, we break the i nherent correlations in the data if τ is large enough. Now we let ˜M(k) be the matrix recording the empirical frequencies of the tra nsitions in the k-th subset: ˜M(k) i,j = 1 Tτ ∑ Tτ −1 l=0 1{(xk+lτ ,xk+1+lτ )=(i,j)} for any pair of states (i,j). Let ˆM(k) be the best r-rank approximation of ˜M(k). As in ( 1), we deﬁne the corresponding ˆP(k). Finally we may aggregate these estimates ˆM = 1 τ ∑ τ k=1 ˆM(k) and ˆP = 1 τ ∑ τ k=1 ˆP(k). W e present below the performance analysis for the estimates coming from a single subset; the analysis of the aggregate estimates easily follows. For anyε> 0, we deﬁne the ε-mixing time of the Markov chain with transition matrix P as τ(ε) = min{t ≥ 1 : max 1≤i≤n 1 2 ∥Pt i,: − ν⊤∥1 ≤ ε}, and its mixing time as τ⋆ = τ(1/4). The next theorem is a simpliﬁed version and a consequence of a more gen eral and tighter theorem presented in App. B.3. T o simplify the presentation, we deﬁne: h(M,T,δ ) = nτ⋆log( n √ T δ ) log(Tν−1 min) max { µ6κ6r3, log2( n√Tτ δ )1{∃ℓ:Tτ ∥ Mℓ,:∥ ∞≤1} log2(1+ 1 Tτ ∥ M∥ ∞ ) } . Theorem 5. Let δ >0. Assume that νmin = min i∈[n] νi >0 and that τ/(τ⋆log(Tν−1 min)) ∈ [c1,c2] for some universal constants c2 >c1 ≥ 2. Introduce: B = µκ √ rτ⋆∥M∥∞ T log (n√Tτ δ ) log ( T νmin ) . 5If (a) n ≥ cτ⋆log3/2(nT3/2/δ) log1/2(Tν−1 min) and (b) T ≥ ch(M,T,δ ) for some universal constant c >0, then there exists a universal constant C >0 such that the following inequalities hold with probability at least 1 − δ: (i) max { ∥U − ˆU( ˆU⊤U)∥2→∞,∥V − ˆV(ˆV⊤V)∥2→∞ } ≤ C κµ2r n∥M∥∞ B, (ii) ∥ ˆM − M∥2→∞ ≤ CκB, ∥ ˆP − P∥1→∞ ≤ Cκ√n νmin B, (iii) ∥ ˆM − M∥∞ ≤ Cκµ2r√n B, (iv) ∥ ˆP − P∥∞ ≤ C B νmin [ √nκ∥M∥∞ νmin + ( 1 + κB√n∥M∥∞ )κµ2r√n ] , where (iv) holds if in addition T ≥ cn∥M∥∞ν−2 minτ⋆rµ2κ4 log(n √ T/δ) log(Tν−1 min). Note that our guarantees hold when τroughly scales as τ⋆log(Tν−1 min). Hence to select τ, one would need an idea of the latter quantity. It can be estimated typic ally using τ⋆ν−1 min samples [ 25] (which is small when compared to the constraint T ≥ ch(M,T,δ ) as soon as νmin = Ω(1 /n)). Further observe that in the theorem, the condition (a) can be removed (refer to App. B.3 for a full statement of the theorem without this condition). Corollary 6.(Homogeneous transition matrices) Assume that M is homogeneous (as deﬁned in Corollary 4). Let τ = log( Tn). If T ≥ cnlog2(nT) for some universal constant c >0, then we have with probability at least 1 − min{n−2,T−1}: max { ∥U − ˆU( ˆU⊤U)∥2→∞,∥V − ˆV(ˆV⊤V)∥2→∞ } ≲ 1√ T log(nT), ∥ ˆM − M∥2→∞ ≲ 1 n √ T log(nT), ∥ ˆM − M∥∞ ≲ 1 n √ nT log(nT), ∥ ˆP − P∥1→∞ ≲ √ n T log(nT), ∥ ˆP − P∥∞ ≲ 1√ nT log(nT). As for the generative model, for a homogeneous frequency mat rix, our estimates achieve a relative error scaling at most as √ n/T up to the logarithmic factor for all metrics. Note that up to a logarithmic factor, the upper bound for ∥ ˆP − P∥1→∞ (and similarly for ˆM) matches the minimax lower bound derived in [ 26]. 3.4 Elements of the proofs The proofs of the three above theorems share similar argumen ts. W e only describe elements of the proof of Theorem 5, corresponding to the most challenging model. The most difﬁ cult result concerns the singular subspace recovery (the upper bounds (i) in our t heorems), and it can be decomposed into the following three steps. The ﬁrst two steps are meant t o deal with the Markovian nature of the data. The third step consists in applying a leave-one-out an alysis to recover the singular subspaces. Step 1: Multinomial approximation of Markovian data. W e treat the matrix ˜M(k) arising from one subset of data, and for simplicity, we remove the supersc ript (k), i.e., ˜M = ˜M(k). Note that Tτ˜M is a matrix recording the numbers of transitions observed in the data for any pair of states: denote by Ni,j this number for (i,j). W e approximate the joint distribution of N = ( Ni,j)(i,j) by a multinomial distribution with n2 components and parameter TτMi,j for component (i,j). Denote by Z = ( Zi,j)(i,j) the corresponding multinomial random variable. Using the m ixing property of the Markov chain and the choice of τ, we establish (see Lemma 21 in App. C) that for any subset Z of {z∈ Nn2 : ∑ (i,j) zi,j = Tτ}, we have P[N ∈ Z ] ≤ 3P[Z ∈ Z ]. Step 2: T owards P oisson random matrices with independent en tries. The random matrix Z does not have independent entries. Independence is however a req uirement if we wish to apply the leave- one-out argument. Consider the random matrix Y whose entries are independent Poisson random 6variables with mean TτMi,j for the (i,j)-th entry. W e establish the following connection between the distribution of Zand that of Y: for any Z ⊂ Nn2 , we have P[Z ∈ Z ] ≤ e√TτP[Y ∈ Z ]. Refer to Lemma 22 in App. C for details. Step 3: The leave-one-out argument for P oisson matrices. Combining the two ﬁrst steps provides a connection between the observation matrix ˜M and a Poisson matrix Y with independent entries. This allows us to apply a leave-one-out analysis to ˜M as if it had independent entries (replacing ˜M by Y). The analysis starts by applying the standard dilation tri ck (see Section 4.10 in [ 22]) so as to make ˜Msymmetric. Then, we can decompose the error ∥U− ˆU( ˆU⊤U)∥2→∞ (see Lemma 32 in App. E) into several terms. The most challenging of these terms is ∥(M − ˜M)(U − ˆU( ˆU⊤U))∥2→∞ = maxl∈[n] ∥(Ml,: − ˜Ml,:)(U − ˆU( ˆU⊤U))∥2 because of inherent dependence between M − ˜M and U − ˆU( ˆU⊤U). The leave-one-out analysis allows us to decouple this stat istical dependency. It consists in exploiting the row and column independence of ma trix ˜M to approximate ∥(Ml,: − ˜Ml,:)(U − ˆU( ˆU⊤U))∥2 by ∥(Ml,: − ˜Ml,:)(U − ˆU(l)(( ˆU(l))⊤U)∥2 where ˆU(l) is the matrix of eigenvectors of matrix ˜M(l) obtained by zeroing the l-th row and column of ˜M. By construction, (Ml,: − ˜Ml,:) and U − ˆU(l)(( ˆU(l))⊤U) are independent, which simpliﬁes the analysis. The proof is completed by a further appropriate decomposition of this term, combined with concentration inequalities for random Poisson matrices (see App. D). 4 Regret Minimization in Low-Rank Bandits Consider a low-rank bandit problem with a homogeneous rank-r reward matrix M. W e wish to devise an algorithm π with low regret. π selects in round t an entry (iπ t,jπ t ) based on previous observations, and receives as a feedback the noisy reward Miπ t,jπ t + ξt. The regret up to round T is deﬁned by Rπ(T) = TMi⋆,j⋆ − E[∑ T t=1 Miπ t,jπ t ], where (i⋆,j⋆) is an optimal entry. One could think of a simple Explore-Then-Commit (ETC) algorith m, where in the ﬁrst phase entries are sampled uniformly at random, and where in a second phase, the algorithm always selects the highest entry of ˆMbuilt using the samples gathered in the ﬁrst phase and obtain ed by spectral decomposition. When the length of the ﬁrst phase is T2/3(n+ m)1/3, the ETC algorithm would yield a regret upper bounded by O(T2/3(n+ m)1/3) for T = Ω(( n+ m) log3(n+ m)). T o get better regret guarantees, we present SME-AE (Success ive Matrix Estimation and Arm Elimination), an algorithm meant to identify the best entry as quickly as possible with a prescribed level of certainty. After the SME-AE has returned the estima ted best entry, we commit and play this entry for the remaining rounds. The pseudo-code of SME- AE is presented in Algorithm 1. The algorithm runs in epochs: in epoch ℓ, it samples Tℓ entries uniformly at random among all entries (in Tℓ, the constant C just depends on upper bounds of the parameters µ, κ, and ∥M∥∞, refer to App. G); from these samples, a matrix ˆM(ℓ) is estimated and Aℓ, the set of candidate arms, is pruned. The pruning procedure is based on the estimated gaps: ˆ∆ (ℓ) i,j = ˆM(ℓ) ⋆ − ˆM(ℓ) i,j where ˆM(ℓ) ⋆ = max i,j ˆM(ℓ) i,j . Algorithm 1: S uccesive Matrix Estimation and Arm Elimination ( SME-AE) Input: Arms [m] × [n], conﬁdence level δ ℓ= 1 ; A1 = [ m] × [n]; while |Aℓ| >1 do δℓ = δ/ℓ2; Tℓ = ⌈ C ( 2ℓ+2)2 (m+ n) log3 ( 22ℓ+4(m+ n)/δℓ )⌉ ; Sample uniformly at random Tℓ entries from A1: (Mit,jt + ξt)t=1,...,Tℓ ; Estimate ˆM(ℓ) via spectral decomposition as described in Section 3.1 ; Aℓ+1 = { (i,j) ∈ A ℓ : ˆ∆ (ℓ) i,j ≤ 2−(ℓ+2) } ; ℓ= ℓ+ 1; end Output:Recommend the remaining pair (ˆıτ,ˆτ) in Aℓ. 7The following theorem characterizes the performance of SME -AE and the resulting regret. T o simplify the notation, we introduce the gaps: for any entry (i,j), ∆ i,j = ( Mi⋆,j⋆ − Mi,j), ∆ min = min (i,j):∆ i,j >0 ∆ i,j, ∆ max = max (i,j) ∆ i,j, and ¯∆ = ∑ (i,j) ∆ i,j/(mn). W e deﬁne the function ψ(n,m,δ ) = c(m+n) log(e/∆ min) ∆ 2 min log3 (e(m+n) log(e/∆ min) ∆ minδ ) for some universal constant c> 0. Theorem 7. (Best entry identiﬁcation) F or any δ ∈ (0,1), SME-AE( δ) stops at time τ and recommends arm (ˆıτ,ˆτ) with the guarantee P ( (ˆıτ,ˆτ) = ( i⋆,j⋆),τ ≤ ψ(n,m,δ ) ) ≥ 1 − δ. Moreover , for any T ≥ 1 and α > 0, the sample complexity τ of SME-AE( 1/Tα) satisﬁes E[τ ∧ T] ≤ ψ(n,m,T −α) + T1−α. (Regret) Let T ≥ 1. Consider the algorithm πthat ﬁrst runs SME-AE( 1/T2) and then commits to its output (ˆıτ,ˆτ) after τ. W e have: Rπ(T) ≤ ¯∆ ( ψ(n,m,T −2) + 1 ) + ∆ max T . The proof of Theorem 7 is given in App. G. Note that the regret upper bounds hold for any time horizon T ≥ 1, and that it scales as O((m + n) log3(T) ¯∆ /∆ 2 min) (up to logarithmic factors in m,n and 1/∆ min). The cubic dependence in log3(T) is an artifact of our proof techniques. More precisely, it is due to the Poisson approximation used to obt ain entry-wise guarantees. Importantly, for any time horizon, the regret upper bound only depends on (m+ n) rather than mn(the number of arms / entries), and hence, the low-rank structure is efﬁc iently exploited. If we further restrict our attention to problems with gap ratio ∆ max/∆ min upper bounded by ζ, our regret upper bound becomes O(ζ(m+ n) log3(T)/∆ min), and can be transformed into the minimax gap-independent upper bound O(ζ((m+ n)T)1/2 log2(T)), see App. G. Finally note that Ω((( m+ n)T)1/2) is an obvious minimax regret lower bound for our low-rank bandit p roblem. A very similar low-rank bandit problem has been investigate d in [ 6]. There, under similar assumptions (see Assumption 1 and Deﬁnition 1), the authors devise an algorithm with both gap- dependent and gap-independent regret guarantees. The latt er are difﬁcult to compare with ours. Their guarantees exhibit a better dependence in T and ∆ min, but worse in the matrix dimensions n and m. Indeed in our model, b⋆in [ 6] corresponds to ∥M∥2→∞ and scales as √n. As a consequence, the upper bounds in [ 6] have a dependence in n and m scaling as √n(n+ m) in the worst case for gap-dependent guarantees and even nm (through the constant C2 in [ 6]) for gap-independent guarantees. 5 Representation Learning in Low-Rank MDPs The results derived for Models II(a) and II(b) are instrumen tal towards representation learning and hence towards model-based or reward-free RL in low-rank MDP s. In this section, we provide an example of application of these results, and mention other e xamples in Section 7. A low-rank MDP is deﬁned by (S,A,{Pa}a∈A,R,γ ) where S, A denote state and action spaces of cardinalities n and A, respectively, Pa denotes the rank- rtransition matrix when taking action a, Ris the reward function, and γ is the discount factor. W e assume that all rewards are in [0,1]. The value function of a policy π: S → A is deﬁned as Vπ R(x) = E[∑ ∞ t=1 γt−1R(xπ t,πt(xπ t))|xπ 1= x] where xπ tis the state visited under πin round t. W e denote by π⋆(R) an optimal policy (i.e., with the highest value function). Reward-free RL.In the reward-free RL setting (see e.g. [ 27, 28, 29]), the learner does not receive any reward signal during the exploration process. The latte r is only used to construct estimates { ˆPa}a∈A of {Pa}a∈A. The reward function Ris revealed at the end, and the learner may compute ˆπ(R) an optimal policy for the MDP (S,A,{ ˆPa}a∈A,R,γ ). The performance of this model-based approach is often assessed through Γ = sup R∥Vπ⋆(R) R − Vˆπ(R) R ∥∞. In tabular MDP , to identify an ǫ-optimal policy for all reward functions, i.e., to ensure th at Γ ≤ ǫ, we believe that the number of samples that have to be collected should be Ω( poly( 1 1−γ)n2A ǫ2 ) (the exact degree of the polynomial in 1/(1 − γ) has to be determined). This conjecture is based on the sample complexity lower bounds derived for reward-free RL in episodic tabular MDP [ 28, 30]. Now for low-rank MDPs, the equivalent lower bound would be Ω( poly( 1 1−γ)nA ǫ2 ) [21] (this minimax lower bound is valid for Block MDPs, a particular case of low-rank MDPs). 8Leveraging our low-rank matrix estimation guarantees, we p ropose an algorithm matching the aforementioned sample complexity lower bound (up to logari thmic factors) at least when the frequency matrices {Ma}a∈A are homogeneous. The algorithm consists of two phases: (1) i n the model estimation phase, it collects Atrajectories, each of length T/A, corresponding to the Markov chains with transition matrices {Pa}a∈A. From this data, it uses the spectral decomposition method described in § 3 to build estimates { ˆPa}a∈A. (2) In the planning phase, based on the reward function R, it computes the best policy ˆπ(R) for the MDP (S,A,{ ˆPa}a∈A,R,γ ). The following theorem summarizes the performance of this algorithm. T o simplify t he presentation, we only provide the performance guarantees of the algorithm for homogeneous tr ansition matrices (guarantees for more general matrices can be derived plugging in the results from Theorem 5). Theorem 8. Assume that for any a ∈ A , Ma is homogeneous (as deﬁned in Corollary 4). If T ≥ cnAlog2(nAT) for some universal constant c >0, then we have with probability at least 1 − min{n−2,T−1}: Γ = sup R∥Vπ⋆(R) R − Vˆπ(R) R ∥∞ ≲ 1 (1−γ)2 √ nA T log(nAT). Theorem 8 is a direct consequence of Corollary 6 and of the fact that for any reward function R: ∥Vπ⋆(R) R − Vˆπ(R) R ∥∞ ≤ 2γ (1−γ)2 maxa∈A ∥Pa − ˆPa∥1→∞, see App. A. The theorem implies that if we wish to guarantee Γ ≤ ǫ, we just need to collect O( nA ǫ2(1−γ)4 ) samples up to a logarithmic factor. This sample complexity is minimax optimal in n, A, and ǫin view of the lower bound presented in [21]. 6 Related W ork Low-rank matrix estimation.Until recently, the main efforts on low-rank matrix recover y were focused on guarantees w .r.t. the spectral or Frobenius norm s, see e.g. [ 31] and references therein. The ﬁrst matrix estimation and subspace recovery guarantee s in ℓ2→∞ and ℓ∞ were established in [ 13], [ 14] via a more involved perturbation analysis than the classic al Davis-Kahan bound. An alternative approach based on a leave-one-out analysis was proposed in [ 16], and further reﬁned in [ 32, 15, 33], see [ 22] for a survey. Some work have also adapted the techniques bey ond the independent noise assumption [ 34, 35, 36], but for very speciﬁc structural dependence. W e deal with a stronger dependence, and in particular with Markovia n data (an important scenario in RL). The estimation of low-rank transition matrices of Markov ch ains has been studied in [ 26, 37] using spectral methods and in [ 38, 39] using maximum-likelihood approaches. [ 26] does not conduct any ﬁne-grained subspace recovery analysis (such as the leave- one-out), and hence the results pertaining to the ∥ · ∥ 1→∞-guarantees are questionable; refer to App. H for a detailed justiﬁcation. All these papers do not present entry-wise guarantees. It is worth mentioning that there exist other methods for matrix estimation that do not rely on spectral decompositions like ours, yet enjoy entry-wise matrix esti mation guarantees [ 40, 41, 42]. However, these methods require different assumptions than ours that may be too strong for our purposes, notably having access to the so-called anchor rows and colum ns. Moreover, we do not know if these methods also lead to guarantees for subspace recovery in the norm ∥ · ∥ 2→∞, nor how to extend those results to settings with dependent noise. Low-rank bandits.Low-rank structure in bandits has received a lot of attentio n recently [ 43, 4, 5, 44, 45, 6, 46, 7]. Different set-ups have been proposed (refer to App. H for a detailed exposition, in particular, we discuss how the settings proposed in [ 5, 6] are equivalent), and regret guarantees in an instance dependent and minimax sense have been both estab lished. T ypically minimax regret guarantees in bandits scale as √ T, but the scaling in dimension may defer when dealing with a low rank structure [ 5, 46, 6]. In [ 5], the authors also leverage spectral methods. They reduce the problem to a linear bandit of dimens ion nmbut where only roughly n+m dimensions are relevant. This entails that a regret lower bo und of order (n+ m) √ T is inevitable. Actually, in their reduction to linear bandits, they only us e a subspace recovery in Frobenius norm, which perhaps explains the scaling (n+ m)3/2 in their regret guarantees. It is worth noting that in [46], the authors manage to improve upon the work [ 5] and obtain a scaling order (m+ n) in the regret. Our algorithm leverages entry-wise guarantees whi ch rely on a stronger subspace recovery guarantee. This allows us to obtain a scaling √n+ min the regret. The work of [ 7] is yet another 9closely related work to ours. There, the authors propose an a lgorithm achieving a regret of order polylog(n+ m) √ T for a contextual bandit problem with low rank structure. How ever, their result only holds for rank 1 and their observation setup is differen t than ours because in their setting, the learner observes m entries per round while in ours the learner only observes one entry per round. In [ 6], the authors use matrix estimation with nuclear norm penal ization to estimate the matrix M. Their regret guarantees are already discussed in § 4. Some instance-dependent guarantees with logarithmic regr et for low rank bandits have been established in [ 43, 4, 44]. However, these results suffer what may be qualiﬁed as seri ous limitations. Indeed, [ 43, 44] provide instance dependent regret guarantees but only con sider low-rank bandits with rank 1, and the regret bounds of [ 43] are expressed in terms of the so-called column and row gaps (see their Theorem 1) which are distinct from the standa rd gap notions. [ 4] extend the results in [ 43] to rank r with the limitation that they require stronger assumptions than ours. Moreover, the computational complexity of their algorithm depends ex ponentially on the rank r; they require a search over spaces of size (m r ) and (n r ) . Our proposed algorithm does not suffer from such limitations. W e wish to highlight that our entry-wise guarantees for matr ix estimation are the key enabling tool that led us to the design and analysis of our proposed algorit hm. In fact, the need for such guarantees arises naturally in the analysis of gap-dependent regret bo unds (see Appendix G.1). Therefore, we believe that such guarantees can pave the way towards better , faster, and efﬁcient algorithms for bandits with low-rank structure. Low-rank Reinforcement Learning.RL with low rank structure has been recently extensively studied but always in the function approximation framework [ 47, 48, 49, 50, 51, 52, 8, 9, 10, 53, 11, 12]. There, the transition probabilities can be written as φ(x,a)⊤µ(x′) where the unknown feature functions φ(x,a),µ(x′) ∈ Rr belong to some speciﬁc class F of functions. The major issue with algorithms proposed in this literature is that they rely on s trong computational oracles (e.g., ERM, MLE), see [ 54, 55, 56] for detailed discussions. In contrast, we do not assume tha t the transition matrices are constructed based on a given restricted class o f functions, and our algorithms do not rely on any oracle and are computationally efﬁcient. In [ 40, 42], the authors also depart from the function approximation framework. There, they consider a l ow rank structure different than ours. Their matrix estimation method enjoys an entry-wise guaran tee, but requires to identify a subset of rows and columns spanning the range of the full matrix. Moreo ver, their results are only limited the generative models, which allows to actually rely on indepen dent data samples. 7 Conclusion and Perspectives In this paper, we have established that spectral methods efﬁciently recover low-rank matrices even in correlated noise. W e have investigated noise correlatio ns that naturally arise in RL, and have managed to prove that spectral methods yield nearly-minima l entry-wise error. Our results for low- rank matrix estimation have been applied to design efﬁcient algorithms in low-rank RL problems and to analyze their performance. W e believe that these resu lts may ﬁnd many more applications in low-rank RL. They can be applied (i) to reward-free RL in ep isodic MDPs (this setting is easier than that presented in § 5 since successive episodes are independent); (ii) to scenar ios corresponding to ofﬂine RL [ 57] where the data consists of a single trajectory generated un der a given behavior policy (from this data, we can extract the transitions (x,a,x ′) where a given action a is involved and apply the spectral method to learn ˆPa); (iii) to traditional RL where the reward function Rhas to be learnt (learning Ris a problem that lies in some sense between the inference pro blems in our Models I and II); (iv) to model-free RL where we would directl y learn the Qfunction as done in [58] under a generative model; (v) to low-rank RL problems with c ontinuous state spaces (this can be done if the transition probabilities are smooth in the sta tes, and by combining our methods to an appropriate discretization of the state space). Acknowledgment This research was supported by the W allenberg AI, Autonomou s Systems and Software Program (W ASP) funded by the Knut and Alice W allenberg Foundation. 10References [1] Michael Laskin, Aravind Srinivas, and Pieter Abbeel. CU RL: Contrastive Unsupervised Representations for Reinforcement Learning. In Proceedings of the 37th International Conference on Machine Learning , volume 119 of Proceedings of Machine Learning Research , pages 5639–5650. PMLR, 13–18 Jul 2020. (Cited on page 1.) [2] Adam Stooke, Kimin Lee, Pieter Abbeel, and Michael Laski n. Decoupling Representation Learning from Reinforcement Learning. In Proceedings of the 38th International Conference on Machine Learning , volume 139 of Proceedings of Machine Learning Research , pages 9870– 9879. PMLR, 18–24 Jul 2021. (Cited on page 1.) [3] Y ash Chandak, Shantanu Thakoor, Zhaohan Daniel Guo, Y un hao T ang, Remi Munos, Will Dabney, and Diana L Borsa. Representations and exploration for deep reinforcement learning using singular value decomposition. In Proc. of ICML , 2023. (Cited on page 1.) [4] Branislav Kveton, Csaba Szepesvari, Anup Rao, Zheng W en , Y asin Abbasi-Y adkori, and S. Muthukrishnan. Stochastic low-rank bandits, 2017. (Cited on pages 1, 9, and 10.) [5] Kwang-Sung Jun, Rebecca Willett, Stephen J. Wright, and Robert D. Nowak. Bilinear bandits with low-rank structure. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machin e Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA , volume 97 of Proceedings of Machine Learning Research, pages 3163–3172. PMLR, 2019. (Cited on pages 1 and 9.) [6] Mohsen Bayati, Junyu Cao, and W anning Chen. Speed up the c old-start learning in two-sided bandits with many arms. arXiv preprint arXiv:2210.00340 , 2022. (Cited on pages 1, 8, 9, 10, and 50.) [7] Prateek Jain and Soumyabrata Pal. Online low rank matrix completion. In Proc. of ICLR , 2023. (Cited on pages 1, 9, and 50.) [8] W en Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal , and John Langford. Model- based RL in Contextual Decision Processes: P AC bounds and Ex ponential Improvements over Model-free Approaches. In Proceedings of the Thirty-Second Conference on Learning Th eory, volume 99 of Proceedings of Machine Learning Research , pages 2898–2933. PMLR, 25–28 Jun 2019. (Cited on pages 1 and 10.) [9] Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and W en Sun. FLAMBE: Structural Complexity and Representation Learning of Low Rank MDPs. In Advances in Neural Information Processing Systems , volume 33, pages 20095–20107. Curran Associates, Inc., 2020. (Cited on pages 1 and 10.) [10] Aditya Modi, Jinglin Chen, Akshay Krishnamurthy, Nan J iang, and Alekh Agarwal. Model- free representation learning and exploration in low-rank m dps. T o appear in Journal of Machine Learning Research (JMLR) , 2023. (Cited on pages 1 and 10.) [11] Masatoshi Uehara, Xuezhou Zhang, and W en Sun. Represen tation Learning for Online and Ofﬂine RL in Low-rank MDPs. In International Conference on Learning Representations , 2022. (Cited on pages 1 and 10.) [12] T ongzheng Ren, Tianjun Zhang, Lisa Lee, Joseph E Gonzal ez, Dale Schuurmans, and Bo Dai. Spectral decomposition representation for reinforcement learning. In Proc. of ICLR , 2023. (Cited on pages 1 and 10.) [13] Jianqing Fan, W eichen W ang, and Y iqiao Zhong. An ℓ∞ eigenvector perturbation bound and its application to robust covariance estimation. Journal of Machine Learning Research , 18(207):1–42, 2018. (Cited on pages 2 and 9.) [14] Justin Eldridge, Mikhail Belkin, and Y usu W ang. Unpert urbed: spectral analysis beyond davis- kahan. In Algorithmic Learning Theory , pages 321–358. PMLR, 2018. (Cited on pages 2 and 9.) [15] Joshua Cape, Minh T ang, and Carey E Priebe. The two-to-i nﬁnity norm and singular subspace geometry with applications to high-dimensional statistic s. The Annals of Statistics , 47(5):2405– 2439, 2019. (Cited on pages 2, 9, 18, 35, and 43.) [16] Emmanuel Abbe, Jianqing Fan, Kaizheng W ang, and Y iqiao Zhong. Entrywise eigenvector analysis of random matrices with low expected rank. Annals of statistics , 48(3):1452, 2020. (Cited on pages 2, 9, and 34.) 11[17] Nathan Srebro and Adi Shraibman. Rank, trace-norm and m ax-norm. In International conference on computational learning theory , pages 545–560. Springer, 2005. (Cited on page 2.) [18] Y uxin Chen, Y uejie Chi, Jianqing Fan, Cong Ma, and Y ulin g Y an. Noisy matrix completion: Understanding statistical guarantees for convex relaxati on via nonconvex optimization. SIAM journal on optimization , 30(4):3098–3121, 2020. (Cited on page 2.) [19] Ohad Shamir and Shai Shalev-Shwartz. Matrix completio n with the trace norm: Learning, bounding, and transducing. The Journal of Machine Learning Research , 15(1):3401–3423, 2014. (Cited on page 2.) [20] Sahand Negahban and Martin J W ainwright. Restricted st rong convexity and weighted matrix completion: Optimal bounds with noise. The Journal of Machine Learning Research , 13(1):1665–1697, 2012. (Cited on page 2.) [21] Y assir Jedra, Junghyun Lee, Alexandre Proutiere, and S e-Y oung Y un. Nearly optimal latent state decoding in block mdps. In International Conference on Artiﬁcial Intelligence and Statistics, pages 2805–2904. PMLR, 2023. (Cited on pages 2, 8, and 9.) [22] Y uxin Chen, Y uejie Chi, Jianqing Fan, Cong Ma, et al. Spe ctral methods for data science: A statistical perspective. F oundations and T rends® in Machine Learning , 14(5):566–806, 2021. (Cited on pages 2, 4, 7, 9, 34, 39, 40, 41, and 43.) [23] Emmanuel J Candès and T erence T ao. The power of convex re laxation: Near-optimal matrix completion. IEEE T ransactions on Information Theory , 56(5):2053–2080, 2010. (Cited on page 3.) [24] Benjamin Recht. A simpler approach to matrix completio n. Journal of Machine Learning Research, 12(12), 2011. (Cited on page 3.) [25] Geoffrey W olfer and Aryeh Kontorovich. Estimating the mixing time of ergodic markov chains. In Alina Beygelzimer and Daniel Hsu, editors, Proceedings of the Thirty-Second Conference on Learning Theory , volume 99 of Proceedings of Machine Learning Research , pages 3120– 3159. PMLR, 25–28 Jun 2019. (Cited on page 6.) [26] Anru Zhang and Mengdi W ang. Spectral State Compression of Markov Processes. IEEE T ransactions on Information Theory , 66(5):3202–3231, 2020. (Cited on pages 6, 9, 18, 21, 44, 49, and 50.) [27] Emilie Kaufmann, Pierre Ménard, Omar Darwiche Domingu es, Anders Jonsson, Edouard Leurent, and Michal V alko. Adaptive Reward-Free Explorati on. In Proceedings of the 32nd International Conference on Algorithmic Learning Theory , volume 132 of Proceedings of Machine Learning Research , pages 865–891. PMLR, 16–19 Mar 2021. (Cited on page 8.) [28] Chi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tia ncheng Y u. Reward-Free Exploration for Reinforcement Learning. In Proceedings of the 37th International Conference on Machine Learning , volume 119 of Proceedings of Machine Learning Research , pages 4870– 4879. PMLR, 13–18 Jul 2020. (Cited on page 8.) [29] Zihan Zhang, Simon Du, and Xiangyang Ji. Near Optimal Re ward-Free Reinforcement Learning. In Proceedings of the 38th International Conference on Machin e Learning , volume 139 of Proceedings of Machine Learning Research , pages 12402–12412. PMLR, 18–24 Jul 2021. (Cited on page 8.) [30] Pierre Menard, Omar Darwiche Domingues, Anders Jonsso n, Emilie Kaufmann, Edouard Leurent, and Michal V alko. Fast active learning for pure exp loration in reinforcement learning. In Proceedings of the 38th International Conference on Machin e Learning , volume 139 of Proceedings of Machine Learning Research , pages 7599–7608. PMLR, 18–24 Jul 2021. (Cited on page 8.) [31] Mark A. Davenport and Justin K. Romberg. An overview of l ow-rank matrix recovery from incomplete observations. IEEE J. Sel. T op. Signal Process. , 10(4):608–622, 2016. (Cited on page 9.) [32] T T ony Cai and Anru Zhang. Rate-optimal perturbation bo unds for singular subspaces with applications to high-dimensional statistics. The Annals of Statistics , 46(1):60–89, 2018. (Cited on page 9.) [33] Y uxin Chen, Jianqing Fan, Cong Ma, and Kaizheng W ang. Sp ectral method and regularized mle are both optimal for top-k ranking. Annals of statistics , 47(4):2204, 2019. (Cited on page 9.) 12[34] Lihua Lei. Uniﬁed ℓ2→∞ eigenspace perturbation theory for symmetric random matri ces. arXiv preprint arXiv:1909.04798 , 2019. (Cited on page 9.) [35] Emmanuel Abbe, Jianqing Fan, and Kaizheng W ang. An ℓp theory of pca and spectral clustering. The Annals of Statistics , 50(4):2359–2385, 2022. (Cited on page 9.) [36] Joshua Agterberg, Zachary Lubberts, and Carey E Priebe . Entrywise estimation of singular vectors of low-rank matrices with heteroskedasticity and d ependence. IEEE T ransactions on Information Theory , 68(7):4618–4650, 2022. (Cited on page 9.) [37] Shujun Bi, Zhen Y in, and Y ihong W eng. A low-rank spectra l method for learning markov models. Optimization Letters , 17(1):143–162, 2023. (Cited on page 9.) [38] Xudong Li, Mengdi W ang, and Anru Zhang. Estimation of ma rkov chain via rank-constrained likelihood. In International Conference on Machine Learning , pages 3033–3042. PMLR, 2018. (Cited on page 9.) [39] Ziwei Zhu, Xudong Li, Mengdi W ang, and Anru Zhang. Learn ing markov models via low-rank optimization. Operations Research , 70(4):2384–2398, 2022. (Cited on page 9.) [40] Devavrat Shah, Dogyoon Song, Zhi Xu, and Y uzhe Y ang. Sam ple Efﬁcient Reinforcement Learning via Low-Rank Matrix Estimation. In Advances in Neural Information Processing Systems, volume 33, pages 12092–12103. Curran Associates, Inc., 20 20. (Cited on pages 9 and 10.) [41] Alekh Agarwal, Nan Jiang, Sham M Kakade, and W en Sun. Rei nforcement learning: Theory and algorithms. CS Dept., UW Seattle, Seattle, W A, USA, T ech. Rep , pages 10–4, 2019. (Cited on pages 9 and 18.) [42] T yler Sam, Y udong Chen, and Christina Lee Y u. Overcomin g the long horizon barrier for sample-efﬁcient reinforcement learning with latent low-r ank structure. ACM SIGMETRICS P erformance Evaluation Review , 50(4):41–43, 2023. (Cited on pages 9 and 10.) [43] Sumeet Katariya, Branislav Kveton, Csaba Szepesvari, Claire V ernade, and Zheng W en. Stochastic Rank-1 Bandits. In Aarti Singh and Jerry Zhu, edi tors, Proceedings of the 20th International Conference on Artiﬁcial Intelligence and St atistics, volume 54 of Proceedings of Machine Learning Research , pages 392–401. PMLR, 20–22 Apr 2017. (Cited on pages 9 and 10.) [44] Cindy Trinh, Emilie Kaufmann, Claire V ernade, and Rich ard Combes. Solving bernoulli rank- one bandits with unimodal thompson sampling. In Aryeh Konto rovich and Gergely Neu, editors, Proceedings of the 31st International Conference on Algori thmic Learning Theory , volume 117 of Proceedings of Machine Learning Research , pages 862–889. PMLR, 08 Feb– 11 Feb 2020. (Cited on pages 9 and 10.) [45] Y angyi Lu, Amirhossein Meisami, and Ambuj T ewari. Low- rank generalized linear bandit problems. In Arindam Banerjee and Kenji Fukumizu, editors, The 24th International Conference on Artiﬁcial Intelligence and Statistics, AIST ATS 2021, April 13-15, 2021, V irtual Event, volume 130 of Proceedings of Machine Learning Research , pages 460–468. PMLR, 2021. (Cited on page 9.) [46] Y ue Kang, Cho-Jui Hsieh, and Thomas Chun Man Lee. Efﬁcie nt frameworks for generalized low-rank matrix bandit problems. Advances in Neural Information Processing Systems , 35:19971–19983, 2022. (Cited on pages 9 and 50.) [47] Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John L angford, and Robert E. Schapire. Contextual decision processes with low Bellman rank are P AC -learnable. In Proceedings of the 34th International Conference on Machine Learning , volume 70 of Proceedings of Machine Learning Research , pages 1704–1713. PMLR, 06–11 Aug 2017. (Cited on page 10.) [48] Christoph Dann, Nan Jiang, Akshay Krishnamurthy, Alek h Agarwal, John Langford, and Robert E Schapire. On Oracle-Efﬁcient P AC RL with Rich Obser vations. In Advances in Neural Information Processing Systems , volume 31. Curran Associates, Inc., 2018. (Cited on page 10.) [49] Simon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarw al, Miroslav Dudik, and John Langford. Provably efﬁcient RL with Rich Observations via L atent State Decoding. In Proceedings of the 36th International Conference on Machin e Learning , volume 97 of Proceedings of Machine Learning Research , pages 1665–1674. PMLR, 09–15 Jun 2019. (Cited on page 10.) 13[50] Dipendra Misra, Mikael Henaff, Akshay Krishnamurthy, and John Langford. Kinematic State Abstraction and Provably Efﬁcient Rich-Observation Reinf orcement Learning. In Proceedings of the 37th International Conference on Machine Learning , volume 119 of Proceedings of Machine Learning Research , pages 6961–6971. PMLR, 13–18 Jul 2020. (Cited on page 10.) [51] Dylan Foster, Alexander Rakhlin, David Simchi-Levi, a nd Y unzong Xu. Instance-Dependent Complexity of Contextual Bandits and Reinforcement Learni ng: A Disagreement-Based Perspective. In Proceedings of Thirty F ourth Conference on Learning Theory , volume 134 of Proceedings of Machine Learning Research , pages 2059–2059. PMLR, 15–19 Aug 2021. (Cited on page 10.) [52] Xuezhou Zhang, Y uda Song, Masatoshi Uehara, Mengdi W an g, Alekh Agarwal, and W en Sun. Efﬁcient Reinforcement Learning in Block MDPs: A Model-fre e Representation Learning Approach. In Proceedings of the 39th International Conference on Machin e Learning , volume 162 of Proceedings of Machine Learning Research , pages 26517–26547. PMLR, 17–23 Jul 2022. (Cited on page 10.) [53] Masatoshi Uehara, Xuezhou Zhang, and W en Sun. Represen tation learning for online and ofﬂine rl in low-rank mdps. arXiv preprint arXiv:2110.04652 , 2021. (Cited on page 10.) [54] Daniel Kane, Sihan Liu, Shachar Lovett, and Gaurav Maha jan. Computational-statistical gap in reinforcement learning. In Proceedings of Thirty Fifth Conference on Learning Theory , volume 178 of Proceedings of Machine Learning Research , pages 1282–1302. PMLR, 02–05 Jul 2022. (Cited on page 10.) [55] Noah Golowich, Ankur Moitra, and Dhruv Rohatgi. Learni ng in Observable POMDPs, without Computationally Intractable Oracles. In Advances in Neural Information Processing Systems , volume 35. Curran Associates, Inc., 2022. (Cited on page 10.) [56] Tianjun Zhang, T ongzheng Ren, Mengjiao Y ang, Joseph Go nzalez, Dale Schuurmans, and Bo Dai. Making Linear MDPs Practical via Contrastive Repres entation Learning. In Proceedings of the 39th International Conference on Machin e Learning , volume 162 of Proceedings of Machine Learning Research , pages 26447–26466. PMLR, 17–23 Jul 2022. (Cited on page 10.) [57] Ming Y in and Y u-Xiang W ang. Optimal Uniform OPE and Mode l-based Ofﬂine Reinforcement Learning in Time-Homogeneous, Reward-Free and T ask-Agnostic Settings. In Advances in Neural Information Processing Systems , volume 34, pages 12890–12903. Curran Associates, Inc., 2021. (Cited on page 10.) [58] Devavrat Shah, Dogyoon Song, Zhi Xu, and Y uzhe Y ang. Sam ple efﬁcient reinforcement learning via low-rank matrix estimation. In Proceedings of the 34th International Conference on Neural Information Processing Systems , NIPS’20. Curran Associates Inc., 2020. (Cited on page 10.) [59] Michael Mitzenmacher and Eli Upfal. Probability and computing: Randomization and probabilistic techniques in algorithms and data analysis . Cambridge university press, 2017. (Cited on pages 22 and 23.) [60] Samuel B Hopkins, Tselil Schramm, Jonathan Shi, and Dav id Steurer. Fast spectral algorithms from sum-of-squares proofs: tensor decomposition and plan ted sparse vectors. In Proceedings of the forty-eighth annual ACM symposium on Theory of Comput ing, pages 178–191, 2016. (Cited on page 26.) [61] George Bennett. Probability inequalities for the sum o f independent random variables. Journal of the American Statistical Association , 57(297):33–45, 1962. (Cited on page 26.) [62] Andrew D McRae and Mark A Davenport. Low-rank matrix com pletion and denoising under poisson noise. Information and Inference: A Journal of the IMA , 10(2):697–720, 2021. (Cited on page 31.) [63] Afonso S Bandeira and Ramon V an Handel. Sharp nonasympt otic bounds on the norm of random matrices with independent entries. The Annals of Probability , 44(4):2479–2506, 2016. (Cited on page 31.) [64] V ivek Farias, Andrew A Li, and Tianyi Peng. Near-optima l entrywise anomaly detection for low-rank matrices with sub-exponential noise. In International Conference on Machine Learning, pages 3154–3163. PMLR, 2021. (Cited on page 34.) 14[65] Joel A Tropp et al. An introduction to matrix concentrat ion inequalities. F oundations and T rends® in Machine Learning , 8(1-2):1–230, 2015. (Cited on page 34.) [66] Kwang-Sung Jun, Rebecca Willett, Stephen Wright, and R obert Nowak. Bilinear bandits with low-rank structure. In International Conference on Machine Learning , pages 3163–3172. PMLR, 2019. (Cited on page 50.) [67] Y uxin Chen, Y uejie Chi, Jianqing Fan, Cong Ma, and Y ulin g Y an. Noisy matrix completion: Understanding statistical guarantees for convex relaxati on via nonconvex optimization. SIAM Journal on Optimization , 30(4):3098–3121, 2020. (Cited on page 50.) 15Contents 1 Introduction 1 2 Models and Objectives 2 3 Matrix Estimation via Spectral Decomposition 3 3.1 Reward matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 3.2 Transition matrices under the generative model . . . . . . . . . . . . . . . . . . . 4 3.3 Transition matrices under the forward model . . . . . . . . . . . . . . . . . . . . . 5 3.4 Elements of the proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 4 Regret Minimization in Low-Rank Bandits 7 5 Representation Learning in Low-Rank MDPs 8 6 Related W ork 9 7 Conclusion and Perspectives 10 A Preliminaries 18 A.1 Matrix norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 A.2 Mixing time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 A.3 V alue difference lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 B Statement and proofs of the main results 19 B.1 Reward matrix estimation – Model I . . . . . . . . . . . . . . . . . . . . . . . . . 19 B.2 Transition matrix estimation under the generative mode l – Model II(a) . . . . . . . 20 B.3 Transition matrix estimation under the forward model – M odel II(b) . . . . . . . . 21 B.4 An additional lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 C Comparison inequalities and the Poisson approximation ar gument 22 C.1 Preliminaries on Poisson approximation . . . . . . . . . . . . . . . . . . . . . . . 22 C.2 Poisson approximation for reward matrices – Model I . . . . . . . . . . . . . . . . 23 C.3 Approximations for transition matrices – Model II . . . . . . . . . . . . . . . . . . 24 C.3.1 Multinomial approximation . . . . . . . . . . . . . . . . . . . . . . . . . 24 C.3.2 Poisson approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 D Concentration of matrices with Poisson and compound Poiss on entries 26 D.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 D.2 Random matrices with compound Poisson entries . . . . . . . . . . . . . . . . . . 27 D.3 Random matrices with Poisson entries . . . . . . . . . . . . . . . . . . . . . . . . 31 E Singular subspace recovery via the leave-one-out argumen t 34 16E.1 Subspace recovery for reward matrices . . . . . . . . . . . . . . . . . . . . . . . . 34 E.2 Subspace recovery for transition matrices . . . . . . . . . . . . . . . . . . . . . . 38 E.3 Error decomposition in the two-to-inﬁnity norm . . . . . . . . . . . . . . . . . . . 41 F Row-wise and entry-wise matrix estimation errors 43 F .1 Bounding ∥M− ˆM∥2→∞ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 F .2 Bounding ∥P − ˆP∥1→∞ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 F .3 Bounding ∥M− ˆM∥∞ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 F .4 Bounding ∥P − ˆP∥∞ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 G Low-rank bandits: proofs of results from Section 4 46 G.1 Gap-dependent guarantees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 G.2 Gap-independent guarantees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 H Related work 49 H.1 Low-rank transition matrix estimation . . . . . . . . . . . . . . . . . . . . . . . . 49 H.2 Low rank bandits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 17A Preliminaries In this section, we present a few results that are used throughout our analysis. A.1 Matrix norms Lemma 9.Let A∈ Rn×m,B ∈ Rm×r. Then: ∥AB∥2→∞ ≤ ∥ A∥1→∞∥B∥2→∞, (2) ∥AB∥2→∞ ≤ ∥ A∥2→∞∥B∥, (3) ∥AB∥∞ ≤ ∥ A∥2→∞∥B⊤∥2→∞. (4) Proof. The proof of the lemma directly follows from Hölder’s inequa lity (see for example Proposition 6.5 in [ 15]). A.2 Mixing time Lemma 10.(Lemma 5 in [ 26]) Let τ(ε) be the ε-mixing time of an irreducible Markov chain. Then if ε≤ δ <1/2, τ(ε) ≤ τ(δ) ( 1 + ⌈ log(δ/ε) log(1/(2δ)) ⌉) . A.3 V alue difference lemmas The following lemmas are used in Section 5 to prove Theorem 8. Recall the deﬁnition of value function of a policy π: Vπ R(x) = E[∑ ∞ t=1 γt−1R(xπ t,πt(xπ t))|xπ 1= x]. The (state, action) value function of πis also deﬁned as: for any state x∈ S and action a∈ A , Qπ R(x,a) = R(x,a) + γEx′∼P(·|x,a)[Vπ R(x′)]. W e denote by ˆQπ Rthe (state, action) value function of π in the MDP where P is replaced by its estimate ˆP, and let ˆπ(R) be the optimal policy for this MDP . Lemma 11. W e have that ∥Vπ⋆(R) R − Vˆπ(R) R ∥∞ ≤ 2 sup π ∥Qπ R− ˆQπ R∥∞ Proof. W e remove the subscript Rto simplify the notation. For any s, we have Vπ⋆ (s) − Vˆπ(s) = Qπ⋆ (s,π⋆(s)) − Qˆπ(s,ˆπ(s)) = [ Qπ⋆ (s,π⋆(s)) − ˆQπ⋆ (s,π⋆(s))] + [ ˆQˆπ(s,ˆπ(s)) − Qˆπ(s,ˆπ(s))] + [ ˆQπ⋆ (s,π⋆(s)) − ˆQˆπ(s,ˆπ(s))] ≤ 2 sup π ∥Qπ − ˆQπ∥∞, since ˆQπ⋆ (s,π⋆(s)) ≤ ˆQˆπ(s,ˆπ(s)) by deﬁnition of ˆπ. Lemma 12. (Proposition 2.1 in [ 41]) F or all policies π: ∥Qπ R− ˆQπ R∥∞ ≤ γ (1 − γ)2 max a∈A ∥Pa − ˆPa∥1→∞ Combining the two lemmas, we get: ∥Vπ⋆(R) R − Vˆπ(R) R ∥∞ ≤ 2γ (1 − γ)2 max a∈A ∥Pa − ˆPa∥1→∞. This inequality is used in the proof of Theorem 8. 18B Statement and proofs of the main results In this appendix, we present the proofs of the main theorems.In Subsection § B.1, we provide the proof of Theorem 1 and Corollary 2. In Subsection § B.2, we give a complete, non-simpliﬁed version of Theorem 3 from which one can deduce Theorem 3 and Corollary 4 given in the main text. Finally, in Subsection § B.3, we present a complete, non-simpliﬁed version of Theorem 5 and from the latter, deduce Theorem 5 and Corollary 6. B.1 Reward matrix estimation – Model I In this subsection, we present the proofs of Theorem 1. The proof of Corollary 2 is in fact immediate from Theorem 1. Proof of Theorem 1. Proof of (i) . Recall the results from Lemma 30: for all δ∈ (0,1), if B = √ nm T (√ (n+ m) log (e(n+ m)T δ ) + log3/2 (e(n+ m)T δ )) , (5) then for all T ≥ c1(µ4κ2r2 + 1)(m+ n) log3 ( e2(m+ n)T/δ ) , the event max(∥U − ˆU( ˆU⊤U)∥,∥V − ˆV(ˆV⊤V)∥) ≤ C1 ∥M∥∥M∥∞ σr(M)2 max(∥V∥2→∞∥U∥2→∞)B holds with probability at least 1 − δ for some universal constants c1,C1 > 0. T o obtain the form presented in Theorem 1, we simply recall the deﬁnitions κ = ∥M∥/σr(M), µ = max( √ m/r∥U∥2→∞, √ n/r∥V∥2→∞) and the bound ∥M∥∞/σr(M) ≤ (µ2κr)/√mn from Lemma 17. W e then substitute in the upper bound above. Note that µ,κ and r are larger than 1 by deﬁnition. Proof of (ii).T o establish the desired bound, we use the decomposition err or established in Lemma 34. Namely, under the event that ∥ ˜M − M∥ ≤ c1σr(M) for some universal constant c1 > 0 sufﬁciently small, there exists a universal constant c2 >0 such that ∥ ˆM− M∥2→∞ ≤ c2σ1(M) [ ∥U − ˆU( ˆU⊤U)∥2→∞ + µ √ r m ∥ ˜M − M∥ σr(M) ] . (6) Hence, we only need high probability bounds on ∥U − ˆU( ˆU⊤U)∥2→∞ which we established in (i), and on ∥ ˜M− M∥ which we also established in Proposition 26 under the compound Poisson entries model described ( 15). W e can extend the latter result under our observation mode l using the Poisson approximation Lemma 20, and ﬁnally write that for all δ∈ (0,1), using the same deﬁnition of B as above in ( 5), for all for all T ≥ c3 log3 ((n+ m)/δ), the following statement ∥ ˜M− M∥ σr(M) ≤ C3∥M∥∞ σr(M) B (7) holds with probability at least 1 − δ, for some universal constants c3,C3 > 0 large enough. Note that under the condition T ≥ c4µ4κ2r2 log3 (e(n+ m)/δ) for some universal constant c4 large enough, the high probability statement in ( 7) holds and in addition we also have ∥ ˜M − M∥ ≤ c1σr(M). There, we used the result of Lemma 17. The statement (ii) in Theorem 1 is obtained by ﬁrst substituting in ( 6), the upper bound we get in (i) and that we get in ( 7), and then, using σ1(M) ≤ √mn∥M∥∞ and the bound ∥M∥∞/σr(M) ≤ (µ2κr)/√mnfrom Lemma 17. Proof of (iii). T o establish the desired bound, we use the decomposition err or established in Lemma 36. Namely, under the event that ∥ ˜M − M∥ ≤ c1σr(M) for some universal constant c1 > 0 sufﬁciently small, there exists a universal constant c2 >0 such that ∥ ˆM− M∥∞ ≤ c2∥M∥2→∞ ( ∥M − ˜M∥ σr(M) ∥V∥2→∞ + ∥V − ˆVWˆV∥2→∞ ) + c2∥M − ˆM∥2→∞(∥V∥2→∞ + ∥V − ˆVWˆV∥2→∞). (8) 19T o upper bound the above error, we need to control: (a) ∥V − ˆVWˆV∥2→∞, which we have already done in (i); (b) ∥M − ˜M∥, which follows from Lemma 20 as established in the proof of (ii) (see the high probability statement ( 7)); and (c) ∥M − ˆM∥2→∞, which again we have already done in (ii). The statement (iii) in Theorem 1 follows from ﬁrst substituting in ( 8), the upper bounds we get from (a), (b) and (c), and then using ∥M∥∞/σr(M) ≤ (µ2κr)/√mn, µ= max( √ m/r∥U∥2→∞, √ n/r∥V∥2→∞), and the basic inequality ∥M∥2→∞ ≤ √m∥M∥∞ ≤√m+ n∥M∥∞. B.2 T ransition matrix estimation under the generative mode l – Model II(a) In this subsection, we present a complete, non-simpliﬁed ve rsion of Theorem 3, from which one can deduce Theorem 3 and Corollary 4 given in the main text. First, let us deﬁne the function gδ : Rn×n → R+ as gδ(M) = 1{∃ℓ:∥Mℓ,:∥∞≤1} log (ne δ ) log−1 ( 1 + 1 ∥M∥∞ ) + 1{∀ℓ:∥Mℓ,:∥∞>1} log (∥M∥∞ ne δ )√ ∥M∥∞. (9) W e also use the following notation:    A = 1 √ T √ ∥M∥1→∞ + ∥M⊤∥1→∞, B′ = µκ√ r n ( A + 1 Tgδ/ √ T(TM) log ( n √ T δ )) + √ r∥M∥∞ T log ( n √ T δ ) . W e ﬁrst recall a standard result quantifying how well ˜M approximates M. Lemma 13. ∀δ∈ (0,1), w .p. at least 1 − δ, ∥ ˜M − M∥ ≤ CA + C Tgδ/ √ T(TM) √ log( n √ T δ ). Proof. The lemma follows directly from Lemma 22 (replacing Tτ by T) and Lemma 28. Theorem 14. Assume that (ν0)min = min i∈[n](ν0)i >0. F or any δ >0, if ∥ ˜M − M∥ ≤ cσr(M), gδ/ √ T(TM) log(n √ T/δ) ≤ cTσr(M) and ∥M∥∞ log(n √ T/δ) ≤ cTσ2 r(M) for some universal constant c> 0, then there exists a universal constant C >0 such that with probability at least 1 − δ holds: (i) max { ∥U− ˆU( ˆU⊤U)∥2→∞,∥V − ˆV(ˆV⊤V)∥2→∞ } ≤ C B′ σr (M) , (ii) ∥ ˆM− M∥2→∞ ≤ CκB′, ∥ ˆP − P∥1→∞ ≤ C κ√n (ν0)min B′, (iii) ∥ ˆM− M∥∞ ≤ C ( ∥M∥2→∞+κB′ σr (M) + κµ√ r n ) B′, (iv) ∥ ˆP − P∥∞ ≤ C B′ (ν0)min [ √nκ∥M∥∞ (ν0)min + ∥M∥2→∞+κB′ σr (M) + κµ√ r n ] , where (iv) holds if in addition ∥ ˆM − M∥1→∞ ≤ 1 2 (ν0)min. Proof. The ﬁrst statement of the theorem follows from Lemma 22 (with T instead of Tτ), Lemmas 28 and 32. The remaining bounds are consequences of (i) and of the results presented in Appendix F. Proof of Theorem 3. Theorem 3 follows from Lemma 13 and Theorem 14 by simplifying the term B′ using B given in Theorem 3. As a result of this simpliﬁcation, as well as of the assumpti ons given in statement of Theorem 14, we obtain bounds on n,T required in Theorem 3. Furthermore, we use simple inequalities (check Lemma 17) to rewrite all terms depending on M as functions of ∥M∥∞ and (ν0)min. Remark 1. It is worth noting that Corollary 4 is a corollary of Theorem 14, and that the lower bound on n required in Theorem 3 is not required for this corollary. Moreover , results prese nted in this corollary are valid for almost all T ≥ cnlog(nT) - in the case when T ≍ [n2−ǫ,n2] for 20arbitrarily small ǫ >0, bounds in Corollary 4 contain additional log term, which is an artifact of our analysis (and splitting concentration into cases T ≲ n2 and T ≳ n2). This discontinuity in the range of T can be resolved, but at the price of a reduced readability. B.3 T ransition matrix estimation under the forward model – M odel II(b) In this subsection, we present a complete, non-simpliﬁed ve rsion of Theorem 5, from which one can deduce Theorem 5 and Corollary 6 given in the main text. Again, we use function the funcion gδ deﬁned in ( 9), and we introduce: B′ = µκ √ r n (√ ∥ν∥∞τ⋆ T log (ne δ ) log(Tν−1 min) + τ⋆ T gδ/√Tτ (TτM) log (n√Tτ δ ) log(Tν−1 min) ) + √ rτ⋆∥M∥∞ T log (n√Tτ δ ) log(Tν−1 min). Our analysis starts from the following lemma stating how wel l ˜M approximates M. Lemma 15. (Lemma 7 in [ 26]) F or τ ≥ 2τ⋆log(Tν−1 min) and for any δ ∈ (0,1), we have with probability at least 1 − δ: ∥ ˜M − M∥ ≤ C √ ∥ν∥∞τ T log (ne δ ) + Cτ T log (ne δ ) . Theorem 16. Assume that νmin = min i∈[n] νi > 0 and that τ/(τ⋆log(Tν−1 min)) ∈ [c1,c2] for some universal constants c2 > c1 ≥ 2. F or any δ > 0, if ∥ ˜M − M∥ ≤ cσr(M), gδ/ √ T(TτM) log(n√Tτ/δ) ≤ cTτσr(M) and ∥M∥∞ log(n√Tτ/δ) ≤ cTτσ2 r(M) for some universal constant c> 0, then there exists a universal constant C >0 such that with probability at least 1 − δ, (i) max { ∥U − ˆU( ˆU⊤U)∥2→∞,∥V − ˆV(ˆV⊤V)∥2→∞ } ≤ C B′ σr (M) , (ii) ∥ ˆM− M∥2→∞ ≤ CκB′, ∥ ˆP − P∥1→∞ ≤ Cκ√n νmin B′, (iii) ∥ ˆM− M∥∞ ≤ C ( ∥M∥2→∞+κB′ σr(M) + κµ√ r n ) B′, (iv) ∥ ˆP − P∥∞ ≤ C B′ νmin [ √nκ∥M∥∞ νmin + ∥M∥2→∞+κB′ σr (M) + κµ√ r n ] , where (iv) holds if in addition ∥ ˆM − M∥1→∞ ≤ 1 2 νmin. Proof. The ﬁrst statement of the theorem follows from Lemmas 21, 22 and 32, whereas the next four bounds follow from (i) and the bounds presented in Appendix F. As for the generative model, Theorem 5 is a direct consequence of Theorem 16, and it is obtained by simplifying the term B′ to B. Corollary 6 is also easily derived from Theorem 16. B.4 An additional lemma Lemma 17.Let M be matrix and m× nmatrix with rank r, incoherence parameter µ >0, and condition number κ> 0. Then, we have ∥M∥∞ ≤ σ1(M) µ2r √nm ≤ σr(M) µ2κr√nm. Proof of Lemma 17. For all (i,j) ∈ [m] × [n], we have |Mi,j| = ⏐ ⏐ ⏐ ⏐ ⏐ r∑ ℓ=1 σℓ(M)ui,ℓvj,ℓ ⏐ ⏐ ⏐ ⏐ ⏐≤ σ1(M) r∑ ℓ=1 |ui,ℓvj,ℓ| ≤ σ1(M)∥U∥2→∞∥V∥2→∞ ≤ σ1(M) µr √nm ≤ σr(M) µκr√nm. The ﬁrst inequality follows from the triangular inequality and the fact that σ1(M) ≥ σ2(M) ≥ · · · ≥ σr(M). The second inequality follows from Cauchy-Schwarz inequa lity. The last inequalities follow by deﬁnition of the incoherence parameter and of the c ondition number. 21C Comparison inequalities and the Poisson approximation ar gument In this section, we state and prove the results related to the Poisson approximations used to handle the noise correlations in the data. W e start by presenting so me of the key tools behind the Poisson approximation argument. This argument comes in the form of c omparison inequalities. The latter are applied and speciﬁed ﬁrst to Model I (reward matrix estim ation), and then to Model II (transition matrix estimation). C.1 Preliminaries on Poisson approximation The Poisson approximation argument comes in the form of an in equality, which is presented in Lemma 19. However, the key idea behind the argument is, roughly speak ing, the equality in distribution between a multinomial distribution with t trials and n outcomes, and the joint distribution of n in dependent Poisson random variables with properly chosen parameters, conditioned on some particular event. This equality of dist ribution is powerful for our purposes precisely because of the independence between the Poisson r andom variables. Below , we present Lemma 18 that represents this idea. Lemma 18. (Heterogeneous analogue of Theorem 5.2 in [ 59]) Let Y(t) i ∼ Poisson(tpi), i = 1 ,...,n , be independent random variables with ∑ n i=1 pi = 1 . Moreover , let (Z(t) 1 ,Z(t) 2 ,...,Z (t) n ) ∼ Multinomial(t,(p1,...,p n)). Then distribution of (Y(t) 1 ,...,Y (t) n ) conditioned on ∑ n i=1 Y(t) i = sis the same as (Z(s) 1 ,...,Z (s) n ) irrespective of t. Proof. The proof follows similar steps as the proof of Theorem 5.2 in [ 59], but we provide it here for the sake of completeness. First, note that from the deﬁni tion of multinomial distributions: P((Z(s) 1 ,...,Z (s) n ) = ( a1,...,a n)) = s! a1! · · · an!pa1 1 · · · pan n (10) if ∑ n i=1 ai = s, and 0 otherwise. Since the sum of Poisson random variables is a Poi sson random variable with parameter equal to the sum of parameters of the initial random variables, we get that the random variable ∑ n i=1 Y(t) i ∼ Poisson(∑ n i=1 tpi) = Poisson( t). Hence we have: P ( (Y(t) 1 ,...,Y (t) n ) = ( a1,...,a n) ⏐ ⏐ ⏐ ⏐ ⏐ n∑ i=1 Y(t) i = s ) = P((Y(t) 1 ,...,Y (t) n ) = ( a1,...,a n)) P(∑ n i=1 Y(t) i = s) = s! exp(−t)ts n∏ i=1 (tpi)ai exp(−tpi) ai! = s! a1! · · · an!pa1 1 · · · pan n (11) where in the last step we used the independence of Y(t) i ’s and ∑ n i=1 ai = s. Note that equations ( 10) and ( 11) are exactly the same, which concludes the proof. Lemma 19. Consider the setting of Lemma 18 and let f : Rp → R+ be any non-negative function. Then: E [ f(Z(t) 1 ,...,Z (t) p ) ] ≤ e √ tE [ f(Y(t) 1 ,...,Y (t) p ) ] 22Proof. The proof is essentially the same as that of Theorem 5.7 in [ 59] with the exception that we use Lemma 18 instead of Theorem 5.6 in [ 59] and we repeat it here for the sake of completeness. E[f(Y(t) 1 ,...,Y (t) p )] = ∞∑ k=0 E [ f(Y(t) 1 ,...,Y (t) p ) ⏐ ⏐ ⏐ p∑ i=1 Y(t) i = k ] P ( p∑ i=1 Y(t) i = k ) ≥ E [ f(Y(t) 1 ,...,Y (t) p ) ⏐ ⏐ ⏐ p∑ i=1 Y(t) i = t ] P ( p∑ i=1 Y(t) i = t ) = E[f(Z(t) 1 ,...,Z (t) p )]P ( p∑ i=1 Y(t) i = t ) (12) where in the second line we used non-negativeness of f, and in the last line we used Lemma 18. Now , since ∑ p i=1 Y(t) i is a Poisson random variable with mean twe have P(∑ p i=1 Y(t) i = t) = tt exp(−t) t! and using simple inequality t! <e √ t( t e)t we can rewrite inequality ( 12) as follows: E[f(Y(t) 1 ,...,Y (t) p )] ≥ E[f(Z(t) 1 ,...,Z (t) p )] 1 e √ t (13) which gives statement of the lemma. C.2 Poisson approximation for reward matrices – Model I W e recall from Section 4 that the deﬁnition of the empirical reward matrix ˜M is given as follows ∀(i,j) ∈ [n] × [m], ˜Mi,j = nm T T∑ t=1 (Mit,jt + ξt)1{(it,jt)=(i,j)} (14) where (it,jt) are sampled uniformly at random from [n] × [m]. Due to independence between (i1,j1),..., (iT,jT) and ξ1,...,ξ T, we note that the observation model ( 14) is equivalent in distribution to the following one ∀(i,j) ∈ [n] × [m], ˜Mi,j = nm T Zi,j∑ t=1 (Mi,j + ξ′ i,j,t) where we for all (i,j) ∈ [n] × [j], (ξ′ i,j,t)t≥1 is a sequence of i.i.d. random variables copies, say of ξ1, and Zi,j = T∑ t=1 1{(it,jt)=(i,j)}. Observe that Z = ( Zi,j)(i,j) is a multinomial random variable whose parameters are deﬁne d by the fact that for all t ∈ [T], P((it,jt) = ( i,j)) = 1 /nm). W e denote P the joint probability of the entries of Zand sequences (ξi,j,t)t≥1, (i,j) ∈ [n] × [m]. Compound Poisson random matrix model. W e deﬁne a random matrix Y ∈ Rn×m generated by a Poisson model as follows: Yi,j ∼ Poisson (T/nm) , (i,j) ∈ [n] × [m] and denote P′ the joint probability of the entries of Y and the sequences (ξ′ i,j,t)t≥1, for (i,j) ∈ [n] × [m]. W e may then consider the matrix model Xi,j = Yi,j∑ t=1 (Mi,j + ξ′ i,j,t). (15) W e note that the entries of the matrix Xare distributed according to compound Poisson distributio ns. Below , we precise the Poisson approximation argument for th e reward matrix model. 23Lemma 20 (Poisson Approximation) . Let (Ω ,F,P) (resp. (Ω ,F,P′)) be the probability space under the matrix-plus-noise model (14) (resp. (15)). Then for any event E ∈ F , we have P (E) ≤ e √ TP′ (E) . Proof of Lemma 20. For convenience, we denote X = (( ξi,j,t)t≥1)i,j∈[n]×[m]. W e set f(Z,X) = 1{E} . Thanks to Lemma 19, given that Zis independent of X, we have E [f(Z,X)|X] ≤ e √ TE [f(Y,X)|X] . W e further take the expectation on X and write P(E) = E [f(Z,X)] ≤ e √ TE [f(Y,X)] = e √ T P′(E). C.3 Approximations for transition matrices – Model II W e restrict our attention to the forward model, Model II(b). The results for the generative model are simpler and can be easily deduced from those for the forwa rd model. Recall from Section 3.3 deﬁnition of matrix ˜M(k) and in the following discussion we ﬁx value of k ∈ [τ]. Deﬁne a matrix N = Tτ˜M(k) and note that it is equal to: Ni,j = Tτ −1∑ l=0 1{(xk+lτ ,xk+1+lτ )=(i,j)}, i,j = 1 ,2,...,n (16) Furthermore, let P1 be joint probability distribution of entries of N. C.3.1 Multinomial approximation Here we deﬁne a matrixZ ∈ Rn×n with entries: Zi,j = Tτ −1∑ t=0 1{(it,jt)=(i,j)}, i,j = 1 ,2,...,n, (17) where P((it,jt) = ( i,j)) = νiPi,j independently over i,j ∈ [n] and t ∈ [Tτ]. Denote by P2 joint probability distribution of entries of Z. Then we have: Lemma 21. Let N and Z be matrices obtained under the models ( 16) and (17), respectively. Then, for any subset Z of {z∈ Nn2 : ∑ (i,j) zi,j = Tτ}, we have P(N ∈ Z ) ≤ 3P(Z ∈ Z ). Proof. Note that by subsampling as explained in Section 3.3, for each kwe obtain a Markov chain with transition kernel Pτ((y,y′)|(x,x′)) = Pτ(y|x′)P(y′|y) and initial distribution ν(k) 0 (x,x′) = ν(k) 0 (x)P(x′|x) with ν(1) 0 (x) = ν0(x) and ν(k) 0 (x) = ∑ y∈[n] ν0(y)Pk−1(x|y) for k= 2 ,...,τ. Moreover, all chains share the same stationary distributio n given by Π ∈ Rn×n with Π x,x′ = ν(x)P(x′|x), x,x′ ∈ [n]. Now , recall deﬁnition of τ from Theorem 5 and note that according to Lemma 10 with δ= 1 4 and ε= νmin/(eT) we have τ(ε) ≤ τ and thus: max 1≤i≤n ∥Pτ i,: − ν⊤∥1 ≤ νmin eT . (18) Now , let z = ( zi,j)n i,j=1 ∈ { z∈ Nn2 : ∑ i,jzi,j = Tτ} be a tuple of ﬁxed integers. Deﬁne a set: S(z) := {(a2l+1,a2l+2)Tτ −1 l=0 ∈ ([n] × [n])Tτ : Tτ −1∑ l=0 1{(a2l+1,a2l+2)=(i,j)} = zi,j,∀i,j ∈ [n]} 24and note that |S(z)| = Tτ!(∏ n i,j=1 zi,j!)−1. By deﬁnition of Markovian and multinomial models, we have: P(N = z) = ∑ ν(k) 0 (xk−1,xk) Tτ −1∏ l=1 Pτ((xk−1+lτ,xk+lτ)|(xk−1+(l−1)τ,xk+(l−1)τ)) where the sum is over (xk−1+lτ,xk+lτ)Tτ −1 l=0 ∈ S (z), and P(Z = z) = Tτ! ∏ n i,j=1 zi,j! n∏ i,j=1 Π zi,j i,j . Now we ﬁx arbitrarily one of the summands in the expression fo r P(N = z) and note that: ⏐ ⏐ ⏐ν(k) 0 (xk−1,xk) Tτ −1∏ l=1 Pτ((xk−1+lτ,xk+lτ)|(xk−1+(l−1)τ,xk+(l−1)τ)) − n∏ i,j=1 Π zi,j i,j ⏐ ⏐ ⏐ = (Tτ −1∏ l=0 P(xk+lτ|xk−1+lτ) )⏐ ⏐ ⏐ν(k) 0 (xk−1) Tτ −1∏ l=1 Pτ(xk−1+lτ|xk+(l−1)τ) − Tτ −1∏ l=0 ν(xk−1+lτ) ⏐ ⏐ ⏐ ≤ (Tτ −1∏ l=0 P(xk+lτ|xk−1+lτ) )(Tτ −1∏ l=0 (ν(xk−1+lτ) + ǫ) − Tτ −1∏ l=0 ν(xk−1+lτ) ) ≤   n∏ i,j=1 Π zi,j i,j   Tτ∑ j=1 ( ǫ νmin )j(Tτ j ) ≤   n∏ i,j=1 Π zi,j i,j   Tτ∑ j=1 (eTτǫ jνmin )j ≤ 2   n∏ i,j=1 Π zi,j i,j   where in ﬁrst inequality we used Equation ( 18), where we then used the bound on binomial coefﬁcients (Tτ j ) ≤ (eTτ/j)j, and where in the last inequality, we used deﬁnition of ǫ. Since this upper bound holds irrespective of the summand, we deduc e that: |P(N = z) − P(Z = z)| ≤ 2 Tτ!∏ n i,j=1 zi,j!   n∏ i,j=1 Π zi,j i,j  = 2 P(Z = z). Now , let Z be any subset of {z∈ Nn2 : ∑ (i,j) zi,j = Tτ}. Then we have: P(N ∈ Z ) = ∑ z∈Z P(N = z) ≤ 3 ∑ z∈Z P(Z = z) = 3 P(Z ∈ Z ) as claimed in the lemma. C.3.2 Poisson approximation W e deﬁne a matrix Y ∈ Rn×n generated by the Poisson model as follows: Yi,j ∼ Poisson(TτMi,j), i,j = 1 ,2,...,n. (19) W e show that rare random events occur with approximately equ al probability for the Poisson and multinomial models: Lemma 22.Let Z and Y be matrices obtained under the models ( 17) and (19), respectively. Then for any Z ⊂ Nn2 , we have P(Z ∈ Z ) ≤ e√TτP(Y ∈ Z ). Proof. Proof of the lemma is a straightforward consequence of Lemma 19 with parameters Tτ, n2 and f = 1{Z} . 25D Concentration of matrices with Poisson and compound Poiss on entries As mentioned in Appendix C, our analysis relies on a Poisson approximation argument. A s a result, we will require tight concentration bounds for random matri ces with entries distributed according to compound Poisson distributions (when estimating the rew ard matrix) and Poisson distributions (when estimating the transition matrices). In § D.1, we present a few simple facts about Poisson and compound Poisson random variables, together with some othe r useful tools. In § D.2, we present two concentration results, required for the model with compoun d Poisson entries. Similarly, in § D.3, we present two concentration results, required for the model w ith Poisson entries. These concentration results will be extensively used in the forthcoming analysi s for the subspace recovery. It is worth noting that our results in § D.3 are sharper than those in § D.2 thanks to Bennett’s inequality. As a consequence, our results for estimating the reward matr ix exhibit a dependence in log3(n+ m) while in the estimation of the transitions, our results exhi bit a dependence in log2(n) and even log(n) in some regimes. D.1 Preliminaries W e ﬁrst present Theorem 23, which can be seen as a version of matrix Bernstein inequalit y. The theorem is borrowed from [ 60] and relies on a truncation trick. The proofs of our concentr ation results in § D.2 and § D.3 rely on this theorem. Theorem 23. (Proposition A.3 in [ 60]) Let {Zt}T t=1 be a sequence of m×nindependent zero-mean real random matrices. Suppose that for all 1 ≤ t≤ T, (i) P (∥Zt∥ ≥ β) ≤ p, and (ii)  E[Zt1{∥Zt∥>β}]  ≤ q, (20) hold for some quantities p∈ (0,1), and q≥ 0. Furthermore, assume there exists v≥ 0, such that (iii) max {     T∑ t=1 E [ ZtZ⊤ t ]     ,      T∑ t=1 E [ Z⊤ t Zt ]      } ≤ v. (21) Then, for all u> 0, P (     T∑ t=1 Zt     ≥ Tq + u ) ≤ Tp + (n+ m) exp ( − u2/2 v+ βu/3 ) . (22) T o apply Theorem 23, we need control of the tails of the entries of the random matr ix we study. In the case of Poisson entries, we will simply use the following standard fact about Poisson random variables. It is a simple consequence of Bennett’s inequali ty [ 61]. Lemma 24. Let Y be a P oisson random variable with mean λ. Then for , all θ ∈ R, we have E[eθY] ≤ exp(λ(eθ − 1)). Furthermore, we have for all u> 0 P(|Y − λ| >u) ≤ 2 exp (−λh(u/λ)) ≤ 2 exp ( − u2/2 λ+ u/3 ) , where h(u) = (1 + u) log(1 + u) − u. In the case of compound Poisson entries, we do not have any res ult similar to Bennett’s inequality. Instead, we derive a Bernstein-type concentration result o n these random variables. Lemma 25. Let (ξt)t≥1 be a sequence of zero-mean, σ2-subgaussian, i.i.d. random variables. Let Y be a P oisson random variables with mean λ. Let M be a positive constant. Then, the moment generating function of the compound P oisson random variabl e Z = ∑ Y i=1(M + ξi) satisﬁes the following: ∀u> 0, P(|Z− λM| >u) ≤ 2 exp ( − min ( u2 16eλL2 , u 4L )) , E [ |Z− λM|2] ≤ 18λL2, where L= max( M,σ). 26Proof of Lemma 25. First, we upper bound the moment generating function of ∑ Y i=10(M+ ξi). Let θ> 0, we have IZ(θ) ≜ E [ eθ(∑ Y i=1(M+ξi)) ] ≤ √ E [e2θMY ] E[e2θ∑ Y i=0 ξi ] ≤ exp (λ(e2θM − 1)2 )√ E[e2θ∑ Y i=0 ξi ] ≤ exp (λ2 ( (2θM)2e2θM + 2θM ))√ E[e2θ∑ Y i=0 ξi ], where in the ﬁrst inequality, we use Cauchy-Schwarz inequal ity, in the second inequality, we use the well known bound on the moment generating function of a Po isson random variable (if Y is a Poisson random variable with mean λ, then for all θ> 0, E[eθY] ≤ exp(λ(eθ − 1))), and in the last inequality, we use the elementary fact that ex − 1 ≤ x2ex + xfor all x∈ R. Next, we have E [ e2θ∑ Y i=1 ξi ] = E [ ∞∑ k=1 1{Y=k} exp ( 2θ k∑ i=1 ξi )] = ∞∑ k=1 P(Y = k)E [ exp ( 2θ k∑ i=1 ξi )] ≤ ∞∑ k=1 P(Y = k) exp(2kθ2σ2) ≤ exp ( λ(e2θ2σ2 − 1) ) ≤ exp ( λ ( 2θ2σ2e2θ2σ2 )) , where we use the fact that the ξi are σ2-subgaussian r.v., and the elementary inequality ex2 − 1 ≤ x2ex2 for all x∈ R. W e conclude that for all θ >0, IZ(θ) ≤ exp ( λ ( 2θ2M2e2θM + 2θ2σ2e2θ2σ2 ) + λθM ) . Next, we introduce L= max( M,σ). Then, for all α> 0, we deduce that IZ(θ) ≤ exp ( 2λθ2L2 ( eα + eα2 ) + λθM ) , ∀|θ| ≤ α 2L. By Markov inequality, and ﬁxing α= 1 , we have P(Z− λM >u) ≤ inf |θ|≤1/(2L) IZ(θ)e−λθM−θu ≤ exp ( − min ( u2 16eλL2 , u 4L )) . Similarly, we have P(λM − Z >u) ≤ exp ( − min ( u2 16eλL2 , u 4L )) . The ﬁnal tail bound follows from a union bound. Finally, stra ightforward computations yield an upper bound on E[|λM − Z|2]. Indeed, we have E[|λM − Z|2] ≤ 2E[|Y − λ|2]M2 + 2E   (Y∑ i=1 ξi )2 ≤ 2λM2 + 16λσ2 ≤ 18λL2. D.2 Random matrices with compound Poisson entries W e list below the two main concentration results that we need for the forthcoming analysis. In Proposition 26, we provide a high probability guarantee on the error betwee n the empirical mean reward matrix and the true matrix in operator norm. In Propos ition 27, we establish another concentration result that will be instrumental in the subsp ace recovery analysis. The proofs of the two results are similar with slight differences and they bot h rely on Theorem 23. The proofs are presented at the end of this subsection. 27Proposition 26. Under the random matrix model (15) with compound P oisson entries, for all δ ∈ (0,1), for all T ≥ 13(n+ m) log3 ((n+ m)/δ), the following statement ∥ ˜M − M∥ ≤ 36 √ 2L √ nm T (√ (n+ m) log (n+ m δ ) + log3/2 (n+ m δ )) holds with probability at least 1 − δ, where L= max( ∥M∥∞,σ). Proposition 27. Let A be a m× 2r nonrandom matrix, and B be a n× 2r nonrandom matrix. Then, under the random matrix model (15) with compound P oisson entries, and denoting L = max(∥M∥∞,σ), we have: (i) for all ℓ∈ [m], for all δ∈ (0,1), for all T ≥ mlog3(en/δ), the following event ∥(˜Mℓ,: − Mℓ,:)A∥ ≤ 73 √ 2L∥A∥2→∞ √ nm T (√ nlog (en δ ) + log3/2 (en δ )) (23) holds with probability at least 1 − δ; (ii) for all k∈ [n], for all δ∈ (0,1), for all T ≥ nlog3(em/δ), the following event ∥(˜M:,k − M:,k)⊤B∥ ≤ 73 √ 2L∥B∥2→∞ √ nm T (√ mlog (em δ ) + log3/2 (em δ )) (24) holds with probability at least 1 − δ. Proof of Proposition 26. T o simplify the notation, introduce the matrices Zi,j = ( ˜Mi,j−Mi,j)eie⊤ j, for all (i,j) ∈ [m] × [n], λ= T/mn, and L= max( ∥M∥∞,σ). W e remark that we can write ˜M − M = ∑ (i,j)∈[m]×[n] Zi,j. Starting from the above expression, we will apply Theorem 23 to obtain the desired result. First, we note that for all (i,j) ∈ [m] × [n], ∥Zi,j∥ = |˜Mi,j − Mi,j| and ˜Mi,j − Mi,j is a centered and normalized compound Poisson random variable. Thus, we have by Lemma 25, for all δ ∈ (0,1), P(∥Zi,j∥ >β ) ≤ δ/(2n2m2), where we deﬁne β = 4 Lmax (√ e λlog (4n2m2 δ ) , 1 λlog (4n2m2 δ )) , ≤ 4Lmax (√ 4e λ log (n+ m δ ) ,4 λlog (n+ m δ )) . Moreover, we have E [ ∥Zi,j∥1{∥Zi,j]∥ >β } ] ≤ √ E [∥Zi,j∥2] E [ 1{∥Zi,j]∥ >β } ] ≤ √ E[|˜Mi,j − Mi,j|2]P(∥Zi,j∥ >β) ≤ √ 9L2δ λn2m2 , where the ﬁrst inequality follows from Cauchy-Schwarz ineq uality, the second inequality follows from the expression of Zi,j, and the third inequality follows from Lemma 25. Next, we have      ∑ (i,j)∈[m]×[n] E [ Zi,jZ⊤ i,j ]       = ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ ∑ i∈[m]  ∑ j∈[n] E [ ( ˜Mi,j − Mi,j )2]  eie⊤ i ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ = max i∈[m] ∑ j∈[n] E [ ( ˜Mi,j − Mi,j )2] ≤ 18nL2 λ . 28By symmetry, we obtain similarly       ∑ (i,j)∈[m]×[n] E [ Z⊤ i,jZi,j ]       ≤ 18mL2 λ . Let us set v= 18( n∧ m)L2/λ. W e conclude using Theorem 23 that, for all u> 0, P ( ∥ ˜M − M∥ > √ 9L2δ λ + u ) ≤ δ 2(nm) + (n+ m) exp ( − u2/2 v+ βu/3 ) ≤ δ 2(nm) + (n+ m) exp ( − 1 4 min (u2 v ,3u β )) . W e re-parametrize by choosing δ= 2( n+ m) exp(−(1/4) min(u2/v,3u/β)), and write P ( ∥ ˜M− M∥ > 3L √ δ√ λ + u ) ≤ δ (25) with u= max (√ 4vlog (2(n+ m) δ ) ,4β 3 log (2(n+ m δ )) ≤ max (√ 8vlog (n+ m δ ) ,8β 3 log (n+ m δ )) . By inspecting the deﬁnition of βand v, we note that under the condition λ= T nm ≥ 45 34 1 n∧ mlog3 (n+ m δ ) (26) then u≤ max (√ 8vlog (n+ m δ ) ,16L √ 2e 3 √ λ log3/2 (2(n+ m) δ )) ≤ L√ λ max (√ 3242(n∧ m) log (n+ m δ ) ,422√e 3 log (n+ m δ )) . After using the upper bound on u in ( 25), and after upper bounding δ by 1, we obtain, under the condition ( 26), ∥ ˜M − M∥ > L√ λ ( 3 + 12 √ 2(n+ m) log (n+ m δ ) + 43√e 3 log3/2 (n+ m δ )) > L√ λ ( 36 √ 2(n+ m) log (n+ m δ ) + 36 log 3/2 (n+ m δ )) > 36 √ 2L√ λ (√ (n+ m) log (n+ m δ ) + log3/2 (n+ m δ )) with probability at most δ. Noting that a stricter condition than ( 26) is T ≥ 13(n+ m) log3 (n+ m δ ) , we complete the proof. 29Proof of Proposition 27. T o simplify the notation, let us denote Zj = ( ˜Mℓ,j − Mℓ,j)Aj,:, λ = mn/T, and L= max( ∥M∥∞,σ). W e remark that we can write (˜Mℓ,: − Mℓ,:)A= ∑ j∈[n] (˜Mℓ,j − Mℓ,j)Aj,: = ∑ j∈[n] Zj. Starting from the above expression, we will apply 23 to obtain the desired result. First, we note that for all j ∈ [n], ∥Zj∥ = |˜Mℓ,j−Mℓ,j|∥Aj,:∥, and ˜Mℓ,j−Mℓ,j is a centered and normalized compound Poisson random variable. Thus, we have by Lemma 25, for all δ∈ (0,1), P (∥Zj∥ >∥A∥2→∞β) ≤ P (∥Zj∥ >∥Aj,:∥β) ≤ δ/(2n2), where we deﬁne β = 4 Lmax (√ e λlog (4n2 δ ) ,1 λlog (4n2 δ )) ≤ 4Lmax (√ 2e λ log (en δ ) ,2 λlog (en δ )) . Moreover, we have E [ ∥Zj∥1{∥Zj ∥>∥A∥2→∞β} ] ≤ √ E[∥Zj∥2]P (∥Zj∥ >∥A∥2→∞β) ≤ ∥ A∥2→∞ √ E[∥ ˜Mℓ,: − Mℓ,:∥2]δ 2n2 ≤ ∥ A∥2→∞ √ 9L2δ λn2 , where in the ﬁrst inequality, we use Cauchy-Schwarz inequal ity, and in the third inequality, the result of Lemma 25 to upper bound the variances. Next, we have      E  ∑ j∈[n] ZjZ⊤ j         ≤ ∑ j∈[n] E [ (˜Mℓ,j − Mℓ,j)2 ] ∥Aj,:∥2 ≤ 18L2∥A∥2 F λ ≤ 18L2n∥A∥2 2→∞ λ , where we simply used the expressions of Zj, j ∈ [n], the triangular inequality, and Lemma 25 to upper bound the variances. Similarly, we have       E  ∑ j∈[n] Z⊤ j Zj         ≤ 18L2n∥A∥2 2→∞ λ . W e set v= 18 L2n∥A∥2 2→∞/λ. Now we are ready to apply Theorem 23. W e get: P ( ∥(˜Mℓ,: − Mℓ,:)A∥ >∥A∥2→∞ √ 9L2δ λ + u ) ≤ δ 2n + nexp ( − 1 4 min (u2 v , 3u ∥A∥2→∞β )) . W e re-parametrize by choosing δ= 2 nexp(−(1/4) min(u2/v,3u/(∥A∥2→∞β))) and we write P ( ∥(˜Mℓ,: − Mℓ,:)A∥ >∥A∥2→∞ √ 9L2δ λ + u ) ≤ δ with u= max (√ 4vlog (2n δ ) ,4∥A∥2→∞β 3 log (2n δ )) ≤ max (√ 4vlog (en δ ) ,4∥A∥2→∞β 3 log (en δ )) . 30By inspecting the deﬁnition of βand v, we note that when the condition λ= T mn ≥ 43 34 1 nlog3 (en δ ) (27) holds, then u≤ max (√ 4vlog (en δ ) ,16 √ 2eL∥A∥2→∞ 3 √ λ log3/2 (en δ )) ≤ L∥A∥2→∞√ λ max (√ 2332nlog (en δ ) ,16 √ 2e 3 log3/2 (en δ )) ≤ 36 √ 2L∥A∥2→∞√ λ max (√ nlog (en δ ) ,log3/2 (en δ )) . (28) After using the upper bound in ( 28), and upper bounding δby 1, we obtain that, under the condition (27), ∥(˜Mℓ,: − Mℓ,:)A∥ > 73 √ 2L∥A∥2→∞√ λ (√ nlog (en δ ) + log3/2 (en δ )) holds with probability at most δ. W e can also reﬁne the condition ( 27) as follows T ≥ mlog3 (en δ ) . This concludes the proof of the statement ( 23) in the proposition. The statement ( 24) follows similarly. Therefore, we omit it. D.3 Random matrices with Poisson entries Recall from Section B.2, the deﬁnition of the function gδ from ( 9) and that A = 1√ T √ ∥M∥1→∞ + ∥M⊤∥1→∞. First we show the following lemma that provides an upper bou nd of the spectral norm. This lemma is used to derive Lemma 13. Lemma 28. Let Y ∈ Rn×n be a matrix with independent entries Yi,j ∼ T−1Poisson(TMij), i,j ∈ [n], and let 0 ≤ δ≤ 1. Then, w .p. at least 1 − δ, ∥Y − M∥ ≤ CA + C Tgδ(TM) √ log( ne δ ). Proof. The proof follows from that of Lemma 29 and that of Lemma 4 in [ 62], which is based on a spectral bound from [ 63]. W e use that the random variables |Yi,j − Mi,j| concentrate well around L = L11{∃ℓ:T∥Mℓ,:∥∞≤1} + L21{∀ℓ:T∥Mℓ,:∥∞>1} where L1 = 4 T−1 log−1(1 + ( T∥M∥∞)−1 ∧ nδ−1) log( ne δ ) and L2 = 4 √ T−1∥M∥∞ log ( T∥M∥∞ ne δ ) using exactly the same argument as in the ﬁrst step of Lemma 29. Moreover, we use upper bound on |E[(Yi,j − Mi,j)1{|Yi,j −Mi,j |<L}]| derived in the second step of Lemma 29. W e also derive upper bounds in the ℓ2→∞ norm. These bounds are used in the analysis of the singular subspace recovery in Lemma 32, and therefore in the proofs of Theorems 3 and 5. Lemma 29. Let Y ∈ Rn×n be a matrix with independent entries Yi,j ∼ T−1Poisson(TMij), i,j ∈ [n], for an arbitrary integer T >0. Let 0 ≤ δ ≤ 1. Then, for any 1 ≤ l ≤ nand any matrix A∈ Rn×p, with p≤ n, and independent of Yl,: we have, if T∥Ml,:∥∞ ≤ 1, ∥(Yl,: − Ml,:)A∥ ≲ ∥A∥F √ ∥Ml,:∥∞ log (ne δ ) √ T + ∥A∥2→∞ log2 (ne δ ) Tlog(1 + ( T∥Ml,:∥∞)−1 ∧ nδ−1)) else if T∥Ml,:∥∞ >1, ∥(Yl,: − Ml,:)A∥ ≲ ∥A∥F √ ∥Ml,:∥∞ log (ne δ ) √ T + ∥A∥2→∞ √ ∥Ml,:∥∞√ T log ( T∥Ml,:∥∞ ne δ ) log (ne δ ) with probability at least 1 − δ/n. 31Proof of Lemma 29. The lemma is an application of the truncated matrix Bernstei n theorem i.e. Theorem 23. In this theorem, T corresponds to nin Lemma 29, nin Theorem 23 corresponds to 1 in Lemma 29, and min Theorem 23 corresponds to nin Lemma 29. First note that for any l, we have (Yl,: −Ml,:)A= ∑ n i=1(Yl,i−Ml,i)Ai,:. Moreover, since each of these nsummands are independent, zero-mean random vectors, we can identify Zi’s from Theorem 23 with (Yl,i − Ml,i)Ai,: ∈ R1×n for i∈ [n]. T o apply Theorem 23, we need to verify its assumptions. This is done below . Step 1: Showing (i) in (20) First, recall Bennett’s concentration inequality from Le mma 24, which in our case implies that for any i,j ∈ [n]: P(|Yi,j − Mi,j| ≥ tMi,j) ≤ 2 exp (−h(t)TMi,j) . (29) Note that ∥Zi∥ in Theorem 23 in our case corresponds to: ∥(Yl,i − Ml,i)Ai,:∥ = |Yl,i − Ml,i|∥Ai,:∥ ≤ | Yl,i − Ml,i|∥A∥2→∞. W e consider two different cases: 1. T∥Ml,:∥∞ ≤ 1: W e let β1 = 4 T−1∥A∥2→∞ log−1(1 + ( T∥Ml,:∥∞)−1 ∧ nδ−1) log( ne δ ) and note that h(t) ≥ 1 2 tlog tfor t≥ 1. Thus, from Equation ( 29), we have: P ( |Yl,i − Ml,i| ≥ β1 ∥A∥2→∞ ) ≤ 2 exp ( − 2 log( ne δ ) log−1(1 + ( T∥Ml,:∥∞)−1 ∧ nδ−1) · log ( 4 log( ne δ ) T∥Ml,:∥∞ log(1 + ( T∥Ml,:∥∞)−1 ∧ nδ−1) )) ≤ δ 2n2 . where, in the second inequality, we show using simple algebr a that log−1(1 + ( T∥Ml,:∥∞)−1 ∧ nδ−1) log ( 4 log( ne δ ) T∥Ml,:∥∞ log(1+(T∥Ml,:∥∞)−1∧nδ−1) ) ≥ 1 for δ≤ 1 and T∥Ml,:∥∞ ≤ 1. 2. T∥Ml,:∥∞ > 1: Here we deﬁne β2 := 4 ∥A∥2→∞ √ T−1∥Ml,:∥∞ log ( T∥Ml,:∥∞ ne δ ) . Then, according to Equation ( 29) and the approximation h(t) ≥ min{t2/4,t} for t≥ 0, we have: P ( |Yl,i − Ml,i| ≥ β2 ∥A∥2→∞ ) ≤ 2 exp ( − 4 min { log2(T∥Ml,:∥∞ ne δ ), √ T∥Ml,:∥∞ log(T∥Ml,:∥∞ ne δ ) }) ≤ 2 exp ( − 4 log(T∥Ml,:∥∞ ne δ ) ) ≤ 1 2T∥Ml,:∥∞ δ n2 . where, in the second inequality, we used that δ ≤ 1 and T∥Ml,:∥∞ > 1. Finally, we deﬁne β = β11{T∥Ml,:∥∞≤1} + β21{T∥Ml,:∥∞>1} and p = δ 2n1{T∥Ml,:∥∞≤1} + 1 2T∥Ml,: ∥∞ δ n1{T∥Ml,:∥∞>1} (since we took union bound over i∈ [n]). Step 2: Showing (ii) in (20) In our case the l.h.s. corresponds to ∥E[(Yl,i − Ml,i)Ai,:1{∥(Yl,i−Ml,i)Ai,:∥>β}]∥ = ∥E[(Yl,i − Ml,i)Ai,:1{∥(Yl,i−Ml,i)Ai,:∥≤β}]∥ which can be upper bounded by ∥A∥2→∞|E[(Yl,i − Ml,i)1{|Yl,i−Ml,i|≤ β ∥ Ai,:∥ }]|. For some integers κmin,κmax, let Yl,i ∈ 1 T[κmin,κmax] be interval of Yl,i for which indicator 1{|Yl,i−Ml,i|≤ β ∥ Ai,:∥ } is active and note that this is a superset of interval for which 1{|Yl,i−Ml,i|≤ β ∥ A∥ 2→∞ } is active. Then from the deﬁnition of Poisson random variables and the bounds derive d previously, we obtain: ⏐ ⏐ ⏐E[(Yl,i − Ml,i)1{|Yl,i−Ml,i|≤ β ∥ Ai,:∥ }] ⏐ ⏐ ⏐= 1 T ⏐ ⏐ ⏐ κmax∑ k=κmin (k− TMl,i)exp(−TMl,i)(TMl,i)k k! ⏐ ⏐ ⏐ = Ml,i ⏐ ⏐ ⏐ κmax−1∑ k=κmin−1 exp(−TMl,i)(TMl,i)k k! − κmax∑ k=κmin exp(−TMl,i)(TMl,i)k k! ⏐ ⏐ ⏐ ≤ Ml,i(P(TYl,i = κmin − 1) + P(TYl,i = κmax)) ≤ 2δ Tn2 min{T∥Ml,:∥∞,1}, 32where we assumed that κmin ≥ 1, otherwise we keep just the second probability term above. T hus, using previous two inequalities, we have: ∥E[(Yl,i − Ml,i)Ai,:1{∥(Yl,i−Ml,i)Ai,:∥>β}]∥ ≤ ∥ A∥2→∞ 2δ Tn2 min{T∥Ml,:∥∞,1} Step 3: Showing (iii) in (21) Using our deﬁnition Zi = ( Yl,i − Ml,i)Ai,: ∈ R1×n, we have that ZiZ⊤ i = ( Yl,i − Ml,i)2∥Ai,:∥2, Z⊤ i Zi = ( Yl,i − Ml,i)2A⊤ i,:Ai,:. Since Aand Yl,: are independent, we have: ∥ n∑ i=1 E[ZiZ⊤ i ]∥ = n∑ i=1 E[ZiZ⊤ i ] = n∑ i=1 ∥Ai,:∥2E(Yl,i − Ml,i)2 ≤ ∥ A∥2 Fmax i E(Yl,i − Ml,i)2 and ∥ n∑ i=1 E[Z⊤ i Zi]∥ = ∥ n∑ i=1 E(Yl,i − Ml,i)2A⊤ i,:Ai,:∥ ≤ n∑ i=1 E(Yl,i − Ml,i)2∥A⊤ i,:Ai,:∥ ≤ ∥ A∥2 Fmax i E(Yl,i − Ml,i)2. Now note that for Yl,i ∼ T−1Poisson(TMl,i), Var(Yl,i) = E(Yl,i − Ml,i)2 = T−1Ml,i. Thus, by setting v= T−1∥A∥2 F∥Ml,:∥∞, we get (iii). Plugging in all obtained quantities into Equation ( 22) ﬁnishes proof of the lemma. 33E Singular subspace recovery via the leave-one-out argumen t In this section, we present Lemma 30 and Lemma 32 providing sharp guarantees for the singular subspace recovery in two-to-inﬁnity norm. Obtaining such g uarantees is not trivial and requires the use of a rather technical analysis, namely the leave-one -out technique [ 16, 22]. However, such technique heavily relies on independence between entries o f the observed random matrix. W e use the Poisson approximation argument to address this, which i n turn requires to reproduce the leave- one-out analysis under a different random matrix observati on models (see ( 15) and ( 19)). W e wish to highlight that Farias et al. [ 64], like us, have also used the leave-one-out argument to obtain entry-wise guarantees for matrix estimation with su b-exponential noise. In our case, we use this argument as a sub-step of our analysis after performing the Poisson approximation. However, we believe that, our ﬁnal results are richer, more precise an d actually needed for our RL applications. Indeed, we are able to obtain guarantees in the norms ∥ · ∥2→∞ and ∥ · ∥1→∞ (these are not provided in [ 64]). Moreover, the entry-wise guarantees in [ 64] are only expressed in terms of the matrix dimensions m and n. Our guarantees on the other hand exhibit dependencies on th e dimensions m,n, the number of observation T and the conﬁdence level δ. Having guarantees with an explicit dependence for all T ≥ 1 and δ ∈ (0,1) is crucial in the design of our algorithm for low-rank bandits. E.1 Subspace recovery for reward matrices Lemma 30.Let δ∈ (0,1). Deﬁne: B = √ nm T (√ (n+ m) log (e(n+ m)T δ ) + log3/2 (e(n+ m)T δ )) . F or all T ≥ c(µ4κ2r2 + 1)(m+ n) log3 ( e2(m+ n)T/δ ) , the event max(∥U− ˆU( ˆU⊤U)∥,∥V − ˆV(ˆV⊤V)∥) ≤ C∥M∥∥M∥∞ σr(M)2 max(∥V∥2→∞∥U∥2→∞)B holds with probability at least 1 − δ, for some universal constants c,C >0. Proof of Lemma 30. The proof follows similar steps as that of Theorem 4.2 in [ 22], which is based on the leave-one-out analysis. Step 1: Dilation trick.In order to apply the leave-one-out analysis, we ﬁrst use a di lation trick [ 65] to reduce the problem to that of symmetric matrices. Deﬁne: S = [ 0 M M⊤ 0 ] and note that for matrix M with SVD M = UΣ V⊤, we have: S = 1√ 2 [ U U V −V ][ Σ 0 0 −Σ ] 1√ 2 [ U U V −V ] ⊤ := QDQ⊤. W e deﬁne, in a similar way, ˜S using ˜M, and let ˆQ ∈ R(n+m)×2r be the matrix of eigenvectors of the best 2r-rank approximation of ˜S. Note that: ∥Q− ˆQ( ˆQ⊤Q)∥2→∞ = max { ∥U − ˆU( ˆU⊤U)∥2→∞,∥V − ˆV(ˆV⊤V)∥2→∞ } . (30) T o keep the notation simple, we will deﬁne WˆQ = ˆQ⊤Q. Further note that ∥ ˜S− S∥ = ∥ ˜M − M∥, σ 1(S) = σ1(M), and σ2r(S) = σr(M). (31) W e start the analysis under the model ( 15) and assume that ˜M has independent entries with compound Poisson distributions. W e will eventually invoke the Poisson approximation argument via Lemma 20 to deduce the ﬁnal result. 34Step 2: Error decomposition. W e apply the decomposition in Lemma 33 to obtain: ∥Q− ˆQWˆQ∥2→∞ ≤ 1 σ2r(S) (4∥ ˜SQ∥2→∞∥E∥ σ2r(S) + ∥EQ∥2→∞ + 2∥ ˜S(Q− ˆQWˆQ)∥2→∞ ) , where we set E = ˜S− S. W e observe that when ∥E∥ ≤ σ2r(S)/2, then ∥Q− ˆQWˆQ∥2→∞ ≤ 1 σ2r(S) (4∥SQ∥2→∞∥E∥ σ2r(S) + 3∥EQ∥2→∞ + 2∥ ˜S(Q− ˆQWˆQ)∥2→∞ ) . (32) Furthermore, we also have ∥ ˜S(Q− ˆQWˆQ)∥2→∞ ≤ ∥ E(Q− ˆQWˆQ)∥2→∞ + ∥S(Q− ˆQWˆQ)∥2→∞ ≤ ∥ E(Q− ˆQWˆQ)∥2→∞ + ∥SQ∥2→∞∥ sin(Q, ˆQ)∥2 ≤ ∥ E(Q− ˆQWˆQ)∥2→∞ + ∥SQ∥2→∞∥E∥2 σ2r(S)2 ≤ ∥ E(Q− ˆQWˆQ)∥2→∞ + ∥SQ∥2→∞∥E∥ 2σ2r(S) , where the ﬁrst inequality follows from the triangular inequ ality, the second inequality follows by the relation between the two-to-inﬁnity norm and the sin theore m (see e.g., [ 15]). The third inequality follows from Davis-Kahan’s theorem. The fourth inequality follows under the condition ∥E∥ ≤ σ2r(S)/2. W e ﬁnally obtain ∥Q− ˆQWˆQ∥2→∞ ≤ 1 σ2r(S) (5∥SQ∥2→∞∥E∥ σ2r(S) + 3∥EQ∥2→∞ + 2∥E(Q− ˆQWˆQ)∥2→∞ ) . (33) Note that in the above inequality, we can control ∥E∥ using Proposition 26 and ∥EQ∥2→∞ using Proposition 27. However, the term ∥E(Q− ˆQWˆQ)∥2→∞ is not easy to control because E and (Q− ˆQWˆQ) are dependent on each other in a non-trivial way. T o control t his term, we use the leave-one-out analysis. Step 3: Leave-one-out analysis.W e deﬁne a matrix ˜S(ℓ) ∈ R(n+m)×(n+m) as follows: ˜S(ℓ) i,j = { ˜Si,j, if i̸= ℓor j ̸= ℓ Si,j, otherwise Then deﬁne ˆQ(ℓ) ∈ Rn×2r as a matrix of eigenvectors corresponding to the 2rgreatest (in absolute value) eigenvalues of matrix ˜S(ℓ). Deﬁne W˜U(ℓ) accordingly. W e have ∥E(Q− ˆQWˆQ)∥2→∞ ≤ max ℓ∈[n+m] ∥Eℓ,:(Q− ˆQ(ℓ)WˆQ(ℓ) )∥2 + ∥E∥2∥ ˆQWˆQ − ˆQ(ℓ)WˆQ(ℓ) ∥F. W e have by Proposition 26 that P (∥E∥ ≲ ∥M∥∞G) ≥ 1 − δ provided that (C1) T ≥ c1 mn m+ nlog3 (e(m+ n) δ ) and where we deﬁne G = √ mn T (√ (m+ n) log (e(m+ n) δ ) + log3/2 (e(m+ n) δ )) . Let us now introduce the event E1 as follows E1 = {∥E∥ ≤ ∥ M∥∞G} . 35Note that if the following condition holds (C2) T ≥ c2(µκr)2 ( (m+ n) log (e(m+ n) δ ) + log3 (e(m+ n) δ )) for c2 large enough then 16∥E∥ ≤ σr(M). Hence, under the event E1, using Lemma 31, we have ∥ ˆQWˆQ − ˆQ(ℓ)WˆQ(ℓ) ∥F ≤ 16∥Eℓ,: ˆQ(ℓ)WˆQ(ℓ) ∥2 + 16∥E∥∥ ˆQWˆQ∥2→∞ σ2r(M) , which further gives by triangular inequality ∥ ˆQWˆQ − ˆQ(ℓ)WˆQ(ℓ) ∥F ≤ 16∥Eℓ,:(Q− ˆQ(ℓ)WˆQ(ℓ) )∥2 σr(M) + 16 ( ∥Eℓ,:Q∥2 + ∥E∥∥Q− ˆQWˆQ∥2→∞ + ∥E∥∥Q∥2→∞ ) σr(M) . Now , by Proposition 27, P ( ∥Eℓ,:(Q− Q(ℓ)WˆQ(ℓ) )∥2 ≲ ∥M∥∞∥Q− Q(ℓ)WˆQ(ℓ) ∥2→∞G ) ≥ 1 − δ as long as the same condition (C1) holds with c1 large enough. So let us introduce the event E2 = { ∥Eℓ,:(Q− Q(ℓ)WˆQ(ℓ) )∥2 ≲ ∥M∥∞∥Q− Q(ℓ)WˆQ(ℓ) ∥2→∞G } . W e further upper bound under the event E1 ∩ E2, ∥Eℓ,:(Q− ˆQ(ℓ)WˆQ(ℓ) )∥2 ≲ ∥M∥∞ ( ∥Q− ˆQWˆQ∥2→∞ + ∥ ˆQWˆQ − ˆQ(ℓ)WˆQ(ℓ) ∥F ) G. Note that, under the condition (C2) with c2 large enough, we can also obtain 16∥M∥∞ σr(M) G ≤ 1 2 which entails that ∥ ˆQWˆQ − ˆQ(ℓ)WˆQ(ℓ) ∥F ≤ 32∥M∥∞∥Q− ˆQWˆQ∥2→∞ σr(M) G + 32(∥Eℓ,:Q∥ + ∥E∥∥Q− ˆQWˆQ∥2→∞ + ∥E∥∥Q∥2→∞) σr(M) . T o simplify the notation, let us deﬁne the three errors as x= ∥Q− ˆQWˆQ∥2→∞, y= ∥EQ∥2→∞ ≥ ∥ Eℓ,:Q∥2, z = ∥E∥∥Q∥2→∞. W e have ∥ ˆQWˆQ − ˆQ(ℓ)WˆQ(ℓ) ∥F ≲ (∥M∥∞G σr(M) + ∥E∥ σr(M) ) x+ 1 σr(M)(y+ z). By plugging the above in the previous inequality, we get ∥Eℓ,:(Q− ˆQ(ℓ)WˆQ(ℓ) )∥2 ≲ ∥M∥∞G (( 1 + ∥M∥∞G σr(M) + ∥E∥ σr(M) ) x+ 1 σr(M)(y+ z) ) which entails ﬁnally ∥E(Q− ˆQWˆQ)∥2→∞ ≲ ( ∥E∥ σr(M) + G∥M∥∞ σ1(M) ) (y+ z) + (∥E∥ + G∥M∥∞) ( 1 + G∥M∥∞ σr(M) + ∥E∥ σr(M) ) x. (34) 36Step 4: Putting everything together . Combining the inequalities ( 33) and ( 34) gives x≤ C1 ( ∥E∥ σr(M) + ∥M∥∞G σr(M) )( 1 + ∥M∥∞G σr(M) + ∥E∥ σr(M) ) x + C2 σr(M) ( 1 + ∥E∥ σr(M) + G∥M∥∞ σr(M) ) y + C3 σr(M) ( ∥M∥ σr(M) + ∥E∥ σr(M) + G∥M∥∞ σr(M) ) z. Under the events E1 and E2 and provided that the conditions (C1) and (C2) hold, for c1 and c2 are large enough, we have C1 ( ∥E∥ σr(M) + ∥M∥∞G σr(M) )( 1 + ∥M∥∞G σr(M) + ∥E∥ σr(M) ) ≤ 1 2, ( 1 + ∥E∥ σr(M) + G∥M∥ σr(M) ) ≤ 3, ( ∥M∥ σr(M) + ∥E∥ σr(M) + G∥M∥∞ σr(M) ) ≤ ( ∥M∥ σr(M) + 2 ) . Thus, we obtain x≤ 1 σr(M) ( y+ ∥M∥ σr(M)z ) . W e note that, under a similar conditions as before , we also ha ve by Proposition 26 and Proposition 27 that y≲ ∥M∥∞∥Q∥2→∞G z ≲ ∥M∥∞∥Q∥2→∞G with probability at least 1 − δ. Thus, we conclude after further simpliﬁcations that for so me C >0 large enough, we have P ( ∥Q− ˆQWˆQ∥2→∞ ≤ C∥M∥∥M∥∞ σr(M)2 ∥Q∥2→∞G ) ≥ 1 − δ provided T ≥ c(µ4κ2r2 + 1)(m+ n) log3 (e(m+ n) δ ) , with G(n,m,T,δ ) = √ nm T (√ (n+ m) log (e(n+ m) δ ) + log3/2 (e(n+ m) δ )) . Step 5: Poisson approximation. T o conclude, we now invoke Lemma 20 which entails that under the true model ( 14), we have P ( ∥Q− ˆQWˆQ∥2→∞ >C ∥M∥∥M∥∞ σr(M)2 ∥Q∥2→∞G(n,m,T,δ ) ) ≤ e √ Tδ provided T ≥ c(µ4κ2r2 + 1)(m+ n) log3 (e(m+ n)/δ). By re-parametrizing with δ′ = e √ Tδ, we obtain P ( ∥Q− ˆQWˆQ∥2→∞ >C ∥M∥∥M∥∞ σr(M)2 ∥Q∥2→∞G(n,m,T,δ ′/e √ T) ) ≤ δ′, again provided that T ≥ c(µ4κ2r2 + 1)( m + n) log3 ( e2(m+ n) √ T/δ′ ) . Recalling that ∥Q∥2→∞ = max( ∥V∥2→∞,∥U∥2→∞), we immediately obtain the ﬁnal result. 37Lemma 31. Under the notation used in the proof of Lemma 30, provided the condition ∥E∥ ≤ σ2r(S)/16, the following inequality holds: ∥ ˆQWˆQ − ˆQ(ℓ)WˆQ(ℓ) ∥F ≤ 16∥Eℓ,: ˆQ(ℓ)WˆQ(ℓ) ∥2 + 16∥E∥∥ ˆQWˆQ∥2→∞ σ2r(S) Proof of Lemma 31. W e have ∥ ˆQWˆQ − ˆQ(ℓ)WˆQ(ℓ) ∥F ≤ ∥ ˆQˆQ⊤ − ˆQ(ℓ)( ˆQ(ℓ))⊤∥F∥Q∥ ≤ 2∥( ˜S− ˜S(ℓ)) ˆQ(ℓ)∥F |σ2r( ˜S(ℓ)) − σ2r+1( ˜S(ℓ))| where the ﬁrst inequality follows the elementary fact that ∥AB∥F ≤ ∥ A∥F∥B∥, and the second inequality follows by Davis-Kahan. Now , by W eyl’s inequali ty, we have for all k ∈ [n + m], |σk( ˜S(ℓ)) − σk(S)| ≤ ∥ E(ℓ)∥ ≤ ∥ E∥, where the error matrix E(ℓ) = ˜Sℓ− S, and more precisely is deﬁned as follows: E(ℓ) i,j = {Ei,j if i̸= ℓor j ̸= ℓ, 0 otherwise. The crude inequality ∥E(ℓ)∥ ≤ ∥ E∥ follows from the fact that ∥E(ℓ)∥ is equal to the operator norm of a submatrix of E which will always be smaller than ∥E∥. Therefore, under the condition that ∥E∥ ≤ σ2r(S)/4, we have |σ2r( ˜S(ℓ)) − σ2r+1( ˜S(ℓ))| ≥ σ2r(S)/2. In summary, we obtain that ∥ ˆQWˆQ − ˆQ(ℓ)WˆQ(ℓ) ∥F ≤ 4∥( ˜S− ˜S(ℓ)) ˆQ(ℓ)∥F σ2r(S) . Now , we further have by triangular inequality and by deﬁniti on of ˜S(ℓ): ∥( ˜S− ˜S(ℓ)) ˆQ(ℓ)∥F = ∥(eℓEℓ,: + (E:,ℓ − Eℓ,ℓeℓ)e⊤ ℓ) ˆQ(ℓ)∥F ≤ ∥ Eℓ,: ˆQ(ℓ)∥2 + ∥E:,ℓ − Eℓ,ℓeℓ∥2∥ ˆQ(ℓ)∥2→∞ ≤ ∥ Eℓ,: ˆQ(ℓ)∥2 + ∥E∥2∥ ˆQ(ℓ)∥2→∞ ≤ ∥ Eℓ,: ˆQ(ℓ)∥2 + 2∥E∥2∥ ˆQ(ℓ)( ˆQ(ℓ))⊤Q∥2→∞ where the last inequality follows under the condition that ∥E∥ ≤ 2σ2r(S). Indeed, we have under such condition that ∥ ˆQ(ℓ)∥2→∞ = ∥ ˆQ(ℓ)( ˆQ(ℓ))⊤Q∥2→∞ + ∥ ˆQ(ℓ)(sgn(( ˆQ(ℓ))⊤Q⊤) − ( ˆQ(ℓ))⊤Q)∥2→∞, and by Davis-Kahan’s inequality ∥sgn(( ˆQ(ℓ))⊤Q⊤) − ( ˆQ(ℓ))⊤Q∥ ≤ 2∥E(ℓ)∥2 (σ2r (S))2 ≤ 2∥E∥2 (σ2r (S))2 ≤ 1 2 . Similarly, we also have ∥Eℓ,: ˆQ(ℓ)∥2 ≤ 2∥Eℓ,: ˆQ(ℓ)( ˆQ(ℓ))⊤Q∥2. Hence, we obtain: ∥( ˜S− ˜S(ℓ)) ˆQ(ℓ)∥F ≤ 2∥Eℓ,: ˆQ(ℓ)WˆQ(ℓ) ∥2 + 2∥E∥(∥ ˆQWˆQ∥2→∞ + ∥ ˆQWˆQ − ˆQ(ℓ)WˆQ(ℓ) ∥2→∞) Which entails under the condition that ∥E∥ ≤ σ2r(S)/16 that ∥ ˆQWˆQ − ˆQ(ℓ)WˆQ(ℓ) ∥F ≤ 8∥Eℓ,: ˆQ(ℓ)WˆQ(ℓ) ∥2 + 8∥E∥∥ ˆQWˆQ∥2→∞ σ2r(S) + ∥ ˆQWˆQ − ˆQ(ℓ)WˆQ(ℓ) ∥2→∞ 2 After rearranging, we obtain ∥ ˆQWˆQ − ˆQ(ℓ)WˆQ(ℓ) ∥F ≤ 16∥Eℓ,: ˆQ(ℓ)WˆQ(ℓ) ∥2 + 16∥E∥∥ ˆQWˆQ∥2→∞ σ2r(S) E.2 Subspace recovery for transition matrices Lemma 32.Let Y ∈ Rn×n be a matrix of independent P oisson entries with Yi,j ∼ 1 TPoisson(TMi,j), and let ˆU, ˆV be the matrices of left and right singular vectors of best r-rank 38approximation of Y. Let gδ be the function deﬁned in (9). Conditioned on the events where ∥Y − M∥ ≤ c1σr(M),gδ(TM) log(ne/δ) ≤ c2Tσr(M), √ ∥M∥∞ log(ne/δ) ≤ c3 √ Tσr(M) for some sufﬁciently small universal constants c1,c2,c3 > 0, we have, with probability at least 1 − δ, max { ∥U− ˆU( ˆU⊤U)∥2→∞,∥V − ˆV(ˆV⊤V)∥2→∞ } ≲ 1 σr(M) [ µ √ r n (σ1(M) σr(M) ∥Y − M∥ + 1 Tgδ(TM) log (ne δ )) + √ r∥M∥∞ T log (ne δ )] . Proof. The proof follows similar steps as the proof of Theorem 4.2 in [22]. In order to apply the leave-one-out technique, we ﬁrst repeat the symmetric dila tion trick as in Step 1 of proof of Lemma 30. W e deﬁne S = [ 0 M M⊤ 0 ] (35) and note that for matrix M with SVD M = UΣ V⊤, we have: S = 1√ 2 [ U U V −V ][ Σ 0 0 −Σ ] 1√ 2 [ U U V −V ] ⊤ := QDQ⊤. W e deﬁne ˜S as the symmetrized version of matrix Y, and let ˆQ ∈ Rn×2r be the matrix of eigenvectors of the best 2r-rank approximation of ˜S. Note that: ∥Q− ˆQ( ˆQ⊤Q)∥2→∞ = max { ∥U − ˆU( ˆU⊤U)∥2→∞,∥V − ˆV(ˆV⊤V)∥2→∞ } . W e will also repeatedly use the properties ( 31). T o keep the notation simple, deﬁne WˆQ = ˆQ⊤Q. Thus, proving Lemma 32 is equivalent to showing: ∥Q− ˆQWˆQ∥2→∞ ≲ 1 σr(M) [ ∥Q∥2→∞(σ1(M) σr(M)∥ ˜S− S∥ + 1 Tgδ(TM) log (ne δ ) ) + √ r∥M∥∞ T log (ne δ )] with high probability. Deﬁne E = ˜S− S. Now , as in Lemma 33, we have: ∥Q− ˆQWˆQ∥2→∞ ≤ 1 σr(M) (4∥ ˜SQ∥2→∞∥E∥ σr(M) + ∥EQ∥2→∞ + 2∥ ˜S(Q− ˆQWˆQ)∥2→∞ ) (36) under the assumption that ∥E∥ ≤ c1σr(M). Indeed, it is straightforward to show the same bounds as in Lemma 4.14 in [ 22] - note that the boundedness assumption is not used in these l emmas. W e bound the three terms in Equation ( 36) as follows: 1. T o bound the ﬁrst term, we use: ∥ ˜SQ∥2→∞ ≤ ∥ SQ∥2→∞ + ∥EQ∥2→∞ ≤ ∥ Q∥2→∞∥S∥ + ∥EQ∥2→∞ (37) where we ﬁrst used the triangle inequality and then ∥SQ∥2→∞ = ∥QD∥2→∞ ≤ ∥ Q∥2→∞∥S∥. 2. For the second term, according to Lemma 29, we obtain with probability at least 1 − δ: ∥EQ∥2→∞ ≲ 1 T [ ∥Q∥F √ T∥M∥∞ log(ne/δ) + gδ(TM) log(ne/δ)∥Q∥2→∞ ] . (38) Moreover, we will use ∥Q∥F ≤ √ 2rand ∥Q∥2→∞ ≤ µ√ r n, which follow from the low-rank and incoherence assumptions. 3. Finally, regarding the last term in Equation ( 36), we split it using the triangle inequality as follows: ∥ ˜S(Q− ˆQWˆQ)∥2→∞ ≤ ∥ S(Q− ˆQWˆQ)∥2→∞ + ∥E(Q− ˆQWˆQ)∥2→∞, and from Step 3 of proof of Theorem 4.2 in [ 22] we have: ∥S(Q− ˆQWˆQ)∥2→∞ ≤ ∥ Q∥2→∞∥S∥∥Q⊤(Q− ˆQWˆQ)∥ ≲ ∥Q∥2→∞∥S∥ ∥E∥2 σ2r(M), (39) 39where we used ∥Q⊤(Q− ˆQWˆQ)∥ = ∥ sin Θ( Q, ˆQ)∥2. The remaining of the proof consists in bounding ∥E(Q− ˆQWˆQ)∥2→∞ = max ℓ=1,...,n∥Eℓ,:(Q − ˆQWˆQ)∥. First note that the matrix Q− ˆQWˆQ depends on E and thus we cannot apply Lemma 29 immediately. Instead, we will use the leave-one-out method, and deﬁne a matrix ˜S(ℓ) ∈ Rn×n as follows: ˜S(ℓ) i,j = { ˜Si,j, if i̸= ℓor j ̸= ℓ Si,j, otherwise Then deﬁne ˆQ(ℓ) ∈ Rn×2r as a matrix of eigenvectors corresponding to 2r greatest (in absolute value) eigenvalues of matrix ˜S(ℓ). Deﬁne WˆQ(ℓ) accordingly. Then we have: ∥E(Q− ˆQWˆQ)∥2→∞ ≤ 2 max 1≤ℓ≤n { ∥Eℓ,:(Q− ˆQ(ℓ)WˆQ(ℓ) )∥,∥Eℓ,:( ˆQ(ℓ)WˆQ(ℓ) − ˆQWˆQ)∥ } . (40) (3a) Since Eℓ,: is statistically independent of Q− ˆQ(ℓ)WˆQ(ℓ) , the ﬁrst term from ( 40) can be bounded according to Lemma 29 for any 1 ≤ ℓ≤ nas follows: ∥Eℓ,:(Q− ˆQ(ℓ)WˆQ(ℓ) )∥ ≲ 1 T [ ∥Q− ˆQ(ℓ)WˆQ(ℓ) ∥F √ T∥M∥∞ log (ne δ ) + gδ(TM) log (ne δ ) ∥Q− ˆQ(ℓ)WˆQ(ℓ) ∥2→∞ ] with probability at least 1 − δ. After applying the triangle inequality to the second term a nd using ∥ · ∥ 2→∞ ≤ ∥ · ∥ F, we get: ∥Q− ˆQ(ℓ)WˆQ(ℓ) ∥2→∞ ≤ ∥ Q− ˆQWˆQ∥2→∞ + ∥ ˆQWˆQ − ˆQ(ℓ)WˆQ(ℓ) ∥F. (41) Thus, combining the last two inequalities yields: ∥Eℓ,:(Q− ˆQ(ℓ)WˆQ(ℓ) )∥ ≲ ∥Q− ˆQWˆQ∥F √ T−1∥M∥∞ log (ne δ ) + T−1gδ(TM) log (ne δ ) ∥Q− ˆQWˆQ∥2→∞ + ∥ ˆQWˆQ − ˆQ(ℓ)WˆQ(ℓ) ∥F (√ T−1∥M∥∞ log (ne δ ) + T−1gδ(TM) log (ne δ )) (42) for all 1 ≤ ℓ≤ n. (3b) The second term from ( 40) can be bounded very roughly as follows: ∥Eℓ,:( ˆQ(ℓ)WˆQ(ℓ) − ˆQWˆQ))∥ ≤ ∥ E∥∥ ˆQ(ℓ)WˆQ(ℓ) − ˆQWˆQ∥F. (43) Similar to the Step 2.2 in the proof of Theorem 4.2 in [ 22], we have: ∥ ˆQ(ℓ)WˆQ(ℓ) − ˆQWˆQ∥F ≤ 16 σr(M)(∥Eℓ,:(Q− ˆQ(ℓ)WˆQ(ℓ) )∥ + ∥EQ∥2→∞ + ∥E∥∥Q− ˆQ(ℓ)WˆQ(ℓ) ∥2→∞ + ∥E∥∥Q∥2→∞). Applying again the inequality ( 41) and moving the term ∥ ˆQ(ℓ)WˆQ(ℓ) − ˆQWˆQ∥F to the left side of inequality, we get under assumption ∥E∥ ≤ c1σr(M) that: ∥ ˆQ(ℓ)WˆQ(ℓ) − ˆQWˆQ∥F ≲ 1 σr(M)(∥Eℓ,:(Q− ˆQ(ℓ)WˆQ(ℓ) )∥ + ∥EQ∥2→∞ + ∥E∥∥Q− ˆQWˆQ∥2→∞ + ∥E∥∥Q∥2→∞). (44) After substitution of the results from ( 37), ( 38), ( 39), ( 40), ( 42), ( 43) and ( 44) into Equation ( 36) and using assumptions stated in the lemma, we obtain the stat ement of the lemma. 40E.3 Error decomposition in the two-to-inﬁnity norm Below , we present a decomposition for the error of subspace recovery in the norm ∥ · ∥ 2→∞. W e borrow this result from [ 22] and provide its proof for completeness. Lemma 33 (Lemma 4.16 in [ 22]). Let S,˜S ∈ Rn×n be symmetric matrices and assume that Sand ˜Sare of rank r. Let Q, ˆQ∈ O n×r be the corresponding rsingular vectors of Sand ˜S, respectively. Denote E = ˜S− S. Under the condition ∥E∥ ≤ σr(S)/2, we have: ∥Q− ˆQˆQ⊤Q∥2→∞ ≤ 4∥ ˜SQ∥2→∞∥E∥ (σr(S))2 + ∥EQ∥2→∞ σr(S) + 2∥ ˜S(Q− ˆQˆQ⊤Q)∥2→∞ σr(S) Proof of Lemma 33. Since Sis a symmetric matrix of rank r, by SVD we write S = QΣ Q⊤, where the matrix Σ = diag(σ1(S),...,σ r(S)). For ease of notations, let us further denote W = ˆQQ⊤. W e have ∥Q− ˆQˆQ⊤Q∥2→∞ = ∥SQΣ −1 − ˆQW∥2→∞ ≤ ∥ ˜SQΣ −1 − ˆQW∥2→∞ + ∥EQΣ −1∥2→∞ ≤ ∥ ˜SQ− ˆQWΣ ∥2→∞ σr(S) + ∥EQ∥2→∞ σr(S) Now , we focus on the term ∥ ˜SQ− ˆQWΣ ∥2→∞. T o that end, we ﬁrst establish the identity ˆQWΣ = ˆQˆQ⊤QΣ = ˆQˆQ⊤SQ = ˆQˆQ⊤ ˆSQ+ ˆQˆQ⊤EQ = ˆQˆΣ ˆQ⊤Q+ ˆQˆQ⊤EQ = ˜SˆQˆQ⊤Q+ ˆQˆQ⊤EQ where we use the identities SQ = QΣ , ˆQ⊤ ˜S = ˆΣ ˆQ⊤, and ˆQˆΣ ˆQ⊤ = ˜SˆQˆQ⊤. Then, we observe that ∥ ˜SQ− ˆQWΣ ∥2→∞ = ∥ ˜SQ− ˜SˆQˆQ⊤Q+ ˆQˆQ⊤EQ∥2→∞ ≤ ∥ ˜S(Q− ˆQˆQ⊤Q)∥2→∞ + ∥ ˆQˆQ⊤EQ∥2→∞ Next, we note that when ∥E∥ ≤ σr(S)/2, we have ∥ ˆQˆQ⊤EQ∥2→∞ = ∥ ˜SˆQˆΣ −1 ˆQ⊤EQ∥2→∞ ≤ ∥ ˜SˆQ∥2→∞∥ˆΣ −1∥∥ ˆQ⊤∥∥E∥∥Q∥ ≤ ∥ ˜SˆQsgn( ˆQ⊤Q)∥2→∞∥E∥ σr( ˜S) . At this point, we try to bound ∥ ˜SˆQ∥2→∞ and σr( ˜S), under the condition ∥E∥ ≤ σr(S)/2. First, we can easily see by W eyl’s inequality we have |σr( ˜S) − σr(S)| ≤ ∥ E∥, which entails under the assumed condition that σr( ˜S) ≥ σr(S)/2. Next, we observe: ∥ ˜SˆQ∥2→∞ = ∥ ˜SˆQsgn( ˆQ⊤Q)∥2→∞ ≤ ∥ ˜SˆQˆQ⊤Q∥2→∞ + ∥ ˜SˆQ∥2→∞∥sgn( ˆQ⊤Q) − ˆQ⊤Q∥ ≤ ∥ ˜SˆQˆQ⊤Q∥2→∞ + 2∥ ˜SˆQ∥2→∞∥E∥2 σr(S)2 ≤ ∥ ˜SˆQˆQ⊤Q∥2→∞ + ∥ ˜SˆQ∥2→∞ 2 41where we used the Davis-Kahan’s inequality and properties o f the sgn(·), to upper bound ∥sgn( ˆQ⊤Q) − ˆQ⊤Q∥ ≤ ∥ sin( ˆQ,Q)∥2 ≤ 2∥E∥2/(σr(S))2. Thus, leading to: ∥ ˜SˆQ∥2→∞ ≤ 2∥ ˜SˆQˆQ⊤Q∥2→∞ Moving forward we obtain: ∥ ˆQˆQ⊤EQ∥2→∞ ≤ 4∥ ˜SˆQˆQ⊤Q∥2→∞∥E∥ σr(S) ≤ 4 ( ∥ ˜S( ˆQˆQ⊤Q− Q)∥2→∞ + ∥ ˜SQ∥2→∞ ) ∥E∥ σr(S) ≤ 2∥ ˜S( ˆQˆQ⊤Q− Q)∥2→∞ + 4 ( ∥ ˜SQ∥2→∞ ) ∥E∥ σr(S) Now , putting everything together we conclude that: ∥Q− ˆQˆQ⊤Q∥2→∞ ≤ 2∥ ˜S( ˆQˆQ⊤Q− Q)∥2→∞ σr(S) + 4∥ ˜SQ∥2→∞∥E∥ (σr(S))2 + ∥EQ∥2→∞ σr(S) 42F Row-wise and entry-wise matrix estimation errors In this appendix, we provide a series of results about quantifying the matrix estimation error using different norms. It is important to note that all these resul ts require a control of the error in the two- to-inﬁnity norm, which in turn requires a subspace recovery guarantee in the two-to-inﬁnity norm. Lemmas 35 and 37 are speciﬁc to our analysis for the estimation of the transit ion matrices. Lemmas 34 and 36 are common to the analysis of both the estimation of reward ma trices and transition matrices. The results presented in this appendix are used in the proofs of the main results, presented in Appendix B. F .1 Bounding ∥M− ˆM∥2→∞ Lemma 34. Let M, ˆM be as in § 2. Assume that there exists a sufﬁciently small universal con stant c1 >0 such that ∥M− ˜M∥ ≤ c1σr(M). Then, there exists a universal constant c2 >0 such that ∥ ˆM− M∥2→∞ ≤ c2σ1(M) [ ∥U − ˆU( ˆU⊤U)∥2→∞ + ∥U∥2→∞ ∥ ˜M − M∥ σr(M) ] . Proof. W e start by using deﬁnition of ˆM as a projection of matrix ˜M, and then use the triangle inequality and the inequality ( 3) to obtain: ∥ ˆM − M∥2→∞ = ∥Π ˆU ˜M − Π UM∥2→∞ = ∥(Π ˆU − Π U)(˜M − M) + Π U(˜M − M) + (Π ˆU − Π U)M∥2→∞ ≤ ∥ (Π ˆU − Π U)(˜M − M)∥2→∞ + ∥Π U(˜M − M)∥2→∞ + ∥(Π ˆU − Π U)M∥2→∞ ≤ ∥ Π ˆU − Π U∥2→∞(∥ ˜M − M∥ + ∥M∥) + ∥Π U∥2→∞∥ ˜M − M∥. (45) Moreover, we note that ∥Π U∥2→∞ = ∥U∥2→∞ (refer to Proposition 6.6 in [ 15]). In the remaining of the proof, we upper bound ∥Π ˆU − Π U∥2→∞ from ( 45). For any orthogonal matrix R ∈ O r×r, we have ∥Π ˆU − Π U∥2→∞ = ∥ ˆUˆU⊤ − UU⊤∥2→∞ = ∥ ˆURR⊤ ˆU⊤ − UR⊤ ˆU⊤ + UR⊤ ˆU⊤ − UU⊤∥2→∞ ≤ ∥ ˆURR⊤ ˆU⊤ − UR⊤ ˆU⊤∥2→∞ + ∥UR⊤ ˆU⊤ − UU⊤∥2→∞ ≤ ∥ ˆUR − U∥2→∞∥R⊤ ˆU⊤∥ + ∥U∥2→∞∥R⊤ ˆU⊤ − U⊤∥ ≤ ∥ U − ˆUR∥2→∞ + ∥U∥2→∞∥U − ˆUR∥. (46) Recall the deﬁnition of sgn function given in the notation presented in § 1 and choose the matrix R as R = sgn( ˆU⊤U). For this choice of Rwe have according to Davis-Kahan’s theorem (Corollary 2.8 in [ 22]): ∥U − ˆUR∥ ≤ √ 2∥ sin Θ( ˆU,U )∥ ≤ 2∥M− ˜M∥ σr(M) . (47) Deﬁne the matrix WˆU = ˆU⊤U. W e use the facts that ∥U − ˆUR∥2→∞ ≤ ∥ U − ˆUWˆU∥2→∞ + ∥ ˆU∥2→∞∥WˆU − R∥ (48) and that WˆU is very close to Raccording to the proof of Lemma 4.15 in [ 22] to show: ∥WˆU − R∥ = ∥ ˆU⊤U − sgn( ˆU⊤U)∥ = ∥ sin Θ( ˆU,U )∥2 ≤ 2∥M− ˜M∥2 σ2r(F) . (49) W e also have σi(R) = 1 for i ∈ [r] and according to W eyl’s inequality σmin(WˆU) ≥ σmin(R) − ∥WˆU− R∥ = 1 − ∥WˆU− R∥. Combining these results under assumption ∥M− ˜M∥ <σr(M)/ √ 2 we obtain: ∥W−1 ˆU ∥ = 1 σmin(WˆU) ≤ 1 1 − ∥WˆU − R∥ ≤ 1 1 − 2∥M−˜M∥2 σ2r (M) . 43Thus: ∥ ˆU∥2→∞ ≤ ∥ ˆUWˆU∥2→∞∥W−1 ˆU ∥ ≤ 1 1 − 2∥M−˜M∥2 σ2r (M) (∥U∥2→∞ + ∥U − ˆUWˆU∥2→∞). (50) Combining Equations ( 48), ( 49), ( 50) we get: ∥U − ˆUR∥2→∞ ≤ 1 1 − 2∥M−˜M∥2 σ2r (M) (∥U − ˆUWˆU∥2→∞ + 2∥M− ˜M∥2 σ2r(M) ∥U∥2→∞) and combining the last equality with ( 46) and ( 47), we have ∥Π ˆU − Π U∥2→∞ ≤ ∥U − ˆUWˆU∥2→∞ 1 − 2∥M−˜M∥2 σ2r (M) +   2∥M−˜M∥2 σ2r (M) 1 − 2∥M−˜M∥2 σ2r (M) + 2∥M− ˜M∥ σr(M)  ∥U∥2→∞. (51) Finally, substituting the obtained bound into Equation ( 45) and using assumption ∥M − ˜M∥ ≤ c1σr(M) for simpliﬁcation, we obtain the statement of the lemma. F .2 Bounding ∥P − ˆP∥1→∞ Lemma 35. Let P, ˆP be as in Model II in § 2. W e have: ∥ ˆP − P∥1→∞ ≤ 2 √n∥ ˆM − M∥2→∞ minj∈[n] ∥Mj,:∥1 . Proof. Starting with the deﬁnition of ˆP, we get: ∥ ˆP − P∥1→∞ = max i∈[n] ∥ ˆPi,: − Pi,:∥1 = max i∈[n]      (ˆMi,:)+ ∥(ˆMi,:)+∥1 − Mi,: ∥Mi,:∥1      1 ≤ 2 max i∈[n] ∥ ˆMi,: − Mi,:∥1 ∥Mi,:∥1 ≤ 2 √nmaxi∈[n] ∥ ˆMi,: − Mi,:∥ minj∈[n] ∥Mj,:∥1 , where the ﬁrst inequality follows from Lemma 2 in [ 26] and the second by equivalence of norms. Moreover, note that the above inequality holds even in the ca se when ∥(ˆMi,:)+∥1 = 0 (and thus ˆPi,: = 1 n1n), but the bound is vacuous in this case. F .3 Bounding ∥M− ˆM∥∞ Lemma 36. Let M, ˆM be as in § 2. Assume that there exists a sufﬁciently small universal con stant c1 >0 such that ∥M− ˜M∥ ≤ c1σr(M). Then, there exists a universal constant c2 >0 such that ∥ ˆM− M∥∞ ≤ c2∥M∥2→∞ ( ∥M − ˜M∥ σr(M) ∥V∥2→∞ + ∥V − ˆVWˆV∥2→∞ ) + c2∥M − ˆM∥2→∞(∥V∥2→∞ + ∥V − ˆVWˆV∥2→∞). Proof. Similarly to the decomposition leading to Equation ( 45), we can upper bound the inﬁnity norm error easily from the following decomposition: ∥ ˆM − M∥∞ = ∥ ˆMΠ ˆV − MΠ V∥∞ ≤ ∥ ˆM∥2→∞∥Π ˆV − Π V∥2→∞ + ∥ ˆM− M∥2→∞∥Π V∥2→∞ ≤ (∥ ˆM − M∥2→∞ + ∥M∥2→∞)∥Π ˆV − Π V∥2→∞ + ∥ ˆM− M∥2→∞∥V∥2→∞, 44where we used the inequality ( 4) together with the triangle inequalities and the fact that p rojection matrices are symmetric. T o bound ∥Π ˆV − Π V∥2→∞, we use the same approach as that used in ( 51) (just replacing U by V), and we obtain: ∥Π ˆV − Π V∥2→∞ ≤ ∥V − ˆVWˆV∥2→∞ 1 − 2∥M−˜M∥2 σ2r (M) +   2∥M−˜M∥2 σ2r (M) 1 − 2∥M−˜M∥2 σ2r (M) + 2∥M− ˜M∥ σr(M)  ∥V∥2→∞. F .4 Bounding ∥P − ˆP∥∞ Lemma 37. Let P, ˆP be as in Model II in § 2. Assume that D = min i∈[n] ∥Mi,:∥1 > 0. If ∥ ˆM − M∥1→∞ ≤ 1 2 D, then ∥ ˆP − P∥∞ ≤ 2 ∥ ˆM− M∥∞ D + 2 √n∥M∥∞ D2 ∥ ˆM− M∥2→∞. Proof. First note that for any i∈ [n]: ⏐ ⏐ ⏐∥(ˆMi,:)+∥1 − ∥Mi,:∥1 ⏐ ⏐ ⏐≤ ∥ (ˆMi,:)+ − Mi,:∥1 ≤ ∥ ˆMi,: − Mi,:∥1 ≤ ∥Mi,:∥1 2 , (52) where the ﬁrst inequality follows from the reverse triangle inequality, the second from | max(0,x)− y| ≤ | x− y| for all y >0 and x ∈ R, and the last inequality follows from the assumption in the lemma. This implies that ∥(ˆMi,:)+∥1 >0 for all i∈ [n], which further implies that ˆP is deﬁned by: for all i∈ [n], ˆPi,: = ( ˆMi,:)+/∥(ˆMi,:)+∥1. (53) Furthermore, we have for all i,j = 1 ,...,n , | ˆPi,j − Pi,j| = ⏐ ⏐ ⏐ ⏐ ⏐ (ˆMi,j)+ ∥(ˆMi,:)+∥1 − Mi,j ∥Mi,:∥1 ⏐ ⏐ ⏐ ⏐ ⏐≤ ⏐ ⏐ ⏐ ⏐ ⏐ ˆMi,j ∥(ˆMi,:)+∥1 − Mi,j ∥Mi,:∥1 ⏐ ⏐ ⏐ ⏐ ⏐ ≤ 1 ∥Mi,:∥1 ⏐ ⏐ ⏐ˆMi,j − Mi,j ⏐ ⏐ ⏐+ |ˆMi,j| ⏐ ⏐ ⏐ ⏐ ⏐ 1 ∥(ˆMi,:)+∥1 − 1 ∥Mi,:∥1 ⏐ ⏐ ⏐ ⏐ ⏐ = 1 ∥Mi,:∥1 ⏐ ⏐ ⏐ˆMi,j − Mi,j ⏐ ⏐ ⏐+ |ˆMi,j| ∥Mi,:∥1 ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ 1 1 + ∥(ˆMi,:)+∥1−∥Mi,:∥1 ∥Mi,: ∥1 − 1 ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ = 1 ∥Mi,:∥1 ⏐ ⏐ ⏐ˆMi,j − Mi,j ⏐ ⏐ ⏐+ |ˆMi,j| ∥Mi,:∥1 ϕ ( ∥(ˆMi,:)+∥1 − ∥Mi,:∥1 ∥Mi,:∥1 ) where we deﬁne ϕ(x) = |x/(1 + x)| for all x∈ R\\{−1}. Note that if |x| <1/2, then ϕ(x) ≤ 2|x|, which combined with ( 52) gives | ˆPi,j − Pi,j| ≤ 1 ∥Mi,:∥1 ⏐ ⏐ ⏐ˆMi,j − Mi,j ⏐ ⏐ ⏐+ 2|ˆMi,j| ∥Mi,:∥1 ⏐ ⏐ ⏐ ⏐ ⏐ ∥(ˆMi,:)+∥1 − ∥Mi,:∥1 ∥Mi,:∥1 ⏐ ⏐ ⏐ ⏐ ⏐ ≤ 1 ∥Mi,:∥1 ⏐ ⏐ ⏐ˆMi,j − Mi,j ⏐ ⏐ ⏐+ 2 ∥Mi,:∥2 1 (⏐ ⏐ ⏐ˆMi,j − Mi,j ⏐ ⏐ ⏐+ |Mi,j| ) ∥ ˆMi,: − Mi,:∥1. Using the assumption ∥ ˆM − M∥1→∞ ≤ 1 2 mini∈[n] ∥Mi,:∥1 again, we can group ﬁrst two terms, and then use ∥ ˆM− M∥1→∞ ≤ √n∥ ˆM− M∥2→∞ to get the statement of the lemma. 45G Low-rank bandits: proofs of results from Section 4 G.1 Gap-dependent guarantees Proof of Theorem 7. First, we prove the result corresponding the best entry iden tiﬁcation problem. W e proceed in several steps. Step 1: entry-wise concentration. W e can easily verify that for all ℓ≥ 1, for all (i,j) ∈ [m] × [n], we have | ˆ∆ (ℓ) i,j − ∆ i,j| ≤ 2∥ ˆM(ℓ) − M⋆∥∞. Therefore, applying Theorem 1, we have, for δ >0, and Tℓ ≥ c1(m+n) log3((e2(m+n)(mn)/δℓ), P ( | ˆ∆ i,j − ∆ i,j| >2C1 √ e(m+ n) Tℓ log3 (e(m+ n)mnTℓ δℓ )) ≤ δℓ mn for some c1,C1 > 0 sufﬁciently large. In particular, we can choose C1 = C(µ11/2κ2r1/2 + µ3κr3/2(m + n)/√mn), and c1 = cµ4κ2r2, but under a homogeneous reward matrix these constants are Θ(1) . Thus, by a union bound and always under the same conditions, we have P ( max (i,j)∈[m]×[n] | ˆ∆ i,j − ∆ i,j| >2C1 √ e(m+ n) Tℓ log3 (e(m+ n)mnTℓ δℓ )) ≤ δℓ. Next, we wish to choose Tℓ so that we have P ( max i,j | ˆ∆ i,j − ∆ i,j| ≤ 2−(ℓ+2) ) ≥ 1 − δℓ. (54) Note that in order for the above guarantee to hold, it is sufﬁc ient to have: Tℓ ≥ c1(m+ n) log3 (e2(m+ n)(mn) δℓ ) , Tℓ ≥ 2√eC2 1 (m+ n)22(ℓ−2) log3 (e(m+ n)(mn) δℓ ) . This can be achieved if we choose Tℓ = ⌈ C322(ℓ−2)(m+ n) log3 (22(ℓ−2)(m+ n) δℓ )⌉ , (55) for some positive constant C3 >0 large enough which can be determined explicitly and only dep end on c1,C1. Indeed, this can be deduced from the basic fact that if T1/3 ℓ ≥ 2alog(2a) + 2 b, then T1/3 ℓ ≥ alog(T1/3 ℓ ) + b. W e spare the reader these tedious calculations and only arg ue that such C3 exists and can be computed explicitly. Step 2: Good events.W e deﬁne Sℓ = { (i,j) ∈ [n] × [m] : ∆ i,j ≤ 2−ℓ} and the good events under which we correctly ﬁnd the best entry as Eℓ = {Aℓ+1 ⊆ Sℓ+1} ∩ { (i⋆,j⋆) ∈ A ℓ+1}. W e show that the good event Eℓ happens with high probability conditionally on E1,..., Eℓ−1. Observe that by independence of the entries sampled at epoch ℓfrom those of the previous epochs, we have based on ( 54) P ( max i,j | ˆ∆ i,j − ∆ i,j| ≤ 2−(ℓ+2) ⏐ ⏐ ⏐Eℓ−1,..., E1 ) ≥ 1 − δℓ Now , conditionally on Eℓ−1,..., E1, under the event that maxi,j| ˆ∆ i,j− ∆ i,j| ≤ 2−(ℓ+2), if (i,j) ∈ Sc ℓ+1 ∩ Aℓ+1 then ˆ∆ (ℓ) i,j ≥ ∆ i,j − 2−(ℓ+2) >2−(ℓ+1) − 2−(ℓ+2) = 2 −(ℓ+2). 46Thus, we have P ( Aℓ+1 ⊆ Sℓ+1 ⏐ ⏐ ⏐Eℓ−1,..., E1 ) ≥ P ( max i,j | ˆ∆ i,j − ∆ i,j| ≤ 2−(ℓ+2) ⏐ ⏐ ⏐Eℓ−1,..., E1 ) ≥ 1 − δℓ. Furthermore, note that under the event maxi,j| ˆ∆ i,j−∆ i,j| ≤ 2−(ℓ+2), we clearly have that ˆ∆ i⋆,j⋆ ≤ 2−(ℓ+2) and since (i⋆,j⋆) ∈ A ℓ conditionally on Eℓ−1 we conclude that P ( Eℓ ⏐ ⏐ ⏐Eℓ−1,..., E1 ) ≥ P ( max i,j | ˆ∆ i,j − ∆ i,j| ≤ 2−(ℓ+2) ⏐ ⏐ ⏐Eℓ−1,..., E1 ) ≥ 1 − δℓ Step 3: Sample complexity . First, we remark that when ℓ = ⌈log2(1/∆ min)⌉, we have Sℓ = {(i⋆,j⋆)}. Therefore, under the event E1 ∩ · · · ∩ E ⌈log2(1/∆ min)⌉ the algorithm will stop after τ rounds, and recommend the optimal (i⋆,j⋆), where τ ≤ ⌈log2(1/∆ min)⌉∑ ℓ=1 Tℓ ≤ ⌈log2(1/∆ min)⌉∑ ℓ=1 ⌈ C322(ℓ−2)(m+ n) log3 (22(ℓ−2)(m+ n) δℓ )⌉ ≤ ⌈log2(1/∆ min)⌉∑ ℓ=1 ⌈ C3 (m+ n) ∆ 2 min log3 ((m+ n)⌈log2 (1/∆ min)⌉2 ∆ 2 minδ )⌉ ≤ log2 ( 1 ∆ min )⌈ C3 (m+ n) ∆ 2 min log3 ((m+ n)⌈log2 (1/∆ min)⌉2 ∆ 2 minδ )⌉ ≤ ψ(n,m,δ ) := C4 (m+ n) log (e/∆ min) ∆ 2 min log3 (e(m+ n) log (e/∆ min) ∆ minδ ) where we recall the deﬁnition of Tℓ in ( 55), that δℓ = δ/ℓ2, and where C4 is a large enough universal constant. Hence, we have P ((iτ,jτ) = ( i⋆,j⋆),τ ≤ ψ(n,m,δ )) ≥ P   ⌈log2(1/∆ min)⌉⋂ ℓ=1 Eℓ  ≥ 1 − δ. (56) This conclude the proof of the guarantee for the best entry id entiﬁcation. Note that we can immediately conclude from the above guarantee ( 56) that the sample complexity of SME-AE (1/Tα) for all T ≥ 1, satisﬁes E[τ ∧ T] ≤ ψ(n,m,T −α) + T1−α. Indeed, we have E[τ ∧ T] = E[(τ ∧ T)1{τ≤ψ(n,m,T−α)}] + E[(τ ∧ T)1{τ>ψ(n,m,T−α)}] ≤ ψ(n,m,T −α) + TP(τ >ψ(n,m,T −α)) ≤ ψ(n,m,T −α) + T1−α, where the upper bound on the probability follows from ( 56) with δ= 1 /Tα. 47Next, we turn our attention to proving the regret upper bound . W e deﬁne Egood = {(ˆıτ,ˆτ) = (i⋆,j⋆),τ ≤ ψ(n,m, 1/T2)}. W e have Rπ(T) = TMi⋆,j⋆ − E [ T∑ t=1 Miπ t,jπ t ] = E [ T∑ t=1 (Mi⋆,j⋆ − Miπ t,jπ t )1{Egood } ] + E [ T∑ t=1 (Mi⋆,j⋆ − Miπ t,jπ t )1{Ec good } ] ≤ E [ T∑ t=1 (Mi⋆,j⋆ − Miπ t,jπ t )1{τ≤ψ(n,m,T−2)} ] + ∆ maxTP(Ec good) ≤ E [ ∞∑ t=1 (Mi⋆,j⋆ − Miπ t,jπ t )1{τ∧ψ(n,m,T−2)>t} ] + ∆ max T ≤ E [ ∞∑ t=1 ¯∆ 1{τ∧ψ(n,m,T−2)>t} ] + ∆ max T ≤ ¯∆ ψ(n,m,T −2) + ∆ max T where in the second to last inequality, we used the tower rule together with the observation that E[(Mi⋆,j⋆ −Miπ t,jπ t )1{τ∧ψ(n,m,T−2)>t}|Ft−1] = ¯∆ 1{τ∧ψ(n,m,T−2)>t} where Ft−1 is the σ-algebra deﬁned by the observations up to time t− 1. This concludes the proof.G.2 Gap-independent guarantees An immediate consequence of the regret bound in Theorem 7 is that we can have a gap-independent bound under some additional assumption. Let us deﬁne ζ = ∆ max/∆ min, then the regret bound becomes Rπ(T) ≤ ζC4(m+ n) log (e/∆ min) ∆ min log3 (e(m+ n) log (e/∆ min) T2 ∆ min ) + ∆ max T . (57) At the same time, we also have the worst case bound Rπ(T) ≤ ζ∆ minT. (58) T aking the best of the two bounds ( 57) and ( 58) with the worst case choice for ∆ min, we get Rπ(T) = ˜O ( ζ √ (n+ m)Tlog2((n+ m)T) ) where the ˜Ohides additional log-log terms in m,n and T. 48H Related work In this section, we ﬁrst discuss the results for the estimation of a low-rank transition matrix presented in [ 26]. W e then give a more detailed account of the related work for low-rank bandits. H.1 Low-rank transition matrix estimation In [ 26], the authors try to estimate a low-rank transition matrix f rom the data consisting of a single trajectory of the corresponding Markov chain. In a sense, th is objective is similar to ours in Model II(b). The main results of [ 26] are presented in Theorem 1. First observe that our results a re more precise since we manage to get entry-wise guarantees. Then i t is also worth noting that, in the case of homogenous transition matrices, the upper bound on ∥ ˆP − P∥1→∞ stated in Theorem 1 in [ 26] are similar to the upper bounds we establish in Corollary 6. However, to obtain such bounds, we believe that it is necessary to ﬁrst derive guarantees for th e singular subspace recovery in the ℓ2→∞ norm, as we do. The authors of [ 26] do not present any step with such guarantees for the estimat ion of the singular subspaces. W e explain below why this step is m issing and where the analysis towards the upper bound ∥ ˆP − P∥1→∞ breaks in [ 26]. Proof of the guarantees for ∥ ˆP − P∥1→∞ in [ 26]. Note that in [ 26], the authors use F in lieu of M. W e keep our notation M below to be consistent with the rest of the manuscript. In the proof of Theorem 1 in [ 26], the authors use the following decomposition: ∥ ˆMi,: − Mi,:∥ ≤ ∥ (ˆMi,: − Mi,:)V∥ + (∥ ˆMi,: − Mi,:∥ + ∥Mi,:∥)C∥ ˜M− M∥ σr(M) . (59) They apply concentration results on ∥(˜M − M)V∥2→∞ (Lemma 8) and ∥M − ˜M∥ (Lemma 7) to bound the two terms from above. More precisely, their proof i ncludes (33) page 3217, a sequence of inequalities where these concentration results are used . In the ﬁfth line of (33), the authors apply (31), the concentration result on ∥(˜M − M)V∥2→∞, but to bound ∥(ˆM − M)V∥2→∞ instead. Replacing ˆM by ˜M is however not possible, and the analysis breaks here. Is there a simple solution? W e argue below that it is not easy to solve the aforementioned issue in the proof. W e ﬁrst claim that the two concentration bounds on ∥(˜M − M)V∥2→∞ and ∥M − ˜M∥ are not sufﬁcient for bounding the ﬁrst term from Equation ( 59). Speciﬁcally, for any row i: ∥(ˆMi,: − Mi,:)V∥ = ∥(˜Mi,: ˆV ˆV⊤ − Mi,:)V∥ = ∥(˜Mi,: − Mi,:)V + ˜Mi,:(ˆV ˆV⊤ − VV ⊤)V∥, and in order to analyze the second term inside the norm, we nee d to deal with dependence between ˜M and ˆV. Doing this naively using the triangle inequality and Cauch y-Schwarz inequality yields: ∥(ˆM − M)V∥2→∞ ≤ ∥ (˜M − M)V∥2→∞ + ∥ ˜M(ˆV ˆV⊤ − VV ⊤)V∥2→∞ ≤ ∥ (˜M − M)V∥2→∞ + ∥ ˜M∥1→∞∥V − ˆV(ˆV⊤V)∥2→∞. (60) It is not clear how bounds on ∥(˜M−M)V∥2→∞ and ∥M− ˜M∥ imply a bound on ∥(ˆM−M)V∥2→∞ since term ∥V − ˆV(ˆV⊤V)∥2→∞ does not seem to be directly bounded by these two terms. W e can think of bounding ∥V − ˆV(ˆV⊤V)∥2 using Davis-Kahan’s inequality: ∥V − ˆV(ˆV⊤V)∥2→∞ ≤ ∥ V − ˆV(ˆV⊤V)∥2 ≲ ∥M − ˜M∥ σr(M) , where we neglect the higher order term (see Equations ( 47),(48),(49)). Then, with the upper bound on ∥M− ˜M∥, we may obtain an upper bound on ∥ ˆP−P∥1→∞ but that does not have a fast decaying rate as that claimed in Theorem 1 in [ 26] or in our main theorems. It is also worth noting that assuming proof of Theorem 1 in [ 26] holds or that more speciﬁcally, the series of inequalities leading to Equation (33) holds, o ne could greatly simplify the singular subspace recovery problem. In particular, since ∥ ˜M(V − ˆV ˆV⊤V)∥2→∞ = ∥(˜M− ˆM)V∥2→∞ ≤ ∥ (ˆM − M)V∥2→∞ + ∥EV∥2→∞ 49we can rewrite Equation ( 36) (wlog for symmetric matrix M with eigenvector matrix V) as: ∥V − ˆV(ˆV⊤V)∥2→∞ ≤ 1 σr(M) ( (2 + 4∥E∥ σr(M) )∥EV∥2→∞ + 2∥(ˆM− M)V∥2→∞ + 4∥MV∥2→∞∥E∥ σr(M) ) . (61) Now if (33) in [ 26] was true, we could use the correspoding bound of the critica l term ∥(ˆM − M)V∥2→∞. This would not only greatly simplify proofs given in litera ture based on leave-one- out-technique, but also extend their work to Markov depende nt random variables (which has not been done before). Lastly, note that we cannot skip estimati on of singular subspaces by combining Equation ( 60) and ( 61) since inequality 2∥ ˜M∥1→∞ <σr(M) does not hold in general. H.2 Low rank bandits Here we survey models for low-rank bandits that have emergedrecently in the literature but that are not directly related to our model. Nonetheless our guarante es can be exported there. [ 66] considers a bi-linear bandit model which seems more genera l than that of considered [ 6]. Indeed, they assume that the observed reward in round t after selecting a pair (x,z) ∈ X × Z , is x⊤Θ z+ ξt where X ⊂ Rm and Z ⊂ Rn are ﬁnite. They assume that Θ ∈ Rm×n is low rank. If we assume that X = {e1,...,e m} and Z = {e1,...,e n}, we then recover our model and that of [ 6] with M = Θ . However, we can also argue that if we restrict our attention to mvectors from X , say X ′ = {x1,...,x m}, that span Rm, and n vectors from Z, say Z′ = {z1,...,z n}, that span Rn, then in our model and that of [ 6], Mi,j = x⊤ iΘ zj, for all (i,j) ∈ [m] × [n]. Note that in this case, the rank of M is equal to that of Θ . In fact, in the ﬁrst phase of the algorithm proposed by [ 66], the authors also restrict their attention to sets X ′ and Z′ such that λmin(∑ m i=1 xix⊤ i) and λmin(∑ m i=1 ziz⊤ i ) are maximized. T o simplify our exposition, we do not use the m odel presented by [ 66], instead we use that of [ 6]. [46] considers a generalized bandit framework with low rank str ucture which is rather different than the bandit framework we consider. There, the algorithm is ba sed on the two stage idea introduced by [66], which consists in ﬁrst estimating the subspace, then redu cing the problem to a low-dimensional linear bandit with ambient dimension nmbut with roughly n+ mrelevant dimensions. They are able obtain a minimax regret scaling as (n+ m) √ T. It is worth noting that both these works do not have gap-dependent bounds. [ 7] is another relevant work. There, the authors consider a low -rank bandit problem similar to ours but slightly more restrictive. At time t, they recommend an arm ρ(j)t for each user j, and they observe the corresponds rewards. In other words they observ e mentries per round, while in our case we only observe one entry per round. They show that with an exp lore-then-commit algorithm, they attain a regret of order polylog (n+ m)T2/3. Their regret guarantees require an entry-wise matrix estimation guarantee with scaling comparable to ours. They use the result of Chen et al. [ 67] which again is only valid for independent entries and does not acco unt for repetitive sampling. T o remedy that they rely on ad-hoc pre-processing steps (see remarks 2 , 3 and 4 in [ 7]). In our case, we believe that our matrix estimation guarantees can be immediately us ed in their setting and this would lead to a regret scaling of order (n+ m)1/3T2/3 with the more reasonable constraint that we only observe one entry at each round. The authors also obtain an polylog (n+ m) √ T guarantee but for rank-1 reward matrices only. 50",
      "meta_data": {
        "arxiv_id": "2310.06793v2",
        "authors": [
          "Stefan Stojanovic",
          "Yassir Jedra",
          "Alexandre Proutiere"
        ],
        "published_date": "2023-10-10T17:06:41Z",
        "pdf_url": "https://arxiv.org/pdf/2310.06793v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses matrix estimation problems in reinforcement learning (RL) with low-rank structure, focusing on achieving low entry-wise error while accommodating inherent data correlations (e.g., Markovian data in MDPs). It introduces three matrix estimation problems (low-rank bandits, low-rank MDPs with generative model, and low-rank MDPs with forward model) and demonstrates that simple spectral-based estimation methods efficiently recover singular subspaces with nearly-minimal entry-wise error. The authors develop and combine leave-one-out arguments and Poisson approximation techniques to handle data correlations. These results are applied to devise a regret-minimization algorithm for low-rank bandits, yielding state-of-the-art performance guarantees (regret scales as O((m+n)log^3(T) * delta_bar / delta_min^2)), and a best policy identification algorithm for reward-free low-rank MDPs, achieving minimax optimal sample complexity (O(nA/epsilon^2) up to logarithmic factors).",
        "methodology": "The core methodology involves spectral-based matrix estimation. For all problems, an empirical matrix (~M) is constructed from observed data, and the final estimate (^M) is obtained as its best rank-r approximation via spectral decomposition. To address inherent data correlations, particularly in Markovian settings (MDPs), the authors develop and combine advanced theoretical tools: leave-one-out arguments for singular subspace recovery and Poisson approximation techniques. This allows for rigorous analysis even when data entries are not independent. For low-rank bandits, the SME-AE (Successive Matrix Estimation and Arm Elimination) algorithm is proposed, which iteratively samples entries, estimates the reward matrix, and prunes candidate arms. For reward-free low-rank MDPs, a two-phase approach is used: (1) model estimation by collecting trajectories for each action and applying the spectral decomposition method, and (2) a planning phase to compute the optimal policy based on the estimated model.",
        "experimental_setup": "The research is primarily theoretical, establishing performance guarantees through rigorous mathematical analysis rather than empirical experiments with specific datasets. The 'experimental setup' refers to the data generation models for which the theoretical bounds are derived: (1) **Model I: Reward matrices in low-rank bandits**, where arms are randomly selected, and noisy rewards are observed. (2) **Model II(a): Transition matrices in low-rank MDPs under a generative model**, where states are sampled from a distribution, and next states are sampled from the transition kernel. (3) **Model II(b): Transition matrices in low-rank MDPs under a forward model**, where a single trajectory of the Markov chain is observed. Performance is validated by theoretical bounds on singular subspace recovery (e.g., 2->infinity norm) and matrix entry-wise error (e.g., 1->infinity and infinity norms), along with regret guarantees for bandit algorithms and sample complexity for MDP policy identification.",
        "limitations": "A notable limitation is that the cubic dependence in log^3(T) in the regret upper bound for low-rank bandits is an artifact of the Poisson approximation used in the proof techniques for entry-wise guarantees. The minimax optimal sample complexity for low-rank MDPs is specifically proven 'at least when the frequency matrices {Ma}a∈A are homogeneous,' implying less general or tighter bounds for heterogeneous cases are not explicitly provided. For the forward model in MDPs, selecting the data splitting parameter 'τ' requires prior knowledge or an estimation of the Markov chain's mixing time, which is a prerequisite for the guarantees to hold. Furthermore, while the proposed methods are computationally efficient and do not rely on strong computational oracles (unlike some related RL algorithms), they do not directly address all assumptions required by other matrix estimation methods (e.g., access to anchor rows/columns) nor is it clear how those methods would extend to dependent noise or subspace recovery in the 2->infinity norm.",
        "future_research_directions": "The authors suggest several future applications and extensions for their low-rank matrix estimation results: (i) to reward-free RL in episodic MDPs, leveraging the independence across episodes; (ii) to offline RL scenarios where data consists of a single trajectory generated under a given behavior policy, enabling learning of individual transition matrices; (iii) to traditional RL settings where the reward function R needs to be learned, positioning it between the inference problems of Models I and II; (iv) to model-free RL by directly learning the Q-function, similar to [58] under a generative model; and (v) to low-rank RL problems with continuous state spaces, by combining their methods with appropriate discretization of the state space if transition probabilities are smooth."
      }
    },
    {
      "title": "Low-rank Optimal Transport: Approximation, Statistics and Debiasing",
      "abstract": "The matching principles behind optimal transport (OT) play an increasingly\nimportant role in machine learning, a trend which can be observed when OT is\nused to disambiguate datasets in applications (e.g. single-cell genomics) or\nused to improve more complex methods (e.g. balanced attention in transformers\nor self-supervised learning). To scale to more challenging problems, there is a\ngrowing consensus that OT requires solvers that can operate on millions, not\nthousands, of points. The low-rank optimal transport (LOT) approach advocated\nin \\cite{scetbon2021lowrank} holds several promises in that regard, and was\nshown to complement more established entropic regularization approaches, being\nable to insert itself in more complex pipelines, such as quadratic OT. LOT\nrestricts the search for low-cost couplings to those that have a\nlow-nonnegative rank, yielding linear time algorithms in cases of interest.\nHowever, these promises can only be fulfilled if the LOT approach is seen as a\nlegitimate contender to entropic regularization when compared on properties of\ninterest, where the scorecard typically includes theoretical properties\n(statistical complexity and relation to other methods) or practical aspects\n(debiasing, hyperparameter tuning, initialization). We target each of these\nareas in this paper in order to cement the impact of low-rank approaches in\ncomputational OT.",
      "full_text": "Low-rank Optimal Transport: Approximation, Statistics and Debiasing Meyer Scetbon CREST, ENSAE meyer.scetbon@ensae.fr Marco Cuturi Apple and CREST, ENSAE cuturi@apple.com Abstract The matching principles behind optimal transport (OT) play an increasingly impor- tant role in machine learning, a trend which can be observed when OT is used to disambiguate datasets in applications (e.g. single-cell genomics) or used to improve more complex methods (e.g. balanced attention in transformers or self-supervised learning). To scale to more challenging problems, there is a growing consensus that OT requires solvers that can operate on millions, not thousands, of points. The low- rank optimal transport (LOT) approach advocated in Scetbon et al. [2021] holds several promises in that regard, and was shown to complement more established entropic regularization approaches, being able to insert itself in more complex pipelines, such as quadratic OT. LOT restricts the search for low-cost couplings to those that have a low-nonnegative rank, yielding linear time algorithms in cases of interest. However, these promises can only be fulﬁlled if the LOT approach is seen as a legitimate contender to entropic regularization when compared on properties of interest, where the scorecard typically includes theoretical properties (statistical complexity and relation to other methods) or practical aspects (debiasing, hyperparameter tuning, initialization). We target each of these areas in this paper in order to cement the impact of low-rank approaches in computational OT. 1 Introduction Optimal transport (OT) is used across data-science to put in correspondence different sets of observa- tions. These observations may come directly from datasets, or, in more advanced applications, depict intermediate layered representations of data. OT theory provides a single grammar to describe and solve increasingly complex matching problems (linear, quadratic, regularized, unbalanced, etc...), making it gain a stake in various areas of science such as as single-cell biology Schiebinger et al. [2019], Yang et al. [2020], Demetci et al. [2020], imaging Schmitz et al. [2018], Heitz et al. [2020], Zheng et al. [2020] or neuroscience Janati et al. [2020], Koundal et al. [2020]. Regularized approaches to OT. Solving OT problems at scale poses, however, formidable chal- lenges. The most obvious among them is computational: the Kantorovich [1942] problem on discrete measures of size nis a linear program that requires O(n3 log n) operations to be solved. A second and equally important challenge lies in the estimation of OT in high-dimensional settings, since it suffers from the curse-of-dimensionality Fournier and Guillin [2015]. The advent of regularized approaches, such as entropic regularization [Cuturi, 2013], has pushed these boundaries thanks for faster algorithms [Chizat et al., 2020, Clason et al., 2021] and improved statistical aspects [Genevay et al., 2018a]. Despite these clear strengths, regularized OT solvers remain, however, costly as they typically scale quadratically in the number of observations. Scaling up OT using low-rank couplings. While it is always intuitively possible to reduce the size of measures (e.g. using k-means) prior to solving an OT between them, a promising line of work proposes to combine both [Forrow et al., 2019, Scetbon et al., 2021, 2022]. Conceptually, these Preprint. Under review. arXiv:2205.12365v2  [stat.ML]  15 Sep 2022low-rank approaches solve simultaneously both an optimal clustering/aggregation strategy with the computation of an effective transport. This intuition rests on an explicit factorization of couplings into two sub-couplings. This has several computational beneﬁts, since its computational cost becomes linear in nif the ground cost matrix seeded to the OT problem has itself a low-rank. While these computational improvements, mostly demonstrated empirically, hold several promises, the theoretical properties of these methods are not yet well established. This stands in stark contrast to the Sinkhorn approach, which is comparatively much better understood. Our Contributions. The goal of this paper is to advance our knowledge, understanding and practical ability to leverage low-rank factorizations in OT. This paper provides ﬁve contributions, targeting theoretical and practical properties of LOT:(i) We derive the rate of convergence of the low-rank OT to the true OT with respect to the non-nnegative rank parameter.(ii) We make a ﬁrst step towards a better understanding of the statistical complexity of LOT by providing an upper-bound of the statistical error, made when estimating LOT using the plug-in estimator; that upper-bound has a parametric rate O( √ 1/n) that is independent of the dimension. (iii) We introduce a debiased version of LOT: as the Sinkhorn divergence [Feydy et al., 2018], we show that debiased LOT is nonnegative, metrizes the weak convergence, and that it interpolates between the maximum mean discrepancy [Gretton et al., 2012] and OT. (iv) We exhibit links between the bias induced by the low-rank factorization and clustering methods. (v) We propose practical strategies to tune the step-length and the initialization of the algorithm in [Scetbon et al., 2021]. Notations. We consider (X,dX) and (Y,dY) two nonempty compact Polish spaces and we denote M+ 1 (X) (resp. M+ 1 (Y)) the space of positive Radon probability measures on X(resp. Y). For all n ≥1, we denote ∆n the probability simplex of size nand ∆∗ n the subset of ∆n of positive histograms. We write 1n ≜ (1,..., 1)T ∈Rn and we denote similarly ∥·∥2 the Euclidean norm and the Euclidean distance induced by this norm depending on the context. 2 Background on Low-rank Optimal Transport Let µ∈M+ 1 (X), ν ∈M+ 1 (Y) and c: X×Y→ R+ a nonnegative and continuous function. The Kantorovitch formulation of optimal transport between µand νis deﬁned by OTc(µ,ν) ≜ min π∈Π(µ,ν) ∫ X×Y c(x,y)dπ(x,y) , (1) where the feasible set is the set of distributions over the product spaceX×Y with marginals µand ν: Π(µ,ν) ≜ { π∈M+ 1 (X×Y ) s.t. P1#π = µ, P2#π = ν } , with P1#π (resp. P2#π), the pushforward probability measure of π using the projection maps P1(x,y) = x(resp. P2(x,y) = y). When there exists an optimal coupling solution of (1) supported on a graph of a function, we call such function a Monge map. In the discrete setting, one can reformulate the optimal transport problem as a linear program over the space of nonnegative matrices satisfying the marginal constraints. More precisely, let aand bbe respectively elements of ∆∗ n and ∆∗ m and let also X ≜ {x1,...,x n}and Y ≜ {y1,...,y m}be respectively two subsets of Xand Y. By denoting µa,X ≜ ∑n i=1 aiδxi and νb,Y ≜ ∑m j=1 bjδyj the two discrete distributions associated and writing C ≜ [c(xi,yj)]i,j, the discrete optimal transport problem can be formulated as OTc(µa,X,νb,Y) = min P∈Πa,b ⟨C,P⟩ where Πa,b ≜ {P ∈Rn×m + s.t. P1m = a,PT1n = b}. (2) Scetbon et al. [2021] propose to constrain the discrete optimal transport problem to couplings that have a low-nonnegative rank: Deﬁnition 1. Given M ∈ Rn×m + , the nonnegative rank of M is deﬁned by: rk+(M) ≜ min{q|M = ∑q i=1 Ri,∀i,rk(Ri) = 1,Ri ≥0}. Note that for any M ∈ Rn×m + , we always have that rk+(M) ≤ min(n,m). For r ≥ 1, we consider the set of couplings satisfying marginal constaints with nonnegative-rank of at most ras Πa,b(r) ≜ {P ∈Πa,b, rk +(P) ≤r}. The discrete Low-rank Optimal Transport (LOT) problem is deﬁned by: LOTr,c(µa,X,νb,Y) ≜ min P∈Πa,b(r) ⟨C,P⟩. (3) 2To solve this problem, Scetbon et al. [2021] show that Problem (3) is equivalent to min (Q,R,g)∈C1(a,b,r)∩C2(r) ⟨C,Q diag(1/g)RT⟩, (4) where C1(a,b,r ) ≜ { (Q,R,g ) ∈Rn×r + ×Rm×r + ×(R∗ +)r s.t. Q1r = a,R1r = b } and C2(r) ≜{ (Q,R,g ) ∈Rn×r + ×Rm×r + ×Rr + s.t. QT1n = RT1m = g } .They propose to solve it using a mirror descent scheme and prove the non-asymptotic stationary convergence of their algorithm. While Scetbon et al. [2021] only focus on the discrete setting, we consider here its extension for arbitrary probability measures. Following [Forrow et al., 2019], we deﬁne the set of rank-rcouplings satisfying marginal constraints by: Πr(µ,ν) ≜ {π∈Π(µ,ν) : ∃(µi)r i=1 ∈M+ 1 (X)r, (νi)r i=1 ∈M+ 1 (Y)r, λ∈∆∗ r s.t. π= r∑ i=1 λiµi⊗νi}. This more general deﬁnition of LOT between µ∈M+ 1 (X) and ν ∈M+ 1 (Y) reads: LOTr,c(µ,ν) ≜ inf π∈Πr(µ,ν) ∫ X×Y c(x,y)dπ(x,y) . (5) Note that this deﬁnition of LOTr,c is consistent as it coincides with the one deﬁned in (3) on discrete probability measures. Observe also that Πr(µ,ν) is compact for the weak topology and therefore the inﬁmum in (5) is attained. See Appendix A for more details. 3 Approximation Error of LOT to original OT as a function of rank Our goal in this section is to obtain a control of the error induced by the low-rank constraint when trying to approximate the true OT cost. We provide ﬁrst a control of the approximation error in the discrete setting. The proof is given in Appendix B.1. Proposition 1. Let n,m ≥2, X ≜ {x1,...,x n}⊂X , Y ≜ {y1,...,y m}⊂Y and a∈∆∗ n and b∈∆∗ m. Then for 2 ≤r≤min(n,m), we have that |LOTr,c(µa,X,νb,Y) −OTc(µa,X,νb,Y)|≤∥ C∥∞ln(min(n,m)/(r−1)) Remark 1. Note that this result improves the control obtained in [Liu et al., 2021], where they obtain that |LOTr,c(µa,X,νb,Y) −OTc(µa,X,νb,Y)|≲ ∥C∥∞ √nm(min(n,m) −r) as we have for any z,z′≥1, |ln(z) −ln(z′)|≤| z−z′|. It is in fact possible to obtain another control of the approximation error by partitioning the space where the measures are supported. For that purpose let us introduce the notion of entropy numbers. Deﬁnition 2. Let (Z,d) a metric space, W⊂Z and k≥1 an integer. Then by denotingBZ(z,ε) ≜ {y∈Z : d(z,y) ≤ε}, we deﬁne the k-th (dyadic) entropy number of Was Nk(W,d) ≜ inf{εs.t. ∃z1,...,z 2k ∈Z : W⊂∪ 2k i=1BZ(zi,ε)}. For example, any compact set Wof Rd admits ﬁnite entropy numbers, and by denoting R ≜ supw∈W∥w∥2, we have Nk(W,∥·∥2) ≤4R/2k/d. We obtain next a control of the approximation error of LOTr,c to the true OT cost using entropy numbers (see proof in Appendix B.2). Proposition 2. Let µ∈M+ 1 (X), ν ∈M+ 1 (Y) and assume that cis L-Lipschitz w.r.t.xand y. Then for any r≥1, we have |LOTr,c(µ,ν) −OTc(µ,ν)|≤ 2Lmax(N⌊log2(⌊√r⌋)⌋(X,dX),N⌊log2(⌊√r⌋)⌋(Y,dY)) This results in the following bound for the p-Wasserstein distance for any p≥1 on Rd. Corollary 1. Let d ≥1, p ≥1, Xa compact subspace of Rd and µ,ν ∈M+ 1 (X). By denoting R≜ supx∈X∥x∥2, we obtain that for any r≥1, |LOTr,∥·∥p 2 (µ,ν) −OT∥·∥p 2 (µ,ν)|≤ 4dp(8R2)p rp/2d . 3As per the Proof of Proposition 2 we can provide a tighter control, assuming a Monge map exists. Corollary 2. Under the same assumptions of Proposition 2 and by assuming in addition that there exists a Monge map solving OTc(µ,ν), we obtain that for any r≥1, |LOTr,c(µ,ν) −OTc(µ,ν)|≤ LN⌊log2(r)⌋(Y,dY) . When X= Yare a subspaces of Rd, a sufﬁcient condition for a Monge map to exists is that either µ or νis absolutely continuous with respect to the Lebesgue measure and that cis of the form h(x−y) where h: X→ R+ is a strictly convex function [Santambrogio, 2015, Theorem 1.17]. Therefore if µis absolutely continuous with respect to the Lebesgue measure, we obtain for any r≥1 and p> 1 |LOTr,∥·∥p 2 (µ,ν) −OT∥·∥p 2 (µ,ν)|≤ 2dp(8R2)p rp/d . 4 Sample Complexity of LOT We now focus on the statistical performance of the plug-in estimator for LOT. In the following we assume that X= Yfor simplicity. Given µ,ν ∈M+ 1 (X), we denote the empirical measures associated ˆµn ≜ 1 n ∑n i=1 δXi and ˆνn ≜ 1 n ∑n i=1 δYi, where (Xi,Yi)n i=1 are sampled independently from µ⊗ν. We consider the plug-in estimator deﬁned as LOTr,c(ˆµn,ˆνn), and we aim at quantifying the rate at which it converges towards the true low-rank optimal transport cost LOTr,c(µ,ν). Before doing so, in the next Proposition we show that this estimator is consistent on compact spaces. The proof is given in Appendix B.3. Proposition 3. Let r≥1 and µ,ν ∈M+ 1 (X), then LOTr,c(ˆµn,ˆνn) −−−−−→ n→+∞ LOTr,c(µ,ν) a.s. Next we aim at obtaining the convergence rates of our plug-in estimator. In the following Proposition, we obtain a non-asymptotic upper-bound of the statistical error. See Appendix B.4 for the proof. Proposition 4. Let r ≥1 and µ,ν ∈M+ 1 (X). Then, there exists a constant Kr such that for any δ >0 and n≥1, we have, with a probability of at least 1 −2δ, that LOTr,c(ˆµn,ˆνn) ≤LOTr,c(µ,ν) + 11∥c∥∞ √r n + Kr∥c∥∞ [√ log(40/δ) n + √rlog(40/δ) n ] . This result is, to the best of our knowledge, the ﬁrst attempt at providing a statistical control of low-rank optimal transport. We provide an upper-bound of the plug-in estimator which converges towards LOTr,c at a parametric rate and which is independent of the dimension on general compact metric spaces. While we fall short of providing a lower bound that could match that upper bound, and therefore provide a complete statistical complexity result, we believe this result might provide a ﬁrst explanation on why, in practice, LOTr,c displays better statistical properties than unregularized OT and its curse of dimensionality [Dudley, 1969]. In addition, that upper bound compares favorably to known results on entropic optimal transport. The rate of entropy regularized OT does not depend on the ambient dimension with respect to n, but carries an exponential dependence in dimension with respect to the regularization parameter ε[Mena and Niles-Weed, 2019]. By contrast, the term associated with the nonnegative rank rin our bound has no direct dependence on dimension. Our next aim is to obtain an explicit rate with respect to rand n. In Proposition 4, we cannot control explicitly Kr in the general setting. Indeed, in our proof, we obtain that Kr ≜ 14/miniλ∗ i where (λ∗ i)r i=1 ∈∆∗ r are the weights involved in the decomposition of one optimal solution of the true LOTr,c(µ,ν). Therefore the control of Kr requires additional assumptions on the optimal solutions of LOTr,c(µ,ν). In the following Proposition, we obtain an explicit upper-bound of the plug-in estimator with respect to rand nin the asymptotic regime. Proposition 5. Let r≥1, δ >0 and µ,ν ∈M+ 1 (X). Then there exists a constant Nr,δ such that if n≥Nr,δ then with a probability of at least 1 −2δ, we have LOTr,c(ˆµn,ˆνn) ≤LOTr,c(µ,ν) + 11∥c∥∞ √r n + 77∥c∥∞ √ log(40/δ) n . 4Note that one cannot recover the result obtained in Proposition 5 from the one obtained in Proposition 4 as we have that Kr ≥14r− −−−− → r→+∞ +∞. In order to prove the above result, we use an extension of the McDiarmid’s inequality when differences are bounded with high probability [Kutin, 2002]. See proof in Appendix B.5 for more details. 5 Debiased Formulation of LOT We introduce here the debiased formulation of LOTr,c and show that it is able to distinguish two distributions, metrize the convergence in law and can be used as a new objective in order to learn distributions. We focus next on the debiasing terms involving measures with themselvesLOTr,c(µ,µ) in this new divergence, and show that they can be interpreted as deﬁning a new clustering method generalizing k-means for any geometry. 5.1 On the Proprieties of the Debiased Low-rank Optimal Transport When it comes to learn (or generate) a distribution in ML applications given samples, it is crucial to consider a divergence that is able to distinguish between two distributions and metrize the convergence in law. In general, LOTr,c(µ,µ) ̸= 0 and the minimum of LOTr,c(ν,µ) with respect to νwill not necessarily recover µ. In order to alleviate this issue we propose a debiased version of LOTr,c deﬁned for any µ,ν ∈M+ 1 (X) as DLOTr,c(µ,ν) ≜ LOTr,c(µ,ν) −1 2[LOTr,c(µ,µ) + LOTr,c(ν,ν)] . Note that DLOTr,c(ν,ν) = 0 . In the next Proposition, we show that, as the Sinkhorn diver- gence [Genevay et al., 2018b, Feydy et al., 2018], DLOTr,c interpolates between the Maximum Mean Discrepancy (MMD) and OT. See proof in Appendix B.6. Proposition 6. Let µ,ν ∈M+ 1 (X). Let us assume that cis symmetric, then we have DLOT1,c(µ,ν) = 1 2 ∫ X2 −c(x,y)d[µ−ν] ⊗d[µ−ν](x,y) . If in addition we assume the cis Lipschitz w.r.t toxand y, then we have DLOTr,c(µ,ν) − −−−− → r→+∞ OTc(µ,ν) . Next, we aim at showing some useful properties of the debiased low-rank OT for machine learning applications. For that purpose, let us ﬁrst recall some deﬁnitions. Deﬁnition 3. We say that the cost c : X×X → R+ is a semimetric on Xif for all x,x′ ∈X , c(x,x′) = c(x′,x) and c(x,x′) = 0 if and only ifx= x′. In addition we say thatchas a negative type if ∀n≥2, x1,...,x n ∈X and α1,...,α n ∈R such that ∑n i=1 αi = 0, ∑n i,j=1 αiαjc(xi,xj) ≤0. We say also thatchas a strong negative type if for allµ,ν ∈M+ 1 (X), µ̸= ν =⇒ ∫ X2 c(x,y)d[µ− ν] ⊗[µ−ν] <0. Note that if chas a strong negative type, then chas a negative type too. For example, all Euclidean spaces and even separable Hilbert spaces endowed with the metric induced by their inner products have strong negative type. Also, onRd, the squared Euclidean distance has a negative type [Sejdinovic et al., 2013]. We can now provide stronger geometric guarantees for DLOTr,c. In the next Proposition, we show that for a large class of cost functions, DLOTr,c is nonnegative, able to distinguish two distributions, and metrizes the convergence in law. The proof is given in Appendix B.8. Proposition 7. Let r ≥1, and let us assume that cis a semimetric of negative type. Then for all µ,ν ∈M+ 1 (X), we have that DLOTr(µ,ν) ≥0 . In addition, if chas strong negative type then we have also that DLOTr,c(µ,ν) = 0 ⇐⇒µ= ν and µn →µ ⇐⇒DLOTr,c(µn,µ) →0 . where the convergence of the sequence of probability measures considered is the convergence in law. 5Observe that when chas strong negative type, ν →DLOTr,c(ν,µ) ≥0 and it admits a unique global minimizer at ν = µ. Therefore, DLOTr,c has desirable properties to be used as a loss. It is also worth noting that, in order to obtain the metrization of the convergence in law, we show the following Proposition. See proof in Appendix B.7. Proposition 8. Let r ≥1 and (µn)n≥0 and (νn)n≥0 two sequences of probability measures such that µn →µand νn →νwith respect to the convergence in law. Then we have that LOTr,c(µn,νn) →LOTr,c(µ,ν) . 5.2 Low-Rank Transport Bias and Clustering We turn next to the debiasing terms appearing inDLOT and exhibit links between LOT and clustering methods. Indeed, in the discrete setting, the low-rank bias of a probability measure µdeﬁned as LOTk,c(µ,µ) can be seen as a generalized version of the k-means method for any geometry. In the next Proposition we obtain a new formulation of LOTk,c(µ,µ) viewed as a general clustering method on arbitrary metric space. See proof in Appendix B.9. Proposition 9. Let n≥k≥1, X ≜ {x1,...,x n}⊂X and a∈∆∗ n. If cis a semimetric of negative type, then by denoting C = (c(xi,xj))i,j, we have that LOTk,c(µa,X,µa,X) = min Q ⟨C,Qdiag(1/QT1n)QT⟩ s.t. Q∈Rn×k + , Q1k = a. (6) Let us now explain in more details the link between (6) and k-means. When Xis a subspace of Rd, c is the squared Euclidean distance and a= 1n, we recover exactly the k-means algorithm. Corollary 3. Let n≥k≥1 and X ≜ {x1,...,x n}⊂ Rd. We have that LOTk,∥·∥2 2 (µ1n,X,µa,X) = 2 min Q,z1,...,zk n∑ i=1 k∑ q=1 Qi,q∥xi −zq∥2 2 s.t. Q∈{0,1}n×k, Q1k = 1n . In the general setting, solving LOTk,c(µa,X,µa,X) for a given geometry c, and a prescribed histro- gram aoffers a new clustering method where the assignment of the points to the clusters is determined by the matrix Q∗solution of (6). 6 Computing LOT: Adaptive Stepsizes and Better Initializations We target in this section practical issues that arises when using [Scetbon et al., 2021, Algo.3] to solve (4). Scetbon et al. [2021] propose to apply a mirror descent scheme with respect to the Kullback- Leibler divergence which boils down to solve at each iteration k≥0 the following convex problem using the Dykstra’s Algorithm [Dykstra, 1983]: (Qk+1,Rk+1,gk+1) ≜ argmin ζ∈C1(a,b,r)∩C2(r) KL(ζ,ξk) . (7) where (Q0,R0,g0) ∈ C1(a,b,r ) ∩ C2(r), ξk ≜ (ξ(1) k ,ξ(2) k ,ξ(3) k ), ξ(1) k ≜ Qk ⊙ exp(−γkCRkdiag(1/gk)), ξ(2) k ≜ Rk ⊙exp(−γkCTQkdiag(1/gk)), ξ(3) k ≜ gk ⊙exp(γkωk/g2 k) with [ωk]i ≜ [QT kCRk]i,i for all i ∈{1,...,r }, KL(w,r) ≜ ∑ iwilog(wi/ri) and (γk)k≥0 is a sequence of positive step sizes. In the general setting, each iteration of their algorithm requires O(nmr) operations and when the ground cost matrix Cadmits a low-rank factorization of the form C = ABT where A ∈Rn×q and B ∈Rm×q with q ≪min(n,m), then the total complexity per iteration becomes linear O((n+ m)rq). Note that for the squared Euclidean cost on Rd, we have that q= d+ 2. In the following we investigate two practical aspects of the algorithm: the choice of the step sizes and the initialization. Adaptive choice of γk. Scetbon et al. [2021] show experimentally that the choice of (γk)k≥0 does not impact the solution obtained upon convergence, but rather the speed at which it is attained. Indeed the larger γk is, the faster the algorithm will converge. As a result, their algorithm simply relies on a ﬁxed γschedule. However, the range of admissible γdepends on the problem considered and it may 6vary from one problem to another. Indeed, the algorithm might fail to converge as one needs to ensure at each iteration kof the mirror descent scheme that the kernels ξk do not admit 0 entries in order to solve (7) using the Dykstra’s Algorithm. Such a situation can occur when the terms involved in the exponentials become too large which may depend on the problem considered. Therefore, it may be of particular interest for practitioners to have a generic range of admissible values forγindependently of the considered problem, in order to alleviate parameter tuning issues. We propose to consider instead an adaptive choice of (γk)k≥0 along iterations. D’Orazio et al. [2021], Bayandina et al. [2018] have proposed adaptive mirror descent schemes where, at each iteration, the step-size is normalized by the squared dual-norm of the gradient. Applying such a strategy in our case amounts to consider at each iteration γk = γ ∥(CRdiag(1/g),CTQdiag(1/g),−D(QTRC)/g2) ∥2∞ , (8) where the initial γ > 0 is ﬁxed. By doing so, we are able to guarantee a lower-bound of the exponential terms involved in the expression of the kernels ξk at each iteration and prevent them from having 0 entries. We recommend to set such as global γ ∈[1,10], and observe that this range works whatever the problem considered. On the choice of the initialization. As LOTr,c(4) is a non-convex optimization problem, the ques- tion of choosing an efﬁcient initialization arises in practice. Scetbon et al. [2021] show experimentally that the convergence of the algorithm does not depend on the initalization chosen if no stopping criterion is used. Indeed, their experimental ﬁndings support that only well behaved local minimas are attractive. However, in practice one needs to use a stopping criterion in order to terminate the algorithm. We do observe in many instances that using trivial initializers may result in spurious local minima, which trigger the stopping criterion early on and prevent the algorithm to reach a good solution. Based on various experimentations, we propose to consider a novel initialization of the algorithm. Our initialization aims at being close to a well-behaved local minimum by clustering the input measures. When the measures are supported on Euclidean space, we propose to ﬁnd r centroids (zi)r i=1 of one of the two input discrete probability measures using k-means and to solve the following convex barycenter problem: min Q,R ⟨CX,Z,Q⟩+ ⟨CY,Z,R⟩−εH(Q) −εH(R) s.t. Q1n = a, R1n = b, QT1r = RT1r, (9) where CX,Z = (c(xi,zj))i,j, CY,Z = (c(yi,zj))i,j, and H(P) = −∑ i,jPi,j(log(Pi,j −1). In practice we ﬁx ε = 1 /10 and we then initialize LOTr,c using (Q,R) solution of (9) and g ≜ QT1r(= RT1r). Note that (Q,R,g ) is an admissible initialization and ﬁnding the centroids as well as solving (9) requires O((n+ m)r) algebraic operations. Therefore such initialization does not change the total complexity of the algorithm. In the general (non-Euclidean) case, we propose to initialize the algorithm by applying our generalized k-means approach deﬁned in (6) on each input measure where we ﬁx the common marginal to be g = 1r/r. More precisely, by denoting CX,X = ( c(xi,xj))i,j and CY,Y = ( c(yi,yj))i,j, we initialize the algorithm by solving: Q∈argmin Q ⟨CX,X,Qdiag(1/QT1n)QT⟩ s.t. Q∈Rn×k + , Q1k = a, QT1n = 1r/r. R∈argmin R ⟨CY,Y ,Rdiag(1/RT1m)RT⟩ s.t. R∈Rm×k + , R1k = b, RT1n = 1r/r. (10) Note that again the(Q,R,g ) obtained is an admissible initialization and the complexity of solving(10) is of the same order as solving (4), thus the total complexity of the algorithm remains the same. 7 Experiments In this section, we illustrate experimentally our theoretical ﬁndings and show how our initialization provide practical improvements. For that purpose we consider 3 synthetic problems and one real world dataset to: (i) provide illustrations on the statistical rates of LOTr,c, (ii) exhibit the gradient ﬂow of the debiased formulation DLOTr,c, (iii) use the clustering method induced by LOTr,c, and (iv) show the effect of the initialization. All experiments were run on a MacBook Pro 2019 laptop. 7101 102 103 sample size 10 2 10 1 DLOT dimension = 2 101 102 103 sample size 10 2 10 1 100 dimension = 5 101 102 103 sample size 10 1 100 dimension = 7 101 102 103 sample size 10 1 100 dimension = 10 Upper Bound (r = 1) DLOT: r = 10 DLOT: r = 50 DLOT: r = 100 Figure 1: In this experiment, we consider a mixture of 10 anisotropic Gaussians supported on Rd and we plot the value of DLOTr,c between two independent empirical measures associated to this mixture when varying the number of samples nand the dimension dfor multiple ranks r. The ground cost considered is the squared Euclidean distance. Note that LOTr(µ,µ) ̸= 0 and therefore we use DLOTr,c(µ,µ) instead to evaluate the rates. Each point has been obtained by repeating 10 times the experiment. We compare the empirical rates obtained with the theoretical one derived in Proposition 4 for r= 1. We observe that our theoretical results match the empirical ones and, as expected, the rates do not depend on d. DLOT, r=100 LOT, r=100 Figure 2: We compare the gradient ﬂows (µt)t≥0 (in red) starting from a Gaussian distribution, µ0, to a moon shape distribution (in blue), ν, in 2D when minimizing either L(µ) ≜ DLOTr,c(µ,ν) or L(µ) ≜ LOTr,c(µ,ν). The ground cost is the squared Euclidean distance and we ﬁx r = 100. We consider 1000 samples from each distribution and and we plot the evolution of the probability measure obtained along the iterations of a gradient descent scheme. We also display in green the vector ﬁeld in the descent direction. We show that the debiased version allows to recover the target distribution while LOTr,c is learning a biased version with a low-rank structure. Statistical rates. We aim at showing the statistical rates of the plug-in estimator of LOTr,c. As LOTr,c(µ,µ) ̸= 0 and as we do not have access to this value given samples from µ, we consider instead the debiased version of the low-rank optimal transport, DLOTr,c. In ﬁgure 1, we show that the empiricial rates match the theoretical bound obtained in Proposition 4. In particular, we show that that these rates does not depend on the dimension of the ground space. Note also that we recover our theoretical dependence with respect to the rank r: the higher the rank, the slower the convergence. Gradient Flows using DLOT. We illustrate here a practical use of DLOT for ML application. In ﬁgure 6, we consider Y1,...,Y n independent samples from a moon shape distribution in 2D, and by denoting ˆνn the empirical measure associated, we show the iterations obtained by a gradient descent scheme on the following optimization problem: min X∈Rn×2 DLOTr,c(µ1n/n,X,ˆνn) . We initialize the algorithm using n= 1000 samples drawn from a Gaussian distribution. We show that the gradient ﬂow of our debiased version is able to recover the target distribution. We also compare it with the gradient ﬂow of the biased version (LOT) and show that it fails to reproduce the target distribution as it is learning a biased one with a low-rank structure. Application to Clustering. In this experiment we show some applications of the clustering method induced by LOTr,c. In ﬁgure 3, we consider 6 datasets with different structure and we aim at recovering the clusters using (6) for some well chosen costs. We compare the clusters obtained when considering either the squared Euclidean cost (which amounts at applying the k-means) and the 8Figure 3: In this experiment, we draw 1000 samples from multiple distributions from the python package scikit-learn [Pedregosa et al., 2011] and we apply the method proposed in (6) for two different costs: in the top row we consider the squared Euclidean distance while in the bottom row, we consider the shortest path distance on the graph associated with the ground cost c(x,y) = 1 −k(x,y) where kis a Gaussian kernel. In the two ﬁrst problem (starting from the left), we ﬁx r= 2, in the next three problem we ﬁx r= 3 and in the last one we ﬁx r= 4. We observe that the ﬂexibility of our method allows to recover the clustering for a well chosen ground cost. 105 107 109 Operations 0.2 0.3 0.4 0.5LOT cost Init: kmeans, r=10 Init: kmeans, r=100 Init: gkmeans, r=10 Init: gkmeans, r=100 Init: rank_2, r=10 Init: rank_2, r=100 Init: random, r=10 Init: random, r=100 106 107 108 109 Operations 10 9 10 7 10 5 10 3 LOT criterion: k Figure 4: In this experiment, we consider the Newsgroup20 dataset [Pedregosa et al., 2011] constituted of texts and we embed them into distributions in 50D using the same pre-processing steps as in [Cuturi et al., 2022]. We compare different initialization when applying the algorithm of [Scetbon et al., 2021] to compare random texts viewed as distributions for multiple choices of rank r. The ground cost considered in the squared Euclidean distance. We repeat the experiments 50 times by sampling randomly multiple problems of similar size (≃250 samples). We normalize the cost matrix by its maximum value in order to have comparable LOT cost. We consider 4 different initialization: the one using k-means algorithm (9), the one using the generalized k-means (10), the rank-2 initialization [Scetbon et al., 2021] and a random initialization where Q,R and gare drawn from Gaussians. We compare both the cost value and the criterion value (∆k) along the iterations of the MD scheme. Note that the curves obtained do not start at the same point in time as we start plotting the curves after obtaining the initial point which in some case requires more algebraic operations (e.g. kmeans methods). First we observe that whatever the initialization considered, the algorithm converges toward the same value. In addition, we observe that both k-means and general k-means are able to initialize well the algorithm by avoiding bad local minima at initialization while the two other initialization are close to spurious local minima at initialization. shortest-path distance on the data viewed as a graph. We show that our method is able to recover the clusters on these settings for well chosen costs and therefore the proposed algorithm in Scetbon et al. [2021] can be seen as a new alternative in order to clusterize data. Effect of the Initialization. Our goal here is to show the effect of the initialization. In ﬁgure 4, we display the evolution of the cost as well as the value of the stopping criterion along the iterations of the MD scheme solving (4) when considering different initialization. The x-axis corresponds to the total number of algebraic operations. This number is computed at each iteration of the outer loop of the algorithm proposed in Scetbon et al. [2021] and is obtained by computing the complexity of all the operations involved in their algorithm to reach it. We consider this notion of time instead of 9CPU/GPU time as we do not want to be architecture/machine dependent. Recall also that the stopping criterion introduced in [Scetbon et al., 2021] is deﬁned for all k≥1 by ∆k ≜ 1 γ2 k (KL((Qk,Rk,gk),(Qk−1,Rk−1,gk−1)) + KL((Qk−1,Rk−1,gk−1),(Qk,Rk,gk))), where ((Qk,Rk,gk))k≥0 is the sequence solution of (7). First, we show that whatever the initializa- tion chosen, the algorithm manages to converge to an efﬁcient solution if no stopping criterion is used. However, the choice of the initialization may impact the termination of the algorithm as some initialization might be too close to some spurious local minima. Indeed, the initial points obtained using a “rank 2” or random initialization can be close to spurious and non-attractive local minima, which may trigger the stopping criterion too early and prevent the algorithm from continuing to run in order to converge towards an attractive and well behaved local minimum. We show also that the initialization we propose in (9) and (10) are sufﬁciently far away from bad local minima and allow the algorithm to converge directly toward the desired solution. The right ﬁgure of Fig.4 shows two main observations: (i) that the initial point obtained using a “rank 2” or random initialization can be close to spurious and non-attractive local minima, which may trigger the stopping criterion too early and prevent the algorithm from continuing to run in order to converge towards an attractive and well behaved local minimum. (ii) When initialiazing the algorithm using kmeans methods, we show that our stopping criterion is a decreasing function of time meaning that the algorithm converges directly towards the desired solution. Conclusion. We assembled in this work theoretical and practical arguments to support low-rank factorizations for OT. We have presented two controls: one concerning the approximation error to the true optimal transport and another concerning the statistical rates of the plug-in estimator. The latter is showed to be independent of the dimension, which is of particular interest when studying OT in ML settings. We have motivated further the use of LOT as a loss by introducing its debiased version and showed that it possesses desirable properties: positivity and metrization of the convergence in law. We have also presented the links between the bias induced by such regularization and clustering methods, and studied empirically the effects of hyperparameters involved in the practical estimation of LOT. The strong theoretical foundations provided in this paper motivate further studies of the empirical behaviour of LOT estimator, notably on ﬁnding suitable local minima and on improvements on the convergence of the MD scheme using other adaptive choices for step sizes. 10Acknowledgements. This work was supported by a \"Chaire d’excellence de l’IDEX Paris Saclay\". The authors would also like to thank Gabriel Peyré and Jaouad Mourtada for enlightening conversa- tions on the topics discussed in this work. References Anastasia Bayandina, Pavel Dvurechensky, Alexander Gasnikov, Fedor Stonyakin, and Alexander Titov. Mirror descent and convex optimization problems with non-smooth inequality constraints. In Large-scale and distributed optimization, pages 181–213. Springer, 2018. Lenaic Chizat, Pierre Roussillon, Flavien Léger, François-Xavier Vialard, and Gabriel Peyré. Faster wasserstein distance estimation with the sinkhorn divergence. Advances in Neural Information Processing Systems, 33, 2020. Christian Clason, Dirk A. Lorenz, Hinrich Mahler, and Benedikt Wirth. Entropic regularization of continuous optimal transport problems. Journal of Mathematical Analysis and Applications, 494 (1):124432, 2021. ISSN 0022-247X. doi: https://doi.org/10.1016/j.jmaa.2020.124432. Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in neural information processing systems, pages 2292–2300, 2013. Marco Cuturi, Laetitia Meng-Papaxanthos, Yingtao Tian, Charlotte Bunne, Geoff Davis, and Olivier Teboul. Optimal transport tools (ott): A jax toolbox for all things wasserstein. arXiv preprint arXiv:2201.12324, 2022. Pinar Demetci, Rebecca Santorella, Björn Sandstede, William Stafford Noble, and Ritambhara Singh. Gromov-wasserstein optimal transport to align single-cell multi-omics data. BioRxiv, 2020. Ryan D’Orazio, Nicolas Loizou, Issam Laradji, and Ioannis Mitliagkas. Stochastic mirror descent: Convergence analysis and adaptive variants via the mirror stochastic polyak stepsize.arXiv preprint arXiv:2110.15412, 2021. Richard Mansﬁeld Dudley. The speed of mean glivenko-cantelli convergence. The Annals of Mathematical Statistics, 40(1):40–50, 1969. Richard L Dykstra. An algorithm for restricted least squares regression. Journal of the American Statistical Association, 78(384):837–842, 1983. Jean Feydy, Thibault Séjourné, François-Xavier Vialard, Shun-Ichi Amari, Alain Trouvé, and Gabriel Peyré. Interpolating between optimal transport and mmd using sinkhorn divergences. arXiv preprint arXiv:1810.08278, 2018. Aden Forrow, Jan-Christian Hütter, Mor Nitzan, Philippe Rigollet, Geoffrey Schiebinger, and Jonathan Weed. Statistical optimal transport via factored couplings. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pages 2454–2465. PMLR, 2019. Nicolas Fournier and Arnaud Guillin. On the rate of convergence in Wasserstein distance of the empirical measure. Probability Theory and Related Fields, 162(3-4):707–738, 2015. Aude Genevay, Lénaic Chizat, Francis Bach, Marco Cuturi, and Gabriel Peyré. Sample complexity of sinkhorn divergences. arXiv preprint arXiv:1810.02733, 2018a. Aude Genevay, Gabriel Peyre, and Marco Cuturi. Learning generative models with sinkhorn diver- gences. In Proceedings of the Twenty-First International Conference on Artiﬁcial Intelligence and Statistics, Proceedings of Machine Learning Research. PMLR, 09–11 Apr 2018b. Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Schölkopf, and Alexander Smola. A kernel two-sample test. Journal of Machine Learning Research, 13(Mar):723–773, 2012. Matthieu Heitz, Nicolas Bonneel, David Coeurjolly, Marco Cuturi, and Gabriel Peyré. Ground metric learning on graphs. Journal of Mathematical Imaging and Vision, pages 1–19, 2020. 11Hicham Janati, Thomas Bazeille, Bertrand Thirion, Marco Cuturi, and Alexandre Gramfort. Multi- subject meg/eeg source imaging with sparse multi-task regression. NeuroImage, page 116847, 2020. Leonid Kantorovich. On the transfer of masses (in russian). Doklady Akademii Nauk, 37(2):227–229, 1942. Sunil Koundal, Rena Elkin, Saad Nadeem, Yuechuan Xue, Stefan Constantinou, Simon Sanggaard, Xiaodan Liu, Brittany Monte, Feng Xu, William Van Nostrand, et al. Optimal mass transport with lagrangian workﬂow reveals advective and diffusion driven solute transport in the glymphatic system. Scientiﬁc reports, 10(1):1–18, 2020. Samuel Kutin. Extensions to mcdiarmid’s inequality when differences are bounded with high probability. Dept. Comput. Sci., Univ. Chicago, Chicago, IL, USA, Tech. Rep. TR-2002-04, 2002. Weijie Liu, Chao Zhang, Nenggan Zheng, and Hui Qian. Approximating optimal transport via low-rank and sparse factorization. arXiv preprint arXiv:2111.06546, 2021. Gonzalo Mena and Jonathan Niles-Weed. Statistical bounds for entropic optimal transport: sample complexity and the central limit theorem. Advances in Neural Information Processing Systems, 32, 2019. Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine learning in python. the Journal of machine Learning research, 12:2825–2830, 2011. Filippo Santambrogio. Optimal transport for applied mathematicians. Birkäuser, NY, 55(58-63):94, 2015. Meyer Scetbon, Marco Cuturi, and Gabriel Peyré. Low-rank sinkhorn factorization, 2021. Meyer Scetbon, Gabriel Peyré, and Marco Cuturi. Linear-time gromov wasserstein distances using low rank couplings and costs. ICML, 2022. Geoffrey Schiebinger, Jian Shu, Marcin Tabaka, Brian Cleary, Vidya Subramanian, Aryeh Solomon, Joshua Gould, Siyan Liu, Stacie Lin, Peter Berube, et al. Optimal-transport analysis of single-cell gene expression identiﬁes developmental trajectories in reprogramming. Cell, 176(4):928–943, 2019. Morgan A Schmitz, Matthieu Heitz, Nicolas Bonneel, Fred Ngole, David Coeurjolly, Marco Cuturi, Gabriel Peyré, and Jean-Luc Starck. Wasserstein dictionary learning: Optimal transport-based unsupervised nonlinear dictionary learning. SIAM Journal on Imaging Sciences, 11(1):643–678, 2018. Dino Sejdinovic, Bharath Sriperumbudur, Arthur Gretton, and Kenji Fukumizu. Equivalence of distance-based and rkhs-based statistics in hypothesis testing. The Annals of Statistics , pages 2263–2291, 2013. Karren Dai Yang, Karthik Damodaran, Saradha Venkatachalapathy, Ali C Soylemezoglu, GV Shiv- ashankar, and Caroline Uhler. Predicting cell lineages using autoencoders and optimal transport. PLoS computational biology, 16(4):e1007828, 2020. Xinye Zheng, Jianbo Ye, James Z Wang, and Jia Li. Scott: Shape-location combined tracking with optimal transport. SIAM Journal on Mathematics of Data Science, 2(2):284–308, 2020. 12Supplementary materials A On the Deﬁnition of LOT r,c Let (X,dX) and (Y,dY) two nonempty compact Polish spaces, µ ∈M+ 1 (X), ν ∈M+ 1 (Y) two probability measures on these spaces and c: X×Y→ R+ a nonnegative and continuous function. We deﬁne the generalized low-rank optimal transport between µand νas LOTr,c(µ,ν) ≜ inf π∈Πr(µ,ν) ∫ X×Y c(x,y)dπ(x,y) . where Πr(µ,ν) ≜ {π∈Π(µ,ν) : ∃(µi)r i=1 ∈M+ 1 (X)r, (νi)r i=1 ∈M+ 1 (Y)r, λ∈∆∗ r s.t. π= r∑ i=1 λiµi⊗νi}. As Xand Yare compact, Πr(µ,ν) is tight, then Prokhorov’s theorem applies and the closure of Πr(µ,ν) is sequentially compact. Let us now show that Πr(µ,ν) is closed. Indeed, Let (πn)n≥0 a sequence of Πr(µ,ν) converging towards π∗. Then by deﬁnition there exists for all k ∈[|1,r|], (µ(k) n )n≥0, (ν(k) n )n≥0 and (λ(k) n )n≥0 such that for all n≥0 πn = r∑ i=1 λ(k) n µ(k) n ⊗ν(k) n . However, (µ(k) n )n≥0 and (ν(k) n )n≥0 are also tight, and Prokhorov’s theorem applies, therefore we can extract a common subsequence such that for all k, µ(k) n →µ(k) ∗ and ν(k) n →ν(k) ∗ In addition as (λn)n≥0 live in the simplex ∆r, we can also extract a sub-sequence, such that λn →λ∗∈∆r. Finally by unicity of the limit we obtain that π∗= r∑ k=1 λ(k) ∗ µ(k) ∗ ⊗ν(k) ∗ . Finally, by denoting I ≜ {k: λ(k) ∗ >0}, and by considering i∗∈I, we obtain that π∗= r∑ i∈I\\{i∗} λ(i) ∗ µ(i) ∗ ⊗ν(i) ∗ + r−|I|+1∑ j=1 λ(i∗) ∗ r−|I|+ 1µ(i∗) ∗ ⊗ν(i∗) ∗ . from which follows that π∗∈Πr(µ,ν). B Proofs B.1 Proof of Proposition 1 Proposition. Let n,m ≥2, X ≜ {x1,...,x n}⊂X , Y ≜ {y1,...,y m}⊂Y and a ∈∆∗ n and b∈∆∗ m. Then for 2 ≤r≤min(n,m), we have that |LOTr,c(µa,X,νb,Y) −OTc(µa,X,νb,Y)|≤∥ C∥∞ln(min(n,m)/(r−1)) Proof. Let P ∈argminP∈Πa,b⟨C,P⟩. As P is a nonnegative matrix, its nonnegative rank cannot exceed min(n,m). Assume for simplicity, that n = m, then there exists (Ri)n i=1 nonnegative matrices of rank 1 such that P = n∑ i=1 Ri . 13As for all i∈[|1,n|], Ri is a rank 1 matrix, there exist ˜qi,˜ri ∈Rn + such that Ri = ˜qi˜rT i . Then by denoting qi = ˜qi/|˜qi|, ri = ˜ri/|˜ri|and λi = |˜qi||˜ri|where for any h∈Rn |h|≜ ∑n i=1 hi, we obtain that P = n∑ i=1 λiqirT i . Without loss of generality, we can consider the case where λ1 ≥···≥ λn. Let us now denote λ := ( λ1,...,λ n), and by using the fact the P is a coupling we obtain that λ ∈∆n. Also, by deﬁnition of λ, we have that for all k∈[|1,n|], λk ≤1/k. Let us now deﬁne ˜P ≜ r−1∑ i=1 λiqirT i + ( n∑ i=r λi ) αrβT r where αr ≜ ∑n i=rλiqi∑n i=rλi βr ≜ ∑n i=rλiri∑n i=rλi Remark that ˜P ∈Πa,b(r), therefore we obtain that |LOTr,c(µa,X,νb,Y) −OTc(µa,X,νb,Y)|= LOTr,c(µa,X,νb,Y) −OTx(µa,X,νb,Y) ≤⟨C, ˜P⟩−⟨C,P⟩ ≤⟨C, ( n∑ i=r λi ) αrβT r ⟩−⟨C, n∑ i=r λiqirT i ⟩ ≤⟨C, ( n∑ i=r λi ) αrβT r ⟩ ≤∥C∥∞ n∑ i=r λi ≤∥C∥∞ n∑ i=r 1 i ≤∥C∥∞ln(n/(r−1)) B.2 Proof of Proposition 2 Proposition 10. Let µ∈M+ 1 (X), ν ∈M+ 1 (Y) and let us assume that cis L-Lipschitz w.r.t.xand y. Then for any r≥1, we have |LOTr,c(µ,ν) −OTc(µ,ν)|≤ 2Lmax(N⌊log2(⌊√r⌋)⌋(X,dX),N⌊log2(⌊√r⌋)⌋(Y,dY)) Proof. As Xand Yare compact, N⌊log2(⌊√r⌋)⌋(X,d),N⌊log2(⌊√r⌋)⌋(Y,d) <+∞and then by de- noting εX ≜ N⌊log2(⌊√r⌋)⌋(X,dX), there exists x1,...,x ⌊√r⌋∈X, such that X⊂ ⋃r i=1 BX(xi,ε) from which we can extract a partition (Si,X)⌊√r⌋ i=1 of X such that for all i ∈ [|1,⌊√r⌋|], and x,y ∈Si,X, dX(x,y) ≤εX. Similarly we can build a partition (Si,Y)⌊√r⌋ i=1 of Y. Let us now deﬁne for all k∈[|1,⌊√r⌋|], µk ≜ µ|Sk,X µ(Sk,X) and νk ≜ ν|Sk,Y ν(Sk,Y) with the convention that 0 0 = 0, we can deﬁne πr ≜ ⌊√r⌋∑ i,j=1 π∗(Si,X×Sj,Y)νj ⊗µi . 14First remarks that πr ∈Πr(µ,ν). Indeed we have for any measurable set B πr(X× B) = ⌊√r⌋2 ∑ j=1 νj(B) r∑ i=1 π∗(Si,X×Sj,Y) = ⌊√r⌋∑ j=1 νj(B)ν(Sj,Y) = ⌊√r⌋∑ j=1 ν|Sj,X(B) = ν(B) , similarly πr(A×Y) = µ(A) and we have that ⌊√r⌋2 ≤r. Therefore we obtain that |LOTr,c(µ,ν) −OTc(µ,ν)|= LOTr,c(µ,ν) −OTc(µ,ν) ≤ ∫ X×Y c(x,y)dπr(x,y) − ∫ X×Y c(x,y)dπ∗(x,y) ≤ ⌊√r⌋∑ i,j=1 ∫ Si,X×Sj,Y c(x,y)d[πr(x,y) −π∗(x,y)] ≤ ⌊√r⌋∑ i,j=1 π∗(Si,X×Sj,Y) ×[ sup (x,y)∈Si,X×Sj,Y c(x,y) − inf (x,y)∈Si,X×Sj,Y c(x,y)] ≤L[εX+ εY] from which the result follows. Corollary. Under the same assumptions of Proposition 2 and by assuming in addition that there exists a Monge map solving OTc(µ,ν), we obtain that for any r≥1, |LOTr,c(µ,ν) −OTc(µ,ν)|≤ LN⌊log2(r)⌋(Y,dY) Proof. Let us denote T a Monge map solution of OTc(µ,ν) and as in the proof above, let us consider a partition of (Si,Y)r i=1 of Ysuch that for all i ∈[|1,r|], and x,y ∈Si,Y, dY(x,y) ≤εY with εY≜ N⌊log2(r)⌋(Y,dY). Let us now deﬁne for all k∈[|1,⌊√r⌋|], µk ≜ µ|T−1(Sk,Y) µ(T−1(Sk,Y)) and νk ≜ ν|Sk,Y ν(Sk,Y) with the convention that 0 0 = 0, we can deﬁne πr ≜ r∑ k=1 π∗(T−1(Sk,Y) ×Sk,Y)νk ⊗µk . 15Again we have that πr ∈Πr(µ,ν), and we obtain that |LOTr,c(µ,ν) −OTc(µ,ν)|= LOTr,c(µ,ν) −OTc(µ,ν) ≤ ∫ X×Y c(x,y)dπr(x,y) − ∫ X×Y c(x,y)dπ∗(x,y) ≤ r∑ k=1 π∗(T−1(Sk,Y) ×Sk,Y) ∫ T−1(Sk,Y)×Sk,Y c(x,y)dµk(y) ⊗νk(y) − r∑ k=1 ∫ T−1(Sk,Y) c(x,T(x))dµ(x) ≤ r∑ k=1 π∗(T−1(Sk,Y) ×Sk,Y) ∫ T−1(Sk,Y)×Sk,Y c(x,y)dµk(y) ⊗νk(y) − r∑ k=1 π∗(T−1(Sk,Y) ×Sk,Y) ∫ T−1(Sk,Y)×Sk,Y c(x,T(x))dµk(x) ⊗νk(y) ≤ r∑ k=1 π∗(T−1(Sk,Y) ×Sk,Y) ∫ T−1(Sk,Y)×Sk,Y [c(x,y) −c(x,T(x))]dµk(x) ⊗νk(x) ≤LεY from which the result follows. Note that to obtain the above inequalities, we use the fact that π∗is supported on the graph of T, and therefore we have have for all k∈[|1,r|], π∗(T−1(Sk,Y) ×Sk,Y) = µ(T−1(Sk,Y)) = ν(Sk,Y). B.3 Proof of Proposition 3 Proposition. Let r≥1 and µ,ν ∈M+ 1 (X), then LOTr,c(ˆµn,ˆνn) −−−−−→ n→+∞ LOTr,c(µ,ν) a.s. Proof. Let π∗solution of LOTr,c(µ,ν). Then there exists λ∗∈∆∗ r, (µ∗ i)r i=1,(ν∗ i)r i=1 ∈M+ 1 (X)r such that π∗= r∑ i=1 λ∗ iµ∗ i ⊗ν∗ i. Note that by deﬁnition, we have that µ= r∑ i=1 λ∗ iµ∗ i and ν = r∑ i=1 λ∗ iν∗ i . Let us now deﬁne πµ and πµ both elements of M+ 1 (X× [|1,r|]) as follows: πµ(A×{k}) ≜ λkµk(A) and πν(A×{k}) ≜ λkνk(A) for any measurable set Aand k∈[|1,r|] . Observe that the right marginals of πµ and πν is the same and we will denote it ρ. We can now deﬁne for all x,y ∈X the family of kernels (kµ(·,x))x∈X ∈M+ 1 ([|1,r|])X and (kν(·,y))y∈X ∈ M+ 1 ([|1,r|])X corresponding to the disintegration with respect to the projection of respectively µ and ν. Let us now consider nindependent samples (Zµ i )n i=1 and (Zν i )n i=1 such that for all i∈[|1,n|], Zµ i ∼kµ(·,Xi) and Zν i ∼kν(·,Yi) and let us deﬁne for all k∈[|1,r|] ˜µk ≜ 1 n n∑ i=1 1Zµ i =kδXi and ˜νk ≜ 1 n n∑ i=1 1Zν i=kδYi . Let us now deﬁne ˜π≜ r−1∑ k=1 min(|˜µk|,|˜νk|) |˜µk||˜νk| ˜µk ⊗˜νk + 1 1 −∑r−1 k=1 min(|˜µk|,|˜νk|) [ ˆµ− r−1∑ k=1 min(|˜µk|,|˜νk|) |˜µk| ˜µk ] ⊗ [ ˆν− r−1∑ k=1 min(|˜µk|,|˜νk|) |˜νk| ˜νk ] 16with the convention that 0 0 = 0. Now it is easy to check that ˜π∈Πr(ˆµ,ˆν), indeed we have that ˜π(A×X) = r−1∑ k=1 min(|˜µk|,|˜νk|) |˜µk| ˜µk(A) + 1 1 −∑r−1 k=1 min(|˜µk|,|˜νk|) [ ˆµ(A) − r−1∑ k=1 min(|˜µk|,|˜νk|) |˜µk| ˜µk(A) ][ 1 − r−1∑ k=1 min(|˜µk|,|˜νk|) ] = ˆµ(A) in addition by construction we have that⏐⏐⏐⏐⏐ˆµ− r−1∑ k=1 min(|˜µk|,|˜νk|) |˜µk| ˜µk ⏐⏐⏐⏐⏐= ⏐⏐⏐⏐⏐ˆν− r−1∑ k=1 min(|˜µk|,|˜νk|) |˜νk| ˜νk ⏐⏐⏐⏐⏐= 1 − r−1∑ k=1 min(|˜µk|,|˜νk|) and both ˆµ−∑r−1 k=1 min(|˜µk|,|˜νk|) |˜µk| ˜µk and ˆν−∑r−1 k=1 min(|˜µk|,|˜νk|) |˜νk| ˜νk are positive measures. Therefore we obtain that LOTr,c(ˆµ,ˆν) ≤ ∫ X2 c(x,y)d˜π(x,y) Now we aim at showing at ∫ X2 c(x,y)d˜π(x,y) →LOTr,c(µ,ν) a.s.. Indeed ﬁrst observe that from the law of large numbers we have that for all k ∈[|1,r|], |˜µk|→ λ∗ k and similarly |˜νk|→ λ∗ k. In addition, for all k,q we have that almost surely, ˜µk ⊗˜νq converges weakly towards λ∗ kλ∗ qµk ⊗νq. Indeed one can consider the following algebra F≜ { (x,y) ∈X2 →f(x)g(y) f,g ∈C(X) } , and then by Stone-Weierstrass, one obtains by density the desired result. Now remark that ∫ X2 c(x,y)d˜π(x,y) = r−1∑ k=1 min(|˜µk|,|˜νk|) |˜µk||˜νk| ∫ X2 c(x,y)d˜µk ⊗˜νk + 1 ˜λr ∫ Z2 c(x,y)d˜µr ⊗˜νr + 1 ˜λr r−1∑ k=1 ( 1 −min(|˜µk|,|˜νk|) |˜νk| )∫ X2 c(x,y)d˜µr ⊗˜νk + 1 ˜λr r−1∑ k=1 ( 1 −min(|˜µk|,|˜νk|) |˜µk| )∫ X2 c(x,y)d˜µk ⊗˜νr + 1 ˜λr r−1∑ k,q=1 ∫ X2 ( 1 −min(|˜µk|,|˜νk|) |˜µk| )( 1 −min(|˜µq|,|˜νq|) |˜νq| ) c(x,y)d˜µk(x)d˜νq(y) from which follows directly that ∫ X2 c(x,y)d˜π(x,y) →LOTr,c(µ,ν) a.s.Let us now denote for all n≥1, πn a solution of LOTr,c(ˆµ,ˆν). Let ω∈Ω an element of the probability space where live the random variables (Xi)i≥0 and (Yi)i≥0 such that ∫ X2 c(x,y)d˜π(ω)(x,y) →LOTr,c(µ,ν). As Xis compact Thanks to Prokhorov’s Theorem, we can extract a sequence such that(π(ω) n )n≥0 converge weakly towards π(ω) ∈Πr(µ,ν). In addition we have that for all n≥1∫ X2 c(x,y)dπ(ω) n (x,y) ≤ ∫ X2 c(x,y)d˜π(ω)(x,y) And by considering the limit we obtain that∫ c(x,y)dπ(ω)(x,y) ≤LOTr,c(µ,ν) However π(ω) ∈Πr(µ,ν) and by optimality we obtain that∫ c(x,y)dπ(ω)(x,y) = LOTr,c(µ,ν) This holds for an arbitrary subsequence of (π(ω) n )n≥0, from which follows that∫ c(x,y)dπ(ω) n (x,y) →LOTr,c(µ,ν). Finally this holds almost surely and the result follows. 17B.4 Proof of Proposition 4 Proposition. Let r ≥1 and µ,ν ∈M+ 1 (X). Then, there exists a constant Kr such that for any δ >0 and n≥1, we have, with a probability of at least 1 −2δ, that LOTr,c(ˆµn,ˆνn) −LOTr,c(µ,ν) ≤11∥c∥∞ √r n + Kr∥c∥∞ [√ log(40/δ) n + √rlog(40/δ) n ] Proof. We reintroduce the same notation as in the proof of Proposition 3. Let π∗ solution of LOTr,c(µ,ν). Then there exists λ∗∈∆∗ r, (µ∗ i)r i=1,(ν∗ i)r i=1 ∈M+ 1 (Z)r such that π∗= r∑ i=1 λ∗ iµ∗ i ⊗ν∗ i. As before let us also consider πµ and πµ deﬁned as πµ(A×{k}) ≜ λkµk(A) and πν(A×{k}) ≜ λkνk(A) for any measurable set Aand k∈[|1,r|] and denote ρtheir common right marginal. We also consider n independent samples (Zµ i )n i=1 and (Zν i )n i=1 such that for all i ∈[|1,n|], Zµ i ∼ kµ(·,Xi) and Zν i ∼kν(·,Yi) and we denote for all k∈[|1,r|] ˜µk ≜ 1 n n∑ i=1 1Zµ i =kδXi and ˜νk ≜ 1 n n∑ i=1 1Zν i=kδYi Let us now deﬁne ˆπ≜ r∑ i=1 1 λ∗ k ˜µk ⊗˜νk . Our goal is to control the following quantity: ⏐⏐⏐⏐LOTr,c(µ,ν) − ∫ Z2 c(x,y)dˆπ(x,y) ⏐⏐⏐⏐, First observe that E [∫ Z2 c(x,y)dˆπ(x,y) ] = r∑ i=1 1 λ∗ k E [∫ Z2 c(x,y)d˜µk(x)d˜νk(y) ] = r∑ i=1 1 λ∗ kn2 × ∑ i,j E [ c(Xi,Yj)1Zµ i =k1Zν j=k ] Moreover, we have that E [ c(Xi,Yj)1Zµ i =k1Zν j=k ] = ∫ (Z×[|1,r|])2 c(x,y)1z=k1z′=kdπµ(x,z)dπν(y,z′) = ∫ (Z×[|1,r|])2 c(x,y)1z=k1z′=kdµz(x)dνz′(y)dρ(z)dρ(z′) = λ2 k ∫ Z2 c(x,y)dµk(x)dνk(y) from which follows that E [∫ Z2 c(x,y)dˆπ(x,y) ] = r∑ i=1 λ∗ k ∫ Z2 c(x,y)dµk(x)dνk(y) = LOTr,c(µ,ν) Now let us deﬁne for all (xi,zi)n i=1,(yi,z′ i) ∈(Z× [|1,r|])n, g((x1,z1),..., (xn,zn),(y1,z′ 1),..., (yn,z′ n)) ≜ r∑ q=1 1 λ∗qn2 ∑ i,j c(xi,yj)1zi=q1z′ j=q , 18since Zis compact and cis continuous, we have that |g(..., (xk,zk),... ) −g(..., (˜xk,˜zk),... )|= ⏐⏐⏐⏐⏐⏐ r∑ q=1 1 λ∗qn2 ∑ j [c(xk,yj)1zk=q −c(˜xk,yj)1˜zk=q]1z′ j=q ⏐⏐⏐⏐⏐⏐ = ⏐⏐⏐⏐⏐⏐ 1 λ∗zkn2 n∑ j=1 c(xk,yj)1z′ j=zk − 1 λ∗ ˜zkn2 n∑ j=1 c(˜xk,yj)1z′ j=˜zk ⏐⏐⏐⏐⏐⏐ ≤∥c∥∞ n2 [∑n j=1 1z′ j=zk λ∗zk + ∑n j=1 1z′ j=˜zk λ∗ ˜zk ] ≤ 2∥c∥∞ min 1≤q≤r λ∗q 1 n Then by applying the McDiarmid’s inequality we obtain that forδ >0, with a probability at least of 1 −δ, we have ⏐⏐⏐⏐LOTr,c(µ,ν) − ∫ Z2 c(x,y)dˆπ(x,y) ⏐⏐⏐⏐≤ 2∥c∥∞ min 1≤q≤r λ∗q √ log(2/δ) n Now we aim at building a coupling ˜π ∈Πr(ˆµ,ˆν) from ˆπ. Let us consider the same as the one introduce in the proof of Proposition B.3, that is ˜π≜ r−1∑ k=1 min(|˜µk|,|˜νk|) |˜µk||˜νk| ˜µk ⊗˜νk + 1 1 −∑r−1 k=1 min(|˜µk|,|˜νk|) [ ˆµ− r−1∑ k=1 min(|˜µk|,|˜νk|) |˜µk| ˜µk ] ⊗ [ ˆν− r−1∑ k=1 min(|˜µk|,|˜νk|) |˜νk| ˜νk ] with the convention that 0 0 = 0. Let us now expand the above expression, and by denoting ˜λr = 1 −∑r−1 k=1 min(|˜µk|,|˜νk|) we obtain that ˜π= r−1∑ k=1 min(|˜µk|,|˜νk|) |˜µk||˜νk| ˜µk ⊗˜νk + 1 ˜λr ˜µr ⊗˜νr + 1 ˜λr ˜µr ⊗ [r−1∑ k=1 ( 1 −min(|˜µk|,|˜νk|) |˜νk| ) ˜νk ] + 1 ˜λr [r−1∑ k=1 ( 1 −min(|˜µk|,|˜νk|) |˜µk| ) ˜µk ] ⊗˜νr + 1 ˜λr [r−1∑ k=1 ( 1 −min(|˜µk|,|˜νk|) |˜µk| ) ˜µk ] ⊗ [r−1∑ k=1 ( 1 −min(|˜µk|,|˜νk|) |˜νk| ) ˜νk ] 19Now we aim at controlling the following quantity ⏐⏐∫ Z2 c(x,y)dˆπ(x,y) − ∫ Z2 c(x,y)d˜π(x,y) ⏐⏐and we observe that ∫ Z2 c(x,y)d[ˆπ(x,y) −˜π(x,y)] = r−1∑ k=1 ∫ Z2 c(x,y) [ 1 λ∗ k −min(|˜µk|,|˜νk|) |˜µk||˜νk| ] d˜µk(x)˜νk(y) (11) + ∫ Z2 c(x,y) [1 λ∗r − 1 ˜λr ] d˜µr(x)˜νr(y) (12) + 1 ˜λr r−1∑ k=1 ∫ Z2 ( 1 −min(|˜µk|,|˜νk|) |˜νk| ) c(x,y)d˜µr(x)d˜νk(y) (13) + 1 ˜λr r−1∑ k=1 ∫ Z2 ( 1 −min(|˜µk|,|˜νk|) |˜µk| ) c(x,y)d˜µk(x)d˜νr(y) (14) + 1 ˜λr r−1∑ k,q=1 ∫ Z2 ( 1 −min(|˜µk|,|˜νk|) |˜µk| )( 1 −min(|˜µq|,|˜νq|) |˜νq| ) c(x,y)d˜µk(x)d˜νq(y) (15) Let us now control each term of the RHS of the above equality. Let us ﬁrst consider the term in Eq. 11, remark that we have ⏐⏐⏐⏐ ∫ Z2 c(x,y) [ 1 λ∗ k −min(|˜µk|,|˜νk|) |˜µk||˜νk| ] d˜µk(x)˜νk(y) ⏐⏐⏐⏐ ≤ ⏐⏐⏐⏐ [ 1 λ∗ k −min(|˜µk|,|˜νk|) |˜µk||˜νk| ]⏐⏐⏐⏐∥c∥∞|˜µk||˜νk| ≤ ⏐⏐⏐⏐ [|˜µk||˜νk| λ∗ k −min(|˜µk|,|˜νk|) ]⏐⏐⏐⏐∥c∥∞ ≤min(|˜µk|,|˜νk|) ⏐⏐⏐⏐ max(|˜µk|,|˜νk|) λ∗ k −1 ⏐⏐⏐⏐∥c∥∞ ≤min(|˜µk|,|˜νk|) λ∗ k |max(|˜µk|,|˜νk|) −λ∗ k|∥c∥∞ ≤min(|˜µk|,|˜νk|) λ∗ k max(∥˜λµ −λ∗∥∞,∥˜λν −λ∗∥∞)∥c∥∞ ≤∥c∥∞max ( ˜λµ λ∗  ∞ ,  ˜λν λ∗  ∞ ) max(∥˜λµ −λ∗∥∞,∥˜λν −λ∗∥∞) where we have denoted ˜λµ ≜ (|˜µk|)r k=1 and ˜λν ≜ (|˜νk|)r k=1. Now observe that P ( max(∥˜λµ −λ∗∥∞,∥˜λν −λ∗∥∞) ≥t ) ≤2P ( ∥˜λµ −λ∗∥∞≥t ) ≤P ( dK(λ∗,˜λµ) ≥t 2 ) ≤4 exp(−nt2/2) where dK is the Kolmogorov distance. In addition we have max ( ˜λµ λ∗  ∞ ,  ˜λν λ∗  ∞ ) ≤1 + 1 min 1≤i≤r λ∗ i max ( ∥˜λµ −λ∗∥∞,∥˜λν −λ∗∥∞ ) Combining the two above controls, we obtain that for all δ >0, with a probability of at least 1 −δ, ⏐⏐⏐⏐ ∫ Z2 c(x,y) [ 1 λ∗ k −min(|˜µk|,|˜νk|) |˜µk||˜νk| ] d˜µk(x)˜νk(y) ⏐⏐⏐⏐≤∥c∥∞ √ 2 ln 8/δ n + ∥c∥∞ n 2 ln 8/δ min 1≤i≤r λ∗ i 20Let us now consider the term in Eq. 12, we have that ⏐⏐⏐⏐ ∫ Z2 c(x,y) [1 λ∗r − 1 ˜λr ] d˜µr(x)˜νr(y) ⏐⏐⏐⏐≤|˜µr||˜νr| λ∗r˜λr ⏐⏐⏐⏐⏐1 − r∑ i=1 min(|˜µk|,|˜νk|) −λr ⏐⏐⏐⏐⏐∥c∥∞ ≤max ( ˜λµ λ∗  ∞ ,  ˜λν λ∗  ∞ )r−1∑ k=1 |λ∗ k −min(|˜µk|,|˜νk|)|∥c∥∞ ≤max ( ˜λµ λ∗  ∞ ,  ˜λν λ∗  ∞ ) ∥c∥∞(∥λ∗−˜λµ∥1 + ∥λ∗−˜λν∥1) ≤2∥c∥∞max ( ˜λµ λ∗  ∞ ,  ˜λν λ∗  ∞ ) max(∥λ∗−˜λµ∥1,∥λ∗−˜λν∥1) However we have that P ( max(∥λ∗−˜λµ∥1,∥λ∗−˜λν∥1) ≥t ) ≤2P ( ∥λ∗−˜λµ∥1 ≥t ) In addition we have that E(∥λ∗−˜λµ∥1) ≤√r n and by applying the McDiarmid’s Inequality, we obtain that for all δ >0, with a probability of 1 −δ ∥λ∗−˜λµ∥1 ≤ √r n + √ 2 ln(2/δ) n Therefore we obtain that with a probability of at least 1 −δ, ⏐⏐⏐⏐ ∫ Z2 c(x,y) [1 λ∗r − 1 ˜λr ] d˜µr(x)˜νr(y) ⏐⏐⏐⏐≤2∥c∥∞   √r n + √ 2 ln(8/δ) n + 2 ln(8/δ) + √ 2rln(8/δ) n× min 1≤i≤r λ∗ i   For the term in Eq. 13 and 14, we obtain that ⏐⏐⏐⏐⏐ 1 ˜λr r−1∑ k=1 ∫ Z2 ( 1 −min(|˜µk|,|˜νk|) |˜νk| ) c(x,y)d˜µr(x)d˜νk(y) ⏐⏐⏐⏐⏐ ≤|˜µr| ˜λr r−1∑ k=1 (|˜νk|−min(|˜µk|,|˜νk|)) ∥c∥∞ ≤|˜µr| ˜λr [˜λr −|˜νr|]∥c∥∞ ≤[|˜λr −λ∗ r|+ |λ∗ r −˜νr|]∥c∥∞ ≤3∥c∥∞max(∥λ∗−˜λµ∥1,∥λ∗−˜λν∥1) Therefore we obtain that with a probability of at least 1 −δ, ⏐⏐⏐⏐⏐ 1 ˜λr r−1∑ k=1 ∫ Z2 ( 1 −min(|˜µk|,|˜νk|) |˜νk| ) c(x,y)d˜µr(x)d˜νk(y) ⏐⏐⏐⏐⏐≤3∥c∥∞ [√r n + √ 2 ln(2/δ) n ] Finally the last term in Eq. 15 can be controlled as the following:⏐⏐⏐⏐⏐⏐ 1 ˜λr r−1∑ k,q=1 ∫ Z2 ( 1 −min(|˜µk|,|˜νk|) |˜µk| )( 1 −min(|˜µq|,|˜νq|) |˜νq| ) c(x,y)d˜µk(x)d˜νq(y) ⏐⏐⏐⏐⏐⏐ ≤∥c∥∞ ˜λr r−1∑ k,q=1 ( 1 −min(|˜µk|,|˜νk|) |˜µk| )( 1 −min(|˜µq|,|˜νq|) |˜νq| ) |˜µk||˜νq| ≤∥c∥∞ ˜λr r−1∑ k=1 (|˜µk|−min(|˜µk|,|˜νk|)) r−1∑ k=1 (|˜νk|−min(|˜µk|,|˜νk|)) ≤3∥c∥∞max(∥λ∗−˜λµ∥1,∥λ∗−˜λν∥1) 21and we obtain that with a probability of at least 1 −δ, ⏐⏐⏐⏐⏐⏐ 1 ˜λr r−1∑ k,q=1 ∫ Z2 ( 1 −min(|˜µk|,|˜νk|) |˜µk| )( 1 −min(|˜µq|,|˜νq|) |˜νq| ) c(x,y)d˜µk(x)d˜νq(y) ⏐⏐⏐⏐⏐⏐ ≤3∥c∥∞ [√r n + √ 2 ln(2/δ) n ] Then by applying a union bound we obtain that with a probability of at least 1 −δ ⏐⏐⏐⏐ ∫ Z2 c(x,y)d[ˆπ(x,y) −˜π(x,y)] ⏐⏐⏐⏐≤∥c∥∞  11 √r n + 12 √ 2 ln 40/δ n + 6 ln(40/δ) + 2 √ 2rln(40/δ) n× min 1≤i≤r λ∗ i   Now observe that LOTr,c(ˆµ,ˆν) −LOTr,c(µ,ν) ≤ ∫ Z2 c(x,y)d˜π(x,y) − ∫ Z2 c(x,y)dπ∗(x,y) ≤ ∫ Z2 c(x,y)d[˜π−ˆπ](x,y) + ∫ Z2 c(x,y)d[ˆπ−π∗](x,y) and by combining the two control we obtain that with a probability of at least 1 −2δ, LOTr,c(ˆµ,ˆν) −LOTr,c(µ,ν) ≤∥c∥∞ [ 11 √r n + 12 √ 2 ln 40/δ n + 1 α ( 2 √ log(2/δ) n + 6 ln(40/δ) + 2 √ 2rln(40/δ) n )] ≤11∥c∥∞ √r n + 14∥c∥∞ α √ log(40/δ) n + 2∥c∥∞max(6, √ 2r) log(40/δ) nα where α≜ min 1≤i≤r λ∗ i and the result follows. B.5 Proof Proposition 5 Proposition. Let r ≥1, δ >0 and µ,ν ∈M+ 1 (X). Then there exists a constant Nr,δ such that if n≥Nr,δ then with a probability of at least 1 −2δ, we have LOTr,c(ˆµn,ˆνn) −LOTr,c(µ,ν) ≤11∥c∥∞ √r n + 77∥c∥∞ √ log(40/δ) n . Proof. We consider the same notations as in the proof of Proposition 4. In particular let us deﬁne for all (xi,zi)n i=1,(yi,z′ i) ∈(Z× [|1,r|])n, g((x1,z1),..., (xn,zn),(y1,z′ 1),..., (yn,z′ n)) ≜ r∑ q=1 1 λ∗qn2 ∑ i,j c(xi,yj)1zi=q1z′ j=q , Recall that we have |g(..., (xk,zk),... ) −g(..., (˜xk,˜zk),... )|≤ ∥c∥∞ n2 [∑n j=1 1z′ j=zk λ∗zk + ∑n j=1 1z′ j=˜zk λ∗ ˜zk ] ≤2∥c∥∞ n max ( ˜λµ λ∗  ∞ ,  ˜λν λ∗  ∞ ) ≤2∥c∥∞ n + 2∥c∥∞ n× min 1≤i≤r λ∗ i max ( ∥˜λµ −λ∗∥∞,∥˜λν −λ∗∥∞ ) In fact if we have a control in probability of the bounded difference we can use an extension of the McDiarmid’s Inequality. For that purpose let us ﬁrst introduce the following deﬁnition. 22Deﬁnition 4. Let (Xi)m i=1, mindependent random variables and ga measurable function. We say that g is weakly difference-bounded with respect to (Xi)m i=1 by (b,β,δ ) if P(|g(X1,...,X m) −g(X′ 1,...,X ′ m)|≤ β) ≥1 −δ with X′ i = Xi except for one coordinate kwhere X′ k is an independent copy of Xk. Furthermore for any (xi)m i=1 and (x′ i)m i=1 where for all coordinate except on xj = x′ j |g(x1,...,x m) −g(x′ 1,...,x ′ m)|≤ b. Let us now introduce an extension of McDiarmid’s Inequality [Kutin, 2002]. Theorem 1. Let (Xi)m i=1, mindependent random variables and g a measurable function which is weakly difference-bounded with respect to (Xi)m i=1 by (b,β/m, exp(−Km)), then if 0 < τ≤ T(b,β,K ) and m≥M(b,β,K,τ ), then P(|g(X1,...,X m) −E(g(X1,...,X m))|≥ τ) ≤4 exp (−τ2m 8β2 ) where T(b,β,K ) ≜ min (14c 2 ,4β √ K, β2K b ) M(b,β,K,τ ) ≜ max (b β,β √ 40,3 (24 K + 3 ) log (24 K + 3 ) ,1 τ ) Given the above Theroem we can obtain an asymptotic control of the deviation ofgfrom its mean. Let δ′>0 and let us denote m≜ 2n b≜ 2∥c∥∞ n× min 1≤i≤r λ∗ i K ≜ log(1/δ′) 2n β ≜ 4∥c∥∞  1 + 1 min 1≤i≤r λ∗ i √ 2 log(4/δ′) n   Observe now that with a probability of at least 1 −exp(−Km) |g(..., (xk,zk),... ) −g(..., (˜xk,˜zk),... )|≤ 2∥c∥∞ n  1 + 1 min 1≤i≤r λ∗ i √ 2 log(4/δ′) n   Let us now ﬁx δ >0 and let us choose δ′such that δ′≜ 4/nand τ ≜ β √ 4 log(4/δ) n , then we obtain that for nsufﬁciently large (such that n≥M(b,β,K,τ )/2 and τ ≤T(b,β,K )), we have that with a probability of at least 1 −δ ⏐⏐⏐⏐LOTr,c(µ,ν) − ∫ Z2 c(x,y)dˆπ(x,y) ⏐⏐⏐⏐≤4∥c∥∞  1 + 1 min 1≤i≤r λ∗ i √ 2 log(n) n   √ 4 log(4/δ) n ≤4∥c∥∞ √ 4 log(4/δ) n + 16 √ 5∥c∥∞ √ log(n) log(4/δ) n× min 1≤i≤r λ∗ i Recall also from the proof of Proposition 4, that we have with a probability of at least 1 −δ ⏐⏐⏐⏐ ∫ Z2 c(x,y)d[ˆπ(x,y) −˜π(x,y)] ⏐⏐⏐⏐≤∥c∥∞  11 √r n + 12 √ 2 ln 40/δ n + 6 ln(40/δ) + 2 √ 2rln(40/δ) n× min 1≤i≤r λ∗ i   23Finally by imposing in addition that √ n log(n) ≥ 1 min 1≤i≤r λ∗ i , √n≥ √ log(40/δ) min 1≤i≤r λ∗ i and √n≥ √r min 1≤i≤r we obtain that for nis large enough (such that (such that n≥M(b,β,K,τ )/2 and τ ≤T(b,β,K )) and satysﬁng the above inequalities, we have with a probability of at least 1 −2δthat LOTr,c(ˆµ,ˆν) −LOTr,c(µ,ν) ≤11∥c∥∞ √r n + 77∥c∥∞ √ log(40/δ) n B.6 Proof Proposition 6 Proposition. Let µ,ν ∈M+ 1 (X). Let us assume that cis symmetric, then we have DLOT1,c(µ,ν) = 1 2 ∫ X2 −c(x,y)d[µ−ν] ⊗d[µ−ν](x,y) . If in addition we assume the cis Lipschitz w.r.t toxand y, then we have DLOTr,c(µ,ν) − −−−− → r→+∞ OTc(µ,ν) . Proof. When r= 1, it is clear that for any µ,ν ∈M+ 1 (X), Πr(µ,ν) = {µ⊗ν}and thanks to the symmetry of c, we have directly that DLOT1,c(µ,ν) = 1 2 ∫ X2 −c(x,y)d[µ−ν] ⊗d[µ−ν](x,y) = 1 2MMD−c(µ,ν) . The limit is a direct consequence of Proposition 2. B.7 Proof of Proposition 8 Proposition. Let r≥1 and (µn)n≥0 and (νn)n≥0 two sequences of probability measures such that µn →µand νn →νwith respect to the convergence in law. Then we have that LOTr,c(µn,νn) →LOTr,c(µ,ν) . Proof. Let us denote πan optimal solution of LOTr,c(µ,ν) and let us denote (µ(i))r i=1, (ν(i))r i=1 and (λ(i))r i=1 the decomposition associated. In the following Lemma, we aim at building speciﬁc decompositions of the sequences (µn)n≥0 and (νn)n≥0. Lemma 1. Let r ≥1, µ ∈ M+ 1 (X) and (µ(i))r i=1 ∈ M+ 1 (X) and (λ(i))r i=1 ∈∆∗ r such that µ= ∑r i=1 λiµ(i). Then for any sequence of probability measures (µn)≥0 such that µn →µ, there exist for all i∈[|1,r|] a sequence of nonnegative measures (µ(i) n )n≥0 such that µ(i) n →λiµ(i) for all i∈[|1,r|] and r∑ i=1 µ(i) n = µn for all n≥0 Proof. For r = 1 the result is clear. Let us now show the result for r = 2. Let us denote (˜µ(1) n ) a sequence converging weakly towards λ1µ(1). Then by denoting µ(1) n ≜ µn −(µn −˜µ(1) n )+ where (·)+ correspond to the non-negative part of the measure, we have that µ(1) n ≥0, µ(1) n →λ1µ(1), µ(2) n ≜ µn −µ(1) n ≥0, µ(2) n →λ2µ(2) and µn = µ(1) n + µ(2) n for all n≥0 24which is the result. Let r≥2 and let us assume that the result holds for all 1 ≤k≤r. Let us now consider a decomposition of µsuch that µ = ∑r+1 i=1 λiµ(i). By denoting ˜µ(1) ≜ ∑r i=1 λiµ(i) ∑r i=1 λi , we obtain that µ= ( r∑ i=1 λi ) ˜µ(1) + λr+1µ(r+1) . Then by recursion we have that there exists sequences of nonnegative measures (˜µ(1) n ) and (µ(r+1) n ) such that ˜µ(1) n → ( r∑ i=1 λi ) ˜µ(1), µ(r+1) n →λr+1µ(r+1) and µn = ˜µ(1) n + µ(r+1) n for all n≥0 Now observe that ˜µ(1) n |˜µ(1) n | →˜µ(1) = ∑r i=1 λi∑r i=1 λi µ(i). Therefore applying the recursion on this problem allows us to obtain a decomposition of ˜µ(1) n of the form ˜µ(1) n |˜µ(1) n | = r∑ i=1 µ(i) n where µ(i) n ≥0 and µ(i) n → λi∑r i=1 λi µ(i) . Therefore we obtain that µn = r∑ i=1 |˜µ(1) n |µ(i) n + µ(r+1) n where µ(i) n ≥0, |˜µ(1) n |µ(i) n →λiµ(i) for all i∈[|1,r|] and µ(r+1) n ≥0, µ(r+1) n →λr+1µ(r+1) from which follows the result. Let us now consider such decompositions of (µn)n≥0 and (νn)n≥0 such that each factor converges toward the target decomposition of µ. Now let us build the following coupling: ˜πn ≜ r−1∑ k=1 min(|µ(k) n |,|ν(k) n |) |µ(k) n ||ν(k) n | µ(k) n ⊗µ(k) n + 1 1 −∑r−1 k=1 min(|µ(k) n |,|ν(k n |) [ |µn|− r−1∑ k=1 min(|µ(k) n |,|ν(k) n |) |µ(k) n | µ(k) n ] ⊗ [ νn − r−1∑ k=1 min(|µ(k) n |,|ν(k) n |) |ν(k) n | ν(k) n ] with the convention that 0 0 = 0. Now it is easy to check that ˜πn ∈Πr(µn,νn), and we have that LOTr,c(µn,νn) ≤ ∫ X2 d(x,y)d˜πn(x,y) →LOTr,c(µ,ν) and by Prokhorov’s theorem and the optimality of the limit of (˜πn)n≥0 (up to an extraction) we obtain that LOTr,c(µn,νn) →LOTr,c(µ,ν). B.8 Proof Proposition 7 Proposition. Let r ≥1, and let us assume that cis a semimetric of negative type. Then for all µ,ν ∈M+ 1 (X), we have that DLOTr(µ,ν) ≥0 . In addition, if chas strong negative type then we have also that DLOTr,c(µ,ν) = 0 ⇐⇒µ= ν and µn →µ ⇐⇒DLOTr,c(µn,µ) →0 . where the convergence of the sequence of probability measures considered is the convergence in law. 25Proof. Let π∗solution of LOTr,c(µ,ν). Then there exists λ∗∈∆∗ r, (µ∗ i)r i=1,(ν∗ i)r i=1 ∈M+ 1 (X)r such that π∗= r∑ i=1 λ∗ iµ∗ i ⊗ν∗ i. Note that by deﬁnition, we have that µ= r∑ i=1 λ∗ iµ∗ i and ν = r∑ i=1 λ∗ iν∗ i, By deﬁnition we have also that LOTr,c(µ,µ) ≤ r∑ k=1 λ∗ k ∫ X2 c(x,y)dµ∗ k ⊗µ∗ k similarly for LOTr,c(ν,ν) we have LOTr,c(ν,ν) ≤ r∑ k=1 λ∗ k ∫ X2 c(x,y)dν∗ k ⊗ν∗ k Therefore we have DLOTr,c(µ,ν) ≥ r∑ k=1 λ∗ k (∫ X2 c(x,y)dµ∗ k ⊗ν∗ k −1 2 [∫ X2 c(x,y)dµ∗ k ⊗µ∗ k + ∫ X2 c(x,y)dν∗ k ⊗ν∗ k ]) ≥ r∑ k=1 λ∗ k ∫ X2 −c(x,y)d[µ∗ k −ν∗ k] ⊗[µ∗ k −ν∗ k] ≥ r∑ k=1 λ∗ k 2 Dc(µ∗ k,ν∗ k) where for any any probability measures α,β on Xwe deﬁne Dc(α,β) ≜ 2 ∫ X2 c(x,y)dα⊗β− ∫ X2 c(x,y)dα⊗α− ∫ X2 c(x,y)dβ⊗β However, as cis assumed to have a negative type, we have that Dc(µ∗ k,ν∗ k) ≥0 ∀k In addition if we assume that chas a strong negative type, then we obtain directly that DLOTr,c(µ,ν) = 0 =⇒ µ∗ k = ν∗ k ∀k. Let us now show that DLOTr,c metrize the convergence in law. The direct implication is a direct consequence of the Proposition 8. Conversely, if DLOTr,c(µn,µ) →0, then by compacity of X and thanks to the Prokhorov’s theorem we can extract a subsequence of µn →µ∗, and thanks to Proposition 8, we also obtain that DLOTr,c(µn,µ) →DLOTr,c(µ∗,µ). Finally we deduce that DLOTr,c(µ∗,µ) = 0 and µ∗= µ. B.9 Proof Proposition 9 Proposition. Let n≥k≥1, X ≜ {x1,...,x n}⊂X and a∈∆∗ n. If cis a semimetric of negative type, then by denoting C = (c(xi,xj))i,j, we have that LOTk,c(µa,X,µa,X) = min Q ⟨C,Qdiag(1/QT1n)QT⟩s.t. Q∈Rn×k + , Q1k = a. (16) Proof. First remarks that one can reformulate the LOTk,c problem as LOTk,c(µ,µ) ≜ min g∈∆∗ k min (x,y)∈K2a,g k∑ i=1 xT i Cyi gi 26where Ka,g ≜ {x ∈Rnk s.t. Ax = [a,g]T, x ≥0} A≜ ( 1T n ⊗Ik IT n ⊗1k ) and xi ≜ [x(i−1)×n+1,...,x i×n]T, yi ≜ [y(i−1)×n+1,...,y i×n]T for all i∈[|1,k|] Indeed the above optimization problem is just a reformulation of LOTk,c(µ,µ) where we have vectorized the couplings in a column-wise order. Let us now show the following lemma from which the result will follow. Lemma 2. Under the same assumption of Proposition 9 we have that for all g∈∆∗ k min (x,y)∈K2a,g k∑ i=1 xT i Cyi gi = min x∈Ka,g k∑ i=1 xT i Cxi gi Proof. Let (x∗,y∗) solution of the LHS optimization problem. Then we have that k∑ i=1 (x∗ i)TCx∗ i gi ≥ k∑ i=1 (x∗ i)TCy∗ i gi k∑ i=1 (y∗ i)TCy∗ i gi ≥ k∑ i=1 (x∗ i)TCy∗ i gi Therefore we obtain that 0 ≤ k∑ i=1 (x∗ i)TCx∗ i gi − k∑ i=1 (x∗ i)TCy∗ i gi = k∑ i=1 (x∗ i)TC(x∗ i −y∗ i) gi 0 ≤ k∑ i=1 (y∗ i)TCy∗ i gi − k∑ i=1 (x∗ i)TCy∗ i gi = k∑ i=1 (y∗ i −x∗ i)TCy∗ i gi Then by symmetry of C, we obtain by adding the two terms that k∑ i=1 (x∗ i −y∗ i)TC(x∗ i −y∗ i) gi ≥0 However, thanks to the linear constraints, we have that for alli∈[|1,k|], n−1∑ q=0 x∗ (i−1)×n+1+q = n−1∑ q=0 y∗ (i−1)×n+1+q = gi Therefore (x∗ i −y∗ i)T1n = 0 and thanks to the negativity of the cost function cwe obtain that (x∗ i −y∗ i)TC(x∗ i −y∗ i) ≤0 Therefore we have that (xi −yi)TC(xi −yi) = 0 from which follows that k∑ i=1 (x∗ i)TCx∗ i gi = k∑ i=1 (x∗ i)TCy∗ i gi = k∑ i=1 (y∗ i)TCy∗ i gi and the result follows. As the above result holds for any g∈∆∗ k, we obtain that LOTk,c(µ,µ) = min g∈∆∗ k min x∈Ka,g k∑ i=1 (x∗ i)TCx∗ i gi Then by formulating back this problem in term of matrices, we obtain that LOTk,c(µ,µ) = min g∈∆∗ k min Q∈Πa,g ⟨C,Qdiag(1/g)QT⟩ from which the result follows. 27C Additional Experiments C.1 Comparison of the γschedules 106 107 108 109 Operations 0.45 0.50 0.55 0.60 0.65LOT cost Uniform Distributions Adaptive :  = 1 Adaptive :  = 10 Adaptive :  = 100 Constant:  = 1 Constant:  = 10 Constant:  = 100 105 106 107 108 109 1010 Operations 2 4 6 Mixture of Gaussians Constant:  = 0.1 Constant:  = 1 Constant:  = 10 Figure 5: In this experiment, we compare two strategies for the choice of the step-size in the MD scheme proposed by Scetbon et al. [2021] on two different problems. More precisely, we compare the constant γschedule with the proposed adaptive one and compare them when the distributions are sampled from either uniform distributions (left) or mixtures of anisotropic Gaussians (right). We show that the range of admissible γwhen considering a constant schedule varies from one problem to another. Indeed, in the right plot, we observe that the algorithm converges only when γ ≤1, while in the left plot, the algorithm manages to converge for γ ≤100. We also observe that our adaptive strategy allows to have a consistent choice of admissible values for γwhatever the problem considered. It is worth noticing that whatever the γchosen, the algorithm converges towards the same value, however the larger γis chosen in its admissible range, the faster the algorithm converges. C.2 Gradient Flows between two Moons DLOT, r=100 LOT, r=100 Figure 6: We compare the gradient ﬂows (µt)t≥0 (in red) starting from a moon shape distribution, µ0, to another moon shape distribution (in blue),ν, in 2D when minimizing eitherL(µ) ≜ DLOTr,c(µ,ν) or L(µ) ≜ LOTr,c(µ,ν). The ground cost is the squared Euclidean distance and we ﬁx r = 100. We consider 1000 samples from each distribution and and we plot the evolution of the probability measure obtained along the iterations of a gradient descent scheme. We also display in green the vector ﬁeld in the descent direction. We show that the debiased version allows to recover the target distribution while LOTr,c is learning a biased version with a low-rank structure. 28",
      "meta_data": {
        "arxiv_id": "2205.12365v2",
        "authors": [
          "Meyer Scetbon",
          "Marco Cuturi"
        ],
        "published_date": "2022-05-24T20:51:37Z",
        "pdf_url": "https://arxiv.org/pdf/2205.12365v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper advances the theoretical understanding and practical application of low-rank factorizations in Optimal Transport (OT). Key contributions include deriving the rate of convergence of low-rank OT (LOT) to true OT based on the non-negative rank, providing a dimension-independent O(sqrt(1/n)) statistical error upper-bound for LOT's plug-in estimator, introducing a debiased LOT (DLOTr,c) that is non-negative, metrizes weak convergence, and interpolates between MMD and OT, establishing links between low-rank bias and generalized clustering methods, and proposing practical strategies for adaptive step-size tuning and improved initialization for the LOT algorithm.",
        "methodology": "The core methodology involves constraining the discrete OT problem to couplings with a low-nonnegative rank (LOTr,c), solved using a mirror descent scheme with Dykstra's Algorithm. A debiased version (DLOTr,c) is introduced to address the non-zero bias of LOTr,c(µ,µ). Statistical properties are analyzed using a plug-in estimator, with convergence rates and consistency proofs based on techniques like McDiarmid’s inequality. The paper also reformulates LOT's internal bias as a generalized k-means clustering method for arbitrary metric spaces. Algorithmic enhancements include an adaptive step-size for the mirror descent (normalized by the squared dual-norm of the gradient) and novel initialization strategies based on k-means (for Euclidean spaces) or the generalized k-means (for general non-Euclidean spaces), aiming to avoid spurious local minima.",
        "experimental_setup": "The research validates its findings using 3 synthetic problems and 1 real-world dataset. Experiments include illustrating statistical rates of DLOTr,c on a mixture of 10 anisotropic Gaussians (varying dimension d) using squared Euclidean distance, demonstrating gradient flows of DLOTr,c (vs. LOT) for transforming Gaussian or moon-shaped distributions to moon-shaped targets in 2D using 1000 samples and squared Euclidean distance, applying the generalized clustering method to 6 diverse scikit-learn datasets (1000 samples each) with squared Euclidean and shortest-path distances, and evaluating the effect of different initialization strategies (k-means, generalized k-means, rank-2, random) on the Newsgroup20 dataset (texts embedded in 50D, ~250 samples) by tracking LOT cost and stopping criterion over algebraic operations. All experiments were conducted on a MacBook Pro 2019 laptop.",
        "limitations": "The computational cost of regularized OT methods, including entropic regularization, still scales quadratically in the number of observations. Theoretical properties of low-rank OT approaches are not as well-established as Sinkhorn's. While an upper-bound for statistical error is provided, a matching lower bound is not, thus a complete statistical complexity result is lacking. The constant Kr in the statistical error bound cannot be explicitly controlled in the general setting without additional assumptions. The LOTr,c optimization problem is non-convex, making it susceptible to spurious local minima, especially when a stopping criterion is used. The optimal step-size (γ) for the mirror descent can vary significantly between different problems, complicating hyperparameter tuning for fixed schedules, which the proposed adaptive method aims to alleviate.",
        "future_research_directions": "Future research should focus on further investigating the empirical behavior of the LOT estimator, particularly in discovering suitable local minima for the non-convex optimization problem. Another direction is to explore improvements in the convergence of the Mirror Descent (MD) scheme by developing and testing other adaptive choices for step sizes beyond those proposed. Additionally, providing a lower bound that matches the current upper bound for the statistical complexity of LOT would complete its theoretical understanding."
      }
    },
    {
      "title": "Low-rank lottery tickets: finding efficient low-rank neural networks via matrix differential equations",
      "abstract": "Neural networks have achieved tremendous success in a large variety of\napplications. However, their memory footprint and computational demand can\nrender them impractical in application settings with limited hardware or energy\nresources. In this work, we propose a novel algorithm to find efficient\nlow-rank subnetworks. Remarkably, these subnetworks are determined and adapted\nalready during the training phase and the overall time and memory resources\nrequired by both training and evaluating them are significantly reduced. The\nmain idea is to restrict the weight matrices to a low-rank manifold and to\nupdate the low-rank factors rather than the full matrix during training. To\nderive training updates that are restricted to the prescribed manifold, we\nemploy techniques from dynamic model order reduction for matrix differential\nequations. This allows us to provide approximation, stability, and descent\nguarantees. Moreover, our method automatically and dynamically adapts the ranks\nduring training to achieve the desired approximation accuracy. The efficiency\nof the proposed method is demonstrated through a variety of numerical\nexperiments on fully-connected and convolutional networks.",
      "full_text": "LOW-RANK LOTTERY TICKETS : FINDING EFFICIENT LOW -RANK NEURAL NETWORKS VIA MATRIX DIFFERENTIAL EQUATIONS Steffen Schotthöfer∗ Karlsruhe Institute of Technology 76131 Karlsruhe (Germany) steffen.schotthoefer@kit.edu Emanuele Zangrando∗ Gran Sasso Science Institute 67100 L’Aquila (Italy) emanuele.zangrando@gssi.it Jonas Kusch University of Innsbruck 6020 Innsbruck (Austria) jonas.kusch1@gmail.com Gianluca Ceruti EPF Lausanne 1015 Lausanne (Switzerland) gianluca.ceruti@epfl.ch Francesco Tudisco Gran Sasso Science Institute 67100 L’Aquila (Italy) francesco.tudisco@gssi.it ABSTRACT Neural networks have achieved tremendous success in a large variety of applications. However, their memory footprint and computational demand can render them impractical in application settings with limited hardware or energy resources. In this work, we propose a novel algorithm to ﬁnd efﬁcient low-rank subnetworks. Remarkably, these subnetworks are determined and adapted already during the training phase and the overall time and memory resources required by both training and evaluating them are signiﬁcantly reduced. The main idea is to restrict the weight matrices to a low-rank manifold and to update the low-rank factors rather than the full matrix during training. To derive training updates that are restricted to the prescribed manifold, we employ techniques from dynamic model order reduction for matrix differential equations. This allows us to provide approximation, stability, and descent guarantees. Moreover, our method automatically and dynamically adapts the ranks during training to achieve the desired approximation accuracy. The efﬁciency of the proposed method is demonstrated through a variety of numerical experiments on fully-connected and convolutional networks. 1 Introduction While showing great performance in terms of classiﬁcation records, most state-of-the-art neural networks require an enormous amount of computation and memory storage both for the training and the evaluation phases [27]. These requirements not only increase infrastructure costs and energy consumption, but also make the deployment of artiﬁcial neural networks to infrastructures with limited resources such as mobile phones or smart devices prohibitive. On the other hand, it is well-known that networks’ weights contain structures and redundancies that can be exploited for reducing the parameter space dimension without signiﬁcantly affecting the overall accuracy [9, 3, 17]. Network pruning is a popular line of research that addresses this problem by removing redundant pa- rameters from pre-trained models. Typically, the initial network is large and accurate, and the goal is to produce a smaller network with similar accuracy. Methods within this area include weight sparsiﬁcation [20, 24, 49] and quantization [60, 10], with different pruning techniques, including search-based heuris- tics [24], reinforcement learning [2, 23] and genetic algorithms [43]. More recent work has considered ∗These authors contributed equally to this work. arXiv:2205.13571v2  [cs.LG]  18 Oct 2022pruning during training, by formulating pruning as a data-driven optimization problem [20, 26, 27]. The resulting “dynamical pruning” boils down to a parameter-constrained training phase which, however, has been mostly focused on requiring sparse or binary weights so far. Rather than enforcing sparsity or binary variables, in this work we constrain the parameter space to the manifold of low-rank matrices. Neural networks’ parameter matrices and large data matrices in general are seldom full rank [53, 56, 48, 15]. Constraining these parameters to lie on a manifold deﬁned by low-rank matrices is thus a quite natural approach. By interpreting the training problem as a continuous-time gradient ﬂow, we propose a training algorithm based on the extension of recent Dynamical Low-Rank Approximation (DLRA) algorithms [5, 6, 4]. This approach allows us to use low-rank numerical integrators for matrix Ordinary Differential Equations (ODEs) to obtain modiﬁed forward and backward training phases that only use the small-rank factors in the low-rank representation of the parameter matrices and that are stable with respect to small singular values. This is a striking difference with respect to recent alternative “vanilla” low-rank training schemes [57, 31] which simply factorize the weight matrices as the product of two low-rank factors UV ⊤and apply a descent algorithm alternatively on the two variables U and V. We perform several experimental evaluations on fully-connected and convolutional networks showing that the resulting dynamical low-rank training paradigm yields low-parametric neural network architectures which compared to their full-rank counterparts are both remarkably less-demanding in terms of memory storage and require much less computational cost to be trained. Moreover, the trained low-rank neural networks achieve comparable accuracy to the original full architecture. This observation is reminiscent of the so-called lottery tickets hypothesis — dense neural networks contain sparse subnetworks that achieve high accuracy [17] — and suggests the presence of low-rank winning tickets: highly-performing low-rank subnetworks of dense networks. Remarkably, our dynamical low-rank training strategy seems to be able to ﬁnd the low-rank winning tickets directly during the training phase independent of initialization. 2 Related work on low-rank methods Low-rank factorization using the SVD and other matrix decomposition techniques have been extensively studied in the scientiﬁc computing and machine learning communities. The challenge of compressing and speeding up large-scale neural networks using low-rank methods has sparked wide-spread research interest in recent years and signiﬁcant effort has been put towards developing low-rank factorization strategies for deep neural networks. Previous works can roughly be categorized in approaches with ﬁxed low rank and variable low rank during training time. Fixed rank approaches decompose weight matrices using SVD or tensor decompositions of pre-trained networks and ﬁne-tune the factorized network [50, 12, 55, 38], constrain weight matrices to have a ﬁxed low rank during training [30, 57, 31], or create layers as a linear combination of layers of different rank [29]. Hence, these methods introduce the rank of the matrix decomposition as another hyperparameter to be ﬁne-tuned. Rank-adaptive methods mitigate this issue by automatic determination and adaption of the low-rank structure after training. In particular, [33, 34] apply heuristics to determine the rank of the matrix decomposition ahead of time, whereas [ 59] encourages low-rank weights via a penalized loss that depends on approximated matrix ranks. Few methods have been proposed recently that adapt the ranks of the weight matrix alongside the main network training phase. In [ 40], the authors set up the neural network training as a constrained optimization problem with an upper bound on the ranks of the weights, which is solved in an alternating approach resulting in an NP-hard mixed integer program. The authors of [ 28] formulate a similar constrained optimization problem resulting in a mixed discrete-continuous optimization scheme which jointly addresses the ranks and the elements of the matrices. However, both these approaches require knowledge of the full weight matrix (and of its singular value decomposition) during training and overall are more computational demanding than standard training. In this work we overcome the above issues and propose a training algorithm with reduced memory and computational requirements. To this end, we reinterpret the optimization problem of a neural network as a gradient ﬂow of the network weight matrices and thus as a matrix ODE. This continuous formulation allows us to use recent advances in DLRA methods for matrix ODEs which aim at evolving the solution 2of the differential equation on a low-rank manifold. The main idea of DLRA [ 35], which originates from the Dirac-Frenkel variational principle [14, 18], is to approximate the solution through a low-rank factorization and derive evolution equations for the individual factors. Thereby, the full solution does not need to be stored and the computational costs can be signiﬁcantly reduced. To ensure robustness of the method, stable integrators have been proposed in [44] and [6]. Instead of evolving individual low-rank factors in time, these methods evolve products of low-rank factors, which yields remarkable stability and exactness properties [32], both in the matrix and the tensor settings [36, 46, 45, 8]. In this work, we employ the “unconventional” basis update & Galerkin step integrator [6] as well as its rank-adaptive extension [4], see also [37, 7]. The rank-adaptive unconventional integrator chooses the approximation ranks according to the continuous-time training dynamics and allows us to ﬁnd highly-performing low-rank subnetworks directly during the training phase, while requiring reduced training cost and memory storage. 3 Low-rank training via gradient ﬂow Consider a feed-forward fully-connected neural network N(x) = zM, with z0 = x ∈ Rn0 , zk = σk(Wkzk−1 + bk) ∈ Rnk, k = 1 ,...,M (the convolutional setting is discussed in the sup- plementary material §6.6). We consider the training of Nbased on the optimization of a loss function L(W1,...,W M; N(x),y) by means of a gradient-based descent algorithm. For example, when using gradient descent, the weights of Nat iteration t∈N are updated via Wt+1 k = Wt k −λ∇WkL(W1,...,W M; N(x),y) ∀k= 1,...,M (1) with a learning rate λ. When the weight matrices Wk are dense, both the forward and gradient evaluations of the network require a large number of full matrix multiplications, with high computational expense and large memory footprint. This renders the training and the use of large-scale neural networks a difﬁcult challenge on limited-resource devices. At the same time, a wealth of evidence shows that dense networks are typically overparameterized and that most of the weights learned this way are unnecessary [48, 15]. In order to reduce the memory and computation costs of training, we propose a method that performs the minimization over the manifold of low-rank matrices. To this end, we assume that the ideal Wk can be well-approximated by a matrix of rank rk ≪nk,nk+1 of the form UkSkV⊤ k ∈Rnk×nk−1 , where Uk ∈Rnk×rk, Vk ∈Rnk−1×rk are thin and tall matrices having orthonormal columns spanning optimal subspaces which capture essential properties of parameters, and Sk ∈Rrk×rk is a tiny full-rank matrix that allows us to extrapolate the useful information from the learned subspaces Uk and Vk. Traditional descent algorithms such as (1) do not guarantee the preservation of the low-rank structure UkSkV⊤ k when updating the weights during training and require knowledge of the whole Wk rather than the factors Uk,Sk,Vk. Here we reinterpret the loss minimization as a continuous-time gradient ﬂow and derive a new training method that overcomes all aforementioned limitations. Minimizing the loss function with respect to Wk is equivalent to evaluating the long time behaviour of the following matrix ODE that allows us to interpret the training phase as a continuous process: ˙Wk(t) = −∇WkL(W1,...,W M; N(x),y), (2) where the “dot” denotes the time-derivative. LetMrk denote the manifold of matrices with rank rk and assume at a certain time t0 the weights are in the manifold, i.e., Wk(t0) ∈Mrk. Using this continuous- time interpretation allows us to derive a strategy to evolve the weights according to the dynamics in (2) so that Wk(t) ∈Mrk for all t ≥t0. To this end, in §3.1 we exploit the fact that Wk(t) admits a time-dependent factorization [13] Wk(t) = Uk(t)Sk(t)Vk(t)⊤to rewrite (2) as a system of matrix ODEs for each of the individual factors Uk,Sk and Vk. Then, in §4 we propose an algorithm to efﬁciently integrate the system of ODEs. We show in §4.1 that such algorithm reduces the loss monotonically and is accurate, in the sense that UkSkV⊤ k ≈Wk, i.e. the learned low-dimensional subspaces Uk and Vk well-match the behaviour of the full-rank network Wk solution of (2), through the action of the learned Sk. Remarkably, using the dynamics of the individual factors will also allow us to adaptively adjust the rank rk throughout the continuous-time training process. 33.1 Coupled dynamics of the low-rank factors via DLRA We consider the dynamical system of a single weight matrix Wk, while the remaining weight matrices are ﬁxed in time and are treated as parameters for the gradient. In the following, we omit writing these parameters down for efﬁciency of exposition. Assuming Wk(t) ∈Mrk, we can formulate (2) as min { ∥˙Wk(t) + ∇WkL(Wk(t))∥F : ˙Wk(t) ∈TWk(t)Mrk } (3) where TWk(t)Mrk is the tangent space of Mrk at position Wk(t). In order to solve (3), we further observe that (3) can be equivalently formulated as the following Galerkin condition [35]: ⟨˙Wk(t) + ∇WkL(Wk(t)),δWk⟩= 0 ∀δWk ∈TWk(t)Mrk . (4) From Wk = UkSkV⊤ k , a generic element δWk of the tangent space TWk(t)Mrk can be written as δWk = δUkSkV⊤ k + UkδSkV⊤ k + UkSkδV⊤ k , where δUk and δVk are generic elements of the tangent space of the Stiefel manifold with rk orthonormal columns at the points Uk and Vk, respectively, and δSk is a generic rk ×rk matrix, see e.g. [35, §2] for details. Additionally, the Gauge conditions U⊤ k δUk = 0 and V⊤ k δVk = 0 must be imposed to ensure orthogonality of the basis matrices, and the uniqueness of the representation of the tangent space elements. Similarly, by the chain rule applied several times we have ˙Wk = d dt { UkSkV⊤ k } = ˙UkSkV⊤ k + Uk ˙SkV⊤ k + UkSk ˙V⊤ k . Now, the Galerkin condition (4) becomes ⟨˙UkSkV⊤ k + Uk ˙SkV⊤ k + UkSk ˙V⊤ k + ∇WkL(Wk(t)),δWk⟩= 0, ∀δWk ∈TWk(t)Mrk (5) with U⊤ k ˙Uk = 0 and V⊤ k ˙Vk = 0. If we choose δWk = UkδSkV⊤ k in (5), we obtain ⟨U⊤ k ˙UkSkV⊤ k Vk + U⊤ k Uk ˙SkV⊤ k Vk + U⊤ k UkSk ˙V⊤ k Vk + U⊤ k ∇WkL(Wk(t))Vk,δSk⟩= 0 . Thus, using the Gauge conditions, we obtain ⟨˙Sk + U⊤ k ∇WkL(Wk(t))Vk,δSk⟩= 0, which has to hold for a generic rk ×rk matrix δSk. We obtain this way an evolution equation for the Sk(t) factor. Similarly, specifying (5) for the two choices δWk = δUkSkV⊤ k and δWk = UkSkδV⊤ k , allows us to obtain the following system of differential equations for the individual factors of Wk:    ˙Sk = −U⊤ k ∇WkL(Wk(t))Vk , ˙Uk = −(I−UkU⊤ k )∇WkL(Wk(t))VkS−1 k , ˙Vk = −(I−VkV⊤ k )∇WkL(Wk(t))⊤UkS−⊤ k . (6) 4 KLS-based training algorithm In order to perform an efﬁcient and robust rank-constrained training step, we numerically integrate the system of ODEs (6). Our approach is based on the “unconventional KLS integrator” [ 6] and its rank-adaptive version [4]. The pseudocode of the proposed training strategy is presented in Algorithm 1. The main idea of the KLS algorithm is to alternately represent the product Wk = UkSkV⊤ k as Wk = KkV⊤ k and Wk = UkL⊤ k, consider the corresponding coupled ODEs from (6), and then perform three main steps: 1,2. K&L-steps (in parallel). Update the current Kk and Lk by integrating the differential equations {˙Kk(t) = −∇WkL(Kk(t)V⊤ k )Vk, K k(0) = UkSk, ˙Lk(t) = −∇WkL(UkLk(t)⊤)⊤Uk, L k(0) = VkS⊤ k , (7) from t= 0 to t= η; then form new orthonormal basis matrices ˜Uk and ˜Vk spanning the range of the computed Kk(η) and Lk(η). 43. S-step. Update the current Sk by integrating the differential equation ˙Sk(t) = −˜U⊤ k ∇WkL( ˜UkSk(t)˜V⊤ k )˜Vk (8) from t= 0 to t= η, with initial value condition Sk(0) = ˜U⊤ k UkSkV⊤ k ˜Vk . An important feature of this algorithm is that it can be extended to rank adaptivity in a relatively straightforward manner [4], letting us dynamically evolve the rank of Sk (and thus the rank of Wk) during the computation. This is particularly useful, as we may expect the weight matrices to have low ranks but we may not know what the “best” ranks for each layer are. Typically, dynamically adapting the ranks of a low-rank optimization scheme is a challenging problem as moving from the manifold Mrk to Mrk±1 introduces singular points [19, 1]. Instead, treating the training problem as a system of matrix differential equations allows us to overcome this issue with a simple trick: at each step of the KLS integrator we double the dimension of the basis matrices ˜Uk and ˜Vk computed in the K- and L-steps by computing orthonormal bases spanning [Kk(η) |Uk] and [Lk(η) |Vk], respectively, i.e. by augmenting the current basis with the basis computed in the previous time step. Then, after the new Sk matrix is computed via the S-step, a truncation step is performed, removing from the newly computed Sk matrix all the singular values that are under a certain threshold ϑ. Of course, adding the rank-adaptivity to the integrator comes at a cost. In that case, each step requires to perform an SVD decomposition of twice the size of the current rank of Sk in order to be able to threshold the singular values. Moreover, the dimension of the bases Uk and Vk may grow, which also may require additional computational effort. However, if the ranks remain small throughout the dynamics, this computational overhead is negligible, as we will further discuss in §4.3 and §5. 4.1 Error analysis and convergence In this section we present our main theoretical results, showing that (a) the low-rank matrices UkSkV⊤ k formed by the weights’ factors computed with Alg. 1 are close to the true solution of (2), and (b) that the loss function decreases during DLRT, provided the singular value threshold ϑis not too large, i.e., is bounded by a constant times the square of the time-step size η (see Theorem 1). In the version we present here, part of the statements are presented in an informal way for the sake of brevity. We refer to the supplementary material §6.1 for details and for the proofs. Assume the gradient ﬂow Fk(Z) = −∇WkL(W1,...,Z,...,W M,N(x),y) in (2) is locally bounded and locally Lipschitz continuous, with constants C1 and C2, respectively. Then, Theorem 1. Fixed xand y, let Wk(t) be the (full-rank) continuous-time solution of (2) and let Uk,Sk,Vk be the factors computed with Algorithm 1 after t steps. Assume that the K,L,S steps (7) and (8) are integrated exactly from 0 to η. Assume moreover that, for any Z ∈Mrk sufﬁciently close to Wk(tη), the whole gradient ﬂow Fk(Z) is “ε-close” to Mrk. Then, ∥UkSkV⊤ k −Wk(tη)∥F ≤c1ε+ c2η+ c3ϑ/η k = 1,...,M where the constants c1, c2 and c3 depend only on C1 and C2. In particular, the approximation bound does not depend on the singular values of the exact nor the approximate solution. Observe that, while the loss function Ldecreases monotonically along any continuous-time solution Wk(t) of (2), it is not obvious that the loss decreases when the integration is done onto the low-rank manifold via Algorithm 1. The next result shows that this is indeed the case, up to terms of the order of the truncation tolerance ϑ. More precisely, we have Theorem 2. Let Wt k = Ut kSt k(Vt k)⊤be the low rank weight matrix computed at step tof Algorithm 1 and let L(t) = L(Wt 1,...,W t M,N(x),y). Then, for a small enough time-step ηwe have L(t+ 1) ≤L(t) −αη+ βϑ where αand βare positive constants that do not depend on t, ηand ϑ. 4.2 Efﬁcient implementation of the gradients All the three K,L,S-steps require the evaluation of the gradient ﬂow of the loss function with respect to the whole matrix Wk. Different approaches to efﬁciently compute this gradient may be used. The strategy we 5Algorithm 1: Dynamic Low Rank Training Scheme (DLRT) Input :Initial low-rank factors S0 k ∼r0 k ×r0 k; U0 k ∼nk ×r0 k;V0 k ∼nk−1 ×r0 k for k= 1,...,M ; iter: maximal number of descent iterations per epoch; adaptive: Boolean ﬂag that decides whether or not to dynamically update the ranks; ϑ: singular value threshold for adaptive procedure. 1 for each epoch do 2 for t= 0 to t= iter do 3 for each layer kdo 4 Kt k ←Ut kSt k /* K-step */ 5 Kt+1 k ←one-step-integrate {˙K(t) = −∇KL(K(t)(Vt k)⊤zk−1 + bt k), K(0) = Kt k } 6 Lt k ←Vt k(St k)⊤ /* L-step */ 7 Lt+1 k ←one-step-integrate {˙L(t) = −∇LL(Ut kL(t)⊤zk−1 + bt k), L(0) = Lt k } 8 if adaptive then /* Basis augmentation step */ 9 Kt+1 k ←[Kt+1 k |Ut k] 10 Lt+1 k ←[Lt+1 k |Vt k] 11 Ut+1 k ←orthonormal basis for the range of Kt+1 k /* S-step */ 12 Mk ←(Ut+1 k )⊤Ut k 13 Vt+1 k ←orthonormal basis for the range of Lt+1 k 14 Nk ←(Vt+1 k )⊤Vt k 15 ˜St k ←MkSt kN⊤ k 16 St+1 k ←one-step-integrate {˙S(t)= −∇SL ( Ut+1 k S(t) (Vt+1 k )⊤zk−1+bt k ) ,S(0)= ˜St k } 17 if adaptive then /* Rank compression step */ 18 P,Σ,Q ←SVD(St+1 k ) 19 St+1 k ←truncate Σ using the singular value threshold ϑ 20 Ut+1 k ←Ut+1 k ˜P where ˜P = [ﬁrst rt+1 k columns of P] 21 Vt+1 k ←Vt+1 k ˜Qwhere ˜Q= [ﬁrst rt+1 k columns of Q] /* Bias update step */ 22 bt+1 k ←one-step-integrate {˙b(t)= −∇bL(Ut+1 k St+1 k (Vt+1 k )⊤zk−1+b(t)),b(0)= bt k } discuss below aims at reducing memory and computational costs by avoiding the computation of the full gradient, working instead with the gradient with respect to the low-rank factors. To this end, we note that for the K-step it holds ∇WkL(Kk(t)V⊤ k )Vk = ∇KkL(Kk(t)V⊤ k ). Hence, the whole gradient can be computed through a forward run of the network with respect to Kk zk = σk ( Kk(t)V⊤ k zk−1 + bk ) , k = 1,...,M (9) and taping the gradient with respect to Kk. In this way, the full gradient does not need to be computed and the overall computational costs are comprised of running a forward evaluation while taping gradients with respect to Kk, analogously to the traditional back-propagation algorithm. The L- and S-steps can be evaluated efﬁciently in the same manner, by evaluating the network while taping the gradients with respect to Lk and Sk, respectively. Hence, instead of a single gradient tape (or chain rule evaluation) of the full weight matrix network, we have three gradient tapes, one for each low rank step, whose combined computational footprint is less than the full matrix tape. We provide detailed formulas for all the three gradient tapes in the supplementary material §6.5. 4.3 Implementation details, computational costs and limitations Each step of Alg. 1 requires the computation of two orthonormal bases for the ranges of Kt+1 k and Lt+1 k . There are of course different techniques to compute such orthonormal matrices. In our implementation 6we use the QR algorithm, which is known to be one of the most efﬁcient and stable approaches for this purpose. In the adaptive strategy the singular values of St+1 k are truncated according to a parameter ϑ. To this end, in our implementation, we use the Frobenius norm of Σ. Precisely, we truncate Σ = diag(σi) at step 19 of Alg. 1 by selecting the smallest principal r×rsubmatrix such that (∑ i≥r+1 σ2 i)1/2 ≤ϑ. Finally, one-step-integrate denotes a numerical procedure that integrates the corresponding ODE from time t= 0 to t= η. In practice one can employ different numerical integrators, without affecting the ability of the algorithm to reduce the loss function (see [4, Thm. 5]) while maintaining the low-rank structure. In our implementation we used two methods: 1. Explicit Euler. This method applied to the gradient ﬂow coincides with one step of Stochastic Gradient Descent (SGD), applied to the three factors Kk,Lk,Sk independently. 2. Adam. Here we formally compute the new factors by modifying the explicit Euler step as in the Adam optimization method. Note that Nesterov accelerated SGD is known to coincide with a particular linear multistep ODE integrator [51]. While Adam does not directly correspond to a numerical integrator to our knowledge, in our tests it resulted in a faster decrease of the loss than both Euler (SGD) and Nesterov accelerated SGD. For both choices, the target time step ηcorresponds to the value of the learning rate, which we set to 0.2 for Euler. For Adam, we use the default dynamical update, setting 0.001 as starting value. Computational cost. To obtain minimal computational costs and memory requirements for the K-step, the ordering of evaluating KkV⊤ k zk−1 in (9) is important. First, we compute ˜z := V⊤ k zk−1 ∈Rrk which requires O(rknk−1) operations. Second, we compute Kk˜z which requires O(rknk) operations. Adding the bias term and evaluating the activation function requires O(nk) operations. Hence, combined over all layers we have asymptotic cost of O(∑ krk(nk + nk+1)). Taping the forward evaluation to compute the gradient with respect to Kk as discussed in §4.2 does not affect the asymptotic costs, i.e. the costs of computing the K-step at layer kassuming a single data point xrequires CK ≲ ∑ krk(nk + nk+1) operations. In a similar manner, we obtain the computational costs of the L- and S-steps, which are again CL,S ≲ ∑ krk(nk + nk+1). Moreover, the QR decompositions used in the K- and L-step require O (∑ kr2 k(nk + nk−1) ) operations and computing the SVD in the truncation step has worst- case cost of O (∑ kr3 k ) . Hence, assuming rk ≪nk,nk+1, the cost per step of our low-rank method is CDLRA ≲ ∑ kr2 k(nk + nk−1), opposed to the dense network training, which requires Cdense ≲∑ knknk+1 operations. In terms of memory cost, note that we only need to store rk(1 + nk + nk+1) parameters per layer during the algorithm, corresponding to the matrices St k,Ut k,V t k. Moreover, at the end of the training we can further compress memory by storing the product of the trained weight factors UkSk, rather than the individual matrices. Limitations. A requirement for DLRT’s efﬁciency is thatrk ≪nk,nk+1. When the truncation threshold ϑis too small, Alg. 1 does not provide advantages with respect to standard training. This is also shown by Fig. 1. Moreover, in the terminology of [54], DLRT is designed to reduce training costs corresponding to model parameters and to the optimizer. To additionally decrease activation costs, DLRT can be combined with micro-batching or checkpointing approaches. Finally, the choice of ϑintroduces one additional hyperparameter which at the moment requires external knowledge for tuning. However, our experiments in §5 show that relatively large values of ϑyield competing performance as compared to a number of baselines, including standard training. 5 Numerical Results We illustrate the performance of DLRT Algorithm 1 on several test cases. The code is implemented in both Tensorﬂow (https://github.com/CSMMLab/DLRANet) and PyTorch (https://github.com/COMPiLELab/DLRT). The networks are trained on an AMD Ryzen 93950X CPU and a Nvidia RTX 3090 GPU. Timings are measured on pure CPU execution. 5.1 Performance analysis on MNIST dataset We partition MNIST dataset [11] in randomly sampled train-validation-test sets of size 50K-10K-10K. Images are pixelwise normalized; no further data augmentation or regularization has been used. 7(a) Training timings  (b) Prediction timings Figure 1: Comparison of batch execution and training times of 5-layer, 5120-neurons low-rank networks of different ranks and a non-factorized reference network with the same architecture on the MNIST dataset. Training times shown correspond to one epoch for a batch of 256 datapoints. Prediction times refer instead to the whole dataset. All the times are the average over 1000 runs. Fixed-rank fully-connected feed-forward network timings. First, we compare the training time of the adaptive DLRT Alg. 1 on a5-layer fully-connected [5120,5120,5120,5120,10] network ﬁxing the ranks of layers 1-4, i.e. choosing a speciﬁc starting rank r0 k for the input weight factors and truncating Σ at line 19 of Alg. 1 to the principal r0 k ×r0 k submatrix, rather than via a threshold. Next, we measure the average prediction time on the whole MNIST dataset over 1000 runs. Fig. 1(a) and 1(b) show that both timings scale linearly with the rank of the factorizations, and that for sufﬁciently small ranks DLRT is faster than the full-rank baseline both in terms of training and of prediction. Rank evolution and performance of DLRT for different singular value thresholds. Next, we demonstrate the capabilities of DLRT to determine the rank of the network’s weight matrices auto- matically during the network training using Algorithm 1. The Adam optimizer with default learning rate is used for the gradient update. We train fully connected5-layer networks, of which the ﬁrst 4 are replaced by low-rank layers in the subsequent tests. The activation function is chosen to be ReLU for the hidden layers, and softmax for the output layer. The training loss is sparse categorical cross entropy and we additionally measure the model’s accuracy. We use batch size256 and train for 250 epochs. We choose ϑ= τ∥Σ∥, thus we truncate the singular values of the current St k by a fraction τ of the total Frobenius norm. The smaller τ, the more singular values are kept. Figures 2 (a) and (b) show the evolution of the rank adaptive layers of a 5-layer 500-neuron network in a long time case study for τ = 0.05 and τ = 0.15. We can see that within the ﬁrst epoch the initial full matrix ranks are reduced signiﬁcantly, to 27 for τ = 0.15, and to ∼85 for τ = 0.05 respectively. Within the ﬁrst 50 epochs, the layer ranks are already close to their ﬁnal ranks. This indicates that the rank adaptive algorithm is only needed for the ﬁrst few training epochs, and can then be replaced by the computationally cheaper ﬁxed-low-rank training (by setting the Boolean variable adaptive to False in Algorithm 1). Figure 3 compares the mean test accuracy of 5-layer networks with 500 and 784 neurons with different levels of low-rank compression, over ﬁve independent runs with randomly sampled train-test-val sets. The networks can be compressed via dynamical low-rank training by more than 95%, while only losing little more than1% test accuracy compared to the dense reference network marked in red. Remark that restricting the space of possible networks to a given rank regularizes the problem, since such a restriction can be understood as adding a PCR regularization term to the loss function. This can be seen from the tendency of not overﬁtting and reaching improved test accuracies compared to the corresponding dense network for moderate compression ratios. Also note that adaptive-low rank training eliminates the need for hyperparameter grid search in terms of layer-weights, due to automatic rank adaptation. The rank dynamics for all conﬁgurations can be seen in the supplementary material §6.3. Finally, in the supplementary material §6.4 we compare the use of DLRT with the vanilla approach which simply thresholds the singular values of the full-rank network. Our results show that advantageous low-rank winning tickets exist, but are not easy to ﬁnd. In fact, the vanilla low-rank subnetworks perform very poorly. From this point of view, our approach can be seen as an efﬁcient dynamical pruning technique, able to determine high-performing low-rank subnetworks in a given dense network. Remarkably, our 8(a) Rank evolution for τ = 0.15  (b) Rank evolution of τ = 0.05 Figure 2: Rank evolution (layers 1-4) of 5-layer [500,500,500,500,10] fully-connected net on MNIST. Figure 3: Mean test accuracy over parameters’ number and compression rate for 5 runs with randomly sampled train-test-val sets on 5-layer fully-connected nets. Red dots denote the full-rank baseline. numerical experiments suggest that low-rank winning tickets can be trained from the start and do not to heavily depend on the initial weight guess. Convolutional layers: LeNet5. Here we compare the proposed dynamical low-rank training scheme on LeNet5 [39] on MNIST, against the full-rank reference and several baselines. SVD prune [ 61] and LRNN [28] are the closest approaches to our DLRT: they dynamically train low-rank layers by adding a rank-penalty to the loss function, and by complementing the standard training step via an SVD projection step in the latter and a pruning step in the former. While computing low-rank factors for each layer, thus reducing memory storage of the network, this training approach is more expensive than training the full network. GAL [42], SSL [62], and NISP [58] are pruning methods which aim at learning optimal sparse weights (rather than low-rank) by adding sparsity-promoting regularization terms to the training loss. As for LRNN, these methods do not reduce the computational cost of the training phase (as indicated with the <0% in Table 1). Analogously to [ 28], our adaptive low-rank training technique is applied to the convolutional layers by ﬂattening the tensor representing the convolutional kernel into a matrix. Details are provided in the supplementary material §6.6. All the models are trained for 120 epochs using SGD with a ﬁxed learning rate of 0.2. Results in Table 1 show that the DLRT algorithm is able to ﬁnd low-rank subnetworks with up to 96.4% less parameters than the full reference, while keeping the test accuracy above 95%. Compared to the baseline methods, we achieve better compression rates but observe lower accuracy. However, unlike the baseline references, DLRT automatically prunes the singular values during training, without requirement to solve any additional optimization problem, thus signiﬁcantly improving time and memory efﬁciency of both forward and backward phases, with respect to the full reference. Robustness with respect to small singular values and comparison with vanilla low-rank parametrization. A direct way to perform training enforcing a ﬁxed rank for the weight matrices is to parameterize each weight as Wk = UkV⊤ k and alternating training with respect to Uk and to Vk. This is the strategy employed for example in [57, 31]. This vanilla low-rank parametrization approach has a number of disadvantages with respect to DLRT, on top of the obvious non-adaptive choice of the rank. First, DLRT guarantees approximation and descent via Theorems 1 and 2. Second, we observe that the vanilla factorization gives rise to an ill-conditioned optimization method when small singular values occur. 90 2 4 6 8 epoch 20 40 60 80 100test accuracy [%] no decay DLRT UVT factorization 0 2 4 6 8 epoch decay DLRT UVT factorization 0 2 4 6 8 epoch 0.0 0.5 1.0 1.5 2.0 2.5train loss no decay DLRT UVT factorization 0 2 4 6 8 epoch decay DLRT UVT factorization         T est Accuracy                                                                                            Train Loss Figure 4: Mean learning curves with standard deviation of Lenet5 on MNIST over 10 runs of DLRT compared to a vanilla layer factorizationWk = UkV⊤ k . Both methods are implemented with ﬁxed learning rate of 0.01, and batch size of 128. The weight matrices are either completely randomly initialized (“no decay”) or are initialized with a random choice forced to have an exponential decay on the singular values (“decay”). Table 1: Results of the training of LeNet5 on MNIST dataset. Effective parameters represent the number of parameters we have to save for evaluating the network and those we need in order to train via the DLRT Alg.1. The compression ratio (c.r.) is the percentage of parameter reduction with respect to the full model (<0% indicates that the ratio is negative). “ft” indicates that the model has been ﬁne-tuned. “LeNet5” denotes the standard LeNet5 architecture trained with SGD. NN metrics Evaluation Train method test acc. ranks params c.r. params c.r. LeNet5 99.2% [20 ,50,500,10] 430500 0% 430500 0% DLRT τ = 0.11 98 .0% [15 ,46,13,10] 47975 88 .86% 50585 88 .25% τ = 0.15 97 .8% [13 ,31,9,10] 34435 92 .0% 35746 91 .7% τ = 0.2 97 .2% [10 ,20,7,10] 25650 94 .04% 26299 93 .89% τ = 0.3 95 .3% [6 ,9,4,10] 15520 96.4% 15753 96.34% SSL [62] (ft) 99.18% 110000 74 .4% <0% NISP [58] (ft) 99.0% 100000 76 .5% <0% GAL [42] 98.97% 30000 93 .0% <0% LRNN [28] 98.67% [3 ,3,9,9] 18075 95 .8% <0% SVD prune [61] 94.0% [2 ,5,89,10] 123646 71 .2% <0% This problem is peculiar to the low-rank manifold itself [35, 16], whose local curvature is proportional to the inverse of the smallest singular value of the weight matrices. In contrast, the numerical integration strategy at the basis of DLRT is designed to take advantage of the structure of the manifold and is robust with respect to small singular values [ 32]. This can be seen from the bound of Theorem 1, where the constants are independent of the singular values of the weight matrices, and is illustrated by Figure 4, where DLRT shows a much faster convergence rate with respect to vanilla SGD performed on each factor of the parametrization UkV⊤ k , when applied to train LeNet5 on MNIST. Both methods are implemented with the same ﬁxed learning rate. 5.2 Results on ImageNet1K and Cifar10 with ResNet-50, AlexNet, and VGG16 Finally, we assess the capability of compressing different architectures on large scale training sets. We train a full-rank baseline model and compare it to DLRT using the same starting weights on an Nvidia A-100 GPU. The used optimizer is SGD with momentum factor 0.1 and no data-augmentation techniques are used. We compare the results on ResNet-50, VGG16, and AlexNet models, on the Cifar10 and ImageNet1k datasets, and with respect to a number of low-parametric alternative baselines methods. For DLRT, the last layers of the networks have been adapted to match the corresponding classiﬁcation tasks. Detailed results are reported in Table 2, where we show the test accuracy (reported as the difference with respect to the full baseline) as well as compression ratios. With Cifar10, we archive a train compression of 77.5% with an accuracy loss of just 1.89% for VGG16 and 84.2% train compression at 1.79% accuracy loss for AlexNet. In the ImageNet1k benchmark, we achieve a train compression rate of 14.2%, with an test accuracy loss of 0.5% in top-5 accuracy on ResNet-50 and 78.4% train compression with 2.19 top-5 accuracy loss on VGG16. 10Table 2: Results on ImageNet1k (left) and Cifar10 (right). The compression ratio is the percentage of parameter reduction with respect to the full model. DLRT is used withτ = 0.1. The number of parameters of the full models are: 33.6M (VGG16); 23.6M (AlexNet); 29.6M (ResNet-50). We report difference in test accuracy (top-5 test accuracy for ImageNet1k) with respect to the full baselines. ImageNet1k test acc.[%] compression rate method (to baseline) eval[%] train[%]ResNet-50 DLRT −0.56 54 .1 14 .2PP-2[52] −0.8 52 .2 <0 PP-1[52] −0.2 44 .2 <0 CP[25] −1.4 50 .0 <0 SFP[22] −0.2 41 .8 <0 ThiNet[47] −1.5 36 .9 <0 VGG16 DLRT −2.19 86 78 .4PP-1[52] −0.19 80 .2 <0 CP[25] −1.80 80 .0 <0 ThiNet[47] −0.47 69 .04 <0 RNP(3X)[41] −2.43 66 .67 <0 Cifar10 test acc.[%] compression rate method (to baseline) eval[%] train[%]VGG16 DLRT −1.89 56 77 .5GAL[42] −1.87 77 <0 LRNN[28] −1.9 60 <0 AlexNet DLRT −1.79 86 .3 84 .2NISP[58] −1.06 − <0 6 Appendix 6.1 Proofs of the main results We provide here a proof of Theorems 1 and 2. The proof is based on a number of classical results as well as recent advances in DLRA theory, including [35, 44, 32, 4, 6]. Recall that, for a ﬁxed layer k, we reinterpret the training phase as a continuous-time evolution of the weights on the manifold of low-rank matrices, as illustrated in Fig. 5(a-b). This boils down to solving the manifold-constrained matrix differential equation min { ∥˙Wk(t) −Fk(Wk(t))∥: ˙Wk(t) ∈TWk(t)Mrk } , (10) where ∥·∥ is the Frobenius norm and Fk denotes the gradient ﬂow of the loss with respect to the k-th matrix variable, namely Fk(Z) = −∇WkL(W1,...,Z,...,W M; N(x),y) . For the sake of simplicity and for a cleaner notation, as all the results we will present hold for a generic k, we drop the subscript kfrom now on. In particular, we assume W is the weight matrix of a generic hidden layer with ninput and moutput neurons. In order for our derivation to hold, we require the following two properties: (a) Discrete time weight update  (b) Continuous time weight update (c) Galerkin condition Figure 5: Panels (a)-(b): Graphical re-interpretation of the weight update step as a time-continuous process. Panel (c): Orthogonal projection onto the tangent space of the low-rank manifold Mr. The dashed line depicts the projection resulting in ˙Wk(t), which is the tangent element minimizing the distance between ∇WkL(Wk(t)) and the tangent space TWk(t)Mr at the approximation Wk(t). 11P1. The gradient ﬂow Fis locally bounded and locally Lipschitz continuous, with constants C1 and C2, respectively. Namely, we assume there existC1,C2 >0 (independent of k) such that ∥F(Z)∥≤ C1 ∥F(Z) −F( ˜Z)∥≤ C2∥Z−˜Z∥ for all Z, ˜Z ∈Rm×n. P2. The whole gradient ﬂow is “not too far” from the rank-rmanifold Mr. Precisely, we assume that for any Z ∈Mr arbitrary close to W(t), the whole gradient ﬂow F(Z) near tis such that ∥(I−P(Z))F(Z)∥≤ ε, where P(Z) denotes the orthogonal projection onto TZMr. Note that both assumptions are valid for low-rank neural network training. In particular, Lipschitz continuity and boundedness of the gradient are standard assumptions in optimization and are satisﬁed by the gradient of commonly used neural networks’ losses. Moreover, assuming the gradient ﬂow to be close to the low-rank manifold is an often encountered empirical observation in neural networks [53, 48, 15]. In order to derive the proof of Theorems 1 and 2 we ﬁrst present a number of relevant background lemmas. The ﬁrst lemma shows that the subspace generated by the K-step in Algorithm 1 after the QR-decomposition is O(η(η+ ε)) close to the range of the exact solution, where ηis the time-step of the integrator and εis the eigenvalue truncation tolerance. Lemma 1 ([6, Lemma 2]) . Let W1 be the solution at time t = η of the full problem (2) with initial condition W0. Let U1 be the matrix obtained with the K-step of the ﬁxed-rank Algorithm 1, after one step. Under the assumptions P1 and P2 above, we have ∥U1U1,⊤W1 −W1∥≤ θ where θ= C1C2(4eC2η + 9)η2 + (3eC2η + 4)εη. Proof. The local error analysis of [32] shows that there exists L1 such that ∥U1L1,⊤−W1∥≤ θ. It follows that, ∥U1L1,⊤−W1∥2 = ∥U1L1,⊤−U1U1,⊤W1 + U1U1,⊤W1 −W1∥2 = ∥U1U1,⊤(U1L1,⊤−W1) + (I−U1U1,⊤)(−W1)∥2 = ∥U1U1,⊤(U1L1,⊤−W1)∥2 + ∥(I−U1U1,⊤)W1∥2. Therefore, ∥U1U1,⊤(U1L1,⊤−W1)∥2 + ∥(I−U1U1,⊤)W1∥2 ≤θ2. Hence, since both terms must be bounded by θ2 individually, we obtain the stated result. In the next lemma we show that also the space generated by the Lstep is close by the exact solution. Namely, combined with the previous result, we have Lemma 2 ([6, Lemma 3]). Let W1, U1 be deﬁned as above. Let V1 be the matrix obtained from the L-step of the ﬁxed-rank Algorithm 1, after one step. The following estimate holds: ∥U1U1,⊤W1V1V1,⊤−W1∥≤ 2θ. Proof. The L-step is obtained as the K-step applied to the transposed function G(Y) = F(Y⊤)⊤. Due to the invariance of the Frobenius norm under transposition, property P1 holds. Similarly, property P2 continues to be satisﬁed because ∥(I−P(Y))G(Y)∥= ∥(I−P(Y⊤))F(Y⊤)∥≤ ε, 12where the equality P(Y)Z⊤= [ P(Y⊤)Z ]⊤has been used [35, §4]. It follows from Lemma 1 that ∥U1U1,⊤W1 −W1∥≤ θ, ∥V1V1,⊤W1,⊤−W1,⊤∥≤ θ. (11) This implies that ∥U1U1,⊤W1V1V1,⊤−W1∥≤∥ U1U1,⊤W1V1V1,⊤−W1V1V1,⊤+ W1V1V1,⊤−W1∥ ≤∥U1U1,⊤W1V1V1,⊤−W1V1V1,⊤∥+ ∥W1V1V1,⊤−W1∥ ≤∥ ( U1U1,⊤W1 −W1) V1V1,⊤∥+ ∥V1V1,⊤W1,⊤−W1,⊤∥ ≤∥U1U1,⊤W1 −W1∥·∥V1V1,⊤∥2 + ∥V1V1,⊤W1,⊤−W1,⊤∥. Because ∥V1V1,⊤∥2 = 1, the stated result follows from (11). With the previous lemmas, we are in the position to derive the local error bound for the ﬁxed-rank KLS integrator of Section 4. Lemma 3 (Local Error, [6, Lemma 4]). Let W1,U1,V 1 be deﬁned as above and let S1 be the matrix obtained with the S-step of Algorithm 1 after one step. The following local error bound holds: ∥U1S1V1,⊤−W1∥≤ η(ˆc1ε+ ˆc2η), where the constants ˆci are independent of the singular values of W1 and S1. Proof. From Lemma 2 and the equality Y1 = U1S1V1,⊤, we have that ∥Y1 −W1∥≤∥ Y1 −U1U1,⊤W1V1V1,⊤∥+ ∥U1U1,⊤W1V1V1,⊤−W1∥ ≤∥U1(S1 −U1,⊤W1V1)V1,⊤∥+ 2θ ≤∥S1 −U1,⊤W1V1∥+ 2θ. The local error’s analysis is reduced to bound the term ∥S1 −U1,⊤W1V1∥. For 0 ≤t ≤η, we thus introduce the following auxiliary quantity: ˜S(t) := U1,⊤W(t)V1. We observe that the term W(t) can be re-written as W(t) = U1U1,⊤W(t)V1V1,⊤+ ( W(t) −U1U1,⊤W(t)V1V1,⊤ ) = U1 ˜S(t)V1,⊤+ R(t), where R(t) denotes the term in big brackets. For 0 ≤t≤η, it follows from Lemma 2 and the bound C1 of the function Fthat ∥W(t) −W(η)∥≤ ∫ η 0 ∥˙W(s)∥ds= ∫ η 0 ∥F(W(s))∥ds≤C1η. Therefore, the term R(t) is bounded by ∥R(t)∥≤∥R (t) −R(η)∥+ ∥R(η)∥≤ 2C1η+ 2θ. We re-cast the function F ( W(t) ) as F ( W(t) ) = F ( U1 ˜S(t)V1,⊤+ R(t) ) = F ( U1 ˜S(t)V1,⊤) + D(t) where the defect D(t) is given by D(t) := F ( U1 ˜S(t)V1,⊤+ R(t) ) −F ( U1 ˜S(t)V1,⊤) . Via the Lipschitz constant C2 of the function F, the defect is bounded by ∥D(t)∥≤ C2∥R(t)∥≤ 2C2(C1η+ θ). 13Now, we compare the two differential equations ˙˜S(t) = U1,⊤F ( U1 ˜S(t)V1,⊤) V1 + U1,⊤D(t)V1, ˜S(0) = U1,⊤W0V1, ˙S(t) = U1,⊤F ( U1S(t)V1,⊤) V1, S (0) = U1,⊤W0V1. The solution S1 obtained in the second differential equation is the same as given by the S-step of the KLS integrator of Section 4. By construction, the solution obtained in the ﬁrst differential equation at time t= ηis ˜S(η) = U1,⊤W1V1. With the Gronwall inequality we obtain ∥S1 −U1,⊤W1V1∥≤ ∫ η 0 eC2(η−s) ∥D(s)∥ds≤eLη2C2(C1η+ θ)η. The result yields the statement of the theorem using the deﬁnition of θ. We are now in the position to conclude the proof of Theorem 1. Proof of Theorem 1. In Lemma 3, the local error for the ﬁxed-rank integrator of §4 has been provided. The local error in time of the rank-adaptive version is directly obtained via a triangle inequality: ∥U1S1V1,⊤−W(η)∥≤ ˆc1εη+ ˆc2η2 + ϑ, where ϑis the tolerance parameter chosen for the truncation procedure. Here, we abuse the notation and we maintain the same nomenclature U1,S1,and V1 also for the novel low-rank approximation obtained via the truncation procedure. Thus, we conclude the proof using the Lipschitz continuity of the function F. We move from the local error in time to the global error in time by a standard argument of Lady Windermere’s fan [21, Section II.3]. Therefore, the error after tsteps of the rank-adaptive Algorithm 1 is given by ∥UtStVt,⊤−W(tη)∥≤ c1ε+ c2η+ c3ϑ/η. To conclude with, we prove that after one step the proposed rank-adaptive DLRT algorithm decreases along the low-rank approximations. We remind that only property P1 needs to be assumed here. Proof of Theorem 2. Let ˆY(t) = U1S(t)V1,⊤. Here, S(t) denotes the solution for t∈[0,η] of the S-step of the rank-adaptive integrator . It follows that d dtL(ˆY(t)) = ⟨∇L(ˆY(t)), ˙ˆY(t)⟩ = ⟨∇L(ˆY(t)),U1 ˙S(t)V1,⊤⟩ = ⟨U1,⊤∇L(ˆY(t))V1, ˙S(t)⟩ = ⟨U1,⊤∇L(ˆY(t))V1, −U1,⊤∇L(ˆY(t))V1⟩= −∥U1,⊤∇L(ˆY(t))V1∥2 . The last identities follow by deﬁnition of the S-step. For t∈[0,η] we have d dtL(ˆY(t)) ≤−α2 (12) where α= min0≤τ≤1 ∥U1,⊤∇L (ˆY(τη) ) V1∥. Integrating (12) from t= 0 until t= η, we obtain L(ˆY1) ≤L(ˆY0) −α2η. Because the subspace U1 and V1 contain by construction the range and co-range of the initial value, we have that ˆY0 = U0S0V0,⊤[4, Lemma 1]. The truncation is such that ∥Y1 −ˆY1∥≤ ϑ. Therefore, L(Y1) ≤L(ˆY1) + βϑ where β = max0≤τ≤1 ∥∇L ( τY 1 + (1 −τ)ˆY1) ∥. Hence, the stated result is obtained. 146.2 Detailed timing measurements Table 3 displays the average batch training times of a 5-layer, 5120-neuron dense network on the MNIST dataset, with a batch size of 500 samples. We average the timings over 200 batches and additionally display the standard deviation of the timings corresponding to the layer ranks. The batch timing measures the full K,L and S steps, including back-propagation and gradient updates, as well as the loss and metric evaluations. Table 3: Average batch training times for ﬁxed low-rank training of a 5-layer fully-connected network with layer widths [5120,5120,5120,5120,10]. Different low-rank factorizations are compared ranks mean time [s] std. deviation [s] full-rank 0.320 ±0.005227 [320,320,320,320,320] 0 .855 ±0.006547 [160,160,160,160,10] 0 .387 ±0.005657 [80,80,80,80,10] 0 .198 ±0.004816 [40,40,40,40,10] 0 .133 ±0.005984 [20,20,20,20,10] 0 .098 ±0.005650 [10,10,10,10,10] 0 .087 ±0.005734 [5,5,5,5,10] 0 .071 ±0.005369 Table 4 shows the average test time of a 5-layer, 5120-neuron dense network, for different low-rank factorizations and the full rank reference network. The timings are averaged over 1000 evaluations of the 60K sample MNIST training data set. We measure the K step forward evaluation of the low-rank networks as well as the loss and prediction accuracy evaluations. Table 4: Average dataset prediction times for ﬁxed low-rank training of a 5-layer fully-connected network with layer widths [5120,5120,5120,5120,10]. Different low-rank factorizations are compared. ranks mean time [s] std. deviation [s] full-rank 1.2476 ±0.0471 [2560,2560,2560,2560,10] 1 .4297 ±0.0400 [1280,1280,1280,1280,10] 0 .7966 ±0.0438 [640,640,640,640,10] 0 .4802 ±0.0436 [320,320,320,320,10] 0 .3286 ±0.0442 [160,160,160,160,10] 0 .2659 ±0.0380 [80,80,80,80,10] 0 .2522 ±0.0346 [40,40,40,40,10] 0 .2480 ±0.0354 [20,20,20,20,10] 0 .2501 ±0.0274 [10,10,10,10,10] 0 .2487 ±0.0276 [5,5,5,5,10] 0 .2472 ±0.0322 6.3 Detailed training performance of adaptive low-rank networks Tables 5 and 6 display a detailed overview of the adaptive low-rank results of §5.1. The displayed ranks are the ranks of the converged algorithm. The rank evolution of the 5-Layer, 500-Neuron test case can be seen in Fig. 6. The Evaluation parameter count corresponds to the parameters of the K step of the dynamical low-rank algorithm, since all other matrices are no longer needed in the evaluation phase. The training parameter count is evaluated as the number of parameters of the Sstep of the adaptive dynamical low rank training, with maximal basis expansion by 2r, where ris the current rank of the network. We use the converged ranks of the adaptive low-rank training to compute the training parameters. Note that during the very ﬁrst training epochs, the parameter count is typically higher until the rank reduction has reached a sufﬁciently low level. 15a) Rank evolution for τ = 0.17  b) Rank evolution for τ = 0.15 c) Rank evolution for τ = 0.13  d) Rank evolution for τ = 0.11 e) Rank evolution for τ = 0.09  f) Rank evolution for τ = 0.07 g) Rank evolution for τ = 0.05  h) Rank evolution for τ = 0.03 Figure 6: Rank evolution of the dynamic adaptive low-rank training algorithm for the 5-layer, 500-neuron dense architecture. 16Table 5: Dynamical low rank training for 5-layer 500-neurons network. c.r. denotes the compression rate relative to the full rank dense network. NN metrics Evaluation Train τ test acc. ranks params c.r. params c.r. full-rank 98.54 ± 0.03% [500 ,500,500,500,10] 1147000 0% 1147000 0% 0.03 98 .49 ± 0.02% [176 ,170,171,174,10] 745984 34 .97% 1964540 -71.27% 0.05 98 .56 ± 0.02% [81 ,104,111,117,10] 441004 61 .56% 1050556 8 .40% 0.07 98 .52 ± 0.08% [52 ,67,73,72,10] 283768 75 .26% 633360 44 .78% 0.09 98 .34 ± 0.14% [35 ,53,51,46,10] 199940 82 .57% 429884 62 .52% 0.11 98 .11 ± 0.46% [27 ,40,37,38,10] 154668 86 .52% 324904 71 .67% 0.13 97 .50 ± 0.23% [20 ,31,32,30,10] 123680 89 .22% 255500 77 .72% 0.15 97 .22 ± 0.29% [17 ,25,26,24,10] 101828 91 .13% 207320 81 .92% 0.17 96 .90 ± 0.45% [13 ,21,24,20,10] 86692 92 .45% 174728 84 .76% Table 6: Dynamical low rank training for 5-layer 784-neurons network. c.r. denotes the compression rate relative to the full rank dense network. NN metrics Evaluation Train τ test acc. ranks params c.r. params c.r. full-rank 98.53 ± 0.04% [784 ,784,784,784,10] 2466464 0% 2466464 0% 0.03 98 .61 ± 0.07% [190 ,190,190,190,10] 1199520 51 .37% 2968800 -20.36% 0.05 98 .59 ± 0.06% [124 ,120,125,126,10] 784000 68 .22% 1805268 26 .80% 0.07 98 .58 ± 0.03% [76 ,86,85,83,10] 525280 78 .71% 1151864 53 .29% 0.09 98 .49 ± 0.05% [56 ,67,63,59,10] 392000 84 .41% 836460 66 .08% 0.11 98 .12 ± 0.21% [35 ,49,47,43,10] 280672 88 .63% 584240 76 .31% 0.13 97 .95 ± 0.23% [29 ,35,38,34,10] 221088 91 .04% 453000 81 .63% 0.15 97 .81 ± 0.17% [22 ,29,27,27,10] 172480 93 .01% 348252 85 .88% 0.17 97 .40 ± 0.25% [17 ,23,22,23,10] 141120 94 .28% 281724 88 .57% 6.3.1 Lenet5 experiment In Table 7 we report the results of ﬁve independent runs of the dynamic low-rank training scheme on Lenet5; we refer to §5.1 for further details. For each column of the table, we report the mean value together with its relative standard deviations. No seed has been applied for splitting the dataset and generating the initial weights conﬁguration. Table 7: Mean results and standard relative deviations of the dynamic low-rank training algorithm over ﬁve independent runs on Lenet5. Adaptive learning rate of 0.05 with 0.96−exponentially decaying tax. NN metrics Evaluation Train τ test acc. ranks params c.r. params c.r. 0.11 95 .420 ±1.865% [15 ,46,13,10] 47975 88 .9% 50585 88 .2% 0.15 95 .527 ±1.297% [13 ,31,9,10] 34435 92 .0% 35746 91 .69% 0.2 95 .009 ±1.465% [10 ,20,7,10] 25650 94 .04% 26299 93 .89% 0.3 92 .434 ±1.757% [6 ,9,4,10] 15520 96 .39% 15753 96 .34% 6.4 Detailed training performance of low-rank pruning The proposed low-rank training algorithm does not need to be applied to train a network from random initial weight guesses. When an already trained network is available, the proposed method can be employed as a memory-efﬁcient pruning strategy. A straightforward approach to reduce a trained fully- connected network to a rank rnetwork is to compute an SVD for all weight matrices and to truncate those decompositions at rank r. However, while this choice is optimal to present weight matrices, it might signiﬁcantly reduce the accuracy of the network. Hence, retraining the determined low-rank subnetwork is commonly necessary to obtain desirable accuracy properties. Three key aspects are important to obtain an efﬁcient pruning method for low-rank methods: 1. Retraining preserves the low-rank structure of the subnetwork. 172. Retraining does not exhibit the memory footprint of the fully connected network. 3. Retraining ﬁnds the optimal network among possible low rank networks. Let us note that the attractor of the proposed dynamical low-rank evolution equations fulﬁlls these three requirements. Recall that for the evolution equations we have (3): min { ∥˙Wk(t) + ∇WkL(Wk(t))∥F : ˙Wk(t) ∈TWk(t)Mrk } . (13) The condition ˙Wk(t) ∈TWk(t)Mrk ensures that the weight matrices remain of low-rank. Moreover, as previously discussed, the training method only requires memory capacities to store low-rank factors. At the attractor, i.e., when ˙Wk = 0, the last condition ensures that the attractor minimizes∥∇WkL(Wk(t))∥F. That is, the attractor is the optimal low-rank subnetwork in the sense that it picks the network with minimal gradient. To underline the effectiveness of our low-rank method as a pruning technique, we take the fully connected network from Table 6. To demonstrate the poor validation accuracy when simply doing an SVD on the full 784 by 784 weight matrices and truncating at a given smaller rank, we perform this experiment for ranks r ∈{10,20,30,40,50,60,70,80,90,100}. It turns out that though reducing memory requirements, this strategy leads to unsatisfactory accuracy of about 10%, see the ﬁrst column of Table 8. Then, we use the proposed low-rank training methods with ﬁxed rank rto retrain the network. As starting points, we use the low-rank networks which have been determined by the truncated SVD. Retraining then reaches desired accuracies that are comparable to the previously determined low-rank networks in Table 6. Table 8: Pruning methods with 784 Neurons per layer test accuracy Evaluation SVD low-rank training ranks params c.r. 98.63% 98 .63% [784 ,784,784,784,10] 2466464 0% 9.91% 98 .16% [100 ,100,100,100,10] 635040 74 .25% 9.67% 98 .44% [90 ,90,90,90,10] 572320 76 .80% 9.15% 98 .47% [80 ,80,80,80,10] 509600 79 .34% 9.83% 98 .58% [70 ,70,70,70,10] 446880 81 .88% 9.67% 98 .41% [60 ,60,60,60,10] 384160 84 .42% 9.83% 98 .39% [50 ,50,50,50,10] 321440 86 .97% 10.64% 98 .24% [40 ,40,40,40,10] 258720 89 .51% 10.3% 98 .24% [30 ,30,30,30,10] 196000 92 .05% 9.15% 97 .47% [20 ,20,20,20,10] 133280 94 .60% 10.9% 95 .36% [10 ,10,10,10,10] 70560 97 .14% 6.5 Detailed derivation of the gradient In this section, we derive the computation of the gradients in the K, L and S steps in detail. For this, let us start with the full gradient, i.e., the gradient of the loss with respect to the weight matrix Wk. We have ∂Wℓ jk L= nM∑ iM=1 ∂zM iM L∂Wℓ jk zM iM = nM∑ iM=1 ∂zM iM L∂Wℓ jk σM  ∑ iM−1 WiMiM−1 zM−1 iM−1 + bM iM   = nM∑ iM=1 ∂zM iM Lσ′ M  ∑ iM−1 WiMiM−1 zM−1 iM−1 + bM iM  ∂Wℓ jk  ∑ iM−1 WiMiM−1 zM−1 iM−1  . (14) For a general α, let us deﬁne σ′ α,iα := σ′ α  ∑ iα−1 Wα iαiα−1 zα−1 iα−1 + bα iα   (15) 18and note that for α̸= ℓ ∂Wℓ jk  ∑ iα−1 Wα iαiα−1 zα−1 iα−1  = ∑ iα−1 Wα iαiα−1 ∂Wℓ jk zα−1 iα−1 , (16) whereas for α= ℓwe have ∂Wℓ jk   nα−1∑ iα−1=1 Wα iαiα−1 zα−1 iα−1  = ∑ iα−1 δjiαδkiα−1 zα−1 iα−1 . (17) Therefore, recursively plugging (15), (16) and (17) into (14) yields ∂Wℓ jk L= nM∑ iM=1 ∂zM iM Lσ′ M,iM ∑ iM−1 Wα iMiM−1 ∂Wℓ jk zM−1 iM−1 = nM∑ iM=1 ∂zM iM Lσ′ M,iM ∑ iM−1 Wα iMiM−1 σ′ M−1,iM−1 ∑ iM−2 Wα iM−1iM−2 ∂Wℓ jk zM−2 iM−2 = ··· = ∑ iℓ,···,iM ∂zM iM L M∏ α=ℓ+1 σ′ α,iαWα iαiα−1 σ′ ℓ,iℓ∂Wℓ jk   nℓ−1∑ iℓ−1=1 Wℓ iℓiℓ−1 zℓ−1 iℓ−1   (18) = ∑ iℓ,···,iM ∂zM iM L M∏ α=ℓ+1 σ′ α,iαWα iαiα−1 σ′ ℓ,iℓ ∑ iℓ−1 δjiℓδkiℓ−1 zℓ−1 iℓ−1 = ∑ iℓ+1,···,iM ∂zM iM L M∏ α=ℓ+1 σ′ α,iαWα iαiα−1 σ′ ℓ,jδjiℓzℓ−1 k Written in matrix notation and making use of the Hadamard product deﬁned as y◦A◦x= (yiAijxj)ij, for A∈Rm×n, x∈Rn and y∈Rm, we have: ∂WℓL=∂zML⊤ ( σ′ ℓ ◦ M∏ α=ℓ+1 W⊤ α ◦σ′ α )⊤ z⊤ ℓ−1 Now, let us move to deriving the K, L and S-steps for the dynamical low-rank training. For the K-step, we represent the weight matrix Wℓ as Wℓ iℓiℓ−1 = ∑ mKℓ iℓmVℓ iℓ−1m. Hence, reusing the intermediate result (18) yields ∂Kℓ jk L= ∑ iℓ,···,iM ∂zM iM L M∏ α=ℓ+1 σ′ α,iαWα iαiα−1 σ′ ℓ,iℓ∂Kℓ jk   nℓ−1∑ iℓ−1=1 ∑ m Kℓ iℓmVℓ iℓ−1mzℓ−1 iℓ−1   = ∑ iℓ,···,iM ∂zM iM L M∏ α=ℓ+1 σ′ α,iαWα iαiα−1 σ′ ℓ,iℓ nℓ−1∑ iℓ−1=1 ∑ m δjiℓδkmVℓ iℓ−1mzℓ−1 iℓ−1 = ∑ iℓ+1,···,iM ∂zM iM L M∏ α=ℓ+1 σ′ α,iαWα iαiα−1 σ′ ℓ,iℓ nℓ−1∑ iℓ−1=1 δjiℓVℓ iℓ−1kzℓ−1 iℓ−1 In matrix notation we obtain ∂KℓL=∂zML⊤ ( σ′ ℓ ◦ M∏ α=ℓ+1 W⊤ α ◦σ′ α )⊤( V⊤ ℓ zℓ−1 )⊤ = ∂WℓLVℓ, 19which is exactly the right-hand side of the K-step. Hence, the K-step can be computed by a forward evaluation of Land recording the gradient tape with respect to Kℓ. Similarly, for the L-step, we represent Wℓ as Wℓ iℓiℓ−1 = ∑ mUℓ iℓmLℓ iℓ−1m. Hence, ∂Lℓ jk L= ∑ iℓ,···,iM ∂zM iM L M∏ α=ℓ+1 σ′ α,iαWα iαiα−1 σ′ ℓ,iℓ∂Lℓ jk   nℓ−1∑ iℓ−1=1 ∑ m Uℓ iℓmLℓ iℓ−1m   = ∑ iℓ,···,iM ∂zM iM L M∏ α=ℓ+1 σ′ α,iαWα iαiα−1 σ′ ℓ,iℓ nℓ−1∑ iℓ−1=1 ∑ m Uℓ iℓmδjiℓ−1 δkmzℓ−1 iℓ−1 = ∑ iℓ,···,iM ∂zM iM L M∏ α=ℓ+1 σ′ α,iαWα iαiα−1 σ′ ℓ,iℓUℓ iℓmzℓ−1 j . In matrix notation, we obtain ∂LℓL=  U⊤ ℓ ∂zML⊤ ( σ′ ℓ ◦ M∏ α=ℓ+1 W⊤ α ◦σ′ α )⊤ z⊤ ℓ−1   ⊤ = (∂WℓL)⊤Uℓ. Lastly, for the S-step we write Wℓ iℓiℓ−1 = ∑ n,mUℓ iℓmSmnVℓ iℓ−1n. Then, ∂Sℓ jk L= ∑ iℓ,···,iM ∂zM iM L M∏ α=ℓ+1 σ′ α,iαWα iαiα−1 σ′ ℓ,iℓ∂Sℓ jk (∑ n,m Uℓ iℓmSmnVℓ iℓ−1n ) = ∑ iℓ,···,iM ∂zM iM L M∏ α=ℓ+1 σ′ α,iαWα iαiα−1 σ′ ℓ,iℓ nℓ−1∑ iℓ−1=1 ∑ m Uℓ iℓmδjmδknVℓ iℓ−1nzℓ−1 iℓ−1 = ∑ iℓ,···,iM ∂zM iM L M∏ α=ℓ+1 σ′ α,iαWα iαiα−1 σ′ ℓ,iℓUℓ iℓjVℓ iℓ−1kzℓ−1 iℓ−1 . In matrix notation, we have ∂SℓL= U⊤ ℓ ∂zML⊤ ( σ′ ℓ ◦ M∏ α=ℓ+1 W⊤ α ◦σ′ α )⊤( V⊤ ℓ zℓ−1 )⊤ = U⊤ ℓ ∂WℓLVℓ. 6.6 Low-rank matrix representation and implementation of convolutional layers A generalized convolutional ﬁlter is a four-mode tensor W ∈RF×C×J×K consisting of F ﬁlters of shape C×J×K, which is applied to a batch of N input C−channels image signals Zof spatial dimensions U ×V as the linear mapping, (Z∗W)(n,f,u,v ) = J∑ j=1 K∑ k=1 C∑ c=1 W(f,c,j,k )Z(n,c,u −j,v −k) . (19) In order to train the convolutional ﬁlter on the low-rank matrix manifold, we reshape the tensor W into a rectangular matrix Wresh ∈RF×CJK. This reshaping is also considered in e.g. [28]. An option is, to see the convolution as the contraction between an three-mode tensor Zunfolded of patches and the reshaped kernel matrix Wresh using Pytorch’s fold-unfold function. We can construct the unfold by stacking the vectorized version of sliding patterns of the kernel on the original input, obtaining in this way a tensor Zunfolded ∈RN×CJK×L, where L denotes the dimension of ﬂatten version of the output of the 2-D 20convolution. Thus, equation 19 can be rewritten as a tensor mode product: (Z∗W)(n,f,u,v ) = J∑ j=1 K∑ k=1 C∑ c=1 Wresh(f,(c,j,k ))Zunfolded(n,(c,j,k ),(u,v)) = r∑ p= U(f,p) r∑ q=1 S(p,q) J∑ j=1 K∑ k=1 C∑ c=1 V((c,j,k ),q)Zunfolded(n,(c,j,k ),(u,v)) (20) As it is shown in (20), we can a decompose the starting weight Wresh = USV ⊤and then do all the training procedure as a function of the factors (U,S,V ), without ever reconstructing the kernel. Then we can apply the considerations of fully connected layers. Acknowledgements. The work of S. Schotthöfer was funded by the Priority Programme SPP2298 “Theoretical Foundations of Deep Learning” by the Deutsche Forschungsgemeinschaft (DFG). The work of J. Kusch was funded by the Deutsche Forschungsgemeinschaft (DFG) – 491976834. The work of G. Ceruti was supported by the SNSF research project “Fast algorithms from low-rank updates”, grant number 200020-178806. The work of F. Tudisco and E. Zangrando was funded by the MUR-PNRR project “Low-parametric machine learning”. Special thanks to Prof. Martin Frank for the PhD mentorship of Steffen Schottöfer. References [1] P.-A. Absil, R. Mahony, and R. Sepulchre. Optimization algorithms on matrix manifolds. Princeton University Press, 2009. [2] A. Ashok, N. Rhinehart, F. Beainy, and K. M. Kitani. N2n learning: Network to network compression via policy gradient reinforcement learning. In International Conference on Learning Representations, 2018. [3] D. Blalock, J. J. Gonzalez Ortiz, J. Frankle, and J. Guttag. What is the state of neural network pruning? Proceedings of machine learning and systems, 2:129–146, 2020. [4] G. Ceruti, J. Kusch, and C. Lubich. A rank-adaptive robust integrator for dynamical low-rank approximation. BIT Numerical Mathematics, 2022. [5] G. Ceruti and C. Lubich. Time integration of symmetric and anti-symmetric low-rank matrices and Tucker tensors. BIT Numerical Mathematics, 60(3):591–614, 2020. [6] G. Ceruti and C. Lubich. An unconventional robust integrator for dynamical low-rank approximation. BIT. Numerical Mathematics, 62(1):23–44, 2022. [7] G. Ceruti, C. Lubich, and D. Sulz. Rank-adaptive time integration of tree tensor networks. arXiv:2201.10291, 2022. [8] G. Ceruti, C. Lubich, and H. Walach. Time integration of tree tensor networks. SIAM Journal on Numerical Analysis, 59(1):289–313, 2021. [9] Y . Cheng, F. X. Yu, R. S. Feris, S. Kumar, A. Choudhary, and S.-F. Chang. An exploration of parameter redundancy in deep networks with circulant projections. In Proceedings of the IEEE international conference on computer vision, pages 2857–2865, 2015. [10] M. Courbariaux, I. Hubara, D. Soudry, R. El-Yaniv, and Y . Bengio. Binarized neural networks: Training deep neural networks with weights and activations constrained to +1 or -1. Advances in neural information processing systems, 2016. [11] L. Deng. The MNIST database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine, 29(6):141–142, 2012. [12] E. L. Denton, W. Zaremba, J. Bruna, Y . LeCun, and R. Fergus. Exploiting linear structure within convolutional networks for efﬁcient evaluation. Advances in neural information processing systems, 27, 2014. [13] L. Dieci and T. Eirola. On smooth decompositions of matrices. SIAM Journal on Matrix Analysis and Applications, 20(3):800–819, 1999. 21[14] P. A. M. Dirac et al. The principles of quantum mechanics. Number 27. Oxford university press, 1981. [15] R. Feng, K. Zheng, Y . Huang, D. Zhao, M. Jordan, and Z.-J. Zha. Rank diminishing in deep neural networks. arXiv:2206.06072, 2022. [16] F. Feppon and P. F. Lermusiaux. A geometric approach to dynamical model order reduction. SIAM Journal on Matrix Analysis and Applications, 39(1):510–538, 2018. [17] J. Frankle and M. Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In International Conference on Learning Representations, 2018. [18] J. Frenkel. Wave mechanics, advanced general theory, volume 1. Oxford, 1934. [19] B. Gao and P.-A. Absil. A riemannian rank-adaptive method for low-rank matrix completion. Computational Optimization and Applications, 81(1):67–90, 2022. [20] Y . Guo, A. Yao, and Y . Chen. Dynamic network surgery for efﬁcient dnns. Advances in neural information processing systems, 29, 2016. [21] E. Hairer, S. P. Nørsett, and G. Wanner.Solving ordinary differential equations. I. Nonstiff problems, volume 8 of Springer Series in Computational Mathematics. Springer-Verlag, Berlin, second edition, 1993. [22] Y . He, G. Kang, X. Dong, Y . Fu, and Y . Yang. Soft ﬁlter pruning for accelerating deep convolutional neural networks, 2018. [23] Y . He, J. Lin, Z. Liu, H. Wang, L.-J. Li, and S. Han. AMC: AutoML for model compression and acceleration on mobile devices. In Proceedings of the European conference on computer vision, pages 784–800, 2018. [24] Y . He, X. Zhang, and J. Sun. Channel pruning for accelerating very deep neural networks. InIEEE International Conference on Computer Vision, pages 1389–1397, 2017. [25] Y . He, X. Zhang, and J. Sun. Channel pruning for accelerating very deep neural networks, 2017. [26] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700–4708, 2017. [27] Z. Huang and N. Wang. Data-driven sparse structure selection for deep neural networks. In Proceedings of the European conference on computer vision (ECCV), pages 304–320, 2018. [28] Y . Idelbayev and M. A. Carreira-Perpiñán. Low-rank compression of neural nets: Learning the rank of each layer. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8046–8056, 2020. [29] Y . Ioannou, D. Robertson, J. Shotton, R. Cipolla, and A. Criminisi. Training CNNs with low-rank ﬁlters for efﬁcient image classiﬁcation. In International Conference on Learning Representations, 2016. [30] M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up convolutional neural networks with low rank expansions. In Proceedings of the British Machine Vision Conference. BMVA Press, 2014. [31] M. Khodak, N. Tenenholtz, L. Mackey, and N. Fusi. Initialization and regularization of factorized neural layers. In International Conference on Learning Representations, 2021. [32] E. Kieri, C. Lubich, and H. Walach. Discretized dynamical low-rank approximation in the presence of small singular values. SIAM Journal on Numerical Analysis, 54(2):1020–1038, 2016. [33] H. Kim, M. U. K. Khan, and C.-M. Kyung. Efﬁcient neural network compression. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12569–12577, 2019. [34] Y .-D. Kim, E. Park, S. Yoo, T. Choi, L. Yang, and D. Shin. Compression of deep convolutional neural networks for fast and low power mobile applications, 2015. [35] O. Koch and C. Lubich. Dynamical low-rank approximation. SIAM Journal on Matrix Analysis and Applications, 29(2):434–454, 2007. [36] O. Koch and C. Lubich. Dynamical tensor approximation. SIAM Journal on Matrix Analysis and Applications, 31(5):2360–2375, 2010. 22[37] J. Kusch and P. Stammer. A robust collision source method for rank adaptive dynamical low-rank approximation in radiation therapy. arXiv:2111.07160, 2021. [38] V . Lebedev, Y . Ganin, M. Rakhuba, I. Oseledets, and V . Lempitsky. Speeding-up convolutional neural networks using ﬁne-tuned cp-decomposition. In International Conference on Learning Representations, 2015. [39] Y . Lecun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. [40] C. Li and C. J. R. Shi. Constrained optimization based low-rank approximation of deep neural networks. In Proceedings of the European Conference on Computer Vision (ECCV), September 2018. [41] J. Lin, Y . Rao, J. Lu, and J. Zhou. Runtime neural pruning. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors,Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. [42] S. Lin, R. Ji, C. Yan, B. Zhang, L. Cao, Q. Ye, F. Huang, and D. Doermann. Towards optimal structured CNN pruning via generative adversarial learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2790–2799, 2019. [43] H. Liu, K. Simonyan, O. Vinyals, C. Fernando, and K. Kavukcuoglu. Hierarchical representations for efﬁcient architecture search. In International Conference on Learning Representations, 2018. [44] C. Lubich and I. V . Oseledets. A projector-splitting integrator for dynamical low-rank approximation. BIT Numerical Mathematics, 54(1):171–188, 2014. [45] C. Lubich, T. Rohwedder, R. Schneider, and B. Vandereycken. Dynamical approximation by hierarchical Tucker and tensor-train tensors. SIAM Journal on Matrix Analysis and Applications, 34(2):470–494, 2013. [46] C. Lubich, B. Vandereycken, and H. Walach. Time integration of rank-constrained Tucker tensors. SIAM Journal on Numerical Analysis, 56(3):1273–1290, 2018. [47] J.-H. Luo, J. Wu, and W. Lin. Thinet: A ﬁlter level pruning method for deep neural network compression, 2017. [48] C. H. Martin and M. W. Mahoney. Implicit self-regularization in deep neural networks: Evidence from random matrix theory and implications for learning. Journal of Machine Learning Research, 22(165):1–73, 2021. [49] P. Molchanov, S. Tyree, T. Karras, T. Aila, and J. Kautz. Pruning convolutional neural networks for resource efﬁcient inference. In International Conference on Learning Representations, 2017. [50] T. N. Sainath, B. Kingsbury, V . Sindhwani, E. Arisoy, and B. Ramabhadran. Low-rank matrix factorization for deep neural network training with high-dimensional output targets. In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 6655–6659, 2013. [51] D. Scieur, V . Roulet, F. Bach, and A. d’Aspremont. Integration methods and accelerated optimization algorithms. In Advances In Neural Information Processing Systems, 2017. [52] P. Singh, V . Kumar Verma, P. Rai, and V . P. Namboodiri. Play and prune: Adaptive ﬁlter pruning for deep model compression. In Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence, IJCAI-19, pages 3460–3466. International Joint Conferences on Artiﬁcial Intelligence Organization, 7 2019. [53] S. P. Singh, G. Bachmann, and T. Hofmann. Analytic insights into structure and rank of neural network Hessian maps. In Advances in Neural Information Processing Systems, volume 34, 2021. [54] N. S. Sohoni, C. R. Aberger, M. Leszczynski, J. Zhang, and C. Ré. Low-memory neural network training: A technical report. arXiv:1904.10631, 2019. [55] A. Tjandra, S. Sakti, and S. Nakamura. Compressing recurrent neural network with tensor train. In 2017 International Joint Conference on Neural Networks (IJCNN), pages 4451–4458. IEEE, 2017. [56] M. Udell and A. Townsend. Why are big data matrices approximately low rank? SIAM Journal on Mathematics of Data Science, 1(1):144–160, 2019. 23[57] H. Wang, S. Agarwal, and D. Papailiopoulos. Pufferﬁsh: communication-efﬁcient models at no extra cost. Proceedings of Machine Learning and Systems, 3:365–386, 2021. [58] W. Wen, C. Wu, Y . Wang, Y . Chen, and H. Li. Learning structured sparsity in deep neural networks. Advances in neural information processing systems, 29, 2016. [59] W. Wen, C. Xu, C. Wu, Y . Wang, Y . Chen, and H. Li. Coordinating ﬁlters for faster deep neural networks. In Proceedings of the IEEE International Conference on Computer Vision, pages 658–666, 2017. [60] J. Wu, C. Leng, Y . Wang, Q. Hu, and J. Cheng. Quantized convolutional neural networks for mobile devices. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4820–4828, 2016. [61] H. Yang, M. Tang, W. Wen, F. Yan, D. Hu, A. Li, H. Li, and Y . Chen. Learning low-rank deep neural networks via singular vector orthogonality regularization and singular value sparsiﬁcation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 678–679, 2020. [62] J. Ye, X. Lu, Z. Lin, and J. Z. Wang. Rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers. In International Conference on Learning Representations, 2018. 24",
      "meta_data": {
        "arxiv_id": "2205.13571v2",
        "authors": [
          "Steffen Schotthöfer",
          "Emanuele Zangrando",
          "Jonas Kusch",
          "Gianluca Ceruti",
          "Francesco Tudisco"
        ],
        "published_date": "2022-05-26T18:18:12Z",
        "venue": "Proceedings NeurIPS 2022",
        "pdf_url": "https://arxiv.org/pdf/2205.13571v2.pdf",
        "github_url": "https://github.com/COMPiLELab/DLRT"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Dynamic Low-Rank Training (DLRT), a novel algorithm that finds and adapts efficient low-rank subnetworks (\"low-rank lottery tickets\") during the neural network training phase. It addresses high memory and computational demands by restricting weight matrices to a low-rank manifold and updating only their low-rank factors via techniques from dynamic model order reduction for matrix differential equations. DLRT dynamically adapts ranks, offers approximation and stability guarantees, significantly reduces training/evaluation costs, and achieves comparable accuracy to full-rank models.",
        "methodology": "DLRT reinterprets neural network training as a continuous-time gradient flow (matrix ODE). Weight matrices are factorized (UkSkV⊤k) and their updates are derived using Dynamical Low-Rank Approximation (DLRA), specifically an \"unconventional KLS integrator\" with a rank-adaptive extension. The KLS algorithm performs parallel K-steps and L-steps for basis updates, followed by an S-step for singular value matrix updates. Rank adaptivity is achieved by augmenting bases and truncating singular values below a threshold (ϑ) using SVD. Gradient computations are optimized by operating directly on low-rank factors. Numerical integration uses Explicit Euler (SGD) or Adam. Convolutional kernels are reshaped into matrices for low-rank factorization.",
        "experimental_setup": "Evaluations were conducted on fully-connected and convolutional networks (LeNet5, ResNet-50, AlexNet, VGG16) using PyTorch and TensorFlow. Datasets included MNIST, Cifar10, and ImageNet1K. Training used Adam or SGD optimizers (with momentum) with specified learning rates and batch sizes. Performance metrics included test accuracy, parameter compression ratio, and training/prediction times. Comparisons were made against full-rank baselines and other low-rank/pruning methods. Rank adaptation utilized a singular value threshold ϑ= τ∥Σ∥.",
        "limitations": "DLRT's efficiency requires the inherent rank of weight matrices (rk) to be much smaller than their dimensions. Setting the singular value truncation threshold (ϑ) too low negates efficiency gains. The method primarily reduces model parameter and optimizer costs, but does not inherently reduce activation costs, requiring combination with techniques like micro-batching or checkpointing. The parameter ϑ introduces an additional hyperparameter needing manual tuning.",
        "future_research_directions": "Not mentioned",
        "experimental_code": "import torch\nimport numpy as np\nfrom tqdm import tqdm\nfrom torch import float16   \n\nclass dlr_opt:\n\n    def __init__(self,NN,tau = 0.01,theta = 0.1,absolute = False,\n                KLS_optim = None,**kwargs):\n\n        \"\"\"\n        initializer for the dlr_opt (dynamical low rank optimizer) class.\n        INPUTS:\n        NN: neural network with custom layers, methods and attributes needed (look at Lenet5 for an example) \n        tau : learning rate (integration step)\n        theta : tolerance for singular values\n        absolute : flag variable, True if theta has to be interpreted as an absolute tolerance  \n        KLS_optim : Pytorch integrator to perform the integration step\n        \"\"\"\n\n        self.NN = NN\n        self.tau = tau\n        self.theta = theta\n        self.absolute = absolute\n        self.kw = dict(kwargs)\n        self.KLS_optim = KLS_optim\n\n        if self.KLS_optim is not None:\n\n            self.integrator = self.KLS_optim(self.NN.parameters(),lr = self.tau,**kwargs)\n\n        else:\n\n            self.integrator = torch.optim.SGD(self.NN.parameters(),lr = self.tau,**kwargs)\n\n\n    @torch.no_grad()\n    def K_postprocess_step(self):\n\n        for l in self.NN.layer:\n\n            if hasattr(l,'lr') and l.lr:\n\n                if not l.fixed:\n                    \n                    U_hat = torch.hstack((l.K[:,:l.dynamic_rank],l.U[:,:l.dynamic_rank]))\n\n                    try:\n                        U_hat,_ = torch.linalg.qr(U_hat)\n                    except:\n                        U_hat,_ = np.linalg.qr(U_hat)\n                        U_hat = torch.tensor(U_hat)\n                    l.U_hat[:,:2*l.dynamic_rank] = U_hat\n                    l.M_hat[:2*l.dynamic_rank,:l.dynamic_rank] = l.U_hat[:,:2*l.dynamic_rank].T@l.U[:,:l.dynamic_rank]\n                \n                else:\n\n                    try:\n                        U_hat,_ = torch.linalg.qr(l.K)\n\n                    except:\n                        U_hat,_ = np.linalg.qr(U_hat)\n                        U_hat = torch.tensor(U_hat)\n                    l.M_hat.data = U_hat.T@l.U.data\n                    l.U.data = U_hat\n\n    @torch.no_grad()\n    def postprocess_step(self):\n        \n        self.K_postprocess_step()\n        self.L_postprocess_step()\n\n    @torch.no_grad()\n    def K_integration_step(self):\n        \n        self.zero_bias_grad()\n        self.integrator.step()\n\n    @torch.no_grad()\n    def zero_bias_grad(self):\n\n        for l in self.NN.layer:\n\n            if hasattr(l,'bias') and l.bias is not None:\n\n                l.bias.grad = None\n\n            if hasattr(l,'weight') and l.weight is not None:\n\n                l.weight.grad = None\n\n    @torch.no_grad()\n    def L_postprocess_step(self):\n\n        for l in self.NN.layer:\n\n            if hasattr(l,'lr') and l.lr:\n\n                if not l.fixed:\n\n                    V_hat = torch.hstack((l.L[:,:l.dynamic_rank],l.V[:,:l.dynamic_rank]))\n                    try :\n                        V_hat,_ = torch.linalg.qr(V_hat)\n                    except:\n                        V_hat,_ = np.linalg.qr(V_hat.detach().numpy())\n                        V_hat= torch.tensor(V_hat)\n                    l.V_hat[:,:2*l.dynamic_rank] = V_hat\n                    l.N_hat[:2*l.dynamic_rank,:l.dynamic_rank] = l.V_hat[:,:2*l.dynamic_rank].T@l.V[:,:l.dynamic_rank]\n\n                else:\n\n                    try :\n                        V_hat,_ = torch.linalg.qr(l.L)\n                    except:\n                        V_hat,_ = np.linalg.qr(V_hat.detach().numpy())\n                        V_hat= torch.tensor(V_hat)\n                    l.N_hat.data = V_hat.T@l.V.data\n                    l.V.data = V_hat\n\n\n    \n    @torch.no_grad()\n    def L_integration_step(self):\n\n\n        self.integrator.step()\n        self.integrator.zero_grad()\n\n    @torch.no_grad()\n    def K_and_L_integration_step(self):\n        \n        self.zero_bias_grad()\n        self.integrator.step()\n\n    @torch.no_grad()\n    def S_preprocess_step(self):\n\n        for l in self.NN.layer:\n\n            if hasattr(l,'lr') and l.lr:\n\n                if not l.fixed:\n\n                    s = l.M_hat[:2 * l.dynamic_rank, :l.dynamic_rank]@l.S_hat[: l.dynamic_rank, :l.dynamic_rank]@l.N_hat[:2 * l.dynamic_rank, :l.dynamic_rank].T\n                    l.S_hat[:2*l.dynamic_rank,:2*l.dynamic_rank] = s\n\n                else:\n\n                    s = l.M_hat@l.S_hat@l.N_hat.T\n                    l.S_hat.data = s\n\n\n\n    @torch.no_grad()\n    def K_preprocess_step(self):\n\n        for l in self.NN.layer:\n\n            if hasattr(l,'lr') and l.lr:\n\n                if not l.fixed:\n                \n                    K = l.U[:,:l.dynamic_rank]@l.S_hat[:l.dynamic_rank,:l.dynamic_rank]\n                    l.K[:,:l.dynamic_rank] = K\n\n                else:\n\n                    K = l.U.data@l.S_hat\n                    l.K.data = K\n\n\n\n    @torch.no_grad()\n    def L_preprocess_step(self):\n\n        for l in self.NN.layer:\n\n            if hasattr(l,'lr') and l.lr:\n\n                if not l.fixed:\n\n                    L = l.V[:,:l.dynamic_rank]@l.S_hat[:l.dynamic_rank,:l.dynamic_rank].T\n                    l.L[:,:l.dynamic_rank] = L\n\n                else:\n\n                    L = l.V.data@l.S_hat.T\n                    l.L.data = L\n\n\n    @torch.no_grad()\n    def S_postprocess_step(self):\n\n        for l in self.NN.layer:\n\n            if hasattr(l,'lr') and l.lr:\n\n                if not l.fixed:\n\n                    # rank adaption\n\n                    s_small = torch.clone(l.S_hat[:2 * l.dynamic_rank, :2 * l.dynamic_rank])\n                    try:\n                        u2, d, v2 = torch.linalg.svd(s_small)\n                    except Exception as e:\n                        print(e)\n                        print(s_small)\n                        u2, d, v2 = np.linalg.svd(s_small)\n\n                    tmp = 0.0\n                    tol = self.theta * torch.linalg.norm(d) if not self.absolute else self.theta \n                    rmax = int(np.floor(d.shape[0] / 2))\n                    for j in range(0, 2 * rmax - 1):\n                        tmp = torch.linalg.norm(d[j:2 * rmax - 1])\n                        if tmp < tol:\n                            rmax = j\n                            break\n\n                    rmax = min([rmax, l.rmax])\n                    rmax = max([rmax, 2])\n\n                    l.S_hat[:rmax,:rmax] = torch.diag(d[:rmax])\n                    l.U[:, :rmax] = l.U_hat[:, :2 * l.dynamic_rank]@u2[:, :rmax]\n                    l.V[:,:rmax] =  l.V_hat[:,:2 * l.dynamic_rank]@(v2[:, :rmax])\n                    l.dynamic_rank = int(rmax)\n\n    \n    @torch.no_grad()\n    def S_integration_step(self):\n\n        self.integrator.step()\n        self.integrator.zero_grad()\n    \n\n    @torch.no_grad()\n    def preprocess_step(self):\n\n        self.K_preprocess_step()\n        self.L_preprocess_step()\n\n    @torch.no_grad()\n    def step(self,closure = None):\n\n        \"\"\"\n        optimizer step for the dlrt.\n        INPUTS:\n        closure : function to compute the loss and backpropagate a second time (Pytorch standard)\n        \"\"\"\n\n        # self.K_integration_step()\n        # self.L_integration_step()\n        self.K_and_L_integration_step()\n        self.K_postprocess_step()\n        self.L_postprocess_step()\n        self.S_preprocess_step()\n        self.zero_grad()\n        if closure is not None:\n            with torch.set_grad_enabled(True):\n                loss = closure()\n                loss.backward()\n        self.S_integration_step()\n        self.S_postprocess_step()\n    \n    @torch.no_grad()\n    def zero_grad(self):\n        for p in self.NN.parameters():\n            if p.requires_grad:\n                p.grad = None\n\n\n    @torch.no_grad()\n    def activate_S_fine_tuning(self):\n\n        params = []\n\n        for l in self.NN.layer:\n\n            if hasattr(l,'lr') and l.lr:\n\n                l.K.requires_grad = False\n                l.L.requires_grad = False\n                l.S_hat = torch.nn.Parameter(l.S_hat[:l.dynamic_rank,:l.dynamic_rank])\n                l.fixed = True\n                l.U = torch.nn.Parameter(l.U[:,:l.dynamic_rank],requires_grad = False)\n                l.V = torch.nn.Parameter(l.V[:,:l.dynamic_rank],requires_grad = False)\n                l.step = 'S'\n                params.append(l.S_hat)\n        params = torch.nn.ParameterList(params)\n        self.integrator = self.KLS_optim(params,lr = self.tau,**self.kw)\n\n\n    @torch.no_grad()\n    def S_finetune_step(self):\n\n        self.integrator.step()\n\n\n# imports \nimport math\nimport warnings\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn import init\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=Warning)\n\n# low rank convolution class \n\nclass Conv2d_lr(torch.nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, dilation=1, padding=0, stride=1,bias = True,step = 'K',rank = None,\n    fixed = False,dtype = None,device = None,load_weights = None,full_rank_construct = False)->None:\n\n        \"\"\"  \n        Initializer for the convolutional low rank layer (filterwise), extention of the classical Pytorch's convolutional layer.\n        INPUTS:\n        in_channels: number of input channels (Pytorch's standard)\n        out_channels: number of output channels (Pytorch's standard)\n        kernel_size : kernel_size for the convolutional filter (Pytorch's standard)\n        dilation : dilation of the convolution (Pytorch's standard)\n        padding : padding of the convolution (Pytorch's standard)\n        stride : stride of the filter (Pytorch's standard)\n        bias  : flag variable for the bias to be included (Pytorch's standard)\n        step : string variable ('K','L' or 'S') for which forward phase to use\n        rank : rank variable, None if the layer has to be treated as a classical Pytorch Linear layer (with weight and bias). If\n                it is an int then it's either the starting rank for adaptive or the fixed rank for the layer.\n        fixed : flag variable, True if the rank has to be fixed (KLS training on this layer)\n        load_weights : variables to load (Pytorch standard, to finish)\n        dtype : Type of the tensors (Pytorch standard, to finish)\n        \"\"\"\n            \n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super(Conv2d_lr, self).__init__()\n\n        self.kernel_size = [kernel_size, kernel_size] if isinstance(kernel_size,int) else kernel_size\n        self.kernel_size_number = kernel_size * kernel_size\n        self.out_channels = out_channels\n        self.dilation = dilation if type(dilation)==tuple else (dilation, dilation)\n        self.padding = padding if type(padding) == tuple else(padding, padding)\n        self.stride = (stride if type(stride)==tuple else (stride, stride))\n        self.in_channels = in_channels\n        self.rank = rank\n        self.device = device\n        self.dtype = dtype\n        self.fixed = fixed\n        self.load_weights = load_weights\n        self.weight = torch.nn.Parameter(torch.empty(tuple([self.out_channels, self.in_channels] +self.kernel_size),**factory_kwargs),requires_grad = True)\n        self.lr = True if self.rank!=None else False\n        self.rmax = int(min([self.out_channels, self.in_channels*self.kernel_size_number]) / 2)\n        self.full_rank_construct = full_rank_construct\n        if not self.fixed:\n            self.rank = None if rank == None else min([rank,self.rmax])\n        else:\n            self.rank = min([rank,self.out_channels,self.in_channels*self.kernel_size_number])\n        self.dynamic_rank = self.rank\n        self.step = step\n\n        if bias:\n            self.bias = torch.nn.Parameter(torch.empty(self.out_channels,**factory_kwargs))\n        else:\n            self.bias = torch.nn.Parameter(torch.zeros(self.out_channels,**factory_kwargs))\n\n        self.reset_parameters()\n    \n        # Weights and Bias initialization\n        if self.load_weights == None:\n            self.reset_parameters()\n        else:\n            param,b = self.load_weights\n            self.bias = torch.nn.Parameter(b)\n            self.weight = torch.nn.Parameter(param,requires_grad = True)\n\n        if self.lr and not self.full_rank_construct:\n\n            if not self.fixed:\n\n                n,m = self.out_channels,self.in_channels*self.kernel_size_number\n\n                _,s_ordered,_ = torch.linalg.svd(torch.diag(torch.abs(torch.randn(2*self.rmax))))\n                U = torch.randn(n,self.rmax)\n                V = torch.randn(m,self.rmax)\n                U,_,_ = torch.linalg.svd(U)\n                V,_,_ = torch.linalg.svd(V)\n                self.U = torch.nn.Parameter(U.to(device) ,requires_grad=False)             \n                self.S_hat = torch.nn.Parameter(torch.diag(s_ordered).to(device))                                       \n                self.V = torch.nn.Parameter(V.to(device),requires_grad=False) \n                self.U_hat = torch.nn.Parameter( torch.randn(n,2*self.rmax).to(device) ,requires_grad = False)\n                self.V_hat = torch.nn.Parameter(torch.randn(m,2*self.rmax).to(device) ,requires_grad = False)\n                self.K = torch.nn.Parameter(torch.randn(n,self.rmax).to(device))\n                self.L = torch.nn.Parameter(torch.randn(m,self.rmax).to(device))\n                self.N_hat = torch.nn.Parameter(torch.randn(2*self.rmax,self.rmax).to(device) ,requires_grad = False)\n                self.M_hat = torch.nn.Parameter( torch.randn(2*self.rmax,self.rmax).to(device) ,requires_grad = False)\n                self.S = torch.nn.Parameter(torch.randn(self.rmax,self.rmax).to(device),requires_grad = False)\n                self.weight = None\n                self.id = id(self.K)\n\n            else:\n\n                n,m = self.out_channels,self.in_channels*self.kernel_size_number\n\n                _,s_ordered,_ = torch.linalg.svd(torch.diag(torch.abs(torch.randn(self.rank))))\n                U = torch.randn(n,self.rank)\n                V = torch.randn(m,self.rank)\n                U,_,_ = torch.linalg.svd(U)\n                V,_,_ = torch.linalg.svd(V)\n                self.U = torch.nn.Parameter(U[:,:self.rank].to(device) ,requires_grad=False)             \n                self.S_hat = torch.nn.Parameter(torch.diag(s_ordered).to(device))                                       \n                self.V = torch.nn.Parameter(V[:,:self.rank].to(device),requires_grad=False)\n                self.K = torch.nn.Parameter(torch.randn(n,self.rank).to(device))\n                self.L = torch.nn.Parameter(torch.randn(m,self.rank).to(device))\n                self.N_hat = torch.nn.Parameter(torch.randn(self.rank,self.rank).to(device) ,requires_grad = False)\n                self.M_hat = torch.nn.Parameter( torch.randn(self.rank,self.rank).to(device) ,requires_grad = False)\n                self.weight = None\n\n    def switch_lowrank(self):\n        \n        w,b = self.weight,self.bias\n        device = self.device\n        if not self.fixed:\n            self.rank = None if self.rank == None else min([self.rank,self.rmax])\n        else:\n            self.rank = min([self.rank,self.out_channels,self.in_channels*self.kernel_size_number])\n        self.dynamic_rank = self.rank\n\n        self.bias = b\n\n        if self.lr:\n\n            if not self.fixed:\n\n                n,m = self.out_channels,self.in_channels*self.kernel_size_number\n\n                U_load,S_load,V_load = torch.linalg.svd(w.view(n,m))\n                V_load = V_load.T\n                r = len(S_load)\n                _,s_ordered,_ = torch.linalg.svd(torch.diag(torch.abs(torch.randn(2*self.rmax-r))))\n                s_ordered = torch.tensor(torch.cat([S_load,s_ordered.to(device)])).to(device)\n                self.U = torch.nn.Parameter(U_load.to(device) ,requires_grad=False)             \n                self.S_hat = torch.nn.Parameter(torch.diag(s_ordered).to(device))                                       \n                self.V = torch.nn.Parameter(V_load.to(device),requires_grad=False)\n                self.U_hat = torch.nn.Parameter( torch.randn(n,2*self.rmax).to(device) ,requires_grad = False)\n                self.V_hat = torch.nn.Parameter(torch.randn(m,2*self.rmax).to(device) ,requires_grad = False)\n                self.K = torch.nn.Parameter(torch.randn(n,self.rmax).to(device))\n                self.L = torch.nn.Parameter(torch.randn(m,self.rmax).to(device))\n                self.N_hat = torch.nn.Parameter(torch.randn(r,r).to(device) ,requires_grad = False)\n                self.M_hat = torch.nn.Parameter( torch.randn(r,r).to(device) ,requires_grad = False)\n                self.weight = None\n                self.id = id(self.K)\n            else:\n                n,m = self.out_channels,self.in_channels*self.kernel_size_number\n\n                U_load,S_load,V_load = torch.linalg.svd(w.view(n,m))\n                V_load = V_load.T\n                r = self.rank\n                self.U = torch.nn.Parameter(U_load[:,:r].to(device) ,requires_grad=False)             \n                self.S_hat = torch.nn.Parameter(torch.diag(S_load[:r]).to(device))                                       \n                self.V = torch.nn.Parameter(V_load[:,:r].to(device),requires_grad=False)\n                self.K = torch.nn.Parameter(torch.randn(n,r).to(device))\n                self.L = torch.nn.Parameter(torch.randn(m,r).to(device))\n                self.N_hat = torch.nn.Parameter(torch.randn(2*self.rmax,self.rmax).to(device) ,requires_grad = False)\n                self.M_hat = torch.nn.Parameter( torch.randn(2*self.rmax,self.rmax).to(device) ,requires_grad = False)\n                self.weight = None\n                self.id = id(self.K)\n\n\n\n    def reset_parameters(self) -> None:\n        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\n        # uniform(-1/sqrt(k), 1/sqrt(k)), where k = weight.size(1) * prod(*kernel_size)\n        # For more details see: https://github.com/pytorch/pytorch/issues/15314#issuecomment-477448573\n        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n         # for testing\n        # self.original_weight = Parameter(self.weight.reshape(self.original_shape))\n        if self.bias is not None:\n            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n            if fan_in != 0:\n                bound = 1 / math.sqrt(fan_in)\n                init.uniform_(self.bias, -bound, bound)  \n\n\n    def forward(self, input):\n\n        \"\"\"  \n        forward phase for the convolutional layer. It has to contain the three different\n        phases for the steps 'K','L' and 'S' in order to be optimizable using dlrt.\n\n        \"\"\"\n        \n        batch_size,_,_,_ = input.shape\n\n        if not self.lr:\n\n            return F.conv2d(input = input,weight = self.weight,bias = self.bias,stride = self.stride,\n                padding = self.padding,dilation = self.dilation)\n\n        else:\n\n            if self.step == 'K':\n\n                if not self.fixed:\n\n                    K,V = self.K[:,:self.dynamic_rank],self.V[:,:self.dynamic_rank] # Corrected from l.dynamic_rank to self.dynamic_rank\n\n                else:\n\n                    K,V = self.K,self.V\n\n                inp_unf = F.unfold(input,self.kernel_size,padding = self.padding,stride = self.stride).to(self.device)\n    \n                if self.bias is None:\n                    out_unf = (inp_unf.transpose(1, 2).matmul(V) )\n                    out_unf = (out_unf.matmul(K.t()) + self.bias).transpose(1, 2)\n                else:\n                    out_h = int(np.floor(((input.shape[2]+2*self.padding[0]-self.dilation[0]*(self.kernel_size[0]-1)-1)/self.stride[0])+1))\n                    out_w = int(np.floor(((input.shape[3]+2*self.padding[1]-self.dilation[1]*(self.kernel_size[1]-1)-1)/self.stride[1])+1))\n\n                    out_unf = (inp_unf.transpose(1, 2).matmul(V) )\n                    out_unf = (out_unf.matmul(K.t()) + self.bias).transpose(1, 2)\n    \n                return out_unf.view(batch_size, self.out_channels, out_h, out_w)\n\n            elif self.step =='L':\n\n                if not self.fixed:\n\n                    U,L = self.U[:,:self.dynamic_rank],self.L[:,:self.dynamic_rank]\n                \n                else:\n\n                    U,L = self.U,self.L\n\n                inp_unf = F.unfold(input,self.kernel_size,padding = self.padding,stride = self.stride).to(self.device)\n    \n                if self.bias is None:\n                    out_unf = (inp_unf.transpose(1, 2).matmul(L) )\n                    out_unf = (out_unf.matmul(U.t()) + self.bias).transpose(1, 2)\n                else:\n                    out_h = int(np.floor(((input.shape[2]+2*self.padding[0]-self.dilation[0]*(self.kernel_size[0]-1)-1)/self.stride[0])+1))\n                    out_w = int(np.floor(((input.shape[3]+2*self.padding[1]-self.dilation[1]*(self.kernel_size[1]-1)-1)/self.stride[1])+1))\n\n                    out_unf = (inp_unf.transpose(1, 2).matmul(L) )\n                    out_unf = (out_unf.matmul(U.t()) + self.bias).transpose(1, 2)\n    \n                return out_unf.view(batch_size, self.out_channels, out_h, out_w)\n            \n            elif self.step == 'S':\n\n                if not self.fixed:\n\n                    U_hat,S_hat,V_hat = self.U_hat[:,:2*self.dynamic_rank],self.S_hat[:2*self.dynamic_rank,:2*self.dynamic_rank],self.V_hat[:,:2*self.dynamic_rank]\n\n                else:\n\n                    U_hat,S_hat,V_hat = self.U,self.S_hat,self.V\n    \n                inp_unf = F.unfold(input,self.kernel_size,padding = self.padding,stride = self.stride).to(self.device)\n\n                if self.bias is None:\n                    out_unf = (inp_unf.transpose(1, 2).matmul(V_hat) )\n                    out_unf = (out_unf.matmul(S_hat.t()))\n                    out_unf = (out_unf.matmul(U_hat.t()) + self.bias).transpose(1, 2)\n                else:\n                    out_h = int(np.floor(((input.shape[2]+2*self.padding[0]-self.dilation[0]*(self.kernel_size[0]-1)-1)/self.stride[0])+1))\n                    out_w = int(np.floor(((input.shape[3]+2*self.padding[1]-self.dilation[1]*(self.kernel_size[1]-1)-1)/self.stride[1])+1))\n\n                    out_unf = (inp_unf.transpose(1, 2).matmul(V_hat) )\n                    out_unf = (out_unf.matmul(S_hat.t()))\n                    out_unf = (out_unf.matmul(U_hat.t()) + self.bias).transpose(1, 2)\n    \n                return out_unf.view(batch_size, self.out_channels, out_h, out_w)\n\n            else:\n\n                raise ValueError(f'incorrect step value {self.step}')\n\n\n# redefinition of the linear layer, adding decomposition attributes\n# to the weight object\n\nimport math       \nimport torch\nfrom torch import Tensor\nimport torch.nn.init as init\nimport torch.nn.functional as F\n\n\nclass Linear(torch.nn.Module):\n    \n\n    __constants__ = ['in_features', 'out_features']\n    in_features: int\n    out_features: int\n    weight: Tensor\n\n    def __init__(self, in_features: int, out_features: int, bias: bool = True,\n                 device=None, dtype=None,rank = None,fixed = False,load_weights = None,step = 'K',full_rank_construct = False) -> None:\n\n        \"\"\"  \n        initializer for the low rank linear layer, extention of the classical Pytorch's Linear\n        INPUTS:\n        in_features : number of inputs features (Pytorch standard)\n        out_features : number of output features (Pytorch standard)\n        bias : flag for the presence of bias (Pytorch standard)\n        device : device in where to put parameters\n        dtype : type of the tensors (Pytorch standard)\n        rank : rank variable, None if the layer has to be treated as a classical Pytorch Linear layer (with weight and bias). If\n                it is an int then it's either the starting rank for adaptive or the fixed rank for the layer.\n        fixed : flag variable, True if the rank has to be fixed (KLS training on this layer)\n        load_weights : variables to load (Pytorch standard, to finish)\n        step : flag variable ('K','L' or 'S') for which forward phase to use\n        \"\"\"\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super(Linear, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = torch.nn.Parameter(torch.empty((out_features, in_features), **factory_kwargs))\n        self.rank = rank\n        self.device = device\n        self.load_weights = load_weights\n        self.fixed = fixed\n        self.lr = True if self.rank!=None else False\n        self.full_rank_construct = full_rank_construct\n        self.rmax = int(min([self.in_features, self.out_features]) / 2)\n        if not self.fixed:\n            self.rank = None if rank == None else min([rank,self.rmax])\n        else:\n            self.rank = min([rank,self.in_features,self.out_features])\n        self.dynamic_rank = self.rank\n        self.step = step\n\n        if bias:\n                self.bias = torch.nn.Parameter(torch.empty(out_features, **factory_kwargs))\n        else:\n                self.register_parameter('bias', None)\n\n        self.reset_parameters()\n\n            \n        if self.lr and not full_rank_construct:\n\n            if not self.fixed:   # initialization for dlrt adaptive\n                _,s_ordered,_ = torch.linalg.svd(torch.diag(torch.abs(torch.randn(2*self.rmax))))\n                U = torch.randn(self.out_features,self.rmax)\n                V = torch.randn(self.in_features,self.rmax)\n                U,_,_ = torch.linalg.svd(U)\n                V,_,_ = torch.linalg.svd(V)\n                self.U = torch.nn.Parameter(U.to(device) ,requires_grad=False)             \n                self.S_hat = torch.nn.Parameter(torch.diag(s_ordered).to(device))                                          \n                self.V = torch.nn.Parameter(V.to(device),requires_grad=False)\n                self.U_hat = torch.nn.Parameter( torch.randn(self.out_features,2*self.rmax).to(device) ,requires_grad = False)\n                self.V_hat = torch.nn.Parameter(torch.randn(self.in_features,2*self.rmax).to(device) ,requires_grad = False)\n                self.K = torch.nn.Parameter(torch.randn(self.out_features,self.rmax).to(device))\n                self.L = torch.nn.Parameter(torch.randn(self.in_features,self.rmax).to(device))\n                self.N_hat = torch.nn.Parameter(torch.randn(2*self.rmax,self.rmax).to(device) ,requires_grad = False)\n                self.M_hat = torch.nn.Parameter( torch.randn(2*self.rmax,self.rmax).to(device) ,requires_grad = False)\n                self.weight = None\n            else:     # initialization for dlrt fixed rank\n                _,s_ordered,_ = torch.linalg.svd(torch.diag(torch.abs(torch.randn(self.rank))))\n                U = torch.randn(self.out_features,self.rank)\n                V = torch.randn(self.in_features,self.rank)\n                U,_,_ = torch.linalg.svd(U)\n                V,_,_ = torch.linalg.svd(V)\n                self.U = torch.nn.Parameter(U[:,:self.rank].to(device) ,requires_grad=False)             \n                self.S_hat = torch.nn.Parameter(torch.diag(s_ordered).to(device))                                          \n                self.V = torch.nn.Parameter(V[:,:self.rank].to(device),requires_grad=False)\n                self.K = torch.nn.Parameter(torch.randn(self.out_features,self.rank).to(device))\n                self.L = torch.nn.Parameter(torch.randn(self.in_features,self.rank).to(device))\n                self.N_hat = torch.nn.Parameter(torch.randn(self.rank,self.rank).to(device) ,requires_grad = False)\n                self.M_hat = torch.nn.Parameter( torch.randn(self.rank,self.rank).to(device) ,requires_grad = False)\n                self.weight = None\n\n    def switch_lowrank(self):\n        \n        w,b = self.weight,self.bias\n        device = self.device\n        if not self.fixed:\n            self.rank = None if self.rank == None else min([self.rank,self.rmax])\n        else:\n            self.rank = min([self.rank,self.out_features,self.in_features])\n\n        self.bias = b\n\n        if self.lr:\n\n            if not self.fixed:\n\n                n,m = self.out_features,self.in_features\n\n                U_load,S_load,V_load = torch.linalg.svd(w.view(n,m))\n                V_load = V_load.T\n                r = len(S_load)\n                _,s_ordered,_ = torch.linalg.svd(torch.diag(torch.abs(torch.randn(2*self.rmax-r))))\n                s_ordered = torch.tensor(torch.cat([S_load,s_ordered.to(device)])).to(device)\n                self.U = torch.nn.Parameter(U_load.to(device) ,requires_grad=False)             \n                self.S_hat = torch.nn.Parameter(torch.diag(s_ordered).to(device))                                       \n                self.V = torch.nn.Parameter(V_load.to(device),requires_grad=False)\n                self.U_hat = torch.nn.Parameter( torch.randn(n,2*self.rmax).to(device) ,requires_grad = False)\n                self.V_hat = torch.nn.Parameter(torch.randn(m,2*self.rmax).to(device) ,requires_grad = False)\n                self.K = torch.nn.Parameter(torch.randn(n,self.rmax).to(device))\n                self.L = torch.nn.Parameter(torch.randn(m,self.rmax).to(device))\n                self.N_hat = torch.nn.Parameter(torch.randn(2*self.rmax,self.rmax).to(device) ,requires_grad = False)\n                self.M_hat = torch.nn.Parameter( torch.randn(2*self.rmax,self.rmax).to(device) ,requires_grad = False)\n                self.weight = None\n                self.id = id(self.K)\n            else:\n\n                n,m = self.out_features,self.in_features\n\n                U_load,S_load,V_load = torch.linalg.svd(w.view(n,m))\n                V_load = V_load.T\n                r = self.rank\n                self.U = torch.nn.Parameter(U_load[:,:r].to(device) ,requires_grad=False)             \n                self.S_hat = torch.nn.Parameter(torch.diag(S_load[:r]).to(device))                                       \n                self.V = torch.nn.Parameter(V_load[:,:r].to(device),requires_grad=False)\n                self.K = torch.nn.Parameter(torch.randn(n,r).to(device))\n                self.L = torch.nn.Parameter(torch.randn(m,r).to(device))\n                self.N_hat = torch.nn.Parameter(torch.randn(2*self.rmax,self.rmax).to(device) ,requires_grad = False)\n                self.M_hat = torch.nn.Parameter( torch.randn(2*self.rmax,self.rmax).to(device) ,requires_grad = False)\n                self.weight = None\n                self.id = id(self.K)\n        \n\n\n    def reset_parameters(self) -> None:\n        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\n        # uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\n        # https://github.com/pytorch/pytorch/issues/57109\n        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        if self.bias is not None:\n            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n            init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, input: Tensor) -> Tensor:\n        \"\"\"  \n        forward phase for the layer (the backward is automatically created by Pytorch since only standard functions are used). To use dlrt\n        training the three kind of forward phases has to be included\n        INPUTS:\n        input: input tensor\n        \"\"\"\n        if not self.lr:\n\n            x = F.linear(input,self.weight,self.bias)\n\n        else : \n\n            if self.step == 'K':\n                if not self.fixed:\n                    K,V = self.K[:,:self.dynamic_rank],self.V[:,:self.dynamic_rank]\n                else:\n                    K,V = self.K,self.V\n                x = input.mm(V)\n                x = x.mm(K.T)\n                \n                if self.bias is not None:\n\n                    x = x+self.bias\n\n            elif self.step == 'L':\n                if not self.fixed:\n                    L,U = self.L[:,:self.dynamic_rank],self.U[:,:self.dynamic_rank]\n                else:\n                    L,U = self.L,self.U\n                x = input.mm(L)\n                x = x.mm(U.T)\n                if self.bias is not None:\n                    x = x+self.bias\n            \n            elif self.step == 'S':\n\n                if not self.fixed:\n\n                    S_hat,U_hat,V_hat = self.S_hat[:2*self.dynamic_rank,:2*self.dynamic_rank],self.U_hat[:,:2*self.dynamic_rank],self.V_hat[:,:2*self.dynamic_rank]\n\n                else:\n\n                    S_hat,U_hat,V_hat = self.S_hat,self.U,self.V\n\n                x = input.mm(V_hat)\n                x = x.mm(S_hat.T)\n                x = x.mm(U_hat.T)\n                if self.bias is not None:\n                    x = x+self.bias\n                    \n            else:\n\n                raise ValueError(f' incorrect step type {self.step}')\n            \n        return x \n\n\n# import custom layers\nimport sys,os\nsys.path.insert(1, os.path.join(sys.path[0], '..'))\nfrom optimizer_KLS.my_conv import Conv2d_lr\nfrom optimizer_KLS.Linear_layer_lr_new import Linear\nimport torch\n\nclass Lenet5(torch.nn.Module):\n    def __init__(self,device = 'cpu'):\n        \"\"\"  \n        initializer for Lenet5.\n        NEEDED ATTRIBUTES TO USE dlr_opt:\n        self.layer\n        NEEDED METHODS TO USE dlr_opt:\n        self.forward : standard forward of the NN\n        self.update_step : updates the step of all the low rank layers inside the neural net\n        self.populate_gradients : method used to populate the gradients inside the neural network in one unique function\n        \"\"\"\n        super(Lenet5, self).__init__()\n        self.device = device\n        self.layer = torch.nn.Sequential(\n            Conv2d_lr(in_channels = 1, out_channels = 20, kernel_size = 5, stride=1,rank = 20,device = self.device),  \n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d(kernel_size = 2, stride=2),\n            Conv2d_lr(in_channels = 20, out_channels = 50, kernel_size = 5, stride=1,rank = 50,device = self.device),  \n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d(kernel_size = 2, stride=2),\n            torch.nn.Flatten(),\n            Linear(800,out_features = 500,rank = 500,device = self.device),  \n            torch.nn.ReLU(),\n            Linear(500,out_features = 10,device = self.device)\n        )\n\n    def forward(self, x):\n        for layer in self.layer:\n            x = layer(x)\n        return x\n\n    def update_step(self,new_step = 'K'):\n        for l in self.layer:\n            if hasattr(l,'lr') and l.lr:\n                l.step = new_step\n\n    def populate_gradients(self,x,y,criterion,step = 'all'):\n\n        if step == 'all':\n        \n            self.update_step(new_step = 'K')\n            output = self.forward(x)\n            loss = criterion(output,y)\n            loss.backward() \n            self.update_step(new_step = 'L')\n            output = self.forward(x)\n            loss = criterion(output,y)\n            loss.backward()\n            return loss,output.detach()\n\n        else:\n            \n            self.update_step(new_step = step)\n            loss = criterion(self.forward(x),y)\n            return loss\n\n\nfrom tqdm import tqdm\nimport torch\nfrom torch import float16\nimport pandas as pd\n\ndef full_count_params(NN,count_bias = False,with_grads = False):\n\n    \"\"\" \n    Function that counts the total number of parameters needed for a full rank version of NN\n    INPUTS:\n    NN: neural network\n    count_bias : flag variable, True if the biases are to be included in the total or not\n\n    OUTPUTS:\n    total_params : total number of parameters in the full rank version of NN\n    \"\"\"\n\n    total_params = 0\n\n    for l in NN.layer:\n\n        n = str(l)\n\n        if 'Linear' in n:\n\n            total_params += 2*l.in_features*l.out_features if with_grads else l.in_features*l.out_features\n\n            if count_bias and l.bias is not None:\n\n                total_params += 2*len(l.bias) if with_grads else len(l.bias)\n\n        if 'Conv' in n:\n\n            total_params += 2*l.kernel_size_number*l.in_channels*l.out_channels if with_grads else l.kernel_size_number*l.in_channels*l.out_channels\n\n            if count_bias and l.bias is not None:\n\n                total_params += 2*len(l.bias) if with_grads else len(l.bias)\n\n    return total_params\n\n\n\n\ndef count_params(T,with_grads = False):\n\n    \"\"\" \n    function to count number of parameters inside a tensor\n    INPUT:1\n    T : torch.tensor or None\n    output:\n    number of parameters contained in T\n    \"\"\"\n\n    if len(T.shape)>1:\n\n        if with_grads:\n\n            return 2*int(torch.prod(torch.tensor(T.shape)))\n\n        else:\n\n            return int(torch.prod(torch.tensor(T.shape)))\n\n    elif T == None:\n\n        return 0\n\n    else:\n\n        if with_grads:\n\n            return 2*T.shape[0]\n        \n        else:\n\n            return T.shape[0]\n\n\ndef count_params_train(NN,count_bias = False,with_grads = False):\n\n    \"\"\" \n    function to count the parameters in the train phase\n    \n    INPUTS:\n    NN : neural network\n    count_bias : flag variable, True if the biases are to be included in the total or not\n    \"\"\"\n\n    total_params = 0\n\n    for l in NN.layer:\n\n        if hasattr(l,'lr') and l.lr:\n\n            if not l.fixed:\n\n                total_params += count_params(l.K[:,:l.dynamic_rank],with_grads)\n                total_params += count_params(l.L[:,:l.dynamic_rank],with_grads)\n                total_params += count_params(l.U[:,:l.dynamic_rank])\n                total_params += count_params(l.V[:,:l.dynamic_rank])\n                total_params += count_params(l.U_hat[:,:2*l.dynamic_rank])\n                total_params += count_params(l.V_hat[:,:2*l.dynamic_rank])\n                total_params += count_params(l.S_hat[:2*l.dynamic_rank,:2*l.dynamic_rank],with_grads)\n                total_params += count_params(l.M_hat[:2*l.dynamic_rank,:l.dynamic_rank])\n                total_params += count_params(l.N_hat[:2*l.dynamic_rank,:l.dynamic_rank])\n                if count_bias:\n                    total_params +=count_params(l.bias)\n\n            else:\n\n                total_params += count_params(l.K[:,:l.dynamic_rank],with_grads)\n                total_params += count_params(l.L[:,:l.dynamic_rank],with_grads)\n                total_params += count_params(l.U[:,:l.dynamic_rank])\n                total_params += count_params(l.V[:,:l.dynamic_rank])\n                total_params += count_params(l.S_hat[:2*l.dynamic_rank,:2*l.dynamic_rank],with_grads)\n                total_params += count_params(l.M_hat[:2*l.dynamic_rank,:l.dynamic_rank])\n                total_params += count_params(l.N_hat[:2*l.dynamic_rank,:l.dynamic_rank])\n                if count_bias:\n                    total_params +=count_params(l.bias)\n\n        else:\n\n            for n,p in l.named_parameters():\n\n                if 'bias' not in n:\n\n                    total_params += count_params(p,with_grads)   # add with grads\n\n                elif 'bias' in n and count_bias:\n\n                    total_params += count_params(p)\n\n    return total_params\n\n\ndef count_params_test(NN,count_bias = False):\n\n    \"\"\" \n    function to count the parameters in the test phase\n    \n    INPUTS:\n    NN : neural network\n    count_bias : flag variable, True if the biases are to be included in the total or not\n    \"\"\"\n\n    total_params = 0\n\n    for l in NN.layer:\n\n        if hasattr(l,'lr') and l.lr:\n\n            total_params += count_params(l.K[:,:l.dynamic_rank])\n            total_params += count_params(l.L[:,:l.dynamic_rank])\n            if count_bias:\n                total_params +=count_params(l.bias)\n\n        else:\n\n            for n,p in l.named_parameters():\n\n                if 'bias' not in n:\n\n                    total_params += count_params(p)\n\n                elif 'bias' in n and count_bias:\n\n                    total_params +=count_params(p)\n\n    return total_params\n\n\n\ndef accuracy(outputs,labels):\n\n    return torch.mean(torch.tensor(torch.argmax(outputs.detach(),axis = 1) == labels,dtype = float16))\n\n            \n\n\ndef train_dlrt(NN,optimizer,train_loader,validation_loader,test_loader,criterion,metric,epochs,\n                metric_name = 'accuracy',device = 'cpu',count_bias = False,path = None,fine_tune = False,scheduler = None):\n\n    \"\"\" \n    INPUTS:\n    NN : neural network with custom layers and methods to optimize with dlra\n    train/validation/test_loader : loader for datasets\n    criterion : loss function\n    metric : metric function\n    epochs : number of epochs to train\n    metric_name : name of the used metric\n    count_bias : flag variable if to count biases in params_count or not\n    path : path string for where to save the results\n\n    OUTPUTS:\n    running_data : Pandas dataframe with the results of the run\n    \"\"\"\n\n    running_data = pd.DataFrame(data = None,columns = ['epoch','theta','learning_rate','train_loss','train_'+metric_name+'(%)','validation_loss',\\\n                                                        'validation_'+metric_name+'(%)','test_'+metric_name+'(%)',\\\n                                                     'ranks','# effective parameters','cr_test (%)','# effective parameters train','cr_train (%)',\\\n                                                     '# effective parameters train with grads','cr_train_grads (%)'])\n\n    total_params_full = full_count_params(NN,count_bias)\n    total_params_full_grads = full_count_params(NN,count_bias,True)\n    #scheduler_rate = optimizer.scheduler_change_rate\n\n    file_name = path\n\n    if not fine_tune:\n\n        if path is not None:\n            file_name += '.csv'#'\\_running_data_'+str(optimizer.theta)+'.csv'\n\n        for epoch in tqdm(range(epochs)):\n\n            print(f'epoch {epoch}---------------------------------------------')\n            loss_hist = 0\n            acc_hist = 0\n            k = len(train_loader)\n\n            for i,data in enumerate(train_loader):  # train\n                NN.zero_grad()\n                optimizer.zero_grad()\n                inputs,labels = data\n                inputs,labels = inputs.to(device),labels.to(device)\n                def closure():\n                    loss = NN.populate_gradients(inputs,labels,criterion,step = 'S')\n                    return loss\n                optimizer.preprocess_step()\n                loss,outputs = NN.populate_gradients(inputs,labels,criterion)\n                loss_hist+=float(loss.item())/k\n                outputs = outputs.to(device)#NN(inputs).detach().to(device)\n                acc_hist += float(metric(outputs,labels))/k\n                optimizer.step(closure = closure)\n\n            optimizer.preprocess_step()   # last update after training\n            NN.update_step()\n\n            with torch.no_grad():\n                k = len(validation_loader)\n                loss_hist_val = 0.0\n                acc_hist_val = 0.0\n                for i,data in enumerate(validation_loader):   # validation \n                    inputs,labels = data\n                    inputs,labels = inputs.to(device),labels.to(device)\n                    outputs = NN(inputs).detach().to(device)\n                    loss_val = criterion(outputs,labels)\n                    loss_hist_val+=float(loss_val.item())/k\n                    acc_hist_val += float(metric(outputs,labels))/k\n\n\n                k = len(test_loader)\n                loss_hist_test = 0.0\n                acc_hist_test= 0.0\n                for i,data in enumerate(test_loader):   # validation \n                    inputs,labels = data\n                    inputs,labels = inputs.to(device),labels.to(device)\n                    outputs = NN(inputs).detach().to(device)\n                    loss_test = criterion(outputs,labels)\n                    loss_hist_test += float(loss_test.item())/k\n                    acc_hist_test += float(metric(outputs,labels))/k\n\n            print(f'epoch[{epoch}]: loss: {loss_hist:9.4f} | {metric_name}: {acc_hist:9.4f} | val loss: {loss_hist_val:9.4f} | val {metric_name}:{acc_hist_val:9.4f}')\n            print('='*100)\n            ranks = []\n            for i,l in enumerate(NN.layer):\n                if hasattr(l,'lr') and l.lr:\n                    print(f'rank layer {i} {l.dynamic_rank}')\n                    ranks.append(l.dynamic_rank)\n            print('\\n')\n\n            params_test = count_params_test(NN,count_bias)\n            cr_test = round(params_test/total_params_full,3)\n            params_train = count_params_train(NN,count_bias)\n            cr_train = round(params_train/total_params_full,3)\n            params_train_grads = count_params_train(NN,count_bias,True)\n            cr_train_grads = round(params_train_grads/total_params_full_grads,3)\n            epoch_data = [epoch,optimizer.theta,round(optimizer.tau,5),round(loss_hist,3),round(acc_hist*100,4),round(loss_hist_val,3),\\\n                        round(acc_hist_val*100,4),round(acc_hist_test*100,4),ranks,params_test,round(100*(1-cr_test),4),\\\n                            params_train,round(100*(1-cr_train),4),params_train_grads,round(100*(1-cr_train_grads),4)]\n\n            running_data.loc[epoch] = epoch_data\n\n            if file_name is not None and (epoch%10 == 0 or epoch == epochs-1):\n\n                running_data.to_csv(file_name)\n\n            if scheduler is not None:\n\n                scheduler.step(loss_hist)\n\n            # if epoch%scheduler_rate == 0:\n\n            #     optimizer.scheduler_step()\n\n            if epoch == 0:\n\n                best_val_loss = loss_hist_val\n\n            if loss_hist_val<best_val_loss:\n\n                torch.save(NN.state_dict(),path+'\\_best_weights_'+str(optimizer.theta)+'.pt')\n\n        return running_data\n\n    else:\n\n        if path is not None:\n            file_name += '_finetune.csv'#'\\_running_data_'+str(optimizer.theta)+'.csv'\n\n        for epoch in tqdm(range(epochs)):\n\n            print(f'epoch {epoch}---------------------------------------------')\n            loss_hist = 0\n            acc_hist = 0\n            k = len(train_loader)\n\n            for i,data in enumerate(train_loader):  # train\n                NN.zero_grad()\n                optimizer.zero_grad()\n                inputs,labels = data\n                inputs,labels = inputs.to(device),labels.to(device)\n                outputs = NN(inputs).to(device)\n                loss = criterion(outputs,labels)\n                loss.backward()\n                loss_hist+=float(loss.item())/k\n                acc_hist += float(metric(outputs.detach(),labels))/k\n                optimizer.S_finetune_step()\n\n\n            with torch.no_grad():\n                k = len(validation_loader)\n                loss_hist_val = 0.0\n                acc_hist_val = 0.0\n                for i,data in enumerate(validation_loader):   # validation \n                    inputs,labels = data\n                    inputs,labels = inputs.to(device),labels.to(device)\n                    outputs = NN(inputs).detach().to(device)\n                    loss_val = criterion(outputs,labels)\n                    loss_hist_val+=float(loss_val.item())/k\n                    acc_hist_val += float(metric(outputs,labels))/k\n\n                k = len(test_loader)\n                loss_hist_test = 0.0\n                acc_hist_test= 0.0\n                for i,data in enumerate(test_loader):   # validation \n                    inputs,labels = data\n                    inputs,labels = inputs.to(device),labels.to(device)\n                    outputs = NN(inputs).detach().to(device)\n                    loss_test = criterion(outputs,labels)\n                    loss_hist_test += float(loss_test.item())/k\n                    acc_hist_test += float(metric(outputs,labels))/k\n\n            print(f'epoch[{epoch}]: loss: {loss_hist:9.4f} | {metric_name}: {acc_hist:9.4f} | val loss: {loss_hist_val:9.4f} | val {metric_name}:{acc_hist_val:9.4f}')\n            print('='*100)\n            ranks = []\n            for i,l in enumerate(NN.layer):\n                if hasattr(l,'lr') and l.lr:\n                    print(f'rank layer {i} {l.dynamic_rank}')\n                    ranks.append(l.dynamic_rank)\n            print('\\n')\n            params_test = count_params_test(NN,count_bias)\n            cr_test = round(params_test/total_params_full,3)\n            params_train = count_params_train(NN,count_bias)\n            cr_train = round(params_train/total_params_full,3)\n            params_train_grads = count_params_train(NN,count_bias,True)\n            cr_train_grads = round(params_train_grads/total_params_full_grads,3)\n            epoch_data = [epoch,optimizer.theta,round(optimizer.tau,5),round(loss_hist,3),round(acc_hist*100,4),round(loss_hist_val,3),\\\n                        round(acc_hist_val*100,4),round(acc_hist_test*100,4),ranks,params_test,round(100*(1-cr_test),4),\\\n                                params_train,round(100*(1-cr_train),4),params_train_grads,round(100*(1-cr_train_grads),4)]\n\n            running_data.loc[epoch+epochs] = epoch_data\n\n            if file_name is not None and (epoch%10 == 0 or epoch == epochs-1):\n\n                running_data.to_csv(file_name)\n\n            if scheduler is not None:\n\n                scheduler.step(loss_hist)\n\n            # if epoch%scheduler_rate == 0:\n\n            #     optimizer.scheduler_step()\n\n            if epoch == 0:\n\n                best_val_loss = loss_hist_val\n\n            if loss_hist_val<best_val_loss:\n\n                torch.save(NN.state_dict(),path+'\\_best_weights_finetune_'+str(optimizer.theta)+'.pt')\n\n        return running_data",
        "experimental_info": "Experimental settings for Lenet5 on MNIST from `Lenet_experiment/run_lenet_mnist.py`:\n- Model: Lenet5, constructed with custom low-rank convolutional (`Conv2d_lr`) and linear (`Linear`) layers (e.g., first Conv2d_lr with rank=20, second with rank=50, first Linear with rank=500).\n- Device: CUDA if available, otherwise CPU.\n- Dataset: MNIST (loaded via `tf.keras.datasets.mnist.load_data`, split using `sklearn.model_selection.train_test_split`).\n    - Training data size: 50000 samples\n    - Validation data size: 10000 samples\n    - Test data size: 10000 samples\n    - Input image dimensions: (1, 28, 28) for grayscale.\n    - Data preprocessing: Normalized to float tensor values between 0 and 1.\n- Training parameters:\n    - Epochs: 20 (configurable via `--epochs` argument, default 100 in parser, but set to 20 in script)\n    - Batch size: 128 (configurable via `--batch_size` argument)\n    - Cross-validation runs: 5 (configurable via `--cv_runs` argument)\n    - Learning Rate (`lr` / `tau`): 0.05 (configurable via `--lr` argument)\n    - Rank Adaptation Threshold (`theta`): [0.4, 0.45] (configurable via `--theta` argument)\n- Optimizer: `dlr_opt` (Dynamical Low-Rank Optimizer) configured to use `torch.optim.SGD` as its internal KLS integrator. `dlr_opt` handles the K, L, and S steps for factor updates and rank adaptivity.\n- Loss Function: `torch.nn.CrossEntropyLoss()`\n- Metric: Accuracy (calculated as `torch.mean(torch.tensor(torch.argmax(outputs.detach(),axis = 1) == labels,dtype = float16))`).\n- Results saving: Training metrics and model weights (best validation loss) are saved to `./results_Lenet5/`.\n"
      }
    },
    {
      "title": "Compressible Dynamics in Deep Overparameterized Low-Rank Learning & Adaptation",
      "abstract": "While overparameterization in machine learning models offers great benefits\nin terms of optimization and generalization, it also leads to increased\ncomputational requirements as model sizes grow. In this work, we show that by\nleveraging the inherent low-dimensional structures of data and compressible\ndynamics within the model parameters, we can reap the benefits of\noverparameterization without the computational burdens. In practice, we\ndemonstrate the effectiveness of this approach for deep low-rank matrix\ncompletion as well as fine-tuning language models. Our approach is grounded in\ntheoretical findings for deep overparameterized low-rank matrix recovery, where\nwe show that the learning dynamics of each weight matrix are confined to an\ninvariant low-dimensional subspace. Consequently, we can construct and train\ncompact, highly compressed factorizations possessing the same benefits as their\noverparameterized counterparts. In the context of deep matrix completion, our\ntechnique substantially improves training efficiency while retaining the\nadvantages of overparameterization. For language model fine-tuning, we propose\na method called \"Deep LoRA\", which improves the existing low-rank adaptation\n(LoRA) technique, leading to reduced overfitting and a simplified\nhyperparameter setup, while maintaining comparable efficiency. We validate the\neffectiveness of Deep LoRA on natural language tasks, particularly when\nfine-tuning with limited data. Our code is available at\nhttps://github.com/cjyaras/deep-lora-transformers.",
      "full_text": "Compressible Dynamics in Deep Overparameterized Low-Rank Learning & Adaptation Can Yaras, Peng Wang, Laura Balzano, and Qing Qu Department of Electrical Engineering & Computer Science, University of Michigan June 11, 2024 Abstract While overparameterization in machine learning models offers great benefits in terms of optimization and generalization, it also leads to increased computational requirements as model sizes grow. In this work, we show that by leveraging the inherent low-dimensional structures of data and compressible dynamics within the model parameters, we can reap the benefits of overparameterization without the computational burdens. In practice, we demonstrate the effec- tiveness of this approach for deep low-rank matrix completion as well as fine-tuning language models. Our approach is grounded in theoretical findings for deep overparameterized low-rank matrix recovery, where we show that the learning dynamics of each weight matrix are con- fined to an invariant low-dimensional subspace. Consequently, we can construct and train com- pact, highly compressed factorizations possessing the same benefits as their overparameterized counterparts. In the context of deep matrix completion, our technique substantially improves training efficiency while retaining the advantages of overparameterization. For language model fine-tuning, we propose a method called “Deep LoRA”, which improves the existing low-rank adaptation (LoRA) technique, leading to reduced overfitting and a simplified hyperparameter setup, while maintaining comparable efficiency. We validate the effectiveness of Deep LoRA on natural language tasks, particularly when fine-tuning with limited data. Our code is available at https://github.com/cjyaras/deep-lora-transformers. 1 Introduction In recent years, there has been a growing interest within the realm of deep learning inoverparam- eterization, which refers to employing a greater number of model parameters than necessary to interpolate the training data. While this may appear counterintuitive initially due to the risk of over- fitting, it has been demonstrated to be an effective modeling approach (Zhang et al., 2021; Wu et al., 2017; Allen-Zhu et al., 2019a; Buhai et al., 2020; Xu et al., 2018), primarily attributed to improved optimization landscape and implicit algorithmic regularization. In the context of large language models (LLMs) (Radford et al., 2019; Brown et al., 2020), empirical scaling laws (Kaplan et al., 2020) suggest that larger models are more sample efficient, often requiring fewer samples to reach the same test loss. Taking the problem of low-rank matrix recovery as an illustrative example, the seminal work of Arora et al. (2019) showed that deeper factorizations better promote low-rank solutions as a function of depth, consequently mitigating overfitting in the overparameterized regime compared to a classical two-layer factorization approach; see Figure 1 (left). On the other hand, increasing 1 arXiv:2406.04112v2  [cs.LG]  10 Jun 2024Figure 1: Benefits of depth & width in overparameterized matrix completion with d = 100 , r∗ = 5, ϵl = 10−3 and 30% of entries observed.Left: Recovery error vs. width for shallow and deep factorizations. Right: Number of GD iterations to converge to10−10 error vs. width. We observe that depth prevents overfitting, while width improves convergence. the width of each layer substantially reduces the number of iterations to reach the same training error; see Figure 1 (right). While overparameterization offers remarkable benefits, it also comes with its computational challenges. The significantly increased number of parameters inevitably results in dramatically higher computational costs. This naturally raises a fundamental question: can we attain the benefits of overparameterization with a substantial reduction in computational costs? In this work, we show that we can achieve this by exploiting low-dimensional structures of data and compressible learning dynamics in the model weights. In the context of low-rank matrix recovery via deep overparameterized factorizations,we discover an interesting phenomenon that for each weight matrix,the learning dynamics only happen within an approximately invariant low-dimensional subspace throughout all iterations. We rigorously prove this for deep matrix factorization, which also allows us to compress the number of training parameters significantly when dealing with deep matrix completion. Consequently, we can construct and train a nearly equivalent, yet much smaller, compressed factorization without sacrificing the advantages of its overparameterized counterpart. Interestingly, we empirically find that the above phenomenon can also be observed when em- ploying deep overparameterized weight updates for fine-tuning language models; see Figure 2 for an illustration. Therefore, we can adapt our idea of compressing deep matrix factorization to improve language model fine-tuning. For fine-tuning large-scale pretrained language models, re- cently low-rank adaptation (LoRA) stands out as the most commonly-used technique due to its effectiveness and efficiency (Hu et al., 2021). The basic idea of LoRA is to freeze the pretrained weights and adapt each one to new tasks by adding and optimizing an update in the form of a two-layer low-rank decomposition. Nonetheless, in practical scenarios, selecting the optimal rank of the decomposition can pose a significant challenge. If the rank is not chosen properly, it may lead to overfitting, particularly when we overestimate the rank or when there is limited downstream data available. We deal with this drawback of LoRA by employing a deep (three-layer) overparameterized factorization for the trainable update, which is constructed and optimized via the compression technique used for deep matrix completion. As such, our new method, which we term as Deep LoRA, enjoys notable advantages over the original LoRA method, namely (i)less overfitting by exploiting depth, and (ii)fewer hyperparameterswithout rankr and scale α having to be carefully tuned across all layers, all while having a comparable parameter efficiency due to compression. 2Figure 2: Invariant low-dimensional subspaces in deep overparameterized adaptation of lan- guage models.Fine-tuning BERT (Devlin et al., 2019) with deep overparameterized adaptation on the STS-B dataset (Cer et al., 2017).Left: Singular valuespectra across all adapted layers at the end of fine-tuning. Middle: Alignment of subspaces formed by top 8 right singular vectors between current adapted weights and final adapted weights throughout training.Right: Training losscon- tinues to decrease in iterations after subspace alignment with final adapted weights. See Section 4 for more details. Contributions. We summarize our contributions below. • Practical contributions. We develop efficient compression methods by exploring compressible learning dynamics in overparameterized factorizations. Our method enjoys the benefits of over- parameterization while significantly improving its efficiency. We demonstrate the effectiveness not only on deep matrix completion, but also for improving LoRA for language model fine-tuning. • Theoretical contributions. Our methods are inspired by our theoretical results for deep matrix factorization. Mathematically, we rigorously prove the existence of invariant low-dimensional subspaces throughout gradient descent for each weight matrix, and show how they can con- structed in practice. Related Works There is a great deal of literature on implicit regularization in the setting of matrix factorization/linear networks (Neyshabur et al., 2015; Gunasekar et al., 2017; Arora et al., 2019; Moroshko et al., 2020; Timor et al., 2023; Ji and Telgarsky, 2019; Gidel et al., 2019; You et al., 2020; Liu et al., 2022), as well as low-rank learning in deep networks (Jaderberg et al., 2014; Sainath et al., 2013; Denil et al., 2013; Khodak et al., 2020; Oymak et al., 2019; Min Kwon et al., 2024; Tarzanagh et al., 2023). Similarly, there is an abundance of work discussing the benefits of overparameterization (Du and Hu, 2019; Arora et al., 2018b; Allen-Zhu et al., 2019b; Arpit and Bengio, 2019). 2 Warm-up Study: Deep Matrix Factorization Towards gaining theoretical insights into the phenomena in Figure 2, we first build some intuition based on the problem of deep matrix factorization. Under simplified settings, we rigorously unveil the emergence of low-dimensionality and compressibility in gradient descent learning dynamics. 2.1 Basic Setup Given a low-rank matrixΦ ∈ Rdx×dy with rank(Φ) = r∗, we approximate the matrixΦ by anL-layer deep overparameterized factorization f(Θ) := WLWL−1 ··· W2W1 = WL:1, (1) 3Figure 3: Evolution of SVD of weight matrices.We visualize the SVD dynamics of the first layer weight matrix of anL = 3 layer deep matrix factorization for a random matrix withd = 30, r∗ = 3, ϵl = 1 throughout GD without weight decay. Left: Magnitude of the i-th singular value σi(t) at iteration t. Middle: Angle ∠(vi(t), vi(0)) between the i-th right singular vector at iteration t and initialization. Right: Angle ∠(ui(t), ui(0)) between the i-th left singular vector at iteration t and initialization. where Θ = (Wl)L l=1 are the parameters with weightsWl ∈ Rdl×dl−1 for l ∈ [L]. We consider the case where the weights are all squared0 = d1 = ··· = dL = d, and learn the parametersΘ by solving min Θ ℓ(Θ) = 1 2∥f(Θ) − Φ∥2 F (2) via gradient descent (GD) from scaledorthogonal initialization, i.e., we initialize parametersΘ(0) such that Wl(0)Wl(0)⊤ = Wl(0)⊤Wl(0) = ϵ2 l Id, l∈ [L] (3) where ϵl > 0. We assume this for ease of analysis, and believe that our results could hold for arbitrary small initialization. For each weight matrix, the GD iterations can be written as Wl(t + 1) = (1 − ηλ)Wl(t) − η∇Wlℓ(Θ(t)), l∈ [L] (4) for t = 0, 1, 2, . . ., where η >0 is the learning rate andλ ≥ 0 is an optional weight decay parameter. 2.2 Main Theorem We show that learning only occurs within an invariant low-dimensional subspace of the weight matrices, whose dimensionality depends on rank(Φ). Theorem 2.1. Let Wl(t) satisfy the initialization scheme (3) and updates (4), and suppose Φ ∈ Rd×d is at most rank r and let m := d − 2r > 0. Then there exist orthogonal matrices (Ul)L l=1 ⊂ Od×d and (Vl)L l=1 ⊂ Od×d (depending only on Θ(0) and Φ) satisfying Vl+1 = Ul for l ∈ [L − 1], such that Wl(t) admits the decomposition Wl(t) = Ul \u0014fWl(t) 0 0 ρl(t)Im \u0015 V ⊤ l (5) for all l ∈ [L] and t ≥ 0, where fWl(t) ∈ R2r×2r with fWl(0) = ϵlI2r, and ρl(t) = ρl(t − 1) · (1 − ηλ − η · Y k̸=l ρ2 k(t − 1)) (6) for all l ∈ [L] and t ≥ 1 with ρl(0) = ϵl. In the following, we discuss several implications of our result and its relationship to previous work. 4• SVD dynamics of weight matrices. The decomposition (5) is closely related to the singular value decomposition (SVD) of Wl(t). Specifically, let Ul = [ Ul,1 Ul,2], Vl = [ Vl,1 Vl,2], where Ul,1, Vl,1 ∈ Od×2r, Ul,2, Vl,2 ∈ Od×(d−2r). Let fWl(t) = eUl(t)eΣl(t) eV ⊤ l (t) be an SVD of fWl(t), where eUl(t), eVl(t) ∈ O2r and eΣl(t) ∈ R2r×2r is a diagonal matrix. Then, by (5) we can write Wl(t) as Wl(t) = h Ul,1 eUl(t) Ul,2 i\u0014eΣl(t) 0 0 ρ(t)Im \u0015h Vl,1 eVl(t) Vl,2 i⊤ which is essentially an SVD of Wl(t) (besides the ordering of singular values). According to this, we can verify thatρ(t) is a (repeated) singular value undergoing minimal changes across iterations illustrated in Figure 3 (left). Additionally, these repeated singular values correspond to invariant subspaces Ul,2, Vl,2 that are stationary throughout GD, as seen in Figure 3 (middle and right). • Low-rank bias.From (6), we can show under mild assumptions that the GD trajectory for each weight matrix either remains or tends towards a solution with rank at most2r. This is true whether we employ implicit or explicit regularization. Indeed, if we use small initializationϵl ≈ 0 with no weight decay λ = 0, then the fact that ρl is a decreasing sequence (w.r.t. iteration) implies that the approximate rank ofWl(t) can be no more than2r throughout the entire trajectory. On the other hand, if we use weight decay withλ >0, then we have ρl(t) → 0 as t → ∞. This forces Wl(t) towards a solution of rank at most2r when the training converges. See Appendix E.2 for a formal statement and proof. This result is consistent with previous findings on low-rank and simplicity bias in deep networks (Huh et al., 2022; Galanti and Poggio, 2022; Li et al., 2020; Chou et al., 2024). • Comparison to prior arts.In contrast to existing work studying implicit bias of GD towards low- rank solutions (Gunasekar et al., 2017; Arora et al., 2019), our result explicitly shows how GD finds these solutions. Moreover, unlike previous work on implicit bias (Min et al., 2021; Gissin et al., 2019; Arora et al., 2019; Vardi and Shamir, 2021), we also examine the effect of weight decay, which is commonly employed during the training of deep networks. Our analysis is distinct from that of (Saxe et al., 2014, 2019), which studied continuous time dynamics under the special (separable) setting WL:1(0) = UV ⊤ with Φ = UΣV ⊤. In comparison, our result applies to discrete time dynamics and holds for initialization that is agnostic to the target matrix. It should also be noted that our result does not depend onbalanced initialization like those in (Arora et al., 2018a), as the initialization scaleϵl for each layer can be arbitrarily different from one another. A sketch of analysis. We now provide a rough sketch for the beginning of the proof of Theorem 2.1 in the special case of small initialization ϵl = ϵ ≈ 0 for all l ∈ [L] and λ = 0 , highlighting the construction of the invariant subspace at initialization. The full proof can be found in Appendix E.1. Proof sketch. Since ϵL ≈ 0, from the gradient ofℓ(Θ) (see Appendix E), we have G1 := ∇W1 ℓ(Θ(0)) ≈ −W⊤ L:2(0)Φ (7) implying that the rank ofG1 is (approximately) at mostr. Now consider the subspaceS = N(G1)∩ N(G⊤ 1 W1(0)), where we have dim S ≥d − 2r. Then, there exist orthonormal sets {vi}d−2r i=1 and {ui}d−2r i=1 which satisfy G1vi = 0, ui ∝ W1(0)vi and therefore G⊤ 1 ui ∝ G⊤ 1 W1(0)vi = 0 so along with the orthogonality ofW1(0), the pairs(ui, vi) form singular vector pairs of bothW1(0) and W1(1) simultaneously as they remain unchanged by the gradient updateG1, giving the last 5Figure 4: Network compression for deep matrix factorization.Comparison of trajectories for opti- mizing the original problem (2) vs. the compressed problem(9) with L = 3, d = 1000, r = r∗ = 5, and ϵl = 10−3. Left: Principal components of end-to-end GD trajectories. Right: Training loss vs. wall-time comparison. d − 2r columns of V1 and U1 respectively. To see that we can takeV2 = U1, for instance, we note that ∇W2 ℓ(Θ(0)) · ui ≈ −W⊤ L:3(0)ΦW⊤ 1 (0)ui ∝ W⊤ L:3(0)Φvi = 0 by (7), showing thatui are invariant under gradient updates in the second layer. 2.3 Compression of Overparameterized Factorization We now show that, as a consequence of Theorem 2.1 and the proof sketch, we can run GD on dramatically fewer parameters to achieve a nearidentical end-to-end trajectory as the original (full- width) factorization; see Figure 4. Constructing the “equivalent” compressed factorization. More specifically, given thatΦ is at most rank r and d − 2r >0, from Theorem 2.1 we observe that WL:1(t) = UL,1 fWL:1(t)V ⊤ 1,1 +  LY l=1 ρl(t) ! · UL,2V ⊤ 1,2 ≈ UL,1 fWL:1(t)V ⊤ 1,1| {z } =:fC( eΘ,UL,1,V1,1) , ∀ t = 1, 2, . . . , (8) when we use initialization of small scale (i.e., (ϵl)L l=1 are small). Here, fWL:1 = fWL fWL−1 ··· fW1 with compressed weightsfWl ∈ R2r×2r. Correspondingly,fC( eΘ, UL,1, V1,1) denotes the compressed function with compressed parameters eΘ = (fWl)L l=1. As such, we can expect that solving min eΘ ℓC( eΘ) = 1 2∥fC( eΘ, UL,1, V1,1) − Φ∥2 F (9) will approximately give the same solution as (2). Constructing the factors(Ul, Vl)L l=1. As Theorem 2.1 only showed the existence of(Ul, Vl)L l=1, to solve (9) via GD, we need a practical recipe for constructing(Ul, Vl)L l=1 efficiently at initialization of small scale ϵl. This can be achieved based upon our proof sketch in Section 2.2: we computeG1 = ∇W1 ℓ(Θ(0)) ∈ Rd×d, find an orthonormal set {vi}d−2r i=1 contained in S = N(G1) ∩ N(G⊤ 1 W1(0)), and complete to an orthonormal basis to yield V1. The remaining Ul, Vl can then be iteratively 6constructed via Ul = Wl(0)Vl/ϵl, Vl+1 = Ul, l= 1, ··· , L− 1, and UL = WL(0)VL/ϵL. Finally, we take the first2r columns of UL and V1 to yield UL,1 and V1,1, respectively. It should be noted that these compressed factors are related to,yet distinct from,spectral initialization, which is well-studied in the literature (Chi et al., 2019; Khodak et al., 2020; St¨oger and Soltanolkotabi, 2021). Since Ul,1, Vl,1 are constructed via orthogonal complements to nullspaces involving the gradient, these directions do indeed correlate with the top singular subspaces ofΦ in the deep matrix factorization case (although we do not use the singular value information). On the other hand, our approach is more general through the lens of compression, as it can be applied to a given deep overparameterized factorization trained on anarbitrary loss. Optimization, complexity, and approximation error. In summary, we can approximately solve the original problem by solving(9) via GD for the compressed parameters eΘ = (fWl)L l=1, starting from small initialization (ϵl ≈ 0). The factors UL,1, V1,1 can be efficiently constructed based upon an iterative scheme that we discussed above from the initial weights. Comparing the parameter counts of the compressedfC( eΘ, UL,1, V1,1) vs. the original f(Θ), we only need to optimize4L ·r2 parameters compared to the originalL ·d2. Since r ≪ d, our approach leads to significant improvement in efficiency during GD; see Figure 4 (right). On the other hand, compression requires some additional computation to construct the factorsUL,1 and V1,1 prior to training, which involves taking a gradient of the first weight in the original factorization followed by an SVD or QR decomposition to compute an orthonormal basis for S. While this requires an additional O(d3) compute, this has the same complexity as a single iteration of GD for the original factorization and is therefore a negligible overhead when comparing the two. Finally, the following result demonstrates that our compression method can achieve an almost identical end-to-end trajectory when we use small initializations; see Figure 4 (left). Proposition 2.2. For r such thatm := d −2r >0, if we run GD on the compressed weightseΘ as described above for the loss(9), we have \r\r\rf(Θ(t)) − fC( eΘ(t), UL,1, V1,1) \r\r\r 2 F ≤ m · LY l=1 ϵ2 l for any iteratet = 0, 1, 2, ··· . Here, ϵl is the initialization scale for the weightWl(0). The key idea of Theorem 2.2 is that GD is invariant under orthogonal transformations, and each factor fWl in the end-to-end factorization in(9) is the result of an orthogonal transformation ofWl. Then, the approximation errorm · QL l=1 ϵ2 l is only due to the approximation we showed in(8). We defer the full proof to Appendix E.3. 3 Application I: Deep Matrix Completion In this section, we show that we can generalize our method in Section 2 from vanilla matrix factor- ization to solving low-rank matrix completion problems (Candes and Recht, 2012; Cand`es and Tao, 2010; Davenport and Romberg, 2016) via compressed deep factorizations. Given a ground-truth Φ ∈ Rd×d with rank r∗ ≪ d, the goal of low-rank matrix completion is to recoverΦ from only a few number of observations encoded by a mask Ω ∈ {0, 1}d×d. Adopting a matrix factorization approach, we minimize the objective ℓmc(Θ) = 1 2∥Ω ⊙ (f(Θ) − Φ)∥2 F , (10) 7Figure 5: Network compression for deep matrix completion.Comparison of trajectories for op- timizing the original problem (10) vs. the compressed problem (11) with γ discrepant updates (γ = 0.01) and ablating γ (γ = 0) with L = 3, d = 1000, r = r∗ = 5, ϵl = 10−3 and 20% of entries observed. Left: Principal components of end-to-end trajectories of each factorization.Middle: Recov- ery error vs. iteration comparison.Right: Recovery error vs wall-time comparison. where f(Θ) is the deep overparameterized factorization introduced in(1). The problem simplifies to deep matrix factorization (2) that we studied earlier when Ω = 1d1⊤ d in the full observation case. Additionally,(10) reduces to vanilla (shallow) matrix factorization whenL = 2, whose global optimality and convergence have been widely studied under various settings (Jain et al., 2013; Zheng and Lafferty, 2016; Sun and Luo, 2016; Ge et al., 2016; Bhojanapalli et al., 2016; Ge et al., 2017; Gunasekar et al., 2017; Li et al., 2019; Chi et al., 2019; Li et al., 2018b; Soltanolkotabi et al., 2023; Sun et al., 2018; Zhang et al., 2020; Ding et al., 2021). A double-edged sword of overparameterization. In practice, the true rank r∗ is not known – instead, we assume to have an upper boundr of the same order asr∗, i.e., r∗ ≤ r ≪ d. Surprisingly, overparameterization has advantages in terms of both depthL and width r: • Benefits of depth: mitigating overfitting.When r > r∗, it has been demonstrated (Arora et al., 2019) that optimizing deeper factorizations (i.e., L ≥ 3) generalize better in the low sample regime, while their shallow counterparts overfit; see Figure 1 (left). • Benefits of width: improving convergence.On the other hand, increasing the width r of the deep factorization beyondr∗ results in accelerated convergence in terms of iterations, see Figure 1 (right). However, the advantages of overparameterization come with the challenges of much higher compu- tational costs. For anL-layer factorization of (full)-widthd, we requireO(L·d3) multiplications per iteration to evaluate gradients and need to storeO(L · d2) parameters, where d is often very large. Using ideas from Section 2, however, we can obtain the benefits of overparameterization without the extra computational costs. Compression for deep matrix completion. Given the similarity between deep matrix factoriza- tion and completion (i.e., Ω = 1d1⊤ d vs arbitrary Ω), it seems straightforward to generalize our compression methods in Section 2.3 to deep matrix completion. However, as shown by the orange trace in Figure 5, direct application does not work well, as the compressed factorization’s trajectory diverges from that of the original. This is because the compressed subspaces UL,1, V1,1 ∈ Rd×2br computed at the initializationΘ(0) via the gradient ∇W1 ℓmc(Θ(0)) ≈ −W⊤ L:2(0)[Ω ⊙ Φ] can be misaligned with the true subspace due to the perturbation by the maskΩ. 8Nonetheless, this issue can be mitigated by slowly updatingUL,1, V1,1 during training. Specifi- cally, compared to (9), we minimize min eΘ,UL,1,V1,1 1 2∥Ω ⊙ (fC( eΘ, UL,1, V1,1) − Φ)∥2 F (11) via GD by updating eΘ, UL,1, V1,1 simultaneously every iteration, with a learning rateη on eΘ along with a discrepant learning rate γη on UL,1, V1,1. Because we updateUL,1, V1,1 slower than updating eΘ, we generally chooseγ >0 to be small and tuned accordingly for the given problem. As a result, we reduce computational costs toO((L + d) · r2) multiplications per iteration for computing gradients, andO(d·r +L·r2) parameters. Yet still the trajectory of the deep compressed factorization ultimately aligns with that of the original, while converging roughly5× faster w.r.t. wall-time, as demonstrated in Figure 5. Moreover, the accelerated convergence induced by the full- width trajectory results in the compressed factorization being3× faster than randomly initialized factorizations of similar width – see Appendix D.1 for more details. 4 Application II: Model Fine-tuning In this section, we show that our compression idea can be further extended to parameter-efficient fine-tuning of pretrained language models, specifically via low-rank adaptation (LoRA) (Hu et al., 2021). In particular, inspired by our approach for deep matrix completion, we propose Deep Low- Rank Adaptation (Deep LoRA), which consistently outperformsvanilla LoRA in the limited sample regime. 4.1 Deep Low-Rank Adaptation (Deep LoRA) Background on LoRA. With the ever-growing size of pretrained models and countless down- stream tasks, full model fine-tuning is often computationally infeasible. Given a pretrained model whose parameters consist of a collection of dense weight matrices {W0k}m k=1 ⊂ Rd×d (e.g., the query/key/value projections of a transformer (Vaswani et al., 2017)), LoRA seeks to adapt each layer to a given task byfreezing the pretrained weight{W0k}m k=1 and optimizing an extra trainable low-rank factorization on top. In other words, the fine-tuned weightWk is given by Wk = W0k + α r W(2) k W(1) k where α > 0 is a tunable scale parameter and W(2) k ∈ Rd×r, W(1) k ∈ Rr×d with r ≪ d, thereby substantially reducing the number of trainable parameters during fine-tuning. Proposed method: Deep LoRA. For vanilla LoRA, if we view adapting each weight matrix of model as an individual low-rank factorization problem, we have demonstrated in previous sections that overparameterization and subsequently compressing factorizations improves generalization with minimal extra computational costs. With this in mind, we can employ a deep overparameter- ized adaptation of each pretrained weight as Wk = W0k + W(L) k ··· W(2) k W(1) k| {z } =:∆Wk (12) where each W(i) k is full-width, i.e., W(i) k ∈ Rd×d. Here, L >0 is the depth, and typically we choose L = 3, which is precisely the setting of Figure 2. From the figure, we can see that (i) all the converged weights {∆Wk}m k=1 are very low-rank (left panel), (ii) the learning dynamics each for each weight 9Figure 6: Deep LoRA shows better performance on few shot fine-tuning over vanilla LoRA, with varying numbers of training samples. For each case, we draw n samples at random from STS-B over 20 trials with different seeds, and measure performance on the validation split of each method using the same train set. Figure 7: Deep LoRA finds lower rank solutionscompared to vanilla LoRA. We plot a his- togram of numerical ranks for Deep LoRA and vanilla LoRA with r = 8 after adapting to STS-B with 256 samples. The numerical rank is computed as the number of singular values σi greater than 10−8 and dσ1ϵ where ϵ is machine epsilon. Figure 8: Deep LoRA is more robust to the choice of rank compared to vanilla LoRA. For each choice of rank r, we draw 16 samples at random from STS-B over 5 trials with differ- ent seeds, and measure perfor- mance on the validation split of each method using the same train set. approximately stay within the same invariant subspace throughout the iterations (middle panel), and (iii) this happens independent of the training loss decreasing (right panel). These observations imply that deep overparameterized factorizations in Deep LoRA arehighly compressible, so we can apply the compression method from deep matrix completion in Section 3 to compress the learning dynamics for each individual weight for model fine-tuning. Here, the major differences of our compression approach for deep LoRA from that of deep matrix completion is that (i) we have a separate compressed factorization for each layer to be adapted, and (ii) the fine-tuned loss function can be tailored for specific tasks (e.g., the cross-entropy) besides theℓ2 loss. Advantages of Deep LoRA. Compared to vanilla LoRA, Deep LoRA has clear advantages that we highlight below. More details are provided in Section 4.2. • Less overfitting in limited data regime. Fine-tuning overparameterized models using LoRA can still result in overfitting in few shot or limited data regime (Sebastian Raschka, 2023). In comparison, the extra depth in (12) of Deep LoRA can help prevent overfitting (see Figure 6), which is similar to deep matrix completion in Figure 1. • Robustness to the hyperparameter r. As shown in Figure 8, by exploiting the intrinsic low- dimensional dynamics in GD via overparameterization in width, our approach is robust to the choice of the rankr in fine-tuning. Deep LoRA only requires3r2 additional trainable parameters for each adapted layer compared to vanilla LoRA, wherer is relatively small (e.g.,r = 8). 4.2 More Experimental Details To evaluate our approach, we use a pretrained BERT (Devlin et al., 2019) base model and apply adaptation on all attention and feedforward weights in the transformer, resulting in 72 adapted layers in total. Unless stated otherwise, we user = 8 for both vanilla and Deep LoRA throughout all experiments, in which case Deep LoRA has roughly 0.01% more parameters (with respect to BERT) 10Table 1:Improvement of Deep LoRA over vanilla LoRA for limited data GLUE fine-tuning.For each task, we draw 1024 samples at random over 10 trials with different seeds, and report the performance gap (with variance) on the validation split between Deep LoRA and vanilla LoRA using the same train set. CoLA MNLI MRPC QNLI QQP ∆ +0 .090±0.002 +0.011±0.0005 +0.0042±0.001 +0.048±0.0009 +0.005±0.0002 RTE SST-2 STS-B Overall ∆ +0 .029±0.002 +0.019±0.0006 +0.018±0.00006 +0.028±0.002 than vanilla LoRA. We utilize Adam (Kingma and Ba, 2014) as an optimizer for both methods. See Appendix B for more details on the experimental setup. Advantage I: Better generalization with limited data. We first evaluate our approach on tasks in the GLUE benchmark (Wang et al., 2018), which is a standard benchmark for natural language understanding. To test the performance in a limited data setting, for one given trial of a single task, we randomly sample 1024 examples from the task data for fine-tuning, a nd compare the difference in performance on the same train set between Deep LoRA and vanilla LoRA on the entire validation split. From the results shown in Table 1, we can see that Deep LoRA delivers significant improvements across most tasks compared to vanilla LoRA, and on average improves performance by nearly 3 points, a notable margin. This improvement in performance becomes more pronounced in scenarios with severely lim- ited data, such as few-shot settings. Applying the same sampling procedure as in the prior study to the STS-B dataset, we assess both approaches using only n ∈ {16, 64, 256} training instances. Experiments in Figure 6 illustrate that Deep LoRA consistently surpasses vanilla LoRA across all sample sizes, with the most significant difference observed whenn = 16. Deep LoRA finds lower rank solutions. We find that at the end of fine-tuning, Deep LoRA finds lower rank solutions for∆Wk than vanilla LoRA, as shown in Figure 7. In the limited data setting (256 samples), we see that all adapted layers in the vanilla LoRA saturate the constrained numerical rankr = 8,while most layers in Deep LoRA are perturbed by matrices with numerical ranks between 0 and 4.1 This suggests that Deep LoRA can adaptively select the appropriate rank for each layer depending on the task. This low-rank bias induces implicit regularization during the fine-tuning process and ultimately prevents overfitting to the task, particularly when only few training samples are available. As a practical consideration, Deep LoRA also requires a fraction of the memory cost to store compared to vanilla LoRA due to the parsimony in adapted weights. Advantage II: Robustness to choice of rank r. Due to the scarcity of the target training data, choosing the rank r in LoRA is a delicate process – it needs to be large enough to capture the com- plexity in modeling the downstream task, while small enough to prevent overfitting. The proposed Deep LoRA, on the other hand, avoids catastrophic overfitting as we increaser, as demonstrated in Figure 8. This observation mirrors the behavior seen in deep matrix completion, as illustrated in Figure 1. For shallow factorizations, an overestimation of rankr leads to an increase in the general- ization error. In contrast, deep factorizations remain resilient to overfitting. 1A majority of them are in fact zero, i.e., no change from pretrained weights. 11Finally, we show that Deep LoRA outperforms vanilla LoRA for few-shot natural language generation fine-tuning in Appendix C. We also provide an ablation study in Appendix D.2 on the compression mechanism for Deep LoRA and show that it is crucial for accelerating training. 5 Conclusion & Future Directions In this work, we have provided an in-depth exploration of low-dimensionality and compressibil- ity in the dynamics of deep overparameterized learning, providing theoretical understandings in the setting of deep matrix factorization and applications to efficient deep matrix completion and language model adaptation. Finally, we outline a couple potential future directions following this work. Compressibility in non-linearsettings. Although the results on network compression in Section 2 exploit the specific gradient structure of deep matrix factorizations, we believe that our analysis can provide meaningful direction for analyzing the fully non-linear case. To sketch an idea, consider the setting of Section 2.1 except with anon-linear factorization, i.e., (1) becomes f(Θ) := WLσ(WL−1 ··· σ(W2σ(W1))) (13) where σ is (for example) the entry-wise ReLU activation. For concreteness, consider theL = 3 case. The gradient of the loss with respect to, e.g.,W2 in (2) is given by ∇W2 ℓ(Θ) = [h(W2σ(W1)) ⊙ (W⊤ 3 E)]σ(W1)⊤ where h is the entry-wise unit step function and E = f(Θ) − Φ. Comparing this to the gradient in the linear setting (14), there is a great deal of shared structure, with the two main differences being the non-linearity applied to W1 in the post factor and a projection on the inner term via h(W2σ(W1)). However, we still have the low-rank structure ofW⊤ 3 E, and the zeroing out of certain entries preserves approximate spectral properties of the matrix (Chatterjee, 2015). Moreover, this projection is akin to the masking via Ω as in deep matrix completion from Section 3, for which we do find compressible dynamics. In Figure 9, we plot the singular value spectrum of the above gradient at small initialization, finding that the topr∗ singular values separate from the rest of the spectrum. This suggests that we may be able to identify a low-dimensional subspace along which we can achieve similar dynamics to the full parameter space. Extensions to Deep LoRA. We have demonstrated the efficacy of Deep LoRA for natural language understanding and generation in Section 4 and Appendix C respectively. However, it would be meaningful to evaluate Deep LoRA in other modalities, e.g., diffusion models, where fine-tuning on limited data is commonplace. Moreover, the high degree of alignment at initialization to the final adapted subspaces shown in Figure 2 suggests that SGD (rather than Adam) can be used for the outer factors of Deep LoRA, further reducing memory costs. Finally, exploring the use of second-order methods to accelerate fine-tuning along the rank- r subspace could be a potential improvement. Implications for representation learning. The low-rank bias in the end-to-end features of deep networks may have important connections to emergent phenomena in representation learning, such as deep neural collapse (Zhai et al., 2024; Zhou et al., 2022b; Yaras et al., 2022; Wang et al., 2022; Zhou et al., 2022a; Zhu et al., 2021; Beaglehole et al., 2024; Li et al., 2024), whereby the last- layer representations exhibit surprisingly simple structures. Moreover, by uncovering the low-rank 12Figure 9: Low-rank gradient of non-linear factorizations at initialization.Singular values spec- trum of ∇W2 ℓ(Θ) at initialization for non-linear factorization with L = 3, d = 1000, r∗ = 5, and ϵl = 10−3. The top 5 singular values separate from the tail of the spectrum. evolution ofindividual weights, we could shed light on more intricate phenomena such asprogressive neural collapse (He and Su, 2023; Wang et al., 2023). Acknowledgement The work of L.B., P.W., and C.Y. were supported in part by DoE award DE-SC0022186, ARO YIP award W911NF1910027, and NSF CAREER award CCF-1845076. Q.Q., P.W., and C.Y. acknowledge support from ONR N00014-22-1-2529 and NSF CAREER CCF-214390. Q.Q. also acknowledges sup- port from NSF CAREER CCF-2143904, NSF CCF-2212066, NSF CCF-2212326, and NSF IIS 2312842, an AWS AI Award, a gift grant from KLA, and MICDE Catalyst Grant. References Armen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. InProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 7319–7328, 2021. Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized neural networks, going beyond two layers.Advances in neural information processing systems, 32, 2019a. Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over- parameterization. In International conference on machine learning, pages 242–252. PMLR, 2019b. Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient descent for deep linear neural networks. InInternational Conference on Learning Representations, 2018a. Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit acceleration by overparameterization. In International Conference on Machine Learning, pages 244–253. PMLR, 2018b. Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix factorization. Advances in Neural Information Processing Systems, 32, 2019. 13Devansh Arpit and Yoshua Bengio. The benefits of over-parameterization at initialization in deep relu networks. arXiv preprint arXiv:1901.03611, 2019. Satanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for MT evaluation with improved correla- tion with human judgments. InProceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65–72, Ann Arbor, Michigan, June 2005. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/W05-0909. Daniel Beaglehole, Peter S´uken´ık, Marco Mondelli, and Mikhail Belkin. Average gradient outer product as a mechanism for deep neural collapse. arXiv preprint arXiv:2402.13728, 2024. Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Global optimality of local search for low rank matrix recovery. Advances in Neural Information Processing Systems, 29, 2016. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. Rares-Darius Buhai, Yoni Halpern, Yoon Kim, Andrej Risteski, and David Sontag. Empirical study of the benefits of overparameterization in learning latent variable models. InInternational Conference on Machine Learning, pages 1211–1219. PMLR, 2020. Emmanuel Candes and Benjamin Recht. Exact matrix completion via convex optimization.Communications of the ACM, 55(6):111–119, 2012. Emmanuel J Cand`es and Terence Tao. The power of convex relaxation: Near-optimal matrix completion. IEEE Transactions on Information Theory, 56(5):2053–2080, 2010. Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. InProceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017). Association for Computational Linguistics, 2017. Sourav Chatterjee. Matrix estimation by universal singular value thresholding.The Annals of Statistics, 43(1): 177–214, 2015. Arnav Chavan, Zhuang Liu, Deepak Gupta, Eric Xing, and Zhiqiang Shen. One-for-all: Generalized lora for parameter-efficient fine-tuning. arXiv preprint arXiv:2306.07967, 2023. Yuejie Chi, Yue M Lu, and Yuxin Chen. Nonconvex optimization meets low-rank matrix factorization: An overview. IEEE Transactions on Signal Processing, 67(20):5239–5269, 2019. Hung-Hsu Chou, Carsten Gieshoff, Johannes Maly, and Holger Rauhut. Gradient descent for deep matrix factorization: Dynamics and implicit bias towards low rank.Applied and Computational Harmonic Analysis, 68:101595, 2024. Jeff Dalton and Atul Deshmane. Artificial neural networks.IEEE Potentials, 10(2):33–36, 1991. Mark A Davenport and Justin Romberg. An overview of low-rank matrix recovery from incomplete observa- tions. IEEE Journal of Selected Topics in Signal Processing, 10(4):608–622, 2016. Misha Denil, Babak Shakibi, Laurent Dinh, Marc’Aurelio Ranzato, and Nando De Freitas. Predicting param- eters in deep learning. Advances in neural information processing systems, 26, 2013. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. InProceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, 2019. 14Lijun Ding, Liwei Jiang, Yudong Chen, Qing Qu, and Zhihui Zhu. Rank overspecified robust matrix recovery: Subgradient method and exact recovery.Advances in Neural Information Processing Systems, 34:26767–26778, 2021. Simon Du and Wei Hu. Width provably matters in optimization for deep linear neural networks. InInterna- tional Conference on Machine Learning, pages 1655–1664. PMLR, 2019. Markus Freitag and Yaser Al-Onaizan. Beam search strategies for neural machine translation. InProceedings of the First Workshop on Neural Machine Translation, pages 56–60, 2017. Tomer Galanti and Tomaso Poggio. Sgd noise and implicit low-rank bias in deep neural networks. Technical report, Center for Brains, Minds and Machines (CBMM), 2022. Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. Advances in neural information processing systems, 29, 2016. Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A unified geometric analysis. In International Conference on Machine Learning, pages 1233–1242. PMLR, 2017. Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien. Implicit regularization of discrete gradient dynamics in linear neural networks. Advances in Neural Information Processing Systems, 32, 2019. Daniel Gissin, Shai Shalev-Shwartz, and Amit Daniely. The implicit bias of depth: How incremental learning drives generalization. In International Conference on Learning Representations, 2019. Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Implicit regularization in matrix factorization. Advances in neural information processing systems, 30, 2017. Hangfeng He and Weijie J Su. A law of data separation in deep learning.Proceedings of the National Academy of Sciences, 120(36):e2221704120, 2023. Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. InInternational Conference on Learning Representations, 2021. Minyoung Huh, Hossein Mobahi, Richard Zhang, Brian Cheung, Pulkit Agrawal, and Phillip Isola. The low-rank simplicity bias in deep networks.Transactions on Machine Learning Research, 2022. Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks with low rank expansions. In Proceedings of the British Machine Vision Conference 2014. British Machine Vision Association, 2014. Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix completion using alternating min- imization. In Proceedings of the forty-fifth annual ACM symposium on Theory of computing, pages 665–674, 2013. Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. In7th International Conference on Learning Representations, ICLR 2019, 2019. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Mikhail Khodak, Neil A Tenenholtz, Lester Mackey, and Nicolo Fusi. Initialization and regularization of factorized neural layers. InInternational Conference on Learning Representations, 2020. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 15Dawid Jan Kopiczko, Tijmen Blankevoort, and Yuki M Asano. VeRA: Vector-based random matrix adaptation. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/ forum?id=NjNfLdxr3A. Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the intrinsic dimension of objective landscapes. In International Conference on Learning Representations, 2018a. Pengyu Li, Xiao Li, Yutong Wang, and Qing Qu. Neural collapse in multi-label learning with pick-all-label loss. In Forty-first International Conference on Machine Learning, 2024. Qiuwei Li, Zhihui Zhu, and Gongguo Tang. The non-convex geometry of low-rank matrix optimization. Information and Inference: A Journal of the IMA, 8(1):51–96, 2019. Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations. In Conference On Learning Theory, pages 2–47. PMLR, 2018b. Zhiyuan Li, Yuping Luo, and Kaifeng Lyu. Towards resolving the implicit bias of gradient descent for matrix factorization: Greedy low-rank learning. InInternational Conference on Learning Representations, 2020. Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. InText Summarization Branches Out, pages 74–81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https: //www.aclweb.org/anthology/W04-1013. Sheng Liu, Zhihui Zhu, Qing Qu, and Chong You. Robust training under label noise by over-parameterization. In International Conference on Machine Learning, pages 14153–14172. PMLR, 2022. Hancheng Min, Salma Tarmoun, Rene Vidal, and Enrique Mallada. On the explicit role of initialization on the convergence and implicit bias of overparametrized linear networks. InInternational Conference on Machine Learning, pages 7760–7768. PMLR, 2021. Soo Min Kwon, Zekai Zhang, Dogyoon Song, Laura Balzano, and Qing Qu. Efficient low-dimensional com- pression of overparameterized models. In Sanjoy Dasgupta, Stephan Mandt, and Yingzhen Li, editors, Proceedings of The 27th International Conference on Artificial Intelligence and Statistics, volume 238 ofProceedings of Machine Learning Research, pages 1009–1017. PMLR, 02–04 May 2024. Edward Moroshko, Blake E Woodworth, Suriya Gunasekar, Jason D Lee, Nati Srebro, and Daniel Soudry. Implicit bias in deep linear classification: Initialization scale vs training accuracy. Advances in neural information processing systems, 33:22182–22193, 2020. Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning. In Workshop at International Conference of Learning Represeatations, 2015. Jekaterina Novikova, Ondˇrej Duˇsek, and Verena Rieser. The e2e dataset: New challenges for end-to-end generation. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, pages 201–206, 2017. Samet Oymak, Zalan Fabian, Mingchen Li, and Mahdi Soltanolkotabi. Generalization guarantees for neural networks via harnessing the low-rank structure of the jacobian,. In ICML Workshop on Generalization in Deep Networks, 2019. Long version at https://arxiv.org/abs/1906.05392. Kishore Papineni, Salim Roukos, Todd Ward, and Wei jing Zhu. Bleu: a method for automatic evaluation of machine translation. pages 311–318, 2002. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 16Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1–67, 2020. Tara N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana Ramabhadran. Low-rank matrix factorization for deep neural network training with high-dimensional output targets. In2013 IEEE international conference on acoustics, speech and signal processing, pages 6655–6659. IEEE, 2013. A Saxe, J McClelland, and S Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. InProceedings of the International Conference on Learning Represenatations 2014. International Conference on Learning Represenatations 2014, 2014. Andrew M Saxe, James L McClelland, and Surya Ganguli. A mathematical theory of semantic development in deep neural networks. Proceedings of the National Academy of Sciences, 116(23):11537–11546, 2019. PhD Sebastian Raschka. Practical tips for finetuning llms using lora (low-rank adaptation), Nov 2023. URL https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms . Mahdi Soltanolkotabi, Dominik St¨oger, and Changzhi Xie. Implicit balancing and regularization: General- ization and convergence guarantees for overparameterized asymmetric matrix sensing. InThe Thirty Sixth Annual Conference on Learning Theory, pages 5140–5142. PMLR, 2023. Dominik St¨oger and Mahdi Soltanolkotabi. Small random initialization is akin to spectral learning: Opti- mization and generalization guarantees for overparameterized low-rank matrix reconstruction.Advances in Neural Information Processing Systems, 34:23831–23843, 2021. Ju Sun, Qing Qu, and John Wright. A geometric analysis of phase retrieval. Foundations of Computational Mathematics, 18:1131–1198, 2018. Ruoyu Sun and Zhi-Quan Luo. Guaranteed matrix completion via non-convex factorization.IEEE Transactions on Information Theory, 62(11):6535–6579, 2016. Davoud Ataee Tarzanagh, Yingcong Li, Christos Thrampoulidis, and Samet Oymak. Transformers as support vector machines. In NeurIPS 2023 Workshop on Mathematics of Modern Machine Learning, 2023. Nadav Timor, Gal Vardi, and Ohad Shamir. Implicit regularization towards rank minimization in relu net- works. In International Conference on Algorithmic Learning Theory, pages 1429–1459. PMLR, 2023. Gal Vardi. On the implicit bias in deep-learning algorithms.Communications of the ACM, 66(6):86–93, 2023. Gal Vardi and Ohad Shamir. Implicit regularization in relu networks with the square loss. InConference on Learning Theory, pages 4224–4258. PMLR, 2021. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,Łukasz Kaiser, and Illia Polosukhin. Attention is all you need.Advances in neural information processing systems, 30, 2017. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi- task benchmark and analysis platform for natural language understanding. InInternational Conference on Learning Representations, 2018. Peng Wang, Huikang Liu, Can Yaras, Laura Balzano, and Qing Qu. Linear convergence analysis of neural collapse with unconstrained features. In OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop), 2022. Peng Wang, Xiao Li, Can Yaras, Zhihui Zhu, Laura Balzano, Wei Hu, and Qing Qu. Understanding deep rep- resentation learning via layerwise feature compression and discrimination.arXiv preprint arXiv:2311.02960, 2023. 17Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac,Tim Rault,R ´emi Louf,Morgan Funtowicz,et al. Huggingface’s transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019. Lei Wu, Zhanxing Zhu, et al. Towards understanding generalization of deep learning: Perspective of loss landscapes. arXiv preprint arXiv:1706.10239, 2017. Ji Xu, Daniel J Hsu, and Arian Maleki. Benefits of over-parameterization with em. Advances in Neural Information Processing Systems, 31, 2018. Lingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui Tao, and Fu Lee Wang. Parameter-efficient fine-tuning methods for pretrained language models: A critical review and assessment.arXiv preprint arXiv:2312.12148, 2023. Can Yaras, Peng Wang, Zhihui Zhu, Laura Balzano, and Qing Qu. Neural collapse with normalized features: A geometric analysis over the riemannian manifold.Advances in neural information processing systems, 35: 11547–11560, 2022. Chong You, Zhihui Zhu, Qing Qu, and Yi Ma. Robust recovery via implicit bias of discrepant learning rates for double over-parameterization. InAdvances in Neural Information Processing Systems, 2020. Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu, Yong Jae Lee, and Yi Ma. Investigating the catastrophic forgetting in multimodal large language model fine-tuning. InConference on Parsimony and Learning, pages 202–227. PMLR, 2024. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learn- ing (still) requires rethinking generalization. Communications of the ACM, 64(3):107–115, 2021. Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. InThe Eleventh International Conference on Learning Representations, 2022. Yuqian Zhang, Qing Qu, and John Wright. From symmetry to geometry: Tractable nonconvex problems. arXiv preprint arXiv:2007.06753, 2020. Zhong Zhang, Bang Liu, and Junming Shao. Fine-tuning happens in tiny subspaces: Exploring intrinsic task-specific subspaces of pre-trained language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1701–1713, 2023. Qinqing Zheng and John Lafferty. Convergence analysis for rectangular matrix completion using burer- monteiro factorization and gradient descent. arXiv preprint arXiv:1605.07051, 2016. Jinxin Zhou, Xiao Li, Tianyu Ding, Chong You, Qing Qu, and Zhihui Zhu. On the optimization landscape of neural collapse under mse loss: Global optimality with unconstrained features. InInternational Conference on Machine Learning, pages 27179–27202. PMLR, 2022a. Jinxin Zhou, Chong You, Xiao Li, Kangning Liu, Sheng Liu, Qing Qu, and Zhihui Zhu. Are all losses created equal: A neural collapse perspective. Advances in Neural Information Processing Systems, 35:31697–31710, 2022b. Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and Qing Qu. A geometric analysis of neural collapse with unconstrained features.Advances in Neural Information Processing Systems, 34:29820–29834, 2021. 18Appendix Appendix A Related Works Appendix B Experimental Details Appendix C Evaluation on Natural Language Generation Appendix D Ablating Compression Mechanism Appendix E Proofs In Appendix A, we provide an in-depth discussion of related works. In Appendix B, we provide further details for experiments in Section 4. In Appendix C, we carry out additional experiments for evaluating Deep LoRA for few-shot natural language generation fine-tuning. In Appendix D, we carry out an ablation study for the compression mechanism presented in the main paper, for both deep matrix completion and Deep LoRA. In Appendix E, we provide proofs of all claims from Section 2. A Related Works & Future Directions Implicit regularization. The first work to theoretically justify implicit regularization in matrix fac- torization (Gunasekar et al., 2017) was inspired by empirical work that looked into the implicit bias in deep learning (Neyshabur et al., 2015) and made the connection with factorization approaches as deep nets using linear activations2. Since then a long line of literature has investigated deep factorizations and their low-rank bias, including (Arora et al., 2019; Moroshko et al., 2020; Timor et al., 2023); in fact there is so much work in this direction that there is already a survey in the Communications of the ACM (Vardi, 2023). Several older works explicitly imposed low-rank factorization in deep networks (Jaderberg et al., 2014; Sainath et al., 2013) or studied a low-rank factorization of the weights after the learning process (Denil et al., 2013). Newer works along these lines discuss initialization and relationships to regularization (Khodak et al., 2020). The work in Oymak et al. (2019) also discusses low-rank learning in deep nets, by studying the singular vectors of the Jacobian and arguing that the “information space” or top singular vectors of the Jacobian are learned quickly. Very recent work has shown that the typical factorization of the weights in an attention layer of a transformer into key and query layers has an implicit bias towards low-rank weights (Tarzanagh et al., 2023). Overparameterization. There is a sizeable body of work discussing the various benefits of overpa- rameterization in deep learning settings, of which we discuss a few. Du and Hu (2019) demonstrate that width is provably necessary to guarantee convergence of deep linear networks. Arora et al. (2018b) show that overparameterization can result in an implicit acceleration in optimization dy- namics for training deep linear networks. Allen-Zhu et al. (2019b) argue that overparameterization 2Of course linear activation has been considered throughout the history of artificial neural nets, but the fact that a multilayer network with linear activation has an equivalent one-layer network meant this architecture was summarily ignored. This is evidenced in (Dalton and Deshmane, 1991): “In summary, it makes no sense to use a multilayered neural network when linear activation functions are used.” 19plays a fundamental role in rigorously showing that deep networks find global solutions in polyno- mial time. Arpit and Bengio (2019) shows that depth in ReLU networks improves a certain lower bound on the preservation of variance in information flowing through the network in the form of activations and gradients. LoRA and its variants. There is a substantial body of existing work in the realm of parameter efficient fine-tuning – see Xu et al. (2023) for a comprehensive survey. However, the method that has arguably gained the most traction in recent years is LoRA (Hu et al., 2021), in which trainable rank decomposition matrices are added on top of frozen pretrained weights to adapt transformers to downstream tasks. Since then, there has been a plethora of variants. Generalized LoRA (Chavan et al., 2023) proposes a general formulation of LoRA that encapsulates a handful of other parameter efficient adapters. AdaLoRA (Zhang et al., 2022) parameterizes the updates in an SVD-like form and iteratively prunes the inner factor to dynamically control the factorization rank. VeRA (Kopiczko et al., 2024) parameterizes the updates via diagonal adapters which are transformed via random projections that are shared across layers. The idea of LoRA was initially inspired by the notion of lowintrinsic dimension of fine-tuning pretrained models. Intrinsic dimension for an objective was first proposed in Li et al. (2018a), where it was defined to be the minimum dimensionality needed for a random subspace to contain a solution to the objective. Using this idea, Aghajanyan et al. (2021) demonstrated that the objective of fine-tuning a pretrained model has a low intrinsic dimension. Building on this, Zhang et al. (2023) learns the intrinsic subspace of a given fine-tune task from the original parameter trajectory to investigate transferability between these task-specific subspaces. B Experimental Details The pretrained BERT and T5 models are retrieved from the HuggingFace transformers library (Wolf et al., 2019) as google-bert/bert-base-cased and google-t5/t5-base respectively. We choose the best learning rate for each method from η ∈ {10−5, 10−4, 10−3, 10−2} on STS-B with 1024 samples, and find thatη = 10−4 and α = 8 works best for vanilla LoRA, whileη = 10−2 with γ = 10−2 works best for Deep LoRA (althoughγ can be chosen relatively freely). We tried using a linear decay learning rate but found worse results in the limited data setting for both vanilla and Deep LoRA. We use a maximum sequence length of 128 tokens for all tasks. Vanilla LoRA is initialized in the same fashion as the original paper (i.e., W(2) k is initialized to all zeros, W(1) k is initialized to be Gaussian with standard deviation 1), whereas Deep LoRA is compressed from a full-width 3-layer factorization with orthogonal initialization of scaleϵl = 10−3. We use a train batch size of 16, and train all models until convergence in train loss, and use the final model checkpoint for evaluation. For generative tasks, we use beam search Freitag and Al-Onaizan (2017) with beam size 4 and maximum generation length of 64. All experiments are carried out on a single NVIDIA Tesla V100 GPU, with time and memory usage reported in Table 2. Table 2: Comparison of step wall-time and memory usage for vanilla and Deep LoRA. Method Iteration Wall-Time (ms) Memory Usage (GB) Vanilla LoRA 102 12.526 Deep LoRA 106 12.648 20C Evaluation on Natural Language Generation In addition to the natural language understanding tasks evaluated in Section 4, we test the effec- tiveness of Deep LoRA compared to vanilla LoRA for few-shot fine-tuning for natural language generation (NLG), specifically on the E2E dataset (Novikova et al., 2017) with the T5 base model (Raffel et al., 2020). All hyperparameters are as reported in Section 4 and Appendix B. The results are shown in Table 3. We observe significant improvements using Deep LoRA in BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) scores, with marginally worse results with respect to METEOR (Banerjee and Lavie, 2005) score. Table 3:Improvement of Deep LoRA over vanilla LoRA for few-shot NLG fine-tuning.On the E2E dataset, we draw 16 samples at random over 10 trials with different seeds, and report the average performance gap on the validation split between Deep LoRA and vanilla LoRA for various metrics using the same train set. BLEU ROUGE-1 ROUGE-2 ROUGE-L ROUGE-LSUM METEOR ∆ +0 .033 +0 .032 +0 .056 +0 .061 +0 .047 −0.00036 D Ablating Compression Mechanism D.1 Deep Matrix Completion Figure 10: Comparing efficiency of compressed networks vs. randomly initialized narrow net- works for deep matrix completionwith different overestimated r and L = 3, d = 1000, r∗ = 5, ϵl = 10−3 and 20% of entries observed.Left: Number of iterations to converge.Right: Wall-time to converge. We compare the training efficiency of deep 2r-compressed factorizations (within a wide net- work of width d ≫ r) with randomly initialized deep factorizations of width 2r. As depicted in Figure 10 (left), the compressed factorization requires fewer iterations to reach convergence, and the number of iterations necessary is almost unaffected byr. Consequently, training compressed factorizations is considerably more time-efficient than training narrow networks of the same size, 21provided that r is not significantly larger than r∗. The distinction between compressed and nar- row factorizations underscores the benefits of wide factorizations, as previously demonstrated and discussed in Figure 1 (right), where increasing the width results in faster convergence. However, increasing the width alone also increases computational costs – by employing compression, we can achieve the best of both worlds. D.2 Deep LoRA Figure 11: Compression enables faster convergence of Deep LoRA.We compare full-width, com- pressed, and narrow deep factorizations for adapting to STS-B with 16 samples.Left: Batch train loss vs. iterations.Right: STS-B evaluation metric (Pearson correlation) vs. iterations. We verify that compression is crucial for the efficiency of Deep LoRA. We compare the perfor- mance of three different approaches: (i)Original, where we use a three-layer full-width factorization as in (12), (ii) Compressed, which is the rank-r compression of (12) (a.k.a. Deep LoRA), and (iii) Random, where the W(i) k in (12) are initialized randomly with W(2) k ∈ Rr×r. We can see that via compression, Deep LoRA can achieve similar convergence behavior to the original overparameter- ized factorization with much fewer parameters, while the randomly initialized version takes much longer to train, similar to the result for deep matrix completion in Appendix D.1. E Proofs The analytic form of the gradient∇Θ ℓ(Θ) is given by ∇Wlℓ(Θ) = W⊤ L:l+1EW ⊤ l−1:1, l∈ [L] (14) where E = f(Θ) − Φ, which when substituted into (4) gives the update rules Wl(t + 1) = (1 − ηλ)Wl(t) − ηW⊤ L:l+1(t)E(t)W⊤ l−1:1(t), l∈ [L] (15) for t = 0, 1, 2, . . ., where E(t) = f(Θ(t)) − Φ. We first establish the following Theorem E.1 – the claim in Theorem 2.1 then follows in a rela- tively straightforward manner. We note that all statements quantified byi in this section implicity hold for all i ∈ [m] (as defined in Theorem 2.1) for the sake of notational brevity. 22E.1 Proof of Theorem 2.1 Lemma E.1. Under the setting of Theorem 2.1, there exist orthonormal sets{u(l) i }m i=1 ⊂ Rd and {v(l) i }m i=1 ⊂ Rd for l ∈ [L] satisfying v(l+1) i = u(l) i for all l ∈ [L − 1] such that the following hold for allt ≥ 0: A(t) : Wl(t)v(l) i = ρl(t)u(l) i ∀l ∈ [L], B(t) : W⊤ l (t)u(l) i = ρl(t)v(l) i ∀l ∈ [L], C(t) : Φ⊤WL:l+1(t)u(l) i = 0 ∀l ∈ [L], D(t) : ΦW⊤ l−1:1(t)v(l) i = 0 ∀l ∈ [L], where ρl(t) = ρl(t − 1) · (1 − ηλ − η · Q k̸=l ρk(t − 1)2) for all t ≥ 1 with ρl(0) = ϵl > 0. Proof. Define Ψ := W⊤ L:2(0)Φ. Since the rank ofΦ is at most r, we have that the rank ofΨ ∈ Rd×d is at most r, which implies thatdim N (Ψ) = dim N \u0000 Ψ⊤\u0001 ≥ d − r. We define the subspace S := N (Ψ) ∩ N \u0010 Ψ⊤W1(0) \u0011 ⊂ Rd. Since W1(0) ∈ Rd×d is nonsingular, we have dim(S) ≥ 2(d − r) − d = m. Let {v(1) i }m i=1 denote an orthonormal set contained inS and set u(1) i := W1(0)v(1) i /ϵ1, where ϵ1 > 0 is the scale of W1(0) – since W1(0)/ϵ1 is orthogonal, {u(1) i }m i=1 is also an orthonormal set. Then we trivially haveW1(0)v(1) i = ϵ1u(1) i , which implies W⊤ 1 (0)u(1) i = ϵ1v(1) i . It follows fromv(1) i ∈ Sthat Ψv(1) i = 0and Ψ⊤W1(0)v(1) i = 0,which is equivalent toW⊤ L:2(0)Φv(1) i = 0and Φ⊤WL:2(0)W1(0)v(1) i = ϵ1Φ⊤WL:2(0)u(1) i = 0 respectively. SinceW⊤ L:2(0) is full column rank, we further have thatΦv(1) i = 0. Now letE(l) denote that we have orthonormal sets{u(l) i }m i=1 and {v(l) i }m i=1 satisfying Wl(0)v(l) i = ϵlu(l) i , W⊤ l (0)u(l) i = ϵlv(l) i , Φ⊤WL:l+1(0)u(l) i = 0, and ΦW⊤ l−1:1(0)v(l) i = 0. From the above argu- ments, we have thatE(1) holds – now supposeE(k) holds for some1 ≤ k < L. Setv(k+1) i := u(k) i and u(k+1) i := Wk+1(0)v(k+1) i /ϵk+1. This implies thatWk+1(0)v(k+1) i = ϵk+1u(k+1) i and W⊤ k+1(0)u(k+1) i = ϵk+1v(k+1) i . Moreover, we have Φ⊤WL:(k+1)+1(0)u(k+1) i = Φ⊤WL:k+1(0)W⊤ k+1(0)u(k+1) i /ϵ2 k+1 = Φ⊤WL:k+1(0)v(k+1) i /ϵk+1 = Φ⊤WL:k+1(0)u(k) i /ϵk+1 = 0, where the first two equalities follow from orthogonality andu(k+1) i = Wk+1(0)v(k+1) i /ϵk+1, and the last equality is due tov(k+1) i = u(k) i . Similarly, we have ΦW⊤ (k+1)−1:1(0)v(k+1) i = ΦW⊤ k−1:1(0)W⊤ k (0)v(k+1) i = ΦW⊤ k−1:1(0)W⊤ k (0)u(k) i = ϵkΦW⊤ k−1:1(0)v(k) i = 0, where the second equality follows fromv(k+1) i = u(k) i and the third equality is due toW⊤ k (0)u(k) i = ϵkv(k) i . Therefore E(k + 1) holds, so we haveE(l) for all l ∈ [L]. As a result, we have shown the base cases A(0), B(0), C(0), and D(0). Now we proceed by induction ont ≥ 0. Suppose that A(t), B(t), C(t), and D(t) hold for some 23t ≥ 0. First, we showA(t + 1) and B(t + 1). We have Wl(t + 1)v(l) i = h (1 − ηλ)Wl(t) − ηW⊤ L:l+1(t)E(t)W⊤ l−1:1(t) i v(l) i = h (1 − ηλ)Wl(t) − ηW⊤ L:l+1(t) (WL:1(t) − Φ) W⊤ l−1:1(t) i v(l) i = (1 − ηλ)Wl(t)v(l) i − ηW⊤ L:l+1(t)WL:1(t)W⊤ l−1:1(t)v(l) i = (1 − ηλ)Wl(t)v(l) i − η · ( Y k̸=l ρ2 k(t))Wl(t)v(l) i = ρl(t) · (1 − ηλ − η · Y k̸=l ρ2 k(t))u(l) i = ρl(t + 1)u(l) i for alll ∈ [L], where the first equality follows from(15), the second equality follows from definition of E(t), the third equality follows from D(t), and the fourth equality follows from A(t) and B(t) applied repeatedly along withv(l+1) i = u(l) i for all l ∈ [L − 1], proving A(t + 1). Similarly, we have W⊤ l (t + 1)u(l) i = h (1 − ηλ)W⊤ l (t) − ηWl−1:1(t)E⊤(t)WL:l+1(t) i u(l) i = h (1 − ηλ)W⊤ l (t) − ηWl−1:1(t) \u0010 W⊤ L:1(t) − Φ⊤ \u0011 WL:l+1(t) i u(l) i = (1 − ηλ)W⊤ l (t)u(l) i − ηWl−1:1(t)W⊤ L:1(t)WL:l+1(t)u(l) i = (1 − ηλ)W⊤ l (t)u(l) i − η · ( Y k̸=l ρ2 k(t))W⊤ l (t)u(l) i = ρl(t) · (1 − ηλ − η · Y k̸=l ρ2 k(t))v(l) i = ρl(t + 1)v(l) i for all l ∈ [L], where the third equality follows fromC(t), and the fourth equality follows fromA(t) and B(t) applied repeatedly along withv(l+1) i = u(l) i for all l ∈ [L − 1], proving B(t + 1). Now, we show C(t + 1). For anyk ∈ [L − 1], it follows fromv(k+1) i = u(k) i and A(t + 1) that Wk+1(t + 1)u(k) i = Wk+1(t + 1)v(k+1) i = ρk+1(t + 1)u(k+1) i . Repeatedly applying the above equality fork = l, l+ 1, . . . , L− 1, we obtain Φ⊤WL:l+1(t)u(l) i = \"L−1Y k=l ρk+1(t) # · Φ⊤u(L) i = 0 which follows from C(t), proving C(t + 1). Finally, we show D(t + 1). For any k ∈ {2, . . . , L}, it follows from v(k) i = u(k−1) i and B(t + 1) that W⊤ k−1(t + 1)v(k) i = W⊤ k−1(t + 1)u(k−1) i = ρk−1(t + 1)v(k−1) i . Repeatedly applying the above equality fork = l, l− 1, . . . ,2, we obtain ΦW⊤ l−1:1(t)v(l) i = \" lY k=2 ρk−1(t) # · Φv(1) i = 0 which follows fromD(t). Thus we have provenD(t + 1), concluding the proof. Proof of Theorem 2.1. By A(t) and B(t) of Theorem E.1, there exists orthonormal matrices{Ul,2}L l=1 ⊂ Rd×m and {Vl,2}L l=1 ⊂ Rd×m for l ∈ [L] satisfying Vl+1,2 = Ul,2 for all l ∈ [L − 1] as well as Wl(t)Vl,2 = ρl(t)Ul,2 and Wl(t)⊤Ul,2 = ρl(t)Vl,2 (16) for all l ∈ [L] and t ≥ 0, where ρl(t) satisfies (6) for t ≥ 1 with ρl(0) = ϵl. First, completeV1,2 to an 24orthonormal basis for Rd as V1 = [V1,1 V1,2]. Then for each l ∈ [L − 1], set Ul = [Ul,1 Ul,2] where Ul,1 = Wl(0)Vl,1/ϵl and Vl+1 = [Vl+1,1 Vl+1,2] where Vl+1,1 = Ul,1, and finally setUL = [UL,1 UL,2] where UL,1 = WL(0)VL,1/ϵL. We note thatVl+1 = Ul for each l ∈ [L − 1] and Ul, Vl are orthogonal since Wl(0)/ϵl is orthogonal for alll ∈ [L]. Then we have U⊤ l,1Wl(t)Vl,2 = ρl(t)U⊤ l,1Ul,2 = 0 (17) for all l ∈ [L] and t ≥ 0, where the first equality follows from (16). Similarly, we also have U⊤ l,2Wl(t)Vl,1 = ρ(t)V ⊤ l,2Vl,1 = 0 (18) for all l ∈ [L] and t ≥ 0, where the first equality also follows from(16). Therefore, combining (16), (17), and (18) yields U⊤ l Wl(t)Vl = \u0002 Ul,1 Ul,2 \u0003⊤ Wl(t) \u0002 Vl,1 Vl,2 \u0003 = \u0014fWl(t) 0 0 ρl(t)Im \u0015 for all l ∈ [L], where fWl(0) = ϵlI2r by construction of Ul,1. This directly implies (5), completing the proof. E.2 Low-rank bias in Theorem 2.1 Here, we verify the claims following Theorem 2.1 and give a precise characterization of the rate of decay of ρl as given by(6) and the conditions on learning rateη needed to achieve such behavior. These are given in the following lemma. Lemma E.2. In the setting of Theorem 2.1, suppose0 < ϵl = ϵ ≤ 1 for all l ∈ [L] and 0 < η≤ 1 λ+ϵ. Then for all t ≥ 0, the updates ofρl(t) in (6) satisfy ρl(t) = ρ(t) for some ρ, and ϵ · (1 − η · (λ + ϵ))t ≤ ρ(t) ≤ ϵ · (1 − ηλ)t. (19) Since λ and ϵ are often chose to be small, the above lemma implies that a small learning rate is not required to achieve a low-rank solution. Moreover, by choice ofη, when weight decay is employed (i.e., λ >0) the above inequality implies thatρ(t) → 0 as t → ∞. When λ = 0, we instead have that ρ is bounded byϵ. Proof of Theorem E.2. If ρl(0) = ϵ for all l ∈ [L], it is clear that ρl(t) = ρ(t) for some ρ for all t ≥ 0, and the updates take the form ρ(t) = ρ(t − 1) · h 1 − η · \u0010 λ + ρ(t − 1)2(L−1) \u0011i for each t ≥ 0. We proceed by induction. Fort = 0, since ρ(0) = ϵ, the claim holds trivially. Now suppose (19) holds for some t ≥ 0. By choice of η, we have that1 − η · (λ + ϵ) ≥ 0, so ρ(t) ≥ 0. It then follows that ρ(t + 1) = ρ(t) · h 1 − η · \u0010 λ + ρ(t)2(L−1) \u0011i ≤ ρ(t) · (1 − ηλ) ≤ ϵ · (1 − ηλ)t+1 by the fact thatρ(t) ≤ ϵ · (1 − ηλ)t. Next, by choice ofη and initial condition, we have thatρ(t) ≤ ϵ, so that ρ(t + 1) = ρ(t) · h 1 − η · \u0010 λ + ρ(t)2(L−1) \u0011i ≥ ρ(t) · (1 − η · (λ + ϵ)) ≥ ϵ · (1 − η · (λ + ϵ))t+1 since ϵ2(L−1) ≤ ϵ by ϵ ≤ 1. The claim follows. 25E.3 Proof of Theorem 2.2 Proof. First, it follows from Theorem 2.1 that for any1 ≤ i ≤ j ≤ L we have Wj:i(t) = Uj,1 fWj:i(t)V ⊤ i,1 + ( jY k=i ρk(t)) · Uj,2V ⊤ i,2 (20) for all t ≥ 0, where Ul,1, Vl,1 ∈ Rd×2r and Ul,2, Vl,2 ∈ Rd×m are the first 2r and last m columns of Ul, Vl ∈ Rd×d respectively. The key claim to be shown here is that cWl(t) = fWl(t) for all l ∈ [L] and t ≥ 0. Afterwards, it follows straightforwardly from (20) that \r\r\rf(Θ(t)) − bf( bΘ(t), UL,1, V1,1) \r\r\r 2 F = \r\r\r\r\rUL,1 fWL:1(t)V ⊤ 1,1 + ( LY l=1 ρl(t)) · UL,2V ⊤ 1,2 − UL,1 cWL:1(t)V ⊤ L,1 \r\r\r\r\r 2 F = \r\r\r\r\rUL,1(fWL:1(t) − cWL:1(t))V ⊤ 1,1 + ( LY l=1 ρl(t)) · UL,2V ⊤ 1,2 \r\r\r\r\r 2 F = \r\r\r\r\r( LY l=1 ρl(t)) · UL,2V ⊤ 1,2 \r\r\r\r\r 2 F ≤ m · LY l=1 ϵ2 l . We proceed by induction. Fort = 0, we have that cWl(0) = U⊤ l,1Wl(0)Vl,1 = fWl(0) for all l ∈ [L] by (20) and choice of initialization. Now suppose cWl(t) = fWl(t) for all l ∈ [L]. Comparing cWl(t + 1) = (1 − ηλ)cWl(t) − η∇cWl bℓ( bΘ(t)) with fWl(t + 1) = U⊤ l,1Wl(t + 1)Vl,1 = U⊤ l,1 [(1 − ηλ)Wl(t) − η∇Wlℓ(Θ(t))] Vl,1 = (1 − ηλ)fWl(t) − ηU⊤ l,1∇Wlℓ(Θ(t))Vl,1 it suffices to show that ∇cWl bℓ( bΘ(t)) = U⊤ l,1∇Wlℓ(Θ(t))Vl,1, ∀l ∈ [L] (21) to yield cWl(t + 1) = fWl(t + 1) for all l ∈ [L]. Computing the right hand side of (21), we have U⊤ l,1∇Wlℓ(Θ(t))Vl,1 = U⊤ l,1W⊤ L:l+1(t)(WL:1(t) − Φ)W⊤ l−1:1(t)Vl,1 = (WL:l+1(t)Ul,1)⊤(WL:1(t) − Φ)(V ⊤ l,1Wl−1:1(t))⊤ where WL:l+1(t)Ul,1 =   UL,1 fWL:l+1(t)V ⊤ l+1,1 + ( LY k=l+1 ρk(t)) · UL,2V ⊤ l+1,2 ! Ul,1 = UL,1 fWL:l+1(t) by (20) and the fact thatUl = Vl+1, and similarly V ⊤ l,1Wl−1:1(t) = V ⊤ l,1   Ul−1,1 fWl−1:1(t)V ⊤ 1,1 + ( l−1Y k=1 ρk(t)) · Ul−1,2V ⊤ 1,2 ! = fWl−1:1(t)V ⊤ 1,1. 26We also have that U⊤ L,1(WL:1(t) − Φ)V1,1 = U⊤ L,1   UL,1 fWL:1(t)V ⊤ 1,1 + ( LY k=1 ρk(t)) · UL,2V ⊤ 1,2 − Φ ! V1,1 = fWL:1(t) − U⊤ L,1ΦV1,1 so putting together the previous four equalities yields U⊤ l,1∇Wlℓ(Θ(t))Vl,1 = (WL:l+1(t)Ul,1)⊤(WL:1(t) − Φ)(V ⊤ l,1Wl−1:1(t))⊤ = fW⊤ L:l+1(t)U⊤ L,1(WL:1(t) − Φ)V1,1 fW⊤ l−1:1(t) = fW⊤ L:l+1(t)(fWL:1(t) − U⊤ L,1ΦV1,1)fW⊤ l−1:1(t). On the other hand, the left hand side of (21) gives ∇cWl bℓ( bΘ(t)) = cWL:l+1(t)⊤U⊤ L,1(UL,1 cWL:1(t)V ⊤ 1,1 − Φ)V1,1 cWl−1:1(t)⊤ = cWL:l+1(t)⊤(cWL:1(t) − U⊤ L,1ΦV1,1)cWl−1:1(t)⊤ so (21) holds by the fact thatcWl(t) = fWl(t) for all l ∈ [L], completing the proof. 27",
      "meta_data": {
        "arxiv_id": "2406.04112v2",
        "authors": [
          "Can Yaras",
          "Peng Wang",
          "Laura Balzano",
          "Qing Qu"
        ],
        "published_date": "2024-06-06T14:29:49Z",
        "pdf_url": "https://arxiv.org/pdf/2406.04112v2.pdf",
        "github_url": "https://github.com/cjyaras/deep-lora-transformers"
      },
      "llm_extracted_info": {
        "main_contributions": "This research demonstrates that by leveraging inherent low-dimensional data structures and compressible dynamics within model parameters, the benefits of overparameterization can be achieved without computational burdens. Theoretically, it proves that learning dynamics for each weight matrix in deep overparameterized low-rank matrix recovery are confined to an invariant low-dimensional subspace, enabling the construction of compact, highly compressed factorizations. Practically, the method significantly improves training efficiency for deep low-rank matrix completion and introduces \"Deep LoRA\" for fine-tuning language models, which reduces overfitting, simplifies hyperparameter setup, and maintains comparable efficiency, particularly with limited data.",
        "methodology": "The core methodology involves exploiting compressible learning dynamics and low-dimensional data structures. For deep matrix factorization, it identifies and theoretically proves the existence of invariant low-dimensional subspaces (rank at most 2r) in each weight matrix's learning dynamics under gradient descent (GD) with scaled orthogonal initialization. Based on this, a compression technique is developed to construct and train a much smaller, yet equivalent, compressed factorization. For deep matrix completion, this compression is generalized by introducing a discrepant learning rate (γ) to slowly update the compressed subspace factors (UL,1, V1,1) during training to mitigate misalignment caused by observation masks. For language model fine-tuning, \"Deep LoRA\" proposes replacing vanilla LoRA's two-layer low-rank update with a deep (e.g., three-layer) full-width overparameterized update, and then applying the developed compression method to these updates for each adapted layer. This allows Deep LoRA to adaptively select appropriate ranks and leverage depth for regularization.",
        "experimental_setup": "The methods are validated across several setups: 1. Deep Matrix Factorization: Theoretical SVD dynamics are visualized on synthetic data (d=30, r*=3). Compression efficiency is demonstrated on L=3 deep matrix factorizations (d=1000, r=r*=5, ϵl=10^-3) by comparing principal components of GD trajectories and training loss vs. wall-time. 2. Deep Matrix Completion: Compression is applied to L=3 deep matrix completion (d=1000, r=r*=5, ϵl=10^-3, 20% observed entries), comparing trajectories and recovery error vs. wall-time for original, compressed with discrepant updates (γ=0.01), and without. 3. Language Model Fine-tuning (Deep LoRA): A pretrained BERT base model is fine-tuned on all attention and feedforward weights (72 layers). Adam optimizer is used, with learning rates (η) and LoRA scale (α) tuned. Benchmarks include the GLUE natural language understanding tasks (CoLA, MNLI, MRPC, QNLI, QQP, RTE, SST-2, STS-B) with limited data (1024 samples) and few-shot settings (n ∈ {16, 64, 256} samples for STS-B). Natural language generation is evaluated on the E2E dataset with a T5 base model using 16 samples. Performance is measured by validation set metrics (e.g., accuracy, Pearson correlation, BLEU, ROUGE, METEOR) and numerical rank analysis, with experiments run on a single NVIDIA Tesla V100 GPU.",
        "limitations": "The theoretical results on network compression specifically exploit the gradient structure of deep linear matrix factorizations. While an empirical observation for non-linear settings is provided, a rigorous theoretical extension to fully non-linear models is a recognized challenge. For deep matrix completion, directly applying the compression method can lead to divergence, necessitating the introduction of a slowly updated factor and a discrepant learning rate (γ) for the factors (UL,1, V1,1), which adds an additional hyperparameter that needs careful tuning. Deep LoRA, while efficient, still requires a fractionally larger memory cost for storage compared to vanilla LoRA (e.g., 12.648GB vs. 12.526GB on the tested hardware).",
        "future_research_directions": "Future work includes extending the compressibility analysis to non-linear settings (e.g., with ReLU activations) by leveraging shared gradient structures and approximate spectral properties, potentially drawing parallels with the masking effect in deep matrix completion. For Deep LoRA, further evaluations in other modalities such as diffusion models, where limited data fine-tuning is prevalent, are suggested. Investigating the use of SGD for the outer factors to further reduce memory costs, given the observed alignment of adapted subspaces at initialization, is another direction. Finally, exploring second-order methods to accelerate fine-tuning along the low-rank subspace is proposed. The research also highlights implications for representation learning, such as connecting low-rank bias to emergent phenomena like deep neural collapse and progressive neural collapse.",
        "experimental_code": "File Path: dlt/models.py\nContent:\nclass MatrixFactorization(nn.Module):\n    shape: Tuple[int, int]\n    init_scale: float\n    depth: int\n    rank: Optional[int]\n\n    def setup(self):\n        assert self.depth >= 2, \"Depth must be at least 2\"\n        set_width = self.rank if self.rank else max(self.shape)\n        misc_utils.check_rank(set_width, self.shape)\n\n        if self.depth == 2:\n            init_fn = nn.initializers.normal(stddev=1)\n            last_init_fn = nn.zeros_init()\n        else:\n            init_fn = nn.initializers.orthogonal(scale=self.init_scale)\n            last_init_fn = init_fn\n\n        layers = []\n        layers.append(\n            self.param(\n                \"W1\",\n                init_fn,\n                (set_width, self.shape[1]),\n            )\n        )\n        for i in range(2, self.depth):\n            layers.append(\n                self.param(\n                    f\"W{i}\",\n                    init_fn,\n                    (set_width, set_width),\n                )\n            )\n        layers.append(\n            self.param(\n                f\"W{self.depth}\",\n                last_init_fn,\n                (self.shape[0], set_width),\n            )\n        )\n        self.layers = layers\n\n    def __call__(self):\n        return jnp.linalg.multi_dot(self.layers[::-1])\n\n\nclass CompressedMatrixFactorization(nn.Module):\n    shape: Tuple[int, int]\n    init_scale: float\n    depth: int\n    rank: int\n\n    def setup(self):\n        self.left_factor = self.param(\n            \"left\", nn.initializers.orthogonal(), (self.shape[0], self.rank)\n        )\n        self.right_factor = self.param(\n            \"right\", nn.initializers.orthogonal(), (self.shape[1], self.rank)\n        )\n        self.mf = MatrixFactorization(\n            (self.rank, self.rank), self.init_scale, self.depth, None\n        )\n\n    def __call__(self):\n        return jnp.linalg.multi_dot([self.left_factor, self.mf(), self.right_factor.T])\n\n\nclass Lora(nn.Module):\n    flat_params_shape_dict: dict\n    init_scale: float\n    depth: int\n    rank: Optional[int]\n    compressed: bool\n\n    def setup(self):\n        mfs = {}\n        for flat_param_path, shape in self.flat_params_shape_dict.items():\n            if self.compressed:\n                assert (\n                    self.rank is not None\n                ), \"Rank must be specified for compressed LoRA\"\n                mf = CompressedMatrixFactorization(\n                    shape=shape,\n                    init_scale=self.init_scale,\n                    depth=self.depth,\n                    rank=self.rank,\n                    name=flat_param_path,\n                )\n                assert self.depth >= 3, \"Depth must be at least 3 for compressed LoRA\"\n            else:\n                mf = MatrixFactorization(\n                    shape=shape,\n                    init_scale=self.init_scale,\n                    depth=self.depth,\n                    rank=self.rank,\n                    name=flat_param_path,\n                )\n            mfs[flat_param_path] = mf\n        self.mfs = mfs\n\n    def __call__(self):\n        return {k: v() for k, v in self.mfs.items()}\n\n    def adapt(self, model_params: ArrayTree) -> ArrayTree:\n        updates = self()\n\n        def f(k, v):\n            flat_k = \"/\".join(k)\n            if flat_k in updates.keys():\n                return v + updates[flat_k]\n            else:\n                return v\n\n        return flax.traverse_util.path_aware_map(\n            f,\n            model_params,  # type: ignore\n        )\n\nFile Path: dlt/train.py\nContent:\ndef create_compressed_lora_train_state(\n    uncompressed_lora_state: LoraState,\n    uncompressed_lora_model: Lora,\n    model_state: ModelState,\n    batch: dict[str, np.ndarray],\n    task_config: TaskConfig,\n):\n    assert task_config.lora_compress, \"Lora compression is not enabled.\"\n    rank = task_config.lora_rank\n    assert rank is not None, \"Rank must be specified.\"\n    assert rank % 2 == 0, \"Rank must be even.\"\n    compressed_lora_model = models.Lora(\n        flat_params_shape_dict=model_utils.get_filtered_flat_params_shape_dict(\n            model_state.params, task_config.lora_adapt_type\n        ),\n        depth=task_config.lora_depth,\n        init_scale=task_config.lora_init_scale,\n        rank=rank,\n        compressed=True,\n    )\n\n    uncompressed_e2e = uncompressed_lora_model.apply(\n        {\"params\": uncompressed_lora_state.params}\n    )\n\n    # Get gradient of uncompressed factors\n    loss_fn = create_lora_loss_fn(\n        model_state,\n        uncompressed_lora_state,\n        task_config.finetune_task_name == \"stsb\",\n        False,\n    )\n\n    uncompressed_grads = jax.grad(loss_fn)(uncompressed_lora_state.params, batch)\n\n    # Move to numpy (do compression on CPU to save GPU memory)\n    uncompressed_lora_params_numpy = jax.tree_map(\n        np.array, uncompressed_lora_state.params\n    )\n    uncompressed_grads_numpy = jax.tree_map(np.array, uncompressed_grads)\n    uncompressed_e2e_numpy = jax.tree_map(np.array, uncompressed_e2e)\n\n    def svd(A):\n        U, s, VT = np.linalg.svd(A, full_matrices=True)\n        return U, s, VT.T\n\n    def get_left_right_factors(W1, W1_grad, e2e):\n\n        m, n = W1.shape\n        assert m == n, \"Need square matrix at this point\"\n\n        half_rank = rank // 2\n        Ugrad, _, Vgrad = svd(W1_grad)\n        Va = W1.T @ Ugrad[:, half_rank:] / task_config.lora_init_scale\n        Vb = Vgrad[:, half_rank:]\n        V0 = Va @ svd(np.concatenate([Va, -Vb], axis=1))[2][: Va.shape[1], n:]\n        V = svd(V0)[0][:, ::-1]\n        right = V[:, :rank]\n        left = e2e @ right / (task_config.lora_init_scale**task_config.lora_depth)\n\n        return left, right\n\n    compressed_lora_params_numpy = {}\n\n    pbar = tqdm(uncompressed_grads_numpy.items())\n    pbar.set_description(\"Compressing LoRA parameters\")\n\n    for k, g in pbar:\n        comp_mf_params = {}\n        m, n = uncompressed_lora_params_numpy[k][\"W1\"].shape\n        if m != n:\n            # WL.T will act like W1\n            right, left = get_left_right_factors(\n                uncompressed_lora_params_numpy[k][f\"W{task_config.lora_depth}\"].T,\n                g[f\"W{task_config.lora_depth}\"].T,\n                uncompressed_e2e_numpy[k].T,\n            )\n        else:\n            left, right = get_left_right_factors(\n                uncompressed_lora_params_numpy[k][\"W1\"],\n                g[\"W1\"],\n                uncompressed_e2e_numpy[k],\n            )\n        comp_mf_params[\"left\"] = left\n        comp_mf_params[\"right\"] = right\n        mf_params = {}\n        for w in g.keys():\n            mf_params[w] = task_config.lora_init_scale * jnp.eye(rank)\n        comp_mf_params[\"mf\"] = mf_params\n        compressed_lora_params_numpy[k] = comp_mf_params\n\n    compressed_lora_params = jax.tree_map(jnp.array, compressed_lora_params_numpy)\n\n    inner_tx = uncompressed_lora_state.tx\n    outer_tx = create_optimizer(\n        create_learning_rate_fn(\n            task_config.num_train_steps,\n            task_config.num_warmup_steps,\n            task_config.lora_gamma * task_config.learning_rate,\n            task_config.decay_ratio,\n        ),\n        task_config.weight_decay,\n    )\n    tx = optax.multi_transform(\n        {\"inner\": inner_tx, \"outer\": outer_tx},\n        flax.traverse_util.path_aware_map(\n            lambda p, _: \"outer\" if p[-1] == \"left\" or p[-1] == \"right\" else \"inner\",\n            compressed_lora_params,\n        ),\n    )\n\n    return LoraState.create(\n        apply_fn=partial(\n            compressed_lora_model.apply, method=compressed_lora_model.adapt\n        ),\n        params=compressed_lora_params,\n        tx=tx,\n        dropout_rng=uncompressed_lora_state.dropout_rng,\n    )",
        "experimental_info": "File Path: dlt/configs.py\nContent:\n@dataclass_json\n@dataclass\nclass TaskConfig:\n    identifier: Optional[str] = None\n\n    # Data hparams\n    task_type: TaskType = TaskType.GLUE\n    finetune_task_name: Union[GlueTaskName, NLGTaskName] = GlueTaskName.STSB\n    max_seq_length: Union[int, Tuple[int, int]] = 128\n    num_train_samples: Optional[int] = None\n\n    # Model hparams\n    pretrain_model: ModelType = ModelType.BERT\n\n    # Lora hparams\n    lora_adapt_type: LoraAdaptType = LoraAdaptType.ONLY_QUERY_VALUE\n    lora_depth: int = 3\n    lora_init_scale: float = 1e-3\n    lora_rank: Optional[int] = None\n    lora_compress: bool = False\n    lora_gamma: float = 0\n\n    # Training hparams\n    num_train_steps: int = 200\n    train_batch_size: int = 1\n    eval_batch_size: int = 32\n    num_warmup_steps: int = 0\n    learning_rate: float = 1e-4\n    weight_decay: float = 0.0\n    decay_ratio: float = 0.1\n\n    # Logging hparams\n    log_eval_steps: int = 200\n    save_step_points: list = field(default_factory=list)\n    save_dir: str = \"checkpoints\"\n\nExperimental Settings Summary:\n- **Model and Task:** Experiments are conducted on BERT for GLUE tasks (e.g., STSB, MNLI) and T5 for NLG tasks (e.g., E2E_NLG).\n- **Deep LoRA Configuration (`lora_depth`):** Vanilla LoRA is typically configured with `lora_depth=2`, while Deep LoRA uses `lora_depth=3` (or higher in `MatrixFactorization` if `rank` is not `None`).\n- **LoRA Adaptation Type (`lora_adapt_type`):** Commonly `LoraAdaptType.ALL_DENSE` is used, adapting all dense layers. Other options include `ONLY_QUERY_VALUE` and `ATTENTION_MLP`.\n- **LoRA Rank (`lora_rank`):** The default `lora_rank` is often `8`, but experiments vary this up to `64` (e.g., `stsb_varying_rank.py`). For uncompressed Deep LoRA (where `lora_rank=None`), the effective rank can be adaptive.\n- **Scaled Orthogonal Initialization (`lora_init_scale`):** Generally set to `1e-3`.\n- **Compression (`lora_compress`, `lora_gamma`):** When compression is enabled (`lora_compress=True`), a `lora_rank` must be specified and must be an even number. A discrepant learning rate `lora_gamma` (e.g., `1e-2` or `1.0`) is applied to the compressed subspace factors (`left` and `right`) to update them slower than the inner matrix factorization. The `dlt/train.py:create_compressed_lora_train_state` function ensures `lora_depth >= 3` for compressed LoRA.\n- **Training Hyperparameters:**\n    - `num_train_steps`: Varies from `200` to `2000` depending on the task and dataset size (e.g., `1000` for GLUE 1024 samples, `400` for NLG 16 samples).\n    - `train_batch_size`: Typically `16`.\n    - `learning_rate`: Commonly `1e-4`.\n    - `weight_decay`: `0.0`.\n    - `decay_ratio`: Often `1.0` (no decay after warmup) or `0.1`.\n    - `num_warmup_steps`: `0` by default."
      }
    },
    {
      "title": "Spectral Adapter: Fine-Tuning in Spectral Space",
      "abstract": "Recent developments in Parameter-Efficient Fine-Tuning (PEFT) methods for\npretrained deep neural networks have captured widespread interest. In this\nwork, we study the enhancement of current PEFT methods by incorporating the\nspectral information of pretrained weight matrices into the fine-tuning\nprocedure. We investigate two spectral adaptation mechanisms, namely additive\ntuning and orthogonal rotation of the top singular vectors, both are done via\nfirst carrying out Singular Value Decomposition (SVD) of pretrained weights and\nthen fine-tuning the top spectral space. We provide a theoretical analysis of\nspectral fine-tuning and show that our approach improves the rank capacity of\nlow-rank adapters given a fixed trainable parameter budget. We show through\nextensive experiments that the proposed fine-tuning model enables better\nparameter efficiency and tuning performance as well as benefits multi-adapter\nfusion.",
      "full_text": "Spectral Adapter: Fine-Tuning in Spectral Space Fangzhao Zhang Electrical Engineering Stanford University zfzhao@stanford.edu Mert Pilanci Electrical Engineering Stanford University pilanci@stanford.edu Abstract Recent developments in Parameter-Efficient Fine-Tuning (PEFT) methods for pre- trained deep neural networks have captured widespread interest. In this work, we study the enhancement of current PEFT methods by incorporating the spectral information of pretrained weight matrices into the fine-tuning procedure. We investigate two spectral adaptation mechanisms, namely additive tuning and or- thogonal rotation of the top singular vectors, both are done via first carrying out Singular Value Decomposition (SVD) of pretrained weights and then fine-tuning the top spectral space. We provide a theoretical analysis of spectral fine-tuning and show that our approach improves the rank capacity of low-rank adapters given a fixed trainable parameter budget. We show through extensive experiments that the proposed fine-tuning model enables better parameter efficiency and tun- ing performance as well as benefits multi-adapter fusion. Code is released at https://github.com/pilancilab/spectral_adapter. 1 Introduction Size of language and vision model undergoes a drastic explosion in recent days and results in billions of parameters up to date. While fine-tuning has been used a lot for adapting pretrained large models to various downstream tasks, fine-tuning tasks become increasingly hard with current size of pretrained models due to the huge demand of computing resource. Meanwhile, exchange and storing of fine- tuned models are also expensive given their enormous size. To alleviate these rising problems for fine-tuning large pretrained models, a recent line of research has digged into the Parameter-Efficient Fine-Tuning (PEFT) model family and harnessed great attention. A high-level philosophy behind those PEFT methods is to train a reduced number of parameters compared to full fine-tuning, which instantly saves computing resource and enables light-weight fine-tuned model exchange. Among all PEFT methods, Low-Rank Adaptation (LoRA) [ 20] model is a huge success attributed to its simplicity and effectiveness. Specifically, LoRA proposes to tune an additive trainable low-rank matrix and brings zero inference latency after merging the adapter into pretrained model weights. Since its emergence, numerous variants of LoRA have been developed. For instance, AdaLoRA [65], IncreLoRA [62], and DyLoRA [ 54] propose to dynamically adjust LoRA rank distribution for improving tuning efficiency, QLoRA [10] combines LoRA with model quantization to further save computing resource, LoRA+ [ 16] and PrecLoRA [ 61] study the optimization landscape of LoRA training, and more recent variant DoRA [32] decomposes pretrained weights into magnitude and direction components and applies LoRA for direction tuning, see Apppendix A for a more comprehensive review of different LoRA variants. Other PEFT methods such as Orthogonal Fine- Tuning (OFT) proposes to multiply pretrained weights by tunable orthogonal matrices for preservation of hypersphere energy between pretrained neurons. Though these different PEFT methods focus on improving fine-tuning efficiency with reduced parameters, rare attention has been paid to utilize pretrained model weights’ information beyond its magnitude in the fine-tuning procedure. Prior research in statistical machine learning such as [36] has studied the Empirical Spectral Distribu- tion (ESD) of deep models’ weight matrices and found that the ESDs for larger model weights are arXiv:2405.13952v2  [cs.LG]  4 Nov 2024Figure 1: Training loss of fine-tuning Llama3 8B model with Orca Math dataset [38] and evaluation score on GSM8K benchmark [7]. We follow experimental setup in [53], see Appendix F.1 for details. All methods except full fine-tuning maintain approximately 0.23% trainable parameters. usually more structured and contain indicative information to distinguish between different training stages. More recent work such as [3] investigates the \"dark matter\" effect of bottom spectral space of model weights and recognizes its critical role in attention sink phenomenon observed in [57]. Both work contributes to decrypting spectral information of model weights and sheds light on building insightful understanding of the connection between weight matrices’ spectral information and model performance. In this work, we explore further the value of model weights’ spectral pattern and unravel its effectiveness in enhancing fine-tuning tasks. We showcase via extensive empirical observation that integration of spectral information of pretrained model weights improves current PEFT methods’ parameter efficiency, tuning effect, and arises as a natural solution to multi-adapter fusion problems. Moreover, the suggested fine-tuning model maintains better practicality compared to prior spectral tuning models, which will be investigated further below. Though any technique for weight fine-tuning can be directly applied to fine-tune singular vector matrices of pretrained model weights, we investigate two specific forms of such extension, namely additive tuning and orthogonal rotating the top singular vector space, which we address as Spectral AdapterA and Spectral AdapterR respectively in later content. The spectral adaptation mechanisms being considered are formally depicted in Section 2. As a warmup, to show that incorporating spectral information is indeed helpful, Figure 1 displays the training loss of fine-tuning Llama3 8B model on HuggingFace Orca Math dataset and validation score on GSM8K benchmark, from which it can be clearly observed that Spectral AdapterA performs superior to recent variants of PEFT methods and behaves closest to full fine-tuning, here we follow experimental setup in [53], see Appendix F.1 for details and more investigation. In below, we first introduce the fine-tuning model being studied in Section 2 and we then provide some theoretic insights in Section 3. After that, we detail the advantage of our spectral adapter in enhancing fine-tuning result, improving model’s parameter efficiency, and helping with multi-adapter fusion as well as address any concern with respect to practicality issues in Section 4. Conclusion and future work is discussed in Section 5. For sake of page limitation, literature review is deferred to Appendix A. To summarize, the proposed spectral adaptation mechanism demonstrates the first attempt to fine-tune spectral space of pretrained model weights in a parameter-efficient and storage-economic way which improves current PEFT methods from aspects involving tuning results, parameter efficiency, and multi-adapter fusion. We hope this work serves as a building block and motivates further and deeper insightful investigation for exploring spectral structure of pretrained model weights, which becomes increasingly meaningful especially in current large model regime. 2 Spectral Adapter: Incorporating Spectral Information into Fine-Tuning Motivated by the intrinsic low-rank of weight shifts in fine-tuning procedure studied in [1], LoRA [20] proposes to add a low-rank factorized trainable matrix to pretrained model weights and tune only these additive parameters for downstream task adaptation, which usually injects far fewer trainable parameters compared to full fine-tuning and results in light-weight tuned adapters. LoRA serves as an outstanding representative of PEFT family and is now widely-used for different fine-tuning tasks. 2Figure 2: Compared to LoRA which proposes to add low-rank trainable matrices to pretrained weights, we study two types of spectral adapters: Spectral AdapterA considers additively tuning the top columns of singular vector matrices and Spectral AdapterR considers orthogonally rotating the top columns of singular vector matrices. Inspired by the parameter efficiency of LoRA and the close connection between matrix rank and its spectral representation, here we study two spectral fine-tuning mechanisms, both are completed via first carrying out Singular Value Decomposition (SVD) of pretrained model weights and then fine- tuning the top columns of singular vector matrices obtained via the SVD. More precisely, consider a pretrained weight matrix with its spectral representation of form W =USV T , we define additive spectral adapter as Spectral AdapterA(W) ∶=[U1 +AU U2]S[V1 +AV V2], and correspondingly the rotational version Spectral AdapterR(W) ∶=[U1RU U2]S[V1RV V2], where U1, V1 denote the top- r columns of U and V and U2, V2 denote the rest of the columns. A =(AU , AV ) consists of trainable matrices of shape same as (U1, V1) and R =(RU , RV ) consists of two trainable orthogonal matrices of shape r by r such that RT U RU =RT V RV =I. As we show in later sections, the orthogonality constraint is efficiently handled with the Cayley parameterization, see Section 4.3 for details. The proposed fine-tuning model architecture can be visualized from Figure 2. Here Spectral AdapterA more resembles LoRA as it is of additive form while Spectral AdapterR more resembles prior Orthogonal Fine-Tuning (OFT) method which we compare further in Section 4. To ensure zero initialization as often done for PEFT methods, we initialize AU and AV both at zero. For rotational spectral adapter, we initialize RU and RV as identity matrices. A more thorough literature review suggests that prior work considering tuning model weights’ spectral representation (FSGAN[ 47], SVDiff [ 15]) has been proposed for alleviating overfitting when fine-tuning different vision models. These methods only look at tuning the singular values of flattened CNN weights and thus have fixed amount of trainable parameters. Moreover, these methods require storing all U, Sand V during training while only the diagonal vector of S is tuned, which nearly doubles the storage requirement compared to pretraining when fine-tuning on downstream tasks. Contrarily, we consider incorporating spectral information in generic fine-tuning procedure for different layers (flattened CNN weights, dense linear weights, etc.) and our method enables flexible parameter budget choices by varying values of r. Methodology-wise, we consider tuning the top-r columns of U and V by additive and rotational tuning, both requiring only these top columns to be stored additionally and the left part can be merged into a single weight matrix. See Section 4.4 for more investigation on practicality of the proposed method. 3 Theoretical Insights After introducing the model architecture of spectral adapter we consider, the main question now remains whether tuning the spectral representation of pretrained weights is indeed an improvement over existing PEFT methods. Before we step into our empirical observations, we first provide some theoretical insights for the proposed spectral adaptation mechanism. In this section, we show advantage of our spectral adapter method compared to LoRA from two theoretic perspectives by 3analyzing both the rank capacity of the adapters (Section 3.1) and the subspace alignment of pretrained weight matrices (Section 3.2). Specifically, we will see that Spectral AdapterA has larger rank capacity than LoRA adapter, which indicates the tuned weight has more adaptation freedom and thus is more desirable. Moreover, the dominant spectral direction of pretrained weight matrix identifies more ideal neuron alignment under the setting we consider in Section 3.2, which justifies the robustness of tuning top singular vectors in our spectral adapter. In Appendix D, we show that Spectral AdapterA is approximately equivalent to DoRA [32] for vector-form weights. 3.1 Adapter Rank Capacity For any pretrained weight matrixW, suppose that the adapter is given by the parameterizationfθ(W) where θ represents trainable weights. For instance with LoRA adapter, fθ(W) =W +ABT , where θ ={A, B} is trainable. We define the rank capacity of an adapter fθ(W) as follows: R(fθ; W) ∶= max θ rank(fθ(W))−min θ rank(fθ(W)), which describes the range of matrix ranks the tuned weight can achieve given a specific adapter form. Then, the following lemma shows that Spectral AdapterA has twice the rank capacity of LoRA adapter under an equal number of trainable parameters. Lemma 3.1. Suppose that W ∈Rn×m is an arbitrary full row-rank matrix and n ≤m without loss of generality. Consider rank-r LoRA and rank-r additive spectral adapter, which have an equal number of trainable parameters. We have R(LoRA; W) =r, R(Spectral AdapterA; W) =2r. See Appendix B for proof. Therefore when pretrained model weight matrix is close to full row-rank, as what has been observed in [20], Spectral AdapterA has nearly double rank capacity compared to LoRA adapter. Furthermore, some prior work explicitly imposes low-rank constraint when training original NNs [50, 43, 66, 22, 68, 24, 9]. Using LoRA adapter to fine-tune such pretrained model weights would destroy their rank constraints while applying spectral adapter preserves the constraints. Next we proceed to show that top spectral space of pretrained weight matrices is more aligned with ideal neuron direction under a simple setting via subspace decomposition analysis of pretrained model weights. This observation corroborates our choice of tuning top singular vectors in our proposed spectral adaptation mechanism. Empirically, we observe that tuning top directions performs superior to tuning bottom ones, see Appendix F.3 and F.5.1 for related experiments. 3.2 Weight Subspace Alignment Figure 3: Top singu- lar vector of pretrained weight recognizes more ideal neuron direction. Il- lustration plot for Section 3.2. Consider two-layer ReLU network with m hidden nodes and univariate output. For squared loss objective, we can write out the training problem explicitly as min W(1),W(2) ∥(XW (1))+W(2) −y∥2 2 +β(∥W(1)∥2 F +∥W(2)∥2 2), where X ∈ Rn×d is the data matrix, (W(1) ∈ Rd×m, W(2) ∈ Rm) are first and second layer weights respectively and y ∈Rn is the label vector. For better visualization, we take d = 3. Consider the case that all data points lie on xy−plane, which mimics the usual observation that data points occupy a low-dimensional manifold. Then we can decompose each first layer neuron W(1) j ∈ Rd into W(1) j = wj1 +wj2 where wj1 ∈ R(X), wj2 ⊥ R(X). With simple algebra, for non-zero weight decay which is often the default setting for current deep learning optimizers, one can derive wj2 =0 and thus W(1) j =wj1 ∈R(X). Therefore all optimal neurons lie also in xy−plane. However, due to optimization errors, some of the trained neurons might be slightly deviated from xy−plane, as illustrated in Figure 3, where ui indicates pretrained neuron directions, though most of them lie in xy−plane, some might deviate (i.e., u4). u⋆ indicates the top singular vector direction of pretrained weight W(1) which here recognizes the xy−plane orientation, and thus fine-tuning u⋆ is noiseless and is expected to be more robust. 44 Empirical Results: The Impact of Spectral Information We experiment our proposed spectral adapter with fine-tuning large language models and diffusion models and compare against various recent PEFT methods. From language model experiments, we observe that Spectral Adapter A performs superior to various PEFT baselines and harnesses higher scores on different benchmarks, which again verifies the effectiveness of incorporating spectral information into the fine-tuning procedure, see Section 4.1 for details. For diffusion model experiments, we will see that the advantage of spectral adapter comes in two-fold: Spectral AdapterA offers a natural solution to existing problems in multi-adapter fusion procedure and Spectral AdapterR manifests finer-grained parameter budgets as well as better parameter efficiency, see Section 4.2 and 4.3 respectively. For a fair comparison with all baselines, we use their official implementation and follow hyperparameter setting in their original reports as long as available. See each individual section for corresponding experimental details. All experiments are done with NVIDIA RTX A6000 GPU. 4.1 Language Model Fine-Tuning: Enhancing Fine-Tuning Results with Spectral Adapter A For large language model experiments, we present experimental results for fine-tuning DeBERTaV3- base model (185M) and Mistral model (7B) on GLUE and GSM8K tasks respectively. Our Spectral AdapterA method achieves superior tuning results compared to various recent PEFT methods in most experiments. DeBERTaV3-base Experiment. Table 1 shows fine-tuning results of DeBERTaV3-base model on GLUE benchmarks with various PEFT methods. For a fair comparison, we use official implemen- tations for LoRA, DoRA, OFT and AdaLoRA in HuggingFace PEFT library, with hyperparameter setting for LoRA [20] and AdaLoRA [65] following their original reports. We use same hyperpa- rameter setting as LoRA for DoRA and follow the setting used in BOFT [33], a variant of OFT, for OFT experiments. We abbreviate Spectral AdapterA as SpectralA for presentation simplicity and we tune hyperparameters for Spectral AdapterA. See Appendix F.2 for hyperparameter details and F.3 for loss/validation plot comparison. We fine-tune all q, k, vmatrices in attention layers. Our Spectral AdapterA achieves highest average score and best scores for most tasks with fewest trainable parameters. Method # Param GLUE MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B Avg. LoRAr=24 0.72% 88.87 95.06 87.00 65.84 91.87 91.45 81.22 90.43 86.47 DoRAr=24 0.73% 88.91 95.29 88.72 65.84 92.01 91.51 80.14 90.10 86.57 OFTr=4 0.72% 89.16 95.06 87.74 66.75 93.28 91.33 78.70 89.72 86.47 AdaLoRAr=24 1.07% 89.44 94.95 89.70 63.06 93.17 91.48 83.75 91.22 87.10 SpectralA r=24 0.72% 89.79 95.75 90.19 69.44 93.35 91.65 83.39 90.64 88.03 Table 1: Accuracy comparison of fine-tuning DeBERTaV3-base with various PEFT methods on GLUE benchmarks. SpectralA is abbreviation for Spectral AdapterA. See Section 4.1 for experimental details. Mistral 7B Experiment. We experiment our Spectral Adapter A with Mistral 7B model [23] fine-tuned for GSM8K task [ 7]. Since all baseline model reports include no fine- tuning tasks with the Mistral family, we use official implementations of all baseline meth- ods for comparison and we fix learning rate to be 2.5e − 5 for all methods following [ 51]. Method #Param GSM8K Pre-Trained − 37.91 ±1.34 LoRAr=8 0.16% 44.81 ±1.37 DoRAr=8 0.17% 43.82 ±1.37 SpectralA r=8 0.16% 49.73 ±1.38 Table 2: Accuracy comparison of fine-tuning Mis- tral 7B model with different PEFT methods on GSM8K benchmark. See Section 4.1 for experi- mental details. We take r = 8 for LoRA, DoRA and Spectral AdapterA to maintain approximately same num- ber of trainable parameters for all methods. Ta- ble 2 presents the accuracy comparison where SpectralA stands for Spectral Adapter A. From the result, we observe that our Spectral AdapterA scores higher than both LoRA and DoRA by a large margin and increases the pretrained model baseline significantly, which verifies the effective- ness of the proposed spectral adaptation mecha- nism. See Appendix F.4 for more about experi- mental details. Note for a different learning rate, DoRA performs better than LoRA while still worse than our method, see also Appendix F.4 for details. 54.2 Diffusion Model Fusion: Improving Multi-Object Fine-Tuning with Spectral Adapter A Figure 4: Distributing different concept tunings along different spectral space helps with identity preservation in multi-adapter fusion, see Section 4.2 for details. Multi-adapter fusion is a current bottleneck in diffusion model fine-tuning tasks with LoRA adapters. Simply adding different LoRA adapters tuned for distinct objects will result in problems involving identity loss and concept binding [12]. To tackle this toughness, different methods emerge such as Gradient Fusion [12] and Orthogonal Adaptation [42]. Specifically, Orthogonal Adaptation method proposes to fix LoRA parameter B to have orthogonal basis and train A solely. Experiments there show that merging LoRA weights with such orthogonal basis helps preserving individual object characteristics compared to its non-orthogonal counterpart. In Orthogonal Adaptation [ 42], the authors maintain B by manually keeping large orthogonal matrices for different layer sizes and sample r columns from corresponding orthogonal matrix to form B for each LoRA adapter. With knowledge from random matrix theory, such sampled matrices are likely to have orthogonal basis. Notably, our Spectral AdapterA naturally operates on orthogonal singular vectors and thus introduces an elegant solution to multi-adapter fusion problems by distributing different concept tunings along different columns of singular vector matrices, which maps to wireless communications where the signals are distributed over non-overlapping frequencies. A subtlety here lies in the choice of column space for different fine-tuning tasks: (1) Sample-based methods can be adopted if data privacy is considered and different tuning tasks are done independently. In Appendix F.5, we show that tuning top columns manifests better generation quality compared to both tuning bottom columns and sampling random orthogonal basis as what has been done in Orthogonal Adaptation [42]. Thus there is a trade-off between high-quality generation and concept collapsing, i.e., sampling from top singular vectors is more encouraged while column overlapping between concepts happens more often compared to sampling from the whole set. (2) On the other hand, if fine-tuning tasks are not isolated and can collaborate on the column scheduling, then more deliberate tuning scheduling can be adopted, for example in a two-concept tuning task with r =4, the first concept can allocate first to fourth columns and the second concept then claims fifth to eighth columns. Figure 4 demonstrates steps for the same method for three-concept tuning task. Since we expect fine-tuned weights to stay close to original weights, though both row space and column space are tuned in spectral adapter, this adaptation mechanism approximates orthogonal-basis tuning for different objects and thus we expect it helps improving identity preservation for multi-adapter fusion. In this section, we investigate this effect via extensive diffusion model experiments. Our experiments follow [42] and build on [12] which studies multi-LoRA fusion. We experiment with multi-object tuning and face generation tasks. Due to space limitation, we present some multi-object tuning results below and we leave the rest to Appendix F.5. For all tasks, we compare against baselines including Gradient Fusion [12], Orthogonal Adaptation [42], and FedAvg [37]. We start with a simple review for these baseline methods. Baseline Review To merge different LoRA adapters, say we have a set of LoRA parameters{∆θ1, . . . ,∆θn} where ∆θi = AiBT i and pretrained parameter θ0, FedAvg [ 37] proposes to merge them in to a single parameter by taking a weighted average as θmerged =θ0 +∑i λi∆θi, where λi is the weight attached to parameter ∆θi and is usually taken to satisfy ∑i λi = 1, i.e., θmerged is a convex combination of individual adapters. Gradient Fusion [12] instead considers solving an auxiliary optimization problem of form θmerged =argminθ ∑n i=1 ∥(θ0 +∆θi)Xi −θXi∥2 F where Xi represents the input activation of the i-th concept. Orthogonal Adaptation [42] follows FedAvg method and replaces original LoRA 6Figure 5: Generation results of Chilloutmix diffusion model [8] with different fused adapters tuned on three custom animal concepts. See Section 4.2 for details. parameters with orthogonal-based LoRA adapters. For our method, to merge different spectral adapters, let θ0 = U0S0V T 0 denote the spectral representation of pretrained model weight. Given a set of spectral adapters {(Ui, Vi), . . . ,(Un, Vn)} with zero-padding to make the shape the same as (U0, V0), we follow FedAvg and compute θmerged = (U0 +∑i λiUi)S0(V0 +∑i λiVi)T . In the following experiments, we take λi =1/n as in [42] for all FedAvg, Orthogonal Adaptation, and our Spectral AdapterA fusion. Notably, all FedAvg, Orthogonal Adaptation, and our Spectral AdapterA fusion can be done approximately instantly while Gradient Fusion usually takes around 10 ∼ 15 minutes for solving its auxiliary optimization problems for all concept adapters. Multi-Object Generation We follow default training setting in [ 12] and fine-tune the Chilloutmix diffusion model [ 8] on three custom animal concepts, see original animals in \"reference\" in Figure 5. For better spatial alignment, we adopt T2I-Adapter [39] with sketch condition and we set guidance equal to one, see also \"reference\" in Figure 5 for the sketch condition being used. LoRA rank r =8 is adopted. For baseline comparisons, we use original code for Gradient Fusion [ 12] and Orthogonal Adaptation [42]. We adapt code of Gradient Fusion for FedAvg method since there is no official implementation available. Custom animal name is replaced with special token < Vanimal> for fine-tuning. For our Spectral AdapterA, we follow the method depicted in Figure 4 and tune first, second, and third top eighth columns of singular vector matrices for different animal concepts. Figure 5 shows the generation results with different methods for selected prompts. Notably, baseline methods sometimes fail to capture the custom animal concepts while Spectral AdapterA recognizes all custom animals and generates visually satisfactory images. For better measurement, we also compute the alignment scores for each generated image with both reference images and prompt texts. It can be witnessed that our method achieves better alignment scores compared to baselines. See Appendix F.7 for details on alignment score computation. 4.3 Diffusion Model Expressiveness: Improving Parameter Efficiency with Spectral AdapterR Spectral AdapterR is closely connected to prior Orthogonal Fine-Tuning (OFT ) [45] method which proposes to multiply the pretrained model weights by trainable orthogonal matrices in the fine- tuning procedure. Motivation behind OFT is to preserve hyperspherical energy which characterizes the pairwise neuron relationship on the unit hypersphere. Unlike OFT which orthogonally rotates neurons, Spectral Adapter R multiplies the top- r columns of singular vector space U and V by orthogonal trainable matrices. For our implementation, several options are available for maintaining a trainable orthogonal matrix such as adding an orthogonality penalty in the objective function considered in [65] or via Cayley parameterization considered in [ 45]. We follow [ 45] and adopt Cayley parameterization which is supported by Pytorch [44]. Specifically, the orthogonal matrix R is 7constructed via R =(I +Q)(I −Q)−1 with a skew-symmetric matrix Q maintained as (A −AT )/2 where A is our trainable parameter. Compared to adding an auxiliary orthogonality penalty, this parametrization is exact and thus the SVD form is preserved after tuning with Spectral AdapterR and can be adopted directly for subsequent fine-tuning tasks, which we state formally as a lemma below: Lemma 4.1. With the Cayley parametrization, Spectral AdapterR is an exact rotation operation and thus preserves the structure of the SVD of the fine-tuned weight. Subsequent fine-tunings can be applied consequently without recomputing the SVD each time. See Appendix C for the proof of above lemma. Unlike LoRA which requires number of trainable parameters to scale with weight size, when tuning top-r columns of U an V , Spectral AdapterR only requires two trainable matrices of size r ×r and thus can be more parameter-efficient especially for large pretrained weight. For common weight size such as W ∈ R1024×1024, LoRA with only r = 1 introduces same number of trainable parameters as Spectral AdapterR with r =32. For a thorough analysis on parameter efficiency improvement brought by Spectral AdapterR, we here also compare with different variants of LoRA which are proposed for trainable parameter savings. We review all baselines in detail below. Baseline Review We compare our Spectral Adapter R with LoRA [ 20], SVDiff [ 15], LiDB [ 48], OFT [ 45], and VeRA [25]. Though the other methods are proposed for vision model tuning, VeRA is originally proposed for LLM tuning and we extend it here to diffusion model tuning due to its parameter efficiency. Consider a pretrained weight W ∈Rn×n, SVDiff originally proposes to tune all singular values of flattened CNN weights, here we extend it to tune all singular values of text encoder and U-Net weights for our comparison, thus trainable parameter attached to W will be of size n and is nonadjustable. LiDB stands for Lightweight Dreambooth and proposes to cut down trainable parameter budget by introducing auxiliary frozen matrixAaux ∈Rn×a and Baux ∈Rb×n, then it mimics LoRA but uses AauxABT Baux in replace of ABT with trainable (A ∈ Ra×r, B∈ Rb×r). Thus with a, b< n, LiDB requires (a +b)r < 2nr trainable parameters. In below, we use a = 50, b= 100 as default in [48]. OFT multiplies the weight matrix by a trainable orthogonal matrix via Cayley parametrization discussed above, thus its complete version requires n2 trainable parameters. For parameter efficiency, OFT proposes to use block-diagonal trainable matrix with all diagonal blocks being orthogonal. Thus with r diagonal blocks, the number of trainable parameter will be r ×(n/r)2. Method Granularity #Param Auxiliary Param LoRA / ∞ 2nr∝n noSVDiff / 1 n∝n noLiDB / ∞ (a+b)r∝r yes OFT / #factors ofn1 (n/r)2 ∝nr no VeRA / ∞ n+r∝n yes Spectral AdapterR , n 2r2 ∝r no 1 Ceiling operation is ignored for this count. Table 3: Baseline methods comparison for parameter effi- ciency. Granularity indicates number of trainable parameter budgets available. See Section 4.3 for details. Further reduction of trainable parame- ter is achieved via sharing the diagonal blocks, which demands only (n/r)2 parameters. In below comparison, we use this shared block-diagonal version for best parameter efficiency of OFT. VeRA proposes to use ΛaAΛbBT in replace of ABT where Λa and Λb are diagonal matrices of size n ×n and r ×r respectively. Thus the total num- ber of trainable parameters by VeRA is (n +r) ∝n. Table 3 compares dif- ferent properties across all methods, where n represents weight size and r represents rank for all methods except for OFT, where r denotes number of diagonal blocks. Parameter Efficiency We fine-tune the Chilloumix diffusion model [8] with various PEFT methods on custom vase concept and present the generation results for prompt \"a <Vvase> on a table\" in Figure 6 for various trainable parameter budgets, where grey dash denotes that the corresponding parameter budget is unobtainable with a given adapter no matter how the hyperparameter is chosen and empty entry without grey dash 8Figure 6: Generation results for prompt “a <Vvase> on a table” after fine-tuning Chilloutmix diffusion model [8] on custom vase images with different PEFT methods. See Section 4.3 for details. represents that there is a way to achieve the corresponding parameter budget though the generation result is skipped for better visualization. We follow default LoRA implementation in [12] for LoRA baseline and adjust it for all other methods. From Figure 6, it can be observed that LoRA, OFT, and LiDB start to generate vase close to custom vase with at least 200k trainable parameters. SVDiff and VeRA are unable to generate ideal vase images even if scaled to large parameter budget. On the contrary, Spectral AdapterR starts to recognize the custom vase concept with only 20k trainable parameters and has finer-grained parameter choices compared to other methods, i.e., notably Spectral AdapterR can have as few as1k parameters while other methods start with at least tens of thousands of trainable parameters. In a word, Spectral AdapterR enjoys finer-grained parameter budget choices and manifests better visual quality with fewer parameters, thus achieves enhanced parameter efficiency compared to various other PEFT methods. Figure 7: Generation results for prompt “a yellow <Vchair>” after fine-tuning Chilloutmix diffusion model [8] on custom chair images with different PEFT methods. Spectral R is abbreviation for Spectral AdapterR. See Section 4.3 for details. Figure 7 above presents generation results of Chilloutmix diffusion model [8] tuned on custom chair concept with different methods under various parameter budgets. The prompt used is \"a yellow <Vchair>\". See \"reference\" in Figure 7 for original chair images. From the generation results, it can be observed that LoRA generates reasonable chairs for all rank r =1, 2, 3 though it already induces 273k parameters even if rank is set to 1. OFT and VeRA start to recognize custom chair with >100k parameters. SVDiff has a single fixed trainable parameter budget of size around 100k. LiDB forms a competitive candidate and generates satisfactory images with smallest trainable parameter budget among all baseline methods. However, our Spectral AdapterR still generates images better aligned to 9reference images with as few as 20k trainable parameters and has finer-grained parameter budget choices compared to LiDB. See Appendix F.6 for hyperparameter setting and Appendix F.7 for alignment score computation details. 4.4 Final Note: A Closer Look at SVD Cost Figure 8: Runtime and GPU storage cost plot. See Section 4.4 for details. To alleviate the concerns with respect to online training cost and show that our pro- posed method is very practical, we provide runtime and GPU storage cost bar plot in Figure 8, which shows runtime and GPU storage cost for LoRA and for our Spec- tral AdapterA when used for fine-tuning diffusion model in Section 4.2 and Mistral 7B model in Section 4.1. Here we adopt rank r = 8 for both LoRA and Spectral AdapterA. It can be observed that our Spec- tral Adapter A introduces negligible run- time and storage overhead for current large model size. Modern numerical tools such as randomized SVD [13] can also be exploited for further runtime reduction and the SVD procedure can be paral- lelized when multiple machines are available. See Appendix E for further investigation. 5 Conclusion and Limitations In this work, we investigate the incorporation of spectral information of pretrained model weights into current PEFT models by introducing a spectral adaptation mechanism which updates only the top singular vectors of pretrained weights. We investigate the additive and rotational variants of such spectral adaptation mechanism. Theoretically, we show the motivation of tuning top singular vectors by comparing the rank capacity of different fine-tuning models and carrying out weight decomposition of pretrained model layers. Empirically, we verify the superiority of our proposed spectral adaptation method compared to various recent PEFT methods from different aspects via extensive experiments. To our best knowledge, this is the first work considering incorporating spectral information as a practical generic paradigm for fine-tuning tasks and enhances fine-tuning results, parameter efficiency, as well as benefits multi-adapter fusion of existing PEFT methods. For future work, fine-tuning spectral representation of different components, i.e., only the attention layer, of current large models is also worth studying. Other PEFT methods such as AdaLoRA [65] can also be dynamically combined with spectral adaptation. A limitation of the current work remains in the choice of tuning top spectral space. Though its validity has been theoretically verified under simple settings, further investigation on tuning different columns of singular vector matrices is critical to understanding the role of spectral information in fine-tuning procedure. Besides, fine-tuning spectral representation of different components, i.e., only the attention layer, of current large models is also worth studying. Moreover, the time consumption of singular value decomposition procedure increases as model grows larger and thus faster singular value decomposition method also benefits. 106 Acknowledgement This work was supported in part by the National Science Foundation (NSF) under Grant DMS- 2134248; in part by the NSF CAREER Award under Grant CCF-2236829; in part by the U.S. Army Research Office Early Career Award under Grant W911NF-21-1-0242; and in part by the Office of Naval Research under Grant N00014-24-1-2164. References [1] A. Aghajanyan, L. Zettlemoyer, and S. Gupta. Intrinsic dimensionality explains the effectiveness of language model fine-tuning, 2020. [2] A. Asai, M. Salehi, M. E. Peters, and H. Hajishirzi. Attempt: Parameter-efficient multi-task tuning via attentional mixtures of soft prompts, 2022. [3] N. Cancedda. Spectral filters, dark signals, and attention sinks, 2024. [4] A. Chavan, Z. Liu, D. Gupta, E. Xing, and Z. Shen. One-for-all: Generalized lora for parameter- efficient fine-tuning, 2023. [5] Y . Chen, D. Hazarika, M. Namazifar, Y . Liu, D. Jin, and D. Hakkani-Tur. Empowering parameter-efficient transfer learning by recognizing the kernel structure in self-attention. arXiv preprint arXiv:2205.03720, 2022. [6] A. Chronopoulou, M. E. Peters, A. Fraser, and J. Dodge. Adaptersoup: Weight averaging to improve generalization of pretrained language models, 2023. [7] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems, 2021. [8] C. M. Creator. Chilloutmix diffusion model. https://civitai.com/models/6424/chilloutmix. [9] M. Denil, B. Shakibi, L. Dinh, M. Ranzato, and N. de Freitas. Predicting parameters in deep learning, 2014. [10] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer. Qlora: Efficient finetuning of quantized llms, 2023. [11] A. Edalati, M. Tahaei, I. Kobyzev, V . P. Nia, J. J. Clark, and M. Rezagholizadeh. Krona: Parameter efficient tuning with kronecker adapter, 2022. [12] Y . Gu, X. Wang, J. Z. Wu, Y . Shi, C. Yunpeng, Z. Fan, W. Xiao, R. Zhao, S. Chang, W. Wu, Y . Ge, S. Ying, and M. Z. Shou. Mix-of-show: Decentralized low-rank adaptation for multi-concept customization of diffusion models. arXiv preprint arXiv:2305.18292, 2023. [13] N. Halko, P.-G. Martinsson, and J. A. Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions, 2010. [14] K. Hambardzumyan, H. Khachatrian, and J. May. Warp: Word-level adversarial reprogramming, 2021. [15] L. Han, Y . Li, H. Zhang, P. Milanfar, D. Metaxas, and F. Yang. Svdiff: Compact parameter space for diffusion fine-tuning, 2023. [16] S. Hayou, N. Ghosh, and B. Yu. Lora+: Efficient low rank adaptation of large models, 2024. [17] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, and G. Neubig. Towards a unified view of parameter-efficient transfer learning, 2022. [18] S. He, R.-Z. Fan, L. Ding, L. Shen, T. Zhou, and D. Tao. Mera: Merging pretrained adapters for few-shot learning. arXiv preprint arXiv:2308.15982, 2023. [19] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. de Laroussilhe, A. Gesmundo, M. At- tariyan, and S. Gelly. Parameter-efficient transfer learning for nlp, 2019. 11[20] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models, 2021. [21] C. Huang, Q. Liu, B. Y . Lin, T. Pang, C. Du, and M. Lin. Lorahub: Efficient cross-task generalization via dynamic lora composition, 2024. [22] M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up convolutional neural networks with low rank expansions. arXiv preprint arXiv:1405.3866, 2014. [23] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed. Mistral 7b, 2023. [24] M. Khodak, N. Tenenholtz, L. Mackey, and N. Fusi. Initialization and regularization of factorized neural layers, 2022. [25] D. J. Kopiczko, T. Blankevoort, and Y . M. Asano. Vera: Vector-based random matrix adaptation, 2024. [26] T. Lei, J. Bai, S. Brahma, J. Ainslie, K. Lee, Y . Zhou, N. Du, V . Zhao, Y . Wu, B. Li, et al. Conditional adapters: Parameter-efficient transfer learning with fast inference. Advances in Neural Information Processing Systems, 36, 2024. [27] B. Lester, R. Al-Rfou, and N. Constant. The power of scale for parameter-efficient prompt tuning, 2021. [28] X. L. Li and P. Liang. Prefix-tuning: Optimizing continuous prompts for generation, 2021. [29] Y . Li, Y . Yu, C. Liang, P. He, N. Karampatziakis, W. Chen, and T. Zhao. Loftq: Lora-fine- tuning-aware quantization for large language models, 2023. [30] Z. Lin, A. Madotto, and P. Fung. Exploring versatile generative language model via parameter- efficient transfer learning. arXiv preprint arXiv:2004.03829, 2020. [31] Q. Liu, X. Wu, X. Zhao, Y . Zhu, D. Xu, F. Tian, and Y . Zheng. Moelora: An moe-based parameter efficient fine-tuning method for multi-task medical applications, 2023. [32] S.-Y . Liu, C.-Y . Wang, H. Yin, P. Molchanov, Y .-C. F. Wang, K.-T. Cheng, and M.-H. Chen. Dora: Weight-decomposed low-rank adaptation, 2024. [33] W. Liu, Z. Qiu, Y . Feng, Y . Xiu, Y . Xue, L. Yu, H. Feng, Z. Liu, J. Heo, S. Peng, Y . Wen, M. J. Black, A. Weller, and B. Schölkopf. Parameter-efficient orthogonal finetuning via butterfly factorization, 2023. [34] X. Liu, Y . Zheng, Z. Du, M. Ding, Y . Qian, Z. Yang, and J. Tang. Gpt understands, too, 2023. [35] R. K. Mahabadi, S. Ruder, M. Dehghani, and J. Henderson. Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks, 2021. [36] C. H. Martin and M. W. Mahoney. Implicit self-regularization in deep neural networks: Evidence from random matrix theory and implications for learning, 2018. [37] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication- efficient learning of deep networks from decentralized data, 2023. [38] A. Mitra, H. Khanpour, C. Rosset, and A. Awadallah. Orca-math: Unlocking the potential of slms in grade school math, 2024. [39] C. Mou, X. Wang, L. Xie, Y . Wu, J. Zhang, Z. Qi, Y . Shan, and X. Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models, 2023. [40] mrm8488. Lora finetune deberta-v3 huggingface blog, 2021. Available at https://huggingface.co/mrm8488/deberta-v3-small-finetuned-mnli/commits/main. [41] J. Pfeiffer, A. Kamath, A. Rücklé, K. Cho, and I. Gurevych. Adapterfusion: Non-destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247, 2020. 12[42] R. Po, G. Yang, K. Aberman, and G. Wetzstein. Orthogonal adaptation for modular customiza- tion of diffusion models, 2023. [43] D. Povey, G. Cheng, Y . Wang, K. Li, H. Xu, M. A. Yarmohammadi, and S. Khudanpur. Semi- orthogonal low-rank matrix factorization for deep neural networks. In Interspeech, 2018. [44] pytorch group. Pytorch orthogonal parameterization method implementation, 2023. [45] Z. Qiu, W. Liu, H. Feng, Y . Xue, Y . Feng, Z. Liu, D. Zhang, A. Weller, and B. Schölkopf. Controlling text-to-image diffusion by orthogonal finetuning, 2023. [46] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from natural language supervision, 2021. [47] E. Robb, W.-S. Chu, A. Kumar, and J.-B. Huang. Few-shot adaptation of generative adversarial networks, 2020. [48] N. Ruiz, Y . Li, V . Jampani, W. Wei, T. Hou, Y . Pritch, N. Wadhwa, M. Rubinstein, and K. Aberman. Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models, 2023. [49] A. Rücklé, G. Geigle, M. Glockner, T. Beck, J. Pfeiffer, N. Reimers, and I. Gurevych. Adapter- drop: On the efficiency of adapters in transformers, 2021. [50] T. N. Sainath, B. Kingsbury, V . Sindhwani, E. Arisoy, and B. Ramabhadran. Low-rank matrix factorization for deep neural network training with high-dimensional output targets. In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 6655–6659, 2013. [51] H. Skogström. Lora finetune mistral 7b valohai blog, 2024. https://valohai.com/blog/finetune- mistral/. [52] A. Tang, L. Shen, Y . Luo, Y . Zhan, H. Hu, B. Du, Y . Chen, and D. Tao. Parameter efficient multi-task model fusion with partial linearization, 2023. [53] K. Turgutlu. Answer.ai qdora report, 2024. https://www.answer.ai/posts/2024-04-26-fsdp-qdora- llama3.html. [54] M. Valipour, M. Rezagholizadeh, I. Kobyzev, and A. Ghodsi. Dylora: Parameter efficient tuning of pre-trained models using dynamic search-free low-rank adaptation, 2023. [55] T. Vu, B. Lester, N. Constant, R. Al-Rfou, and D. Cer. Spot: Better frozen model adaptation through soft prompt transfer. arXiv preprint arXiv:2110.07904, 2021. [56] Z. Wang, R. Panda, L. Karlinsky, R. Feris, H. Sun, and Y . Kim. Multitask prompt tuning enables parameter-efficient transfer learning, 2023. [57] G. Xiao, Y . Tian, B. Chen, S. Han, and M. Lewis. Efficient streaming language models with attention sinks, 2024. [58] L. Xu, H. Xie, S.-Z. J. Qin, X. Tao, and F. L. Wang. Parameter-efficient fine-tuning methods for pretrained language models: A critical review and assessment, 2023. [59] Y . Xu, L. Xie, X. Gu, X. Chen, H. Chang, H. Zhang, Z. Chen, X. Zhang, and Q. Tian. Qa-lora: Quantization-aware low-rank adaptation of large language models, 2023. [60] A. X. Yang, M. Robeyns, X. Wang, and L. Aitchison. Bayesian low-rank adaptation for large language models, 2024. [61] F. Zhang and M. Pilanci. Riemannian preconditioned lora for fine-tuning foundation models, 2024. [62] F. F. Zhang, L. Li, J.-C. Chen, Z. Jiang, B. Wang, and Y . Qian. Increlora: Incremental parameter allocation method for parameter-efficient fine-tuning. ArXiv, abs/2308.12043, 2023. 13[63] L. Zhang, L. Zhang, S. Shi, X. Chu, and B. Li. Lora-fa: Memory-efficient low-rank adaptation for large language models fine-tuning, 2023. [64] M. Zhang, H. Chen, C. Shen, Z. Yang, L. Ou, X. Yu, and B. Zhuang. Loraprune: Pruning meets low-rank parameter-efficient fine-tuning, 2023. [65] Q. Zhang, M. Chen, A. Bukharin, N. Karampatziakis, P. He, Y . Cheng, W. Chen, and T. Zhao. Adalora: Adaptive budget allocation for parameter-efficient fine-tuning, 2023. [66] Y . Zhang, E. Chuangsuwanich, and J. Glass. Extracting deep neural network bottleneck features using low-rank matrix factorization. In2014 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 185–189. IEEE, 2014. [67] H. Zhao, H. Tan, and H. Mei. Tiny-attention adapter: Contexts are more important than the number of parameters, 2022. [68] Y . Zhao, J. Li, and Y . Gong. Low-rank plus diagonal adaptation for deep neural networks. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5005–5009. IEEE, 2016. [69] Y . Zhu, J. Feng, C. Zhao, M. Wang, and L. Li. Counter-interference adapter for multilingual machine translation, 2021. [70] B. Zi, X. Qi, L. Wang, J. Wang, K.-F. Wong, and L. Zhang. Delta-lora: Fine-tuning high-rank parameters with the delta of low-rank matrices, 2023. 14Appendix A Prior Work Here we provide an overview of recent PEFT methods. Dating back to 2019, Houlsby et al. [ 19] develop the idea of parameter-efficient fine-tuning and introduce Adapter model, which injects trainable components between pretrained model layers, though the number of trainable parameters has been reduced due to the small size of adapters, this method incurs inference latency and is thus not desirable. Later improvement of Adapter fine-tuning focuses on improving inference latency [49, 26], fusing multiple adapters [6, 41, 18], modifying adapter model architecture [67], introducing parallelism [17, 69], and creating task-specific and layer-specific adapter [ 35, 30]. Another line of fine-tuning is prompt-tuning [27] which usually adds the trainable components into the prompt. Variants of prompt-tuning involve WARP [14], prefix-tuning [28], P-tuning [34], and ATTEMPT [2] which consider injecting different forms of trainable components. Multitask prompt-tuning is considered in [55, 56]. The more relevant PEFT methods to our spectral adaptation mechanism involves LoRA [20] and OFT [45], which inspires our Spectral AdapterA and Spectral AdapterR respectively. LoRA originates from the observation that model fine-tuning is intrinsically low-rank [1]. Variants of LoRA involve different methods proposing dynamic allocation of LoRA rank budgets [54, 62, 65, 5]. LoRA has been combined with model pruning [64] and quantization [10, 59, 29]. Some other variants further cut down the trainable parameter budget or activation storage by modifying LoRA model [25, 11, 63]. DoRA [32] fixes LoRA’s low-rank limitation by decomposing pretrained model weights and isolating their magnitudes. Laplace-LoRA [ 60] incorporates Bayesian inference into LoRA parameters to improve calibration. LoRAHub [21], MOELoRA [31], and L-LoRA [52] consider multitask LoRA. Delta-LoRA [70] updates pretrained weights simultaneously from information of LoRA parameters. GLoRA [4] generalizes LoRA by introducing a prompt module. Another line of variants focuses on analyzing the optimization scheme of LoRA model [ 61, 16]. OFT studies the multiplicative fine-tuning and its variant BOFT [33] improves OFT by utilizing butterfly parametrization for better information delivery efficiency. [58] offers a comprehensive review of recent development of PEFT methods. B Rank Capacity Proof Proof. Consider weight matrix W ∈ Rn×m with n ≤ m of full row rank. For LoRA parameter A ∈Rm×r, B∈Rn×r with n ≥r, final weight matrix W +ABT has rank in [n −r, n]. With Spectral AdapterA parameters AS ∈ Rm×r, BS ∈ Rn×r where n ≥ 2r. Let Xr denote the first r columns of any matrix X and X−r denote the rest columns, final weight matrix ((Ur +AS)Sr(Vr +BS)T )+ U−rS−rV T −r has rank in [n−2r, n]. Therefore, R(LoRA; W) =r and R(Spectral AdapterA; W) = 2r can be derived trivially. C Cayley Parameterization Proof Proof. With any trainable square matrix A, we set Q = (A −AT )/2 and thus Q = −QT and Q is skew-symmetric thereby. Now we show that for any skew-symmetric Q, (I +Q)(I −Q)−1 is orthogonal. Let O =(I +Q)(I −Q)−1, then OT O =((I +Q)(I −Q)−1)T (I +Q)(I −Q)−1 =(I −QT )−1(I +QT )(I +Q)(I −Q)−1 by Q skew-symmetric, =(I +Q)−1(I −Q)(I +Q)(I −Q)−1 since (I −Q) and (I +Q) have same eigen-basis and are commutable, =I, which shows that the Cayley parametrization is exact and no re-SVD is needed for orthogonality preservation. 15D Connection to DoRA In DoRA [32], the authors observe that plain LoRA method tends to either increase or decrease the magnitude and direction updates proportionally and thus lacks ability to make slight direction change together with large magnitude change, to come across this limitation, the authors propose to decompose pretrained model weights into magnitude and direction and update them separately. The magnitude is replaced with a trainable scalar and the direction is updated with original LoRA method. Experiments in [32] show that such decomposition helps improve effectiveness of LoRA significantly. Here we show that our Spectral AdapterA is closely connected to the weight decomposition trick used in DoRA when pretrained model weight is of vector form. We note that in DoRA, after the weight decomposition, each column becomes unit-length while in Spectral AdapterA, we also operates on matrices with unit-length columns. Specifically, consider a pretrained model weight w0 ∈Rn×1, then DoRA becomes w =w w0 +ba ∥w0 +ba∥2 , where w is a trainable scalar initialized at ∥w0∥2. band a are trainable parameters of size n ×1 and 1 ×1 respectively, with ba =0 at initialization. Comparably, Spectral AdapterA becomes w =( w0 ∥w0∥2 +a′)∥w0∥2(1 +b′), with trainable vectora′ ∈Rn×1 and trainable scalarb′ both initialized at zero. We can thus equivalently view ∥w0∥2(1 +b′) as a single trainable scalar initialized at ∥w0∥2, which then plays the role of magnitude adapter as w in DoRA. a′ is adopted for directional adaptation since it directly operates on the normalized base vector. E Cost Investigation (More Detailed) Here we address the potential concern about the overhead of our proposed spectral adaptation mechanism. Firstly, we note that spectral adapter introduces similar number of trainable parameters and can be merged into original model weights, thus it is lightweight for sharing and introduces no additional inference latency, which preserves the strengths of additive fine-tuning methods. Therefore, the major overhead concern exists in the runtime and GPU storage overhead during online training. Note our method involves only matrix multiplication in the forward procedure and thus should run as quick as LoRA. Though the SVD procedure can bring additional runtime overhead, it needs to be done only once for a single model and can be reused for later fine-tuning on various downstream tasks. Besides, modern numerical tools such as randomized SVD [ 13] can also be exploited and the SVD procedure can be parallelized when multiple machines are available. As for GPU storage, unlike SVDiff [15] where all SVD components are required for training procedure thus introducing significant GPU storage burden, our method requires only the top spectral space to be stored additionally and consumes similar GPU storage to LoRA for relatively small tuning ranks (which is usually the case). F Supplemental Materials for Experiments F.1 Experimental Setup for Figure 1 For Figure 1 experiments, we follow QDoRA [53] experimental setup for fine-tuning Llama3 8B model, where all k_proj, q_proj, v_proj, up_proj, down_proj, and gate_proj weights are tuned. We adopt the same data processing method and train on 10K Orca Math data (shuffled) as in [53]. We fix learning rate as 1e −5 for all methods as in QDoRA and train for one epoch with batch size 8. r =8 is adopted for LoRA, DoRA, AdaLoRA, and Spectral AdapterA while for OFT, we set number of diagonal blocks to be 800 to maintain similar amount of trainable parameters. LoRA alpha is set to be 16 following DoRA [32] convention and AdaLoRA hyperparameter is set following what has been used for MNLI benchmark in the original AdaLoRA report [65] with regularization set to 1e −3 which we find works better. For evaluation, we test on GSM8K [7] benchmark for exact matching. For more comparisons, Figure 9 provides training loss for smaller rank r = 4 (oft_r = 1600) and larger rank r =64 (oft_r =95). All settings are the same except that LoRA alpha is always kept as 16Figure 9: More experiments with Llama3 8B model with different number of trainable parameters. In the left plot, the training loss of LoRA and DoRA overlaps. See Appendix F.1 for details. twice as rank number. From Figure 9 we can observe that though increasing trainable parameters closes the gap between different tuning methods, our spectral adapter method is always superior to other PEFT methods and stays closest to full fine-tuning. F.2 Hyperparameter Setting for DeBERTaV3-base Experiment (Section 4.1) Dataset learning rate batch size #epochs optimizer weight decay MNLI 1e −4 32 1 AdamW 0.01 RTE 3e −4 32 10 AdamW 0.01 QNLI 1e −4 32 1 AdamW 0.01 MRPC 7e −4 32 13 AdamW 0.01 QQP 1e −4 32 10 AdamW 0.01 SST-2 1e −4 32 5 AdamW 0.01 CoLA 3e −4 32 8 AdamW 0.01 STS-B 5e −4 32 30 AdamW 0.01 Table 4: Hyperparameters for DeBERTaV3-base model fine-tuning with Spectral AdapterA in Section 4.1 Table 4 shows the hyperparameter setting for our Spectral AdapterA used for fine-tuning DeBERTaV3- base model in Section 4.1. We set number of diagonal blocks to be 4 and enable block sharing for OFT to maintain similar amount of trainable parameters. F.3 More About DeBERTaV3-base Experiment Left plot in Figure 10 presents the training loss and validation score comparisons of LoRA, SVDiff and our Spectral AdapterA for fine-tuning DeBERTaV3-base model on CoLA benchmark. We set learning rates for both LoRA and Spectral AdapterA as what has been used in popular public blog [40] for LoRA fine-tuning with DeBERTaV3-base model, which is not tuned in favor of our method. For SVDiff, since it is originally proposed for vision model tuning, we extend it to this experiment by tuning all singular values of pretrained weights. We find the same learning rate leads to poor fine-tuning results with SVDiff, we thus pick the best learning rate among [1e −3, 1e −4, 1e −5] according to validation performance and set learning rate to be 1e −3. We use r = 8 for LoRA and Spectral AdapterA. From Figure 10, it can be observed that Spectral AdapterA achieves better training and validation performance compared to both LoRA and SVDiff. Interestingly, in LoRA [20], the authors provide a correlation analysis between the LoRA additive component △W = ABT and original pretrained weight matrix W (see Section H.3 in [ 20]), and they find that the additive component does not contain the top singular directions of W. The authors therefore conclude that the learned LoRA component amplifies \"task-specific\" directions which are not emphasized in the pretrained weight matrix. Naively, this seems to suggest that tuning top singular subspace of pretrained weights is not ideal and one should identify the desired \"task-specific\" directions to improve LoRA. Here we show that this is not the case and fine-tuning top directions provides a significant improvement to LoRA. In the right plot of Figure 10 above, we experiment 17Figure 10: Left plot presents training loss and validation results for fine-tuning DeBERTaV3-base model with LoRA, SVDiff, and Spectral AdapterA on CoLA benchmark. Right plot compares the same statistics between LoRA and spectral adapter with top ranks and bottom ranks tuned respectively. tuning the top eighth rank and the bottom eighth rank of singular vector space in our Spectral AdapterA, which we present as \"Spectral Top\" and \"Spectral Bottom\" respectively. Remarkably, \"Spectral Top\" converges faster and scores higher than LoRA, which is then superior to \"Spectral Bottom\". This result unravels the fact that tuning different part of spectral space brings different tuning effect and tuning the top columns of singular vector space improves LoRA tuning significantly. See Section 3 for more theoretic insights. F.4 Hyperparameter Setting for Mistral 7B Experiment (Section 4.1) Method lr lora alpha batch size #epochs lora dropout weight decay LoRA 2.5e −5 16 4 2 0.05 0.01 DoRA 2.5e −5 16 4 2 0.05 0.01 Spectral AdapterA 2.5e −5 - 4 2 - 0.01 Table 5: Hyperparameters for Mistral 7B model fine-tuning task in Section 4.1 Table 5 shows training hyperparameter setting for fine-tuning Mistral 7B model in Section 4.1. We train with bfloat16 precision and fine-tune all q_proj, k_proj, v_proj, o_proj, and gate_proj weights. We evaluate with lm-evaluation-harness [47]. Table 6 shows accuracy comparison of different tuning methods with learning rate 1e −5. Our Spectral AdapterA still exceeds both LoRA and DoRA. F.5 Supplemental Materials for Multi-Adapter Fusion Experiment (Section 4.2) F.5.1 Comparison of Single Object Generation We present more experimental results to show that Spectral AdapterA with top ranks tuned behaves at least as good as LoRA with same parameter budget and is better than Orthogonal Adaptation [42], which is likely due to that Orthogonal Adaptation fixes LoRA parameter B and thus has limited expressiveness. We also show that tuning bottom ranks in spectral adapter behaves worse than all other methods. Figure 11 shows generation results for custom toy concept tuning, where Orthogonal Adaptation and Spectral AdapterA (bottom) generate inaccurate happy-face octopus, sad-face octopus, and green tortoise. Figure 12 shows generation results for custom animal concept tuning, where Orthogonal Adaptation and Spectral AdapterA (bottom) sometimes miss first dog concept. Method #Param GSM8K Pre-Trained − 38.82 LoRAr=8 0.16% 43.29 ±1.36 DoRAr=8 0.17% 43.52 ±1.37 SpectralA r=8 0.16% 46.47 ±1.37 Table 6: Supplemental experiments of fine-tuning Mistral 7B model with different PEFT methods with a different learning rate on GSM8K benchmark. See Section F.4 for experimental details. 18Figure 11: Generation results for single toy concept tuning with LoRA, Orthogonal Adaptation, and Spectral AdapterA with top and bottom ranks tuned respectively. F.5.2 More Multi-Adapter Fusion Generation Results Here we present more results for multi-adapter fusion generation. Figure 13 shows generation results for multi-object generation for custom toy concepts and Figure 14 presents generation results for multi-character generation for three computer scientists. See below for experimental details. Multi-Object Generation. As in Section 4.2, we fine-tune Chilloutmix diffusion model [8] on four custom toy concepts, see \"reference\" in Figure 13 for original toy images. We use r =8 for all methods and tune first, second, third, and fourth top eighth columns of singular vector space of pretrained weights for first, second, third, and fourth toys in our Spectral AdapterA. We follow all default experimental settings in [ 12] and tune all embedding layer, U-Net, and text-encoder. For better spatial alignment, we employ T2I-Adapter with sketch condition listed in \"reference\" in Figure 13. We randomly select three scenes and prompt fused-adapters for the results, see \"prompts\" in Figure 13 for individual prompt being used. From Figure 13, it can be observed that FedAvg and Orthogonal Adaptation generate unsatisfactory happy-face octopus and green tortoise toys. On the contrary, our spectral adapter generates high-quality images similar to Gradient Fusion while saving 19Figure 12: Generation results for single animal concept tuning with LoRA, Orthogonal Adaptation, and Spectral AdapterA with top and bottom ranks tuned respectively. Figure 13: Generation results of Chilloutmix diffusion model [8] tuned on four custom toy concepts with different fused adapters. See Appendix F.5.2 for details. much more time. Multi-Character Generation. We also experiment fine-tuning Chilloutmix diffusion model [ 8] with photos of three computer scientists Yoshua Bengio, Yann LeCun, and Geoffrey Hinton. As in multi-object generation, we use r = 8 for all methods and tune first, second, and third top eighth columns of singular vector space of pretrained weights for Bengio, Lecun, and Hinton in our Spectral AdapterA. We use T2I-Adapter [ 39] with keypose condition. See \"reference\" in Figure 14 for scientists’ photos and keypose condition being used. Figure 14 shows generation results for prompt 20\"<Vbengio> and <Vlecun> and <Vhinton>, standing near a lake, 4K, high quality, high resolution\" with different fused adapters, from which it can be observed that our spectral adapter generates picture of most consistent styles across characters and renders all scientists’ faces clearly. Figure 14: Generation results of Chilloutmix diffusion model [8] tuned on photos of three computer scientists with different fused adapters. See Appendix F.5.2 for details. F.6 Supplemental Materials for Parameter Efficiency Experiment (Section 4.3) In this section, we present more tuning results with various parameter budgets for parameter efficiency experiment studied in Section 4.3, see Section 4.3 for baseline method explanation. Table 7 shows the learning rates used for each baseline method and Table 8 shows learning rates used for our method, the rest experimental settings are default as in [12]. Method text encoder lr unet lr LoRA 1e −5 1e −4 VeRA (r =1) 1e −3 1e −4 VeRA (r =1024, 4096) 5e −3 1e −4 OFTA 1e −5 1e −4 LiDB 5e −4 1e −4 SVDiff 1e −3 1e −4 Table 7: Hyperparameters for baseline methods for diffusion model fine-tuning task in Section 4.3 Method vase chair table text unet text unet text unet Spectral AdapterR (r =2, 40) 1e −3 1e −2 1e −2 1e −2 1e −3 1e −2 Spectral AdapterR (r =4) 5e −3 5e −3 1e −3 1e −2 Spectral AdapterR (r =8) 5e −4 5e −2 1e −3 1e −2 1e −3 1e −2 Spectral AdapterR (r =16) 1e −2 1e −3 1e −3 1e −2 Spectral AdapterR (r =24) 1e −4 1e −2 1e −3 1e −3 1e −4 1e −2 Spectral AdapterR (r =32) 1e −4 5e −2 Table 8: Hyperparameters for Spectral AdapterR for diffusion model fine-tuning task in Section 4.3 Figure 15 shows generation results of Chilloutmix diffusion model [8] fine-tuned on custom table concept with different methods under various parameter budgets. The prompt used is “a <Vtable>”. LoRA generates acceptable images for all rank r =1, 2, 3 though it starts with 273k parameters even if rank is set to 1. OFT generates desirable images only for parameter budget > 400k. VeRA and LiDB start to generate reasonable images with >300k trainable parameters and SVDiff has only a single fixed parameter budget. Meanwhile, our Spectral AdapterR recognizes the shape of custom table with as few as 6k parameters and produces ideal images since 100k parameters. See Appendix F.7 for alignment score computation details. 21Figure 15: Generation results for prompt “a <Vtable>” after fine-tuning Chilloutmix diffusion model [8] on custom table images with different PEFT methods. Spectral R is abbreviation for Spectral AdapterR. See Appendix F.6 for details. F.7 Alignment Score Computation For better quantitative measurement, we compute alignment scores for our Figure 5,6,7,15 results. Specifically, we first compute CLIP [46] embedding for all generated/reference images and prompt texts, then we compute the cosine similarity between generated images’ embedding and reference images’ embedding to serve as their alignment score. Likewise, text score stands for cosine similarity between generated images’ embeddings and their corresponding prompt texts’ embeddings. Intuition here is that if an image is close to another image (or text), their CLIP vectors are expected to stay close as well. For Figure 5 alignment score computation, we crop each generated image vertically into three columns, then we compute their alignment scores to each corresponding reference animal, we finally take the mean of these three scores. For Figure 6, 7, 15 scores, we compute average score over three random trials, with each trial consisting of 8 generated images. 22",
      "meta_data": {
        "arxiv_id": "2405.13952v2",
        "authors": [
          "Fangzhao Zhang",
          "Mert Pilanci"
        ],
        "published_date": "2024-05-22T19:36:55Z",
        "pdf_url": "https://arxiv.org/pdf/2405.13952v2.pdf",
        "github_url": "https://github.com/pilancilab/spectral_adapter"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Spectral Adapter, a novel Parameter-Efficient Fine-Tuning (PEFT) method that integrates spectral information of pretrained weight matrices into the fine-tuning process. It proposes two mechanisms: Spectral AdapterA (additive tuning) and Spectral AdapterR (orthogonal rotation) of the top singular vectors, both applied after Singular Value Decomposition (SVD) of pretrained weights. Theoretically, it's shown that Spectral AdapterA offers twice the rank capacity of LoRA for a given parameter budget and that tuning top singular vectors aligns with ideal neuron directions. Experimentally, the method demonstrates superior parameter efficiency, improved tuning performance in LLMs, and enhanced multi-adapter fusion in diffusion models compared to existing PEFT approaches.",
        "methodology": "The core methodology involves applying Singular Value Decomposition (SVD) to pretrained model weight matrices (W = USV^T) and then fine-tuning only the top-r columns of the singular vector matrices (U and V). Two specific adaptation mechanisms are explored: Spectral AdapterA, which additively tunes the top-r columns with trainable matrices AU and AV initialized to zero; and Spectral AdapterR, which orthogonally rotates the top-r columns using trainable orthogonal matrices RU and RV, initialized as identity matrices. The orthogonality constraint for Spectral AdapterR is efficiently handled using Cayley parameterization. Unlike prior spectral tuning methods, this approach is applicable to generic layers, allows flexible parameter budgets by varying 'r', and minimizes storage by only requiring the top columns to be stored.",
        "experimental_setup": "Experiments were conducted on various large models including Llama3 8B, DeBERTaV3-base (185M), Mistral 7B for Language Model Fine-Tuning, and Chilloutmix diffusion model for Diffusion Model Fusion and Expressiveness. Datasets and benchmarks included Orca Math dataset and GSM8K for Llama3 8B, GLUE benchmarks (MNLI, SST-2, MRPC, CoLA, QNLI, QQP, RTE, STS-B) for DeBERTaV3-base, and GSM8K for Mistral 7B. For diffusion models, custom concepts (animals, toys, vase, chair) and multi-character generation (computer scientists) were used. Comparisons were made against various PEFT baselines: LoRA, DoRA, OFT, AdaLoRA, SVDiff, LiDB, VeRA, Gradient Fusion, Orthogonal Adaptation, and FedAvg. All experiments utilized NVIDIA RTX A6000 GPUs. Hyperparameters generally followed official implementations and original reports, with specific learning rates, batch sizes, epochs, optimizers, and LoRA alpha values mentioned for different tasks and models. Validation involved training loss, accuracy (for LLMs), qualitative visual generation results, and quantitative alignment scores (CLIP embedding cosine similarity for images and texts) for diffusion models.",
        "limitations": "A primary limitation lies in the current choice of exclusively tuning the top spectral space. While theoretically supported under simple settings, further comprehensive investigation into tuning different columns (e.g., bottom spectral space) of singular vector matrices is needed to fully understand the role of spectral information in fine-tuning. Additionally, the time consumption of the Singular Value Decomposition procedure increases with larger models, posing a challenge.",
        "future_research_directions": "Future research could explore fine-tuning the spectral representation of specific components within large models, such as only the attention layer. There is also potential to dynamically combine spectral adaptation with other existing PEFT methods like AdaLoRA. Further in-depth investigation into tuning different columns of singular vector matrices beyond just the top ones is crucial. Lastly, developing faster Singular Value Decomposition methods would be beneficial to alleviate the computational overhead as models continue to grow in size.",
        "experimental_code": "class SpectralLinearLayer_OFT(nn.Module):\n    def __init__(self, name, original_module, rank=4, alpha=1, top=True, idx=0, revised_r=-1):\n        rank = 8\n        super().__init__()\n        self.name = name\n        if original_module.__class__.__name__ == 'Conv2d':\n            self.conv = True\n            in_channels, out_channels = original_module.in_channels, original_module.out_channels\n        else:\n            self.conv = False\n            in_channels, out_channels = original_module.in_features, original_module.out_features\n        W = original_module.weight.data.view(out_channels, in_channels)\n        U, S, V = torch.svd(W)\n        self.U = torch.nn.Parameter(U, requires_grad=False)\n        self.S = torch.nn.Parameter(S, requires_grad=False)\n        self.V = torch.nn.Parameter(V, requires_grad=False)\n        self.spectral_A = torch.nn.Parameter(torch.zeros(revised_r,revised_r), requires_grad=True)\n        self.spectral_B = torch.nn.Parameter(torch.zeros(revised_r,revised_r), requires_grad=True)\n        self.spectral_C = torch.nn.Parameter(torch.ones(revised_r), requires_grad=True)\n        original_module.forward = self.forward\n        self.original_module = original_module\n        self.top = top\n        self.idx = idx\n        assert revised_r>0\n        self.rank = revised_r\n\n    def cayley(self, data: torch.Tensor) -> torch.Tensor:\n        r, _ = data.shape\n        skew = 0.5 * (data - data.T)\n        I = torch.eye(r, device=data.device)\n        Q = torch.mm(I - skew, torch.inverse(I + skew))\n        return Q\n\n    def forward(self, hidden_states):\n        if self.top:\n            pad_U = self.U.clone()\n            pad_U[:,self.idx*self.rank:(self.idx+1)*self.rank] = self.U[:,self.idx*self.rank:(self.idx+1)*self.rank]@self.cayley(self.spectral_A)\n            pad_S = self.S.clone()\n            pad_S[self.idx*self.rank:(self.idx+1)*self.rank] = self.S[self.idx*self.rank:(self.idx+1)*self.rank]*self.spectral_C\n            pad_V = self.V.clone()\n            pad_V[:,self.idx*self.rank:(self.idx+1)*self.rank] = self.V[:,self.idx*self.rank:(self.idx+1)*self.rank]@self.cayley(self.spectral_B)\n        else:\n            raise Exception('')\n        pad_W = pad_U@pad_S.diag()@pad_V.T\n        if self.conv :\n            raise Exception('')\n        else:\n            return F.linear(hidden_states, pad_W, bias=self.original_module.bias)\n\ndef merge_spectraloft_into_weight(original_state_dict, lora_state_dict, model_type, alpha, top=True, idx=0):\n    def get_spectral_A_name(original_layer_name):\n        if model_type == 'text_encoder':\n            spectral_A_name = original_layer_name.replace('q_proj.weight', 'q_proj.spectral_A')                 .replace('k_proj.weight', 'k_proj.spectral_A')                 .replace('v_proj.weight', 'v_proj.spectral_A')                 .replace('out_proj.weight', 'out_proj.spectral_A')                 .replace('fc1.weight', 'fc1.spectral_A')                 .replace('fc2.weight', 'fc2.spectral_A')\n        else:\n            spectral_A_name = k.replace('to_q.weight', 'to_q.spectral_A')                 .replace('to_k.weight', 'to_k.spectral_A')                 .replace('to_v.weight', 'to_v.spectral_A')                 .replace('to_out.0.weight', 'to_out.0.spectral_A')                 .replace('ff.net.0.proj.weight', 'ff.net.0.proj.spectral_A')                 .replace('ff.net.2.weight', 'ff.net.2.spectral_A')                 .replace('proj_out.weight', 'proj_out.spectral_A')                 .replace('proj_in.weight', 'proj_in.spectral_A')\n\n        return spectral_A_name\n    \n    def cayley(data):\n        r, _ = data.shape\n        skew = 0.5 * (data - data.T)\n        I = torch.eye(r, device=data.device)\n        Q = torch.mm(I - skew, torch.inverse(I + skew))\n        return Q\n\n    assert model_type in ['unet', 'text_encoder']\n    new_state_dict = copy.deepcopy(original_state_dict)\n    load_cnt = 0\n    for k in new_state_dict.keys():\n        spectral_A_name = get_spectral_A_name(k)\n        spectral_B_name = spectral_A_name.replace('spectral_A', 'spectral_B')\n        spectral_C_name = spectral_A_name.replace('spectral_A', 'spectral_C')\n        U_name = spectral_A_name.replace('spectral_A', 'U')\n        S_name = spectral_A_name.replace('spectral_A', 'S')\n        V_name = spectral_A_name.replace('spectral_A', 'V')\n        if spectral_B_name in lora_state_dict:\n            load_cnt += 1\n            original_params = new_state_dict[k]\n            spectral_A_params = lora_state_dict[spectral_A_name].to(original_params.device)\n            spectral_B_params = lora_state_dict[spectral_B_name].to(original_params.device)\n            spectral_C_params = lora_state_dict[spectral_C_name].to(original_params.device)\n            U_params = lora_state_dict[U_name].to(original_params.device)\n            S_params = lora_state_dict[S_name].to(original_params.device)\n            V_params = lora_state_dict[V_name].to(original_params.device)\n            r = spectral_A_params.shape[0]\n            if top:\n                pad_U = U_params \n                pad_U[:,idx*r:(idx+1)*r] = U_params[:,idx*r:(idx+1)*r]@(alpha*(cayley(spectral_A_params)-torch.eye(r).to(spectral_A_params.device))+torch.eye(r).to(spectral_A_params.device))\n                pad_V = V_params\n                pad_V[:,idx*r:(idx+1)*r] = V_params[:,idx*r:(idx+1)*r]@(alpha*(cayley(spectral_B_params)-torch.eye(r).to(spectral_A_params.device))+torch.eye(r).to(spectral_A_params.device))\n                pad_S = S_params \n                pad_S[idx*r:(idx+1)*r] = S_params[idx*r:(idx+1)*r]*(alpha*(spectral_C_params-torch.ones(r).to(spectral_A_params.device))+torch.ones(r).to(spectral_A_params.device))\n            else:\n                raise Exception('')\n            if len(original_params.shape) == 4:\n                raise Exception('')\n            else:\n                spectral_param = pad_U@pad_S.diag()@pad_V.T\n            new_state_dict[k] = spectral_param\n    print(f'load {load_cnt} Spectrals of {model_type}')\n    return new_state_dict",
        "experimental_info": "The core methodology involves Singular Value Decomposition (SVD) on pretrained model weight matrices (W = USV^T) within a Stable Diffusion model's text encoder and UNet layers. Specifically, the `SpectralLinearLayer_OFT` is designed for adaptation. The original U, S, and V matrices obtained from SVD are kept fixed (non-trainable). The adaptation is applied by introducing three sets of trainable parameters: `self.spectral_A` and `self.spectral_B` (both `revised_r x revised_r` matrices, initialized to zeros) and `self.spectral_C` (a vector of size `revised_r`, initialized to ones). Cayley parameterization (`self.cayley` function) is used to ensure orthogonality for the updates to U and V, effectively implementing Spectral AdapterR. The top-r columns of the U and V singular vector matrices are orthogonally rotated using these trainable matrices. The corresponding singular values in S are scaled by `self.spectral_C`. The `revised_r` parameter, explicitly set to `rank` in `__init__`, controls the adaptable parameter budget (the 'r' value). An `alpha` parameter further scales the magnitude of the adapter's contribution to the original weights, where the scaled contribution is `alpha * (Cayley(spectral_params) - Identity)` for U/V and `alpha * (spectral_C_params - ones)` for S, which are then added to identity/ones before being applied. This approach allows flexible parameter budgets and is applied to 'Linear' layers (and potentially 'Conv2d' layers with 1x1 kernel size, though not fully implemented for Conv2d in the provided code)."
      }
    },
    {
      "title": "Adapters Strike Back",
      "abstract": "Adapters provide an efficient and lightweight mechanism for adapting trained\ntransformer models to a variety of different tasks. However, they have often\nbeen found to be outperformed by other adaptation mechanisms, including\nlow-rank adaptation. In this paper, we provide an in-depth study of adapters,\ntheir internal structure, as well as various implementation choices. We uncover\npitfalls for using adapters and suggest a concrete, improved adapter\narchitecture, called Adapter+, that not only outperforms previous adapter\nimplementations but surpasses a number of other, more complex adaptation\nmechanisms in several challenging settings. Despite this, our suggested adapter\nis highly robust and, unlike previous work, requires little to no manual\nintervention when addressing a novel scenario. Adapter+ reaches\nstate-of-the-art average accuracy on the VTAB benchmark, even without a\nper-task hyperparameter optimization.",
      "full_text": "Adapters Strike Back Jan-Martin O. Steitz1 Stefan Roth1,2 1Department of Computer Science, TU Darmstadt 2 hessian.AI Abstract Adapters provide an efficient and lightweight mechanism for adapting trained transformer models to a variety of dif- ferent tasks. However, they have often been found to be outperformed by other adaptation mechanisms, including low-rank adaptation. In this paper, we provide an in-depth study of adapters, their internal structure, as well as vari- ous implementation choices. We uncover pitfalls for using adapters and suggest a concrete, improved adapter architec- ture, called Adapter+, that not only outperforms previous adapter implementations but surpasses a number of other, more complex adaptation mechanisms in several challenging settings. Despite this, our suggested adapter is highly robust and, unlike previous work, requires little to no manual inter- vention when addressing a novel scenario. Adapter+ reaches state-of-the-art average accuracy on the VTAB benchmark, even without a per-task hyperparameter optimization.† 1. Introduction Transfer learning from an off-the-shelf model, pre-trained on a large dataset like ImageNet [55] to a downstream task by fully fine-tuning the model’s parameters is a common paradigm. A typical CNN architecture, like a ResNet [24], has several tens of millions of parameters. However, since the introduction of transformers [59] into the realm of com- puter vision [4, 5, 13, 51, 52, 63], model sizes have grown exponentially from around a hundred million parameters for a vision transformer (ViT) [ 13] to more than a billion parameters [10, 46]. This leads to huge storage requirements when fine-tuning on multiple downstream tasks because a complete set of the model’s parameters needs to be saved per task. Additionally, large models require correspondingly large datasets [e.g., 56] to be trained to their full potential, yet tend to overfit easily if the target dataset in transfer learn- ing is too small. One solution is linear probing [12], where only the linear classifier is trained, but this usually yields inferior results compared to full fine-tuning. As a consequence, there is a growing interest in parameter- efficient tuning methods. The main idea is to freeze the †Code is available at https://github.com/visinf/adapter_plus. 0 0.1 0.2 0.3 0.4 0.5 0.6 72 74 76 78 Fine-tuning accuracy Linear probing # Parameters (M) Accuracy (%) Adapter+ (ours) SPT -Adapter [21]SSF [39] Adapter+ opt. (ours)SPT -Adapter⟳ SSF⟳ FacT -TK [32] Consolidator [20]VPT [31] FacT -TK⟳ LoRA [29] VPT⟳ Figure 1. Parameter-accuracy characteristics of adaptation methods on the VTAB [65] test sets. We report original results and re-evaluations ( ⟳) after a complete training schedule with suitable data normalization. Our Adapter+ has clearly the best parameter-accuracy trade-off. The vertical, dashed line shows the possible minimal number of tunable parameters when only the classifiers are trained, i.e., using linear probing (61% accuracy). parameters of the pre-trained model and add a compara- tively small amount of parameters to the model, which are then tuned together with the classifier to adapt the model to the downstream task at hand. Representative methods with different underlying concepts include VPT [31], which prepends the sequence of image tokens in the attention with trainable tokens to learn a prompt tuning, LoRA [29], where the attention weights are updated with learnable low-rank decomposition matrices, and Adapters [28], which are small bottleneck modules that are added to every transformer layer of the network. Adapters were first proposed for CNNs by Rebuffi et al. [53] and various formulations [22, 28, 49] exist for the now common ViT architecture. Recent work on parameter-efficient transfer learning [e.g., 20, 21, 31, 32, 39, 67] presents adapters as a baseline method for the adaptation to downstream tasks in computer vision. However, we identified various common issues in their imple- mentations, which we find to have a negative influence on the To appear in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Seattle, W A, USA, 2024. © 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. arXiv:2406.06820v1  [cs.CV]  10 Jun 2024LoRAVPT ⟳ SSF ⟳ FacT ⟳ Consol.SPT ⟳ Adapter+ 80 81 82 83 84 82.4 82.5 80.3 82.6 82.4 82.2 84.0 Accuracy (%) Natural LoRAVPT ⟳ SSF ⟳ FacT ⟳ Consol.SPT ⟳ Adapter+ 85 86 84.3 84.6 85.7 84.7 86.3 85.3 86.5 Accuracy (%) Specialized LoRAVPT ⟳ SSF ⟳ FacT ⟳ Consol.SPT ⟳ Adapter+ 58 60 62 60.1 62.1 58.0 62.3 60.9 60.5 63.3 Accuracy (%) Structured Figure 2. Average accuracy for VTAB subgroups on thetest sets. For methods marked with ⟳, we report results of our re-evaluation after a complete training schedule with suitable data normalization to ensure a fair comparison. Adapter+ is evaluated with rank r∈[1..32]. adaptation performance. For further details, refer to the sup- plemental material. Additionally, while adapters have been well studied in natural language processing (NLP), there is no study that broadly examines the different adapter config- urations for vision transformers. As a result, adapters have seemed to underperform in comparison to recent parameter- efficient adaptation methods, e.g., reported accuracies of adapters on VTAB of 73.9% in [67] and 60.8% in [31]. In this work, we therefore revisit the idea of adapters and investigate how they can perform at their best in con- nection with ViTs. Our contribution hereby is threefold: (1) We show the first in-depth and systematic study on the effects of the adapter position in the transformer and of the adapter’s inner structure with ViTs, as well as evaluate differ- ent variants of parameter initialization. (2) We further pro- pose a learnable, channel-wise scaling as extension to plain adapters, which proves to be beneficial for computer vision tasks. (3) Finally, we present Adapter+, an adapter configu- ration with an excellent parameter-accuracy trade-off com- pared to other work, as shown in Fig. 1. Adapter+ reaches a state-of-the-art average accuracy of 77.6% on VTAB [65] without any hyperparameter optimization per task and 3.7 percentage points (pp) over previous adapter baselines. We also reach a state-of-the-art accuracy of 90.7% on FGVC [31] with the lowest number of parameters compared to other methods. Finally, Adapter+ shows the best robustness in terms of accuracy across the VTAB subgroups, see Fig. 2. 2. Related work One possibility to adapt a pre-trained network to a novel task, apart from full fine-tuning, is to only selectively tune some of the parameters, e.g., only training the classifier [12]. Cai et al. [3] proposed to tune only the biases of an otherwise frozen network to adapt it to a downstream task. BitFit [ 64] then showed the efficacy of this method for NLP transformers. Modular adaptation. The concept of adding small, train- able modules with only a few parameters to an otherwise frozen network was first proposed for adapting CNNs by Rebuffi et al. [53] and called adapters. Other approaches replaced all convolutions in the network with depth-wise separable convolutions and only tuned their spatial parts [19], learned binary masks to prune a pre-trained network per target task [ 41], or created a student network by aug- menting the original network with adapter-like modules and skip connections, which then mimicked a teacher network by disabling parts of its pre-trained and added modules [43]. Following the rise of transformers in NLP [ 11, 50, 59], Houlsby et al. [28] proposed adapter modules in the form of bottlenecks for transformer layers. Pfeiffer et al. [49] conducted an architecture search on NLP tasks to find a more parameter-efficient configuration of adapter modules that only acts on the transformer’s feed-forward network (FFN), thus saving roughly half of the parameters over [28]. Prompt tuning. Inspired by changing the output of a net- work for NLP with hand-crafted textual prompts, which modifies the attention over the original input tokens, Lester et al. [37] proposed prompt tuning: A set of learnable to- kens is added to the input sequence and trained with back- propagation to prompt a frozen language model to perform downstream tasks. Li and Liang [38] extended on prompt tuning by adding learnable tokens at every transformer layer of the model, which they termed prefix tuning. Jia et al. [31] applied prompt tuning to vision transformers, then called visual prompt tuning (VPT), by preprending the sequence of image patch embeddings with such trainable tokens (VPT- Shallow). They also showed a variant resembling prefix tuning with stronger adaptation capabilities that adds tokens at every layer of the network (VPT-Deep). Low-rank approaches. Also focusing on the attention part of the transformer layers, Hu et al. [29] proposed low-rank adaptation (LoRA) where the attention weights are updated with low-rank decomposition matrices. The matrices can be merged with the attention weights for inference. The structure of LoRA is very similar to an adapter, which can be seen as a superset of LoRA acting on the transformer’s FFN. He et al. [22] proposed a formalism to unify LoRA, adapters, and prefix tuning [ 38]. It allowed them to combine the beneficial aspects of all three methods into a scaled parallel adapter (Scaled PA) for NLP tasks. AdaptFormer [6] then applied the concept of Scaled PA to vision transformers. 2Other related work. Newer approaches for vision trans- formers proposed different techniques to further enhance the parameter-accuracy trade-off in adaptation. NOAH [67] per- forms an architecture search for a combination of adapters, LoRA, and VPT for each task. SSF [39] scales and shifts the features in the network after every operation, i.e., attention, FFN, layer normalization, with task-specific, trainable mod- ules. Jie and Deng [32] aggregate the weights of a ViT into a single 3D tensor. Task-specific weight updates of this tensor are learned as a matrix decomposed into parameter-efficient factors, hence they termed their method factor-tuning (FacT). SPT [21] measures the importance of the weights of a pre- trained network for a downstream task. Based on a desired parameter budget, the most important parameters are chosen for tuning and adapters or LoRA are used for weight matrices that contain enough parameters of importance. Consolidator [20] adapts weights in multiple orderings of channel-wise groups. The updates for all groups are merged for efficient storage and inference. Despite these new developments, we show that the sim- ple concept of adapters exhibits an even better parameter- accuracy trade-off in combination with vision transformers – if done right and with the addition of a channel-wise scaling. 3. Adapters for vision transformers 3.1. Vision transformer basics In this work, we concentrate on the parameter-efficient adaptation of vision transformers (ViT) [ 13]. The ViT is closely modeled after the transformer model for natural lan- guage processing (NLP) proposed by Vaswani et al. [59]. A learned linear projection embeds non-overlapping and flat- tened patches of the input image into a sequence of n tokens x ∈ Rn×d, where d is called the hidden dimension of the transformer. A positional encoding is added to the embed- dings and the sequence is prepended with a trainable [CLS] token. The sequence length and the dimension of the tokens stay fixed throughout the architecture. The sequence is sent through consecutive transformer layers that each consist of a multi-head self-attention and a feed-forward network (FFN). For the self-attention, the tokens are projected to queries, keys, and values (Q, K, and V ) and the output of each of the M attention heads is calculated as Attention(x) =Softmax \u0012Q(x)K(x)T √ d′ \u0013 V (x), (1) with d′ = d/M being the inner dimension of the head. The FFN consists of a multilayer perceptron with two linear layers (with weights Wi and biases bi) and a GELU [ 26] non-linearity as activation in between: FFN(x) =GELU(xW1 + b1)W2 + b2. (2) Both attention and FFN are employed with a preceding layer normalization (LN) [1] and a skip connection and, therefore, transform an input sequence x sequentially as x 7→ Attention(LN(x)) +x (3a) x 7→ FFN(LN(x)) +x. (3b) To keep the notation concise, we will omit the LNs of atten- tion and FFN in the following; each attention and FFN is assumed to be always preceded by an LN. 3.2. Adapters and their inner structure Adapters [28] are small modules that are added to the trans- former layers. They allow to tailor a network to a new task or domain, where instead of tuning the parameters of the whole network, only the adapter parameters and the classi- fier are trained. Adapters take the form of bottlenecks with an inner dimension of r ≪ d. We call r the rank of the adapter. In detail, a down-projection to dimension r with weights Wdown ∈ Rd×r and biases bdown ∈ Rr is followed by a non-linear activation function σ(·), typically a GELU [26] as used throughout the ViT, and an up-projection with weights Wup ∈ Rr×d and biases bup ∈ Rd back to the hid- den dimension d of the transformer layer. This yields a base adapter module Adapterbase(x) =σ(xWdown + bdown)Wup + bup . (4) The base adapter module can be further enhanced with a normalization layer, e.g., a layer normalization (LN) [ 1]. Additionally, the output of the bottleneck can be scaled by s as Adapter(x) =s · Adapterbase \u0000 LN(x) \u0001 . (5) For layer-wise scaling, the factor s is taken to be a scalar, i.e. s ∈ R, and can be either fixed as a hyperparameter or learned during training. Layer-wise scaling was proposed by He et al. [22] and Hu et al. [29] but deemed not effective compared to a fixed scaling for tasks in NLP. Here, we additionally propose to use a channel-wise, learned scaling where s ∈ Rd. We investigate its capabilities in Sec. 4.3. In most cases, the adapter is used with a skip connection, hence the complete feature transformation becomes x 7→ Adapter(x) +x. (6) The complete inner structure of an adapter including its skip connection is visualized in Fig. 3a. 3.3. Adapter positions Although the architecture of bottleneck adapters for trans- formers is rather simple, there are various ways to plug them into the transformer layer. Previous work has not yet in- vestigated what the optimum position is for the use with a ViT [13]. Here, we evaluate four possible adapter positions, shown in Figs. 3b to 3e. We postulate that it is easier for an adapter to learn to modify features previously transformed 3FF down FF up Act LN Scaling (a) Inner structure FFN Adapter (b) Pre FFN Adapter (c) Post FFN Adapter (d) Parallel FFN Adapter (e) Intermediate Figure 3. Illustrations of (a) the inner structure of an adapter with feed-forward layers (FF), activation layer (Act), and optional layer normalization (LN) and scaling, (b)–(d) different possible adapter positions to connect the adapter to the FFN section of the transformer layer. Modules with trainable parameters are shown in red and frozen modules in blue. by a frozen module in the network rather than to anticipate what changes to the features are needed in adapting for a frozen module that follows the adapter. Putting it differently, we argue that the adapter should follow a frozen module. Pre-Adapter. The first adapter position we analyze ap- plies the adapter to the output x of the attention section of the transformer layer before it is passed into the FFN, but with the skip connection of the attention already added (Fig. 3b). The feature transformation of the FFN section with the adapter attached, therefore, becomes x 7→ FFN \u0000 Adapter(x) +x \u0001 + \u0000 Adapter(x) +x \u0001 . (7) Note that the two occurrences of Adapter(x) in Eq. (7) refer to the same instantiation. In this configuration, the adapter has the full information from the feature transformation hap- pening in the attention but needs to estimate the transforma- tion that will be happening in the FFN that follows. As a result, especially the last FFN before the linear classifier will be hard to adapt. To the best of our knowledge, this adapter position has not been considered in the literature. Post-Adapter. In this case, the adapter is positioned at the very end of the transformer layer on the output of the FFN with its skip connection added as x 7→ Adapter \u0000 FFN(x) +x \u0001 + \u0000 FFN(x) +x \u0001 , (8) where the FFNs refer to the same intantiation (Fig. 3c). That way, the adapter has access to the feature transformation happening in the FFN and the unmodified features via the skip connection. This position has been proposed by Pfeiffer et al. [49] as the result of an architecture search, but only for adapting transformers for NLP tasks and not for a ViT. Parallel-Adapter. Next, we consider a parallel setting as proposed by [22], where the adapter is located parallel to the FFN and both share a skip connection (Fig. 3d): x 7→ FFN(x) +Adapter(x) +x. (9) Therefore, both adapter and FFN work on the output of the attention section of the transformer layer and the adapter needs to learn the necessary residual transformation to the one produced by the frozen FFN. Intermediate-Adapter. Finally, we consider the original adapter position as proposed by Houlsby et al. [28]. The adapter is plugged behind the FFN but before the skip con- nection of the FFN is added (Fig. 3e). The adapter addition- ally possesses its own skip connection: x 7→ Adapter \u0000 FFN(x) \u0001 + FFN(x) +x. (10) Note that the two occurrences of FFN(x) in Eq. (10) refer to the same instantiation. The adapter sees the transformed features coming from the FFN but cannot access the features added later on by the skip connection of the FFN. 3.4. Initialization of adapter parameters Since training a deep learning model is a non-convex opti- mization problem, the initialization of parameters is impor- tant. In this work, we evaluate three different variants of parameter initializations for adapters proposed in the litera- ture. All of them have the goal to initialize the adapters in a way that minimizes the initial influence of the adapters at the start of their training. This is a sensible goal since adapters extend an already pre-trained frozen network. Houlsby initialization. Houlsby et al. [28] propose to draw the weights of the projection matrices from a zero-centered Gaussian distribution with a standard deviation of σ = 0.01, truncated at 2σ, and use zero for their biases. BERT initialization. For the BERT model [11], the initial- ization works similar to [28] but the Gaussian distribution has a standard deviation of σ = 0.02 and is not truncated. This form of initialization is used by Pfeiffer et al. [49]. LoRA initialization. LoRA [29] initializes the weights and biases of the down-projection with a uniform Kaiming He ini- tialization [23]; the weights and biases of the up-projection 4are initialized to zero. Therefore, the output of the adapter at the beginning of training equals zero and the adapter initially does not contribute. 3.5. Data normalization in pre-processing Data normalization is common practice during image pre- processing. It is typically done by shifting and scaling of each input pixel xij for each channel c as ˆxijc = (xijc − µc)/σc . (11) Most widely used are the mean µ = (0.485, 0.456, 0.406)T and standard deviation σ = (0.229, 0.224, 0.225)T of the ImageNet dataset [55], commonly referred to as ImageNet normalization. Another option is using 0.5 for every element of µ and σ, which is commonly referred to as Inception normalization because it is used for the Inception family of CNN architectures, starting with Inception-v3 [58]. The Im- ageNet normalization aims to center the input data around 0 with a standard deviation of 1. The Inception normalization, on the other hand, transforms the input values such they are strictly in range [−1, 1]. Because we try to adapt to a target domain on a very low parameter budget, it is important to use the data normaliza- tion the network saw during its pre-training. Otherwise, the parameter-efficient transfer method of choice needs to first compensate for the shift in input data statistics and loses parts of its capacity to adapt to the target domain. 4. Experiments 4.1. Datasets In order to carry out a detailed study of the utility of adapters in the context of ViT models, we experiment with two stan- dard benchmarks for task adaptation. VTAB. The Visual Task Adaptation Benchmark (VTAB) [65] consists of 19 tasks, which are further grouped into three categories: Natural, Specialized, and Structured. The Natural group contains natural images captured using stan- dard photographic equipment. The Specialized group is built from datasets of images captured with specialized equip- ment, from remote sensing and medical domains. Lastly, the Structured group is for evaluating the understanding of the scene structure. Here, the majority of the datasets are compiled from synthetic images with scenes that are easy to assess for humans but have a large domain gap to natural image datasets. Each task of VTAB consists of 800 train- ing and 200 validation images. The test sets have the same number of images as the test sets in the original datasets. FGVC. Following Jia et al. [31], we compile five datasets for fine-grained visual classification (FGVC): CUB-200- 2011 [61], NABirds [ 27], Oxford Flowers [ 45], Stanford Dogs [34], and Stanford Cars [17]. Because VTAB bench- marks task adaptation in a low-data regime in terms of the Table 1. Adapter position. We report the average accuracy in % (± std. dev.) on the VTABval sets for different adapter positions. Adapterbase with Houlsby initialization and rank r=8 is used in all experiments. Position Natural Specialized Structured Average Pre 82.4 ± 0.4 86.2 ± 0.8 57.5 ± 0.5 75.3 ± 0.3 Intermediate 83.0 ± 0.4 85.0 ± 0.8 57.2 ± 0.5 75.1 ± 0.3 Parallel 83.0 ± 0.3 86.2 ± 0.6 57.7 ± 0.6 75.6 ± 0.3 Post 83.0 ± 0.3 85.7 ± 0.4 59.1 ± 0.3 76.0 ± 0.2 number of available training images, we use FGVC to eval- uate adaptation methods in settings where training data is abundant. Where validation sets are not available in FGVC, we follow Jia et al. [31] to create the validation splits. For further details regarding the dataset properties of VTAB and FGVC, see supplemental material. 4.2. Experimental settings For all our experiments, we use a ViT-B/16 network [13] that was pre-trained on ImageNet-21k [55]. We follow its pre- training settings, in particular, regarding input data normal- ization. We train all models with an AdamW [40] optimizer with a learning rate of 10−3, a weight decay of 10−4, and a batch size of 64, following [ 67]. For full fine-tuning, we use a learning rate of 10−4, which we found leads to better results. We use a cosine learning rate schedule with a linear warm-up over the first 10 epochs and train for 100 epochs in total. We use stochastic depth with linearly increasing drop rates as a function of network depth from 0 to 0.1 for the frozen network and with a drop rate of 0.1 for the adapters during training. Apart from data normalization (cf . Sec. 3.4), we resize input images to 224×224 px for VTAB and use a randomly resize crop to 224×224 px and horizontal flipping for FGVC. For the ablations and to determine hyperparam- eters, we evaluate on the validation splits. We include the validation sets in the training data for producing final results. 4.3. Exploring adapter configurations Adapter position. We first evaluate the four possible posi- tions to connect an adapter to the FFN section of the trans- former layer, as described in Sec. 3.3. In our ablation, we use Adapterbase (cf . Eq. (4)) with rank r=8 and use the Houlsby initialization. In this experiment, the adapters neither have a layer normalization nor use scaling. The results on the VTAB validation set for all four adapter positions are presented in Tab. 1. The Post-Adapter yields the best result with 76.0% average accuracy over all VTAB subgroups. It confirms our hypothesis that the adapter should follow the frozen FFN module because it can then post-hoc modify the features flowing through the network. The par- allel configuration comes in second with 75.6% average accuracy, receiving the same input as the FFN but having to 5Table 2. Inner adapter structure. We evaluate the different com- ponents of the adapter structure, e.g., normalization layer (Norm), layer-wise and channel-wise learnable scaling on the VTAB val sets. The difference to Adapterbase (first row) is shown in ∆base. Bias Norm Scaling Initialization Accuracy (%) ∆base ✓ Houlsby 76.0 ± 0.2 0.0 Houlsby 75.6 ± 0.4 −0.4 ✓ LoRA 75.5 ± 0.3 −0.5 ✓ BERT 75.8 ± 0.3 −0.2 ✓ ✓ Houlsby 75.9 ± 0.3 −0.1 ✓ ✓ layer Houlsby 75.9 ± 0.3 −0.1 ✓ layer Houlsby 76.2 ± 0.3 +0.2 ✓ ✓ channel Houlsby 75.8 ± 0.3 −0.2 ✓ channel Houlsby 76.5 ± 0.2 +0.5 learn a residual modification to the FFN instead of a subse- quent one. Pre-Adapter and Intermediate-Adapter are subpar compared to the other positions. They either do not have access to the feature transformation happening afterwards in the FFN or to the features of the skip connection containing the output of the attention. Inner structure. Next, we investigate the impact of the in- ner structure of adapters including their initialization. Tab. 2 shows our findings with average accuracies calculated over the three VTAB subgroups. Removing the biases from the linear layers leads to a decrease in accuracy of 0.4 percent- age points (pp). We find that the Houlsby initialization of the adapter parameters is best while BERT and LoRA initializa- tions reduce the accuracy by 0.2 pp and 0.5 pp. Adding layer normalization (LN) to the adapter is slightly detrimental for all settings, both with scaling and without, while addition- ally adding 2d parameters per layer. We find that a learned scaling is in general beneficial for image-classification tasks. Adding layer-wise scaling leads to a gain of 0.2 pp. The inclusion of a learned, channel-wise scaling, as proposed here, gives the strongest improvement of 0.5 pp, reaching an accuracy of 76.5% on the VTAB validation set while only adding half of the parameters compared to LN. What makes a great adapter? From our systematic explo- ration of possible adapter configurations, we conclude that adapter modules in the Post-Adapter position with a learn- able, channel-wise scaling and Houlsby initialization work best for computer vision tasks. We call our proposed adapter configuration Adapter+. The addition of layer normaliza- tion, as suggested by Pfeiffer et al. [49], is not necessary and even leads to detrimental effects in our setting. Configurations from previous work. Different configu- rations of adapters have been established in previous work. We compare their configurations to our systematic approach with rank r=8 on the VTAB validation sets. Using our own implementations already leads to better results than reported in literature but enables us to compare on equal footing. Houlsby et al. [28] use an Intermediate-Adapter with their Table 3. Comparison of Adapter+ with adapter configurations from previous work. We report the average accuracy in % (± std. dev.) of each subgroup and across all groups on the VTAB val sets. Configuration # Param (M) Natural Specialized Structured Average Houlsby [28], r=8 0.39 82.9 ± 0.2 85.5 ± 0.3 58.9 ± 0.8 75.8 ± 0.3 Houlsby [28], r=4 0.24 82.9 ± 0.4 84.9 ± 0.3 58.3 ± 0.6 75.4 ± 0.3 Pfeiffer [49] 0.21 82.9 ± 0.3 86.1 ± 0.9 58.4 ± 0.7 75.8 ± 0.4 AdaptFormer [6] 0.19 83.0 ± 0.4 85.0 ± 0.2 57.4 ± 0.5 75.2 ± 0.2 Adapter+ 0.20 83.0 ± 0.2 86.8 ± 0.6 59.7 ± 0.4 76.5 ± 0.2 proposed initialization both at the FFN section as well at the attention part of the transformer layer. Additionally, they adapt the LN parameters of the backbone. We, therefore, compare their setting additionally with r = 4to compare on roughly the same parameter budget. Pfeiffer et al. [49] suggest a Post-Adapter like us but with a BERT initialization and they employ a layer normalization inside the adapter. AdaptFormer [6] has the same configuration as a scaled parallel adapter (Scaled PA) [22], which was proposed for NLP tasks, the only difference being the layer-wise scalings. Scaled PA uses a fixed scaling of s = 4 for the adapters whereas AdaptFormer suggests to use s = 0.1 for vision tasks. Optimizing s for VTAB may lead to better results. Our results are presented in Tab. 3. We see a clear advantage of our Adapter+ configuration, gaining at least 0.7 pp over all previous adapter realizations considered despite having the second lowest number of trainable parameters. 4.4. Main results VTAB. We evaluate Adapter+ on the VTAB test sets and compare to other methods in Tab. 4. We provide results for full fine-tuning and tuning only the linear classifier while freezing the rest of the backbone [12] as a baseline of classi- cal fine-tuning methods. As competing parameter-efficient tuning methods, we include LoRA [29], VPT [31], NOAH [67], SSF [39], FacT [32], Consolidator [20], and SPT [21]. Wherever possible, we re-evaluate the other methods with a suitable data normalization for the pre-trained backbone and after the full training schedule to enable a fair compar- ison. For LoRA, we use our own implementation because the original work does not cover VTAB. For VPT, we adopt the number of tokens per task from their hyperparameter optimization but find that we do not need to tune learning rate and weight decay per task. Additionally, deviating from the original implementation, we optimize with AdamW [40] instead of SGD [54] and change to an appropriate data nor- malization. We present the original results from [ 31] on VTAB together with our re-evaluation. Our improved imple- mentation of VPT increases the average accuracy by 4.4 pp from 72.0% to 76.4%. SSF, FacT, and SPT released code to evaluate on VTAB. For FacT and SPT, we change the data normalization to match the backbone; SSF already uses the correct one. We re-run the provided code and present the 6Table 4. Detailed results on the VTAB test sets. We report original results and re-evaluations (⟳) in % after a complete training schedule with suitable data normalization. Grayed out numbers are not included in the ranking for best and second best results. †: Early-stopping based on the test set, •: unsuitable data normalization, E: per-task hyperparameter optimization. 1Average across the average accuracies of the VTAB groups, following previous work. 2No complete code release for Consolidator, hence training and evaluation details are unknown. Natural Specialized Structured # Param (M) Cifar100 [35] Caltech101 [15] DTD [8] Flower102 [45] Pets [47] SVHN [44] Sun397 [62] Average Camelyon [60] EuroSAT [25] Resisc45 [7] Retinopathy [14] Average Clevr-Count [33] Clevr-Dist. [33] DMLab [2] KITTI-Dist. [18] dSpr-Loc. [42] dSpr-Ori [42] sNORB-Azi. [36] sNORB-Ele. [36] Average Global Average1 Full 85.8 73.2 92.6 70.4 97.9 86.2 90.6 39.6 78.6 87.1 96.6 87.5 74.0 86.3 66.6 61.0 49.8 79.7 82.6 51.9 33.5 37.0 57.8 74.2 Linear 0.04 78.1 88.1 69.0 99.1 90.0 36.0 56.9 73.9 79.8 90.7 73.7 73.7 79.5 32.4 30.5 35.9 61.9 11.2 26.2 14.3 24.5 29.6 61.0 LoRA [29] 0.29 83.0 91.7 71.6 99.2 90.9 83.8 56.7 82.4 86.2 95.7 83.5 71.9 84.3 77.7 62.3 49.0 80.2 82.2 51.7 31.0 47.0 60.1 75.6 VPT-Deep E• [31] 0.60 78.8 90.8 65.8 98.0 88.3 78.1 49.6 78.5 81.8 96.1 83.4 68.4 82.4 68.5 60.0 46.5 72.8 73.6 47.9 32.9 37.8 55.0 72.0 VPT-Deep E⟳ 0.60 83.0 93.0 71.2 99.0 91.3 84.1 56.0 82.5 84.9 96.6 82.5 74.5 84.6 77.5 58.7 49.7 79.6 86.2 56.1 37.9 50.7 62.1 76.4 NOAH E†•◦ [67] 0.43 69.6 92.7 70.2 99.1 90.4 86.1 53.7 80.2 84.4 95.4 83.9 75.8 84.9 82.8 68.9 49.9 81.7 81.8 48.3 32.8 44.2 61.3 75.5 SSF E† [39] 0.24 69.0 92.6 75.1 99.4 91.8 90.2 52.9 81.6 87.4 95.9 87.4 75.5 86.6 75.9 62.3 53.3 80.6 77.3 54.9 29.5 37.9 59.0 75.7 SSF E⟳ 0.24 61.9 92.3 73.4 99.4 92.0 90.8 52.0 80.3 86.5 95.8 87.5 72.8 85.7 77.4 57.6 53.4 77.0 78.2 54.3 30.3 36.1 58.0 74.6 FacT-TK8 E†• [32] 0.05 70.3 88.7 69.8 99.0 90.4 84.2 53.5 79.4 82.8 95.6 82.8 75.7 84.2 81.1 68.0 48.0 80.5 74.6 44.0 29.2 41.1 58.3 74.0 FacT-TK8 E⟳ 0.05 74.9 92.7 73.7 99.1 91.3 85.5 57.7 82.1 86.8 94.9 84.1 70.9 84.2 81.9 64.1 49.2 77.2 83.8 53.1 28.2 44.7 60.3 75.5 FacT-TK≤32 E†• [32] 0.10 70.6 90.6 70.8 99.1 90.7 88.6 54.1 80.6 84.8 96.2 84.5 75.7 85.3 82.6 68.2 49.8 80.7 80.8 47.4 33.2 43.0 60.7 75.6 FacT-TK≤32 E⟳ 0.10 74.6 93.7 73.6 99.3 90.6 88.7 57.5 82.6 87.6 95.4 85.5 70.4 84.7 84.3 62.6 51.9 79.2 85.5 52.0 36.4 46.6 62.3 76.5 Consolidator 2 [20] 0.30 74.2 90.9 73.9 99.4 91.6 91.5 55.5 82.4 86.9 95.7 86.6 75.9 86.3 81.2 68.2 51.6 83.5 79.8 52.3 31.9 38.5 60.9 76.5 SPT-Adapter †• [21] 0.23 72.9 93.2 72.5 99.3 91.4 84.6 55.2 81.3 85.3 96.0 84.3 75.5 85.3 82.2 68.0 49.3 80.0 82.4 51.9 31.7 41.2 60.8 75.8 SPT-Adapter ⟳ 0.22 74.7 94.1 73.0 99.1 91.2 84.5 57.5 82.0 85.7 94.9 85.7 70.2 84.1 81.3 63.2 49.1 80.7 83.5 52.0 26.4 41.5 59.7 75.3 SPT-Adapter †• [21] 0.43 72.9 93.2 72.5 99.3 91.4 88.8 55.8 82.0 86.2 96.1 85.5 75.5 85.8 83.0 68.0 51.9 81.2 82.4 51.9 31.7 41.2 61.4 76.4 SPT-Adapter ⟳ 0.43 74.9 93.2 71.6 99.2 91.1 87.9 57.2 82.2 87.0 95.4 86.5 72.4 85.3 81.1 63.2 50.3 80.2 84.4 51.4 31.5 42.2 60.5 76.0 Adapter+, r=1 0.07 85.4 92.4 73.1 99.1 91.3 83.1 58.1 83.2 87.2 96.6 85.3 72.6 85.5 80.7 60.6 50.9 79.9 83.3 55.6 27.1 43.0 60.1 76.3 Adapter+, r=2 0.09 85.4 93.0 72.7 99.2 90.6 85.3 58.0 83.5 87.9 96.8 85.5 71.4 85.4 83.2 61.0 51.6 80.1 86.1 56.3 30.7 46.5 61.9 76.9 Adapter+, r=4 0.13 84.8 93.8 72.7 99.2 90.6 86.5 57.4 83.6 87.5 96.9 85.9 71.5 85.4 83.4 61.6 53.6 81.4 87.3 55.3 34.4 48.1 63.1 77.4 Adapter+, r=8 0.20 84.6 94.2 72.3 99.3 90.7 87.6 56.7 83.6 87.7 97.0 86.7 72.3 85.9 83.2 60.9 53.8 80.3 88.1 55.6 35.7 47.7 63.1 77.6 Adapter+, r=16 0.35 83.7 94.2 71.5 99.3 90.6 88.2 55.8 83.3 87.5 97.0 87.4 72.9 86.2 82.9 60.9 53.7 80.8 88.4 55.2 37.3 46.9 63.3 77.6 Adapter+, r∈[1..4] E 0.11 85.4 93.8 72.7 99.1 90.6 86.5 58.1 83.7 87.5 96.8 85.9 71.4 85.4 83.4 61.0 53.6 81.4 87.3 55.3 34.4 48.1 63.1 77.4 Adapter+, r∈[1..8] E 0.16 85.4 93.8 72.7 99.1 90.7 87.6 58.1 83.9 87.7 96.8 86.7 72.3 85.9 83.4 60.9 53.8 80.3 88.1 55.3 35.7 47.7 63.1 77.7 Adapter+, r∈[1..32] E 0.27 85.4 93.8 72.7 99.1 90.7 88.2 58.1 84.0 87.5 96.8 87.8 73.9 86.5 83.4 60.9 53.8 80.3 87.2 55.3 37.9 47.7 63.3 77.9 results after a full training schedule. For completeness, we also report the results from the original publications. How- ever, we found that the code releases of [21, 32, 39] use early stopping based on the best result on the test set. We argue that tuning hyperparameters such as the number of training epochs on the test set goes against established practices in machine learning; rather the validation set should be used for early stopping. Yet, due to the limited size of the training and validation sets in VTAB, it is not feasible to report test results without also training on the validation data. Hence, we chose to complete a full training schedule of 100 epochs instead of using early stopping. Training SSF for the full schedule leads to a decrease in average accuracy of 1.1 pp over the original publication and re-evaluating SPT leads to a decrease of up to 0.5 pp, even with a corrected data normalization. FacT on the other hand benefits from our re-revaluation, since the accuracy decrease from training a complete schedule is offset by improvements from applying the appropriate data normalization. There was no complete code release with configurations to train Consolidator on VTAB at the time of writing, hence we report results as-is. Adapter+ shows the best parameter-accuracy trade-off among all methods evaluated. This can also be clearly seen in Fig. 1. Additionally, Adapter+ sets a new state of the art with an average accuracy of up to 77.6% over all VTAB subgroups even without any per-task hyperparameter optimization. If we determine the optimal rankr per task on the validation set, we can further improve the accuracy to 77.9%. Optimizing the rank leads to a better parameter-accuracy trade-off than using a fixed rank across all tasks. In Fig. 2, we compare the average accuracy on the sub- groups of VTAB. Wherever possible, we present the results of re-evaluating methods after the last training epoch and matching the data normalization to the backbone. The aver- age accuracies of Adapter+ with r ∈ [1..32] are consistently higher than those of the competing methods. Note that the accuracies of other methods except SPT differ drastically across the different VTAB subgroups. Adapter+, on the other hand, shows a high degree of robustness to the domain shifts between groups. FGVC. Next, we present our results on the FGVC bench- mark in Tab. 5. From the contenders, only SSF [ 39] has released code and hyperparameter configurations for train- ing on FGVC at the time of writing. As we know from the code releases for VTAB, the reported numbers show the accuracy for early stopping based on the test set. There- fore, we expect a similar evaluation for FGVC. While we do not endorse early stopping based on the test set, we ad- 7Table 5. Detailed results on the FGVC test sets. We report original results and re-evaluations (⟳) in % after a complete training schedule with suitable data normalization. Grayed out numbers are not included in the ranking for best and second best results. # Param (M) CUB200 [61] NABirds [27] Oxford Flowers [45] Stanford Dogs [34] Stanford Cars [17] Average Full 86.0 88.0 81.5 99.2 85.6 90.6 89.0 Linear 0.18 88.9 81.8 99.5 92.6 52.8 83.1 VPT-Deep [31] 0.85 88.5 84.2 99.0 90.2 83.6 89.1 VPT-Deep ⟳ 0.85 90.1 83.3 99.6 90.3 85.0 89.7 SSF [39] 0.39 89.5 85.7 99.6 89.6 89.2 90.7 SSF ⟳ 0.39 88.9 85.0 99.6 88.9 88.9 90.3 SPT-Adapter [21] 0.40 89.1 83.3 99.2 91.1 86.2 89.8 SPT-LoRA [21] 0.52 88.6 83.4 99.5 91.4 87.3 90.1 Adapter+, r∈[1..32] 0.34 90.0 83.2 99.6 91.6 89.1 90.7 Adapter+ (best epoch) 0.34 90.4 85.0 99.7 92.6 89.1 91.4 ditionally provide numbers for that setting in Tab. 5 for the sake of comparability. Even when training for a complete schedule, Adapter+ shows the best average accuracy with 90.7% over all five datasets in FGVC,0.4 pp over the second best method under similar evaluation. When early stopping with the test set, Adapter+ reaches 91.4% average accuracy, 0.7 pp over the second best method and 2.4 pp better than full fine-tuning. This demonstrates that Adapter+ also yields state-of-the-art results for task adaptation when training data is abundant while having the best parameter efficiency. 4.5. Ablations Data normalization. We showcase the effect of using an unsuitable data normalization for the chosen ViT in Tab. 6. The gap between ImageNet and Inception normalization (see Sec. 3.5) is largest for VPT [31], with a 3.4 pp difference in average accuracy, which explains around two-thirds of the gain for our re-evaluation as shown in Fig. 1. We suspect that VPT has less of an ability to scale and shift the data because the learnable tokens only act on the attention mechanism. LoRA [29], FacT [32], and adapters all employ linear layers that can directly scale and shift the features of the frozen backbone and thus compensate better for improper data nor- malization. It is worth mentioning that our Adapter+ is the most robust to improper normalization out of the methods evaluated, with a gap of only 2.6 pp average accuracy. Training regularization. We investigate the importance of training regularization methods like stochastic depth [30] and dropout [16] for training adapters on a frozen ViT backbone and evaluate on the VTAB validation sets. We use linearly increasing drop rates as a function of network depth from 0 to 0.1 for the frozen layers of the ViT model, and a drop rate Table 6. Effects of ImageNet vs. Inception data normalization. All methods are evaluated on the VTABval sets. In column ∆Average we report the increase in accuracy in pp across all VTAB subgroups. ImageNet norm Inception normNatural Specialized Structured Average Natural Specialized Structured Average ∆Average VPT 79.2 83.0 53.8 72.0 82.2 86.2 57.9 75.4 3.4 LoRA 78.4 84.1 53.2 71.9 82.0 85.8 56.4 74.7 2.8 FacT-TK 78.0 83.3 56.1 72.4 81.6 85.6 58.1 75.1 2.7 Adapter+ 80.5 85.0 56.0 73.9 83.0 86.8 59.7 76.5 2.6 Table 7. Influence of training regularization. We evaluate accu- racy in % with Adapterbase with rank r=8 on the VTAB val sets. Adapter Stochastic Depth Dropout None ViT Stochastic Depth 76.0 75.4 75.3 None 74.5 74.3 73.7 of 0.1 when using dropout or stochastic depth for the adapter modules. The results in Tab. 7 show a clear benefit for using stochastic regularization for the frozen layers as well as the adapters during training. Using dropout in the adapters is only slightly better than no regularization for adapters, with a gain of only 0.1 pp. With an increase in accuracy of 0.7 pp, stochastic depth is the preferred regularization method for adapters. However, our results show that the more important part is the stochastic depth regularization for the frozen modules of the ViT backbone. Disabling it in training leads to a loss of 1.5 pp accuracy compared to a training where stochastic depth is used throughout the model. 5. Conclusion Applied at the right position and with an optimal inner struc- ture, the simple concept of adapters produces state-of-the-art results for task adaptation. To understand how adapters can “strike back”, we conducted the first systematic and in-depth study on how to best construct adapters and integrate them with vision transformers. This allowed us to determine the optimal connection point for the adapter in the transformer layer. Further, we proposed to use a learnable, channel-wise scaling and showed its benefit for computer vision tasks. Our insights led us to the creation of Adapter+ that yields the highest accuracy and the best parameter-accuracy trade-off on VTAB (77.6%, 0.2M) without any per-task hyperparame- ter optimization and on FGVC (90.7%, 0.34M), showing its superiority over more complicated methods. Acknowledgements. This work has been funded by the LOEWE initiative (Hesse, Germany) within the emergenCITY center. 8References [1] Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. arXiv:1607.06450 [stat.ML], 2016. [2] Charles Beattie, Joel Z. Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich Küttler, Andrew Lefrancq, Si- mon Green, Víctor Valdés, Amir Sadik, Julian Schrittwieser, Keith Anderson, Sarah York, Max Cant, Adam Cain, Adrian Bolton, Stephen Gaffney, Helen King, Demis Hassabis, Shane Legg, and Stig Petersen. DeepMind Lab. arXiv:1612.03801 [cs.AI], 2016. [3] Han Cai, Chuang Gan, Ligeng Zhu, and Song Han. TinyTL: Reduce memory, not parameters for efficient on-device learn- ing. In NeurIPS*2020. [4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End- to-end object detection with transformers. In ECCV, pages 213–229, 2020. [5] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg- ing properties in self-supervised vision transformers. InICCV, pages 9630–9640, 2021. [6] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yib- ing Song, Jue Wang, and Ping Luo. AdaptFormer: Adapt- ing vision transformers for scalable visual recognition. In NeurIPS*2022. [7] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark and state of the art. Proc. IEEE, 105(10):1865–1883, 2017. [8] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In CVPR, pages 3606–3613, 2014. [9] Ekin Dogus Cubuk, Barret Zoph, Jonathon Shlens, and Quoc Le. RandAugment: Practical automated data augmentation with a reduced search space. In NeurIPS*2020. [10] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdul- mohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme Ruiz, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin Fathy Elsayed, Aravindh Mahen- dran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bast- ings, Mark Collier, Alexey A. Gritsenko, Vighnesh Birod- kar, Cristina Nader Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip Pavetic, Dustin Tran, Thomas Kipf, Mario Lucic, Xiaohua Zhai, Daniel Keysers, Jeremiah J. Harmsen, and Neil Houlsby. Scaling vision transformers to 22 billion parameters. In ICML, pages 7480–7512, 2023. [11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional trans- formers for language understanding. In NAACL-HLT, pages 4171–4186, 2019. [12] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. DeCAF: A deep convolutional activation feature for generic visual recognition. In ICML, pages 647–655, 2014. [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. [14] Emma Dugas, Jorge Jared, and Will Cukierski. Diabetic retinopathy detection. Kaggle, 2015. [15] Li Fei-Fei, Robert Fergus, and Pietro Perona. One-shot learn- ing of object categories. IEEE T. Pattern Anal. Mach. Intell., 28(4):594–611, 2006. [16] Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model uncertainty in deep learn- ing. In ICML, pages 1050–1059, 2016. [17] Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, and Li Fei-Fei. Fine-grained car detection for visual census estimation. In AAAI, pages 4502–4508, 2017. [18] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The KITTI dataset. Int. J. Robotics Res., 32(11):1231–1237, 2013. [19] Yunhui Guo, Yandong Li, Liqiang Wang, and Tajana Rosing. Depthwise convolution is all you need for learning multiple visual domains. In AAAI, pages 8368–8375, 2019. [20] Tianxiang Hao, Hui Chen, Yuchen Guo, and Guiguang Ding. Consolidator: Mergable adapter with group connections for visual adaptation. In ICLR, 2023. [21] Haoyu He, Jianfei Cai, Jing Zhang, Dacheng Tao, and Bohan Zhuang. Sensitivity-aware visual parameter-efficient tuning. In ICCV, 2023. [22] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg- Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. In ICLR, 2022. [23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level perfor- mance on imagenet classification. In ICCV, pages 1026–1034, 2015. [24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. InCVPR, pages 770–778, 2016. [25] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. EuroSAT: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE J. Sel. Top. Appl. Earth Obs. Remote. Sens., 12(7):2217–2226, 2019. [26] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs). arXiv:1606.08415 [cs.LG], 2023. [27] Grant Van Horn, Steve Branson, Ryan Farrell, Scott Haber, Jessie Barry, Panos Ipeirotis, Pietro Perona, and Serge J. Be- longie. Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection. In CVPR, pages 595–604, 2015. [28] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In ICML, pages 2790–2799, 2019. [29] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen- Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 9LoRA: Low-rank adaptation of large language models. In ICLR, 2022. [30] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q. Weinberger. Deep networks with stochastic depth. In ECCV, pages 646–661, 2016. [31] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge J. Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In ECCV, pages 709–727, 2022. [32] Shibo Jie and Zhi-Hong Deng. FacT: Factor-tuning for lightweight adaptation on vision transformer. In AAAI, pages 1060–1068, 2023. [33] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross B. Girshick. CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. In CVPR, pages 1988–1997, 2017. [34] Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Li Fei-Fei. Novel dataset for fine-grained image catego- rization. In CVPR Workshop on Fine-grained Visual Classifi- cation, 2011. [35] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, Canadian Institute for Ad- vanced Research, 2009. [36] Yann LeCun, Fu Jie Huang, and Léon Bottou. Learning methods for generic object recognition with invariance to pose and lighting. In CVPR, pages 97–104, 2004. [37] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In EMNLP, pages 3045–3059, 2021. [38] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In ACL/IJCNLP, pages 4582–4597, 2021. [39] Dongze Lian, Daquan Zhou, Jiashi Feng, and Xinchao Wang. Scaling & shifting your features: A new baseline for efficient model tuning. In NeurIPS*2022. [40] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. [41] Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggy- back: Adapting a single network to multiple tasks by learning to mask weights. In ECCV, pages 72–88, 2018. [42] Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dSprites: Disentanglement testing sprites dataset. https://github.com/deepmind/dsprites-dataset, 2017. [43] Pedro Morgado and Nuno Vasconcelos. NetTailor: Tuning the architecture, not just the weights. In CVPR, pages 3044–3054, 2019. [44] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y . Ng. Reading digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011. [45] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In ICVGIP, pages 722–729, 2008. [46] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy V o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po- Yao Huang, Shang-Wen Li, Ishan Misra, Michael G. Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jégou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. arXiv:2304.07193 [cs.CV], 2023. [47] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V . Jawahar. Cats and dogs. InCVPR, pages 3498–3505, 2012. [48] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance deep learning library. In NeurIPS*2019, pages 8024–8035. [49] Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. AdapterFusion: Non- destructive task composition for transfer learning. In EACL, pages 487–503, 2021. [50] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by gener- ative pre-training. Technical report, OpenAI, 2018. [51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, pages 8748–8763, ICML. [52] René Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi- sion transformers for dense prediction. InICCV, pages 12159– 12168, 2021. [53] Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters. In NIPS*2017, pages 506–516. [54] Frank Rosenblatt. The perceptron: A probabilistic model for information storage and organization in the brain. Psychol. Rev., 65(6):386–408, 1958. [55] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San- jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet large scale visual recognition challenge. Int. J. Comput. Vision, 115(13):211–252, 2015. [56] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Lud- wig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION- 5B: An open large-scale dataset for training next generation image-text models. In NeurIPS*2022. [57] Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer. How to train your ViT? Data, augmentation, and regularization in vision transformers. Trans. Mach. Learn. Res., 2022. [58] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the In- 10ception architecture for computer vision. In CVPR, pages 2818–2826, 2016. [59] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkor- eit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS*2017, pages 5998–6008. [60] Bastiaan S. Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equivariant CNNs for digital pathology. In MICCAI, pages 210–218, 2018. [61] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The Caltech-UCSD Birds-200-2011 dataset. Technical report, California Institute of Technology, 2011. [62] Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. SUN database: Large-scale scene recognition from abbey to zoo. In CVPR, pages 3485–3492, 2010. [63] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, José M. Álvarez, and Ping Luo. SegFormer: Simple and efficient design for semantic segmentation with transformers. In NeurIPS*2021, pages 12077–12090. [64] Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. In ACL, 2022. [65] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, Lucas Beyer, Olivier Bachem, Michael Tschannen, Marcin Michalski, Olivier Bousquet, Sylvain Gelly, and Neil Houlsby. A large-scale study of representation learning with the visual task adaptation benchmark. arXiv:1910.04867 [cs.CV], 2020. [66] Hongyi Zhang, Moustapha Cissé, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimiza- tion. In ICLR, 2018. [67] Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. Neural prompt search. arXiv:2206.04673 [cs.CV], 2022. 11Adapters Strike Back Supplementary Material In this appendix, we provide further details and results, which could not be included in the main paper due to space limitations. A. Why did adapters underperform for ViTs? First, we want to shed more light on why adapters do not rank well in the literature for parameter-efficient transfer learning for vision tasks. By comparison of numbers reported for adapters on VTAB in the publications referenced in Tab. 4 of the main paper, we found that they essentially stem from only two sources. The first source is VPT [31], where results for an adapter with a reduction factor of 256, amongst other configurations, are reported. For a ViT-B/16 with a hidden dimension of d=768, this is equal to an adapter with rank r=3. Despite citing Pfeiffer et al. [49], who suggest a Post-Adapter po- sition, the actual implementation in the code base * equals an Intermediate-Adapter that performs worse on VTAB (see Sec. 3.3 of the main paper). The initialization used for the adapter parameters most resembles a LoRA initialization but sets the adapter parameters to zero everywhere. Therefore, there is no randomization in the initialization of the adapter parameters, and different seeds only affect the initialization of the classifier. Additionally, the intermediate features in the adapter bottlenecks then become all zero, leading to iden- tical gradients in the up-projections at the start of training, which hinders optimization. As a result, the adapter baseline used by VPT only reaches 60.0% average accuracy on the VTAB test sets. This is a gap of 17.6 percentage points (pp) compared to our Adapter+ with rank r=8 (77.6% average accuracy). Even when considering the loss of around 2 pp to 3 pp caused by an unsuitable data normalization in the VPT implementation, this is still a very significant gap. The numbers for an adapter with rank r =3 from VPT are also reported in [39] as a baseline. The second source for adapter baseline results is the NOAH pre-print [67]. There, an adapter with rank r = 8 is used. Its implementation† performs the following feature transformation: x 7→ Adapter \u0000 FFN(x) \u0001 + x. (12) This is closest to the Intermediate-Adapter (cf . Eq. (10) of the main paper) but misses the skip connection bypassing the adapter and containing FFN(x). Thus, the adapter does not learn a residual function to an identity mapping but instead must learn a more complex mapping to transform its input. *https://github.com/KMnP/vpt †https://github.com/ZhangYuanhan-AI/NOAH Therefore, the adapter becomes harder to train [24], leading to an average accuracy of 73.9% on the VTAB test sets or 3.7 pp behind our Adapter+. For the NOAH adapter results, we see a proliferation to the publications of FacT [32] and SPT [21]. The adapter implementation from NOAH is also used in the code released for Consolidator ‡ [20] but their results are produced with rank r=16, giving a slightly better average accuracy of 74.3%, or 3.3 pp less than Adapter+. In summary, the examined baseline implementations dif- fer from the configurations proposed by Houlsby et al. [28] and Pfeiffer et al. [49] and introduce issues that lead to their underperformance. In our paper, we show that adapters are capable of reaching 77.6% average accuracy for rank r=8 and 77.9% for our optimized version of Adapter+, uplifting adapters from an easy-to-beat baseline to a state-of-the-art transfer method. B. Dataset properties In Tabs. 8 and 9, we show the statistics of each task in VTAB [65] and FGVC [31] with regard to the number of classes and the number of images in the train, validation, and test splits. The tables are largely “borrowed” from [31]. Table 8. Dataset details for VTAB. Group Task # Classes Splits Train Val Test Natural CIFAR-100 [35] 100 800 200 10 000 Caltech-101 [15] 102 6 084 DTD [8] 47 1 880 Oxford Flowers [45] 102 6 149 Pets [47] 37 3 669 SVHN [44] 10 26 032 Sun397 [62] 397 21 750 Specialized Patch Camelyon [60] 2 800 200 32 768 EuroSAT [25] 10 5 400 RESISC45 [7] 45 6 300 Diabetic Retinopathy [14] 5 42 670 Structured CLEVR-Count [33] 8 800 200 15 000 CLEVR-Distance [33] 6 15 000 DMLab [2] 6 22 735 KITTI-Distance [18] 4 711 dSprites-Location [42] 16 73 728 dSprites-Orientation [42] 16 73 728 smallNORB-Azimuth [36] 18 12 150 smallNORB-Elevation [36] 9 12 150 ‡https://github.com/THU-MIG/Consolidator iTable 9. Dataset details for FGVC.For datasets markedwith *, we follow [31] to randomly sample train and validation splits because validation sets are not available from the original datasets. Dataset # Classes Splits Train Val Test CUB-200-2011* [61] 200 5 394 600 5 794 NABirds* [27] 555 21 536 2 393 6 084 Oxford Flowers [45] 102 1 020 1 020 6 149 Stanford Dogs* [34] 120 10 800 1 200 8 580 Stanford Cars* [17] 196 7 329 815 8 041 C. More experimental settings For all experiments conducted with our implementation, we average the results over three seeds. This includes the (re-)evaluations of LoRA and VPT. We built our implemen- tation on PyTorch [48], PyTorch Lightning,§ and timm.¶ We run experiments with bfloat16 mixed precision on a NVIDIA RTX A6000 GPU. For our experiments in the main paper, we report results for a fixed adapter rank r as well as ranks optimized per task. For the per-task optimization of Adapter+, we use a hyper- parameter sweep over the set of ranks r∈{1, 2, 4, 8, 16, 32}. We evaluate on the validation sets of VTAB and FGVC and choose the per-task ranks from the specified range(s) to steer the number of average parameters. The ranks we used to produce the results on the VTAB and FGVC test sets (see Tabs. 4 and 5 in the main paper) are shown in detail in Tab. 10 and Tab. 11, respectively. D. Calculation of no. of trainable parameters Suppose we have a ViT with a hidden dimension d, N trans- former layers, and adapters with rank r. The total num- ber of learnable parameters for Adapter base modules ( cf . Eq. (4) of the main paper) attached to the FFN of every transformer layer then amounts to N(2dr + r + d). Includ- ing layer normalization in the adapter modules amounts to N2d additional parameters. The addition of learned, layer- wise scaling amounts to N extra parameters and choosing learned, channel-wise scaling instead adds Nd extra parame- ters. Adapter+ (see Sec. 4.3 of the main paper) thus amounts to N(2dr + 2d + r) total parameters. Additionally, for a task with c classes, we add a classifier with dc + c learnable parameters. E. Vision transformer pre-training As we add only very few parameters to an otherwise frozen backbone, the generalization capability of the feature repre- sentations produced by the backbone is important. For ViTs, there are a number of off-the-shelf models available with §https://lightning.ai/pytorch-lightning ¶https://github.com/huggingface/pytorch-image-models Table 10. Adapter rank r for each VTAB task for optimized versions of Adapter+ with different ranges of permitted ranks. Natural Specialized Structured # Param (M) CIFAR-100 [35] Caltech-101 [15] DTD [8] Flowers [45] Pets [47] SVHN [44] Sun397 [62] Camelyon [60] EuroSAT [25] RESISC45 [7] Retinopathy [14] CLEVR-Count [33] CLEVR-Dist. [33] DMLab [2] KITTI-Dist. [18] dSpr-Loc. [42] dSpr-Ori. [42] sNORB-Azi. [36] sNORB-Ele. [36] r∈[1..4] 0.11 1 4 2 1 4 4 1 4 2 4 2 4 2 4 4 4 4 4 4 r∈[1..8] 0.16 1 4 2 1 8 8 1 8 2 8 8 4 8 8 8 8 4 8 8 r∈[1..32] 0.27 1 4 2 1 8 16 1 16 2 32 32 4 8 8 8 32 4 32 8 Table 11. Adapter rank r for each FGVC dataset for optimized versions of Adapter+ with different ranges of permitted ranks. # Param (M) CUB-200 [61] NABirds [27] Oxford Flowers [45] Stanford Dogs [34] Stanford Cars [17] r∈[1..32] 0.34 2 2 1 1 32 differences in their training procedures. Here, we examine three different pre-trainings as examples: (1) Original: The ViT-B/16 weights used in the main paper, pre-trained with su- pervision on ImageNet-21k [55] following the training proce- dure of the original ViT publication [13],|| (2) ImageNet-1k: the same ViT weights further fine-tuned on ImageNet-1k [55],** and (3) AugReg: weights from a pre-training with stronger data augmentation in the form of Mixup [66] and RandAugment [9] following [57].†† In Tab. 12, we summarize our results for Adapter+ with rank r=8 evaluated on the VTAB validation sets. We notice that additional fine-tuning on ImageNet-1k gives a slight edge (83.4% average accuracy over 83.0% for second best) in adaption for tasks that contain natural images. However, the fine-tuning is detrimental for the Specialized and Structured group. Not fine-tuning on ImageNet-1k is beneficial for the Structured group with a large increase of 3.7 pp. The Aug- Reg training setting improves the transfer to the Specialized group but is worse than the other settings for natural images. Overall, the original supervised training on ImageNet-21k generalizes best across all tasks in VTAB with an average accuracy of 76.5%, 0.3 pp better than AugReg training and 1.2 pp better than ImageNet-1k fine-tuning. ||https://storage.googleapis.com/vit_models/imagenet21k/ ViT-B_16.npz **https://storage.googleapis.com/vit_models/imagenet21k+ imagenet2012/ViT-B_16-224.npz ††https://storage.googleapis.com/vit_models/augreg/B_16- i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.0-sd_0.0.npz iiTable 12. Influence of ViT pre-training. We use Adapter+ with rank r=8 for the evaluation and report the average accuracy in % for each subgroup and across all groups on the VTAB val sets. Pre-training Natural Specialized Structured Average ImageNet-1k 83.4 86.5 56.0 75.3 AugReg 81.6 87.2 59.7 76.2 Original 83.0 86.8 59.7 76.5 Table 13. Adapter position with DINO backbone. We report average accuracy in % ( ± std. dev.) on the VTAB val sets for different adapter positions. Adapterbase with Houlsby initialization and rank r=8 is used in all experiments. Position Natural Specialized Structured Average Pre 76.8 ± 0.4 86.2 ± 0.6 53.6 ± 0.7 72.2 ± 0.3 Intermediate 76.8 ± 0.4 85.8 ± 0.8 52.6 ± 0.9 71.8 ± 0.4 Parallel 76.7 ± 0.3 86.8 ± 0.4 54.1 ± 0.7 72.5 ± 0.3 Post 76.9 ± 0.2 86.3 ± 0.5 55.3 ± 0.7 72.8 ± 0.3 Table 14. Comparison of Adapter+ with adapter configurations from previous work with DINO backbone.We report the average accuracy in % (± std. dev.) of each subgroup and across all groups on the VTAB val sets. Configuration # Param Natural Specialized Structured Average Houlsby [28], r=8 0.39 77.4 ± 0.4 86.5 ± 0.7 52.9 ± 0.8 72.3 ± 0.4 Houlsby [28], r=4 0.24 77.2 ± 0.5 86.2 ± 0.5 53.2 ± 0.8 72.2 ± 0.3 Pfeiffer [49] 0.21 76.8 ± 0.4 86.2 ± 0.3 54.4 ± 1.0 72.5 ± 0.4 AdaptFormer [6] 0.19 76.5 ± 0.4 85.8 ± 0.4 53.0 ± 0.5 71.8 ± 0.3 Adapter+ 0.20 76.7 ± 0.3 86.4 ± 0.5 55.4 ± 0.8 72.8 ± 0.3 F. Generality of the conclusions Using DINO [5] as an example of a ViT trained with self- supervision, we show in Tab. 13 that the orders of best-to- worst adapter position is consistent with that of a supervised backbone in terms of average accuracy, albeit with a higher standard deviation. The ranking also stays the same for the comparison of Adapter+ with adapter configurations from previous work as presented in Tab. 14. This shows that our conclusions generalize beyond backbones with super- vised pre-training to backbones based on self-supervised pre-training. iii",
      "meta_data": {
        "arxiv_id": "2406.06820v1",
        "authors": [
          "Jan-Martin O. Steitz",
          "Stefan Roth"
        ],
        "published_date": "2024-06-10T22:07:57Z",
        "pdf_url": "https://arxiv.org/pdf/2406.06820v1.pdf",
        "github_url": "https://github.com/visinf/adapter_plus"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper provides an in-depth and systematic study of adapters for Vision Transformers (ViTs), identifying pitfalls in previous implementations and proposing an improved architecture called Adapter+. The main contributions are: (1) the first comprehensive study on adapter position, inner structure, and initialization for ViTs, (2) the proposal of a learnable, channel-wise scaling as an extension beneficial for computer vision tasks, and (3) the introduction of Adapter+, which achieves state-of-the-art average accuracy on the VTAB benchmark (77.6%) without per-task hyperparameter optimization and on FGVC (90.7%) with the lowest number of parameters, demonstrating superior parameter-accuracy trade-off and robustness compared to other adaptation mechanisms.",
        "methodology": "The study systematically investigates bottleneck adapter modules, their integration into ViT transformer layers, and various implementation choices. Key methodological aspects include: (1) evaluating four adapter positions (Pre, Post, Parallel, Intermediate) relative to the Feed-Forward Network (FFN), with Post-Adapter identified as optimal, (2) analyzing inner adapter structures, proposing a learnable, channel-wise scaling as a beneficial alternative to layer-wise scaling, and (3) comparing parameter initialization methods (Houlsby, BERT, LoRA), with Houlsby initialization found to be best. The final Adapter+ configuration combines the Post-Adapter position, learnable channel-wise scaling, and Houlsby initialization. The methodology also emphasizes the importance of using data normalization consistent with the pre-trained backbone.",
        "experimental_setup": "Experiments were conducted using a ViT-B/16 network pre-trained on ImageNet-21k, with additional ablations on ImageNet-1k fine-tuned and AugReg pre-trained backbones. Two standard benchmarks were used: the Visual Task Adaptation Benchmark (VTAB) comprising 19 tasks grouped into Natural, Specialized, and Structured categories (800 train, 200 validation images per task), and a Fine-Grained Visual Classification (FGVC) benchmark (CUB-200-2011, NABirds, Oxford Flowers, Stanford Dogs, Stanford Cars) for abundant data scenarios. Models were trained with an AdamW optimizer (learning rate 10^-3, weight decay 10^-4, batch size 64) for 100 epochs, using a cosine learning rate schedule with linear warm-up. Stochastic depth regularization was applied to both frozen layers and adapters. Input images were resized to 224x224px. For fair comparison, many competing methods (LoRA, VPT, SSF, FacT, SPT) were re-evaluated using consistent data normalization and full training schedules, correcting for issues like early stopping on test sets found in original implementations.",
        "limitations": "The study primarily focuses on the ViT-B/16 architecture, which may limit the direct generalizability of the findings to other Vision Transformer variants or sizes without further empirical validation. While Adapter+ performs well without per-task hyperparameter optimization, its peak performance still benefits from per-task optimization of the adapter rank 'r', indicating some degree of hyperparameter sensitivity. The extensive re-evaluation of other methods revealed that previous comparisons often suffered from unfair practices (e.g., early stopping on test sets, unsuitable data normalization), suggesting that Adapter+'s reported superiority is partly attributable to a more rigorous and fair evaluation methodology. Furthermore, the paper notes that layer normalization within the adapter, while suggested by previous work in NLP, was found to be detrimental in their vision task settings, which might imply a specific context dependency.",
        "future_research_directions": "Not mentioned",
        "experimental_code": "import torch\nfrom torch import nn\nfrom timm.models.layers import DropPath\nfrom timm.models.vision_transformer import Block\nimport pytorch_lightning as pl\nfrom typing import Optional\n\n# Core Adapter module with scaling and initialization logic\nclass Adapter(nn.Module):\n    def __init__(\n        self,\n        embed_dim,\n        bottleneck_dim=8,\n        drop_path=0.0,\n        dropout=0.0,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm, # For Adapter+ set to None\n        scaling=1.0, # For Adapter+ set to \"channel\"\n        init=\"houlsby\",\n        bias=True,\n        pre_dropout=False,\n    ):\n        super().__init__()\n        self.bottleneck = nn.Sequential(\n            nn.Dropout(dropout) if dropout > 0 and pre_dropout else nn.Identity(),\n            nn.Linear(embed_dim, bottleneck_dim, bias=bias),\n            act_layer() if act_layer else nn.Identity(),\n            nn.Dropout(dropout) if dropout > 0 and not pre_dropout else nn.Identity(),\n            nn.Linear(bottleneck_dim, embed_dim, bias=bias),\n        )\n        self.norm_a = norm_layer(embed_dim) if norm_layer else nn.Identity()\n        self.drop_path_a = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n\n        # Learnable channel-wise scaling\n        if scaling == \"channel\":\n            self.scaling = nn.Parameter(torch.ones(embed_dim))\n        else:\n            self.scaling = scaling\n\n        # Houlsby initialization\n        if init == \"houlsby\":\n            std = 0.01\n            nn.init.trunc_normal_(self.bottleneck[1].weight, std=std, a=-2 * std, b=2 * std)\n            if self.bottleneck[1].bias is not None: nn.init.zeros_(self.bottleneck[1].bias)\n            nn.init.trunc_normal_(self.bottleneck[4].weight, std=std, a=-2 * std, b=2 * std)\n            if self.bottleneck[4].bias is not None: nn.init.zeros_(self.bottleneck[4].bias)\n\n    def forward(self, x: torch.Tensor, skip: Optional[torch.Tensor] = None) -> torch.Tensor:\n        x = self.norm_a(x)\n        x = self.drop_path_a(self.bottleneck(x))\n        x = x * self.scaling # Apply scaling\n        y = x\n        if skip is not None: y = y + skip\n        return y\n\n# Adapter integration into a transformer block (Post-Adapter position)\nclass AdapterBlock(Block): # Block class from timm.models.vision_transformer\n    def __init__(self, dim: int, num_heads: int, mlp_ratio: float = 4.0, adapter_config=None, **kwargs):\n        super().__init__(dim, num_heads, mlp_ratio=mlp_ratio, **kwargs)\n        self.adapter_config = adapter_config\n        if adapter_config is not None:\n            # Instance of Adapter with specific config, e.g., scaling=\"channel\", init=\"houlsby\", norm_layer=None\n            self.adapter = Adapter(\n                dim,\n                bottleneck_dim=adapter_config.dim,\n                dropout=adapter_config.dropout,\n                drop_path=adapter_config.drop_path,\n                act_layer=kwargs.get('act_layer') if adapter_config.act_layer else None,\n                norm_layer=kwargs.get('norm_layer') if adapter_config.norm_layer else nn.Identity(), # Expected None for Adapter+\n                bias=adapter_config.bias,\n                scaling=adapter_config.scaling, # \"channel\" for Adapter+\n                init=adapter_config.init, # \"houlsby\" for Adapter+\n            )\n\n    def forward_post(self, x: torch.Tensor) -> torch.Tensor:\n        x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))\n        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n        x = self.adapter(x, skip=x) # Adapter applied after FFN\n        return x\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if self.adapter_config and self.adapter_config.config == \"post\":\n            return self.forward_post(x)\n        # ... (other adapter positions or no adapter logic)\n        return super().forward(x)\n\n# Model setup for training adapters\nclass AdapterModel(pl.LightningModule):\n    def __init__(self, cfg, num_classes=1000, **kwargs):\n        super().__init__()\n        # timm.create_model is patched to use VisionTransformerAdapter and AdapterBlock\n        self.vit = timm.create_model(\n            cfg.vit.model,\n            adapter=True, # Activates adapter integration\n            pretrained=True,\n            num_classes=num_classes,\n            adapter_config=cfg.get(\"adapter\", None), # Configuration for Adapter module\n            # ...\n        )\n\n        if cfg.get(\"adapter\", None): # If adapter is configured\n            if not cfg.vit.finetune:\n                self.vit.requires_grad_(False) # Freeze backbone\n            self.vit.head.requires_grad_(True) # Unfreeze classifier head\n            for m in self.vit.modules():\n                if isinstance(m, Adapter):\n                    m.requires_grad_(True) # Unfreeze Adapter modules\n                if cfg.train.train_ln and isinstance(m, nn.LayerNorm):\n                    m.requires_grad_(True) # Optionally unfreeze LayerNorms\n\n# Function to handle weight decay, specifically for learnable scaling parameters\ndef add_weight_decay(model, weight_decay=1e-5, skip_list=(), exclude_list=()):\n    decay = []\n    no_decay = []\n    for name, param in model.named_parameters():\n        if not param.requires_grad or name in exclude_list:\n            continue\n        if (\n            len(param.shape) == 1\n            or name.endswith(\".bias\")\n            or name.endswith(\".scaling\") # Skip learnable scaling parameters from weight decay\n            or name in skip_list\n        ):\n            no_decay.append(param)\n        else:\n            decay.append(param)\n    return [\n        {\"params\": no_decay, \"weight_decay\": 0.0},\n        {\"params\": decay, \"weight_decay\": weight_decay},\n    ]",
        "experimental_info": "The optimal adapter configuration (Adapter+) is used, integrating bottleneck adapter modules into ViT transformer layers. The adapter is placed in the **Post-Adapter position** (after the Feed-Forward Network, FFN). The inner structure of the adapter employs **learnable, channel-wise scaling** (`scaling=\"channel\"`), with Layer Normalization within the adapter module explicitly set to `None`. **Houlsby initialization** (`std=0.01`) is used for the adapter's weights.\n\nThe **ViT backbone weights are largely frozen** (`requires_grad_(False)`), except for the adapter modules and the final classification head, which are made trainable (`requires_grad_(True)`). Optionally, all ViT Layer Normalization layers can also be set to be trainable. During optimization, the learnable scaling parameters of the adapter are excluded from weight decay.\n\nInput images are preprocessed with **data normalization consistent with the pre-trained backbone**, typically using ImageNet's default mean and standard deviation values. The optimization typically uses the **AdamW optimizer** and a **Linear Warmup Cosine Annealing learning rate scheduler**.\n\nDefault hyperparameters for the adapter include a **bottleneck dimension of 8** (`bottleneck_dim=8`) and a **dropout rate of 0.0** within the adapter module. The overall model's drop path rate is configurable."
      }
    },
    {
      "title": "Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from Backbone",
      "abstract": "Parameter-efficient tuning has become a trend in transferring large-scale\nfoundation models to downstream applications. Existing methods typically embed\nsome light-weight tuners into the backbone, where both the design and the\nlearning of the tuners are highly dependent on the base model. This work offers\na new tuning paradigm, dubbed Res-Tuning, which intentionally unbinds tuners\nfrom the backbone. With both theoretical and empirical evidence, we show that\npopular tuning approaches have their equivalent counterparts under our\nunbinding formulation, and hence can be integrated into our framework\neffortlessly. Thanks to the structural disentanglement, we manage to free the\ndesign of tuners from the network architecture, facilitating flexible\ncombination of various tuning strategies. We further propose a memory-efficient\nvariant of Res-Tuning, where the bypass i.e., formed by a sequence of tuners)\nis effectively detached from the main branch, such that the gradients are\nback-propagated only to the tuners but not to the backbone. Such a detachment\nalso allows one-time backbone forward for multi-task inference. Extensive\nexperiments on both discriminative and generative tasks demonstrate the\nsuperiority of our method over existing alternatives from the perspectives of\nefficacy and efficiency. Project page:\n$\\href{https://res-tuning.github.io/}{\\textit{https://res-tuning.github.io/}}$.",
      "full_text": "Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from Backbone Zeyinzi Jiang1 Chaojie Mao1 Ziyuan Huang2 Ao Ma1 Yiliang Lv1 Yujun Shen3 Deli Zhao1 Jingren Zhou1 1Alibaba Group 2National University of Singapore 3Ant Group {zeyinzi.jzyz, chaojie.mcj, dave.ma, yiliang.lyl, jingren.zhou}@alibaba-inc.com {ziyuan.huang}@u.nus.edu {shenyujun0302, zhaodeli}@gmail.com Abstract Parameter-efficient tuning has become a trend in transferring large-scale foundation models to downstream applications. Existing methods typically embed some light- weight tuners into the backbone, where both the design and the learning of the tuners are highly dependent on the base model. This work offers a new tuning paradigm, dubbed Res-Tuning, which intentionally unbinds tuners from the backbone. With both theoretical and empirical evidence, we show that popular tuning approaches have their equivalent counterparts under our unbinding formulation, and hence can be integrated into our framework effortlessly. Thanks to the structural disentanglement, we manage to free the design of tuners from the network architecture, facilitating flexible combination of various tuning strategies. We further propose a memory-efficient variant of Res-Tuning, where the bypass (i.e., formed by a sequence of tuners) is effectively detached from the main branch, such that the gradients are back-propagated only to the tuners but not to the backbone. Such a detachment also allows one-time backbone forward for multi-task inference. Extensive experiments on both discriminative and generative tasks demonstrate the superiority of our method over existing alternatives from the perspectives of efficacy and efficiency. Project page: https://res-tuning.github.io/. 1 Introduction Recently, foundation models have demonstrated strong generalization capability across numerous visual [21, 2], language [ 47, 60] and multi-modal tasks [ 40, 1]. Pre-trained on a large corpus of data, a foundation model offers a good initialization for downstream adaptation. Unfortunately, the increasing model scale makes it expensive and almost infeasible to fully fine-tune such a model for every task. Hence, parameter-efficient transfer learning (PETL) [37, 41, 27] is often resorted to as an efficient approach for downstream adaptation without incurring an unaffordable computation burden. Popular existing approaches for parameter-efficient tuning introduce additional tunable structures (which we term as tuners) to the pre-trained base model [ 37, 41, 27]. Compared to the fully-fine- tuned counterparts, the light-weight tuners significantly reduce the training cost while maintaining a competitive performance [32, 28]. However, the current designs of tuners are deeply coupled with their base structures, as shown in Fig. 1a, thus restricting the design flexibility and impeding the extension to new approaches. For example, prefix tuning [ 41] is embedded into the self-attention operation, and prompts [28, 85] could only be introduced at the beginning or between layers, etc. In this work, we introduce Res-Tuning, a new tuning paradigm for flexible and efficient transfer learning. As in Fig. 1b, ourRes-Tuning framework unbinds the tuners from the base model, such that it is possible to decouple both the design and the learning of the tuners from the base structure. Since 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2310.19859v1  [cs.CV]  30 Oct 2023MHA FFN Res- Tuner Tuner MHA Res- Tuner FFN Block Block (b) (c) (a) Res- Tuner Res- Tuner Res- Tuner Res- Tuner × × Figure 1: Concept comparison between existing methods and our Res-Tuning framework. (a) Existing PETL methods are deeply embedded into original structures. (b) Our Res-Tuning framework can decouple PETL methods from the backbone. (c) Backpropagation only through a bypass consisting of Res-Tuner can be achieved by detaching the connections. the design of the tuners is no longer dependent on the base structure in our Res-Tuning framework, we explore various possibilities. With both theoretical and empirical evidence, we first show that our framework can seamlessly encompass popular tuning approaches such as prefix-tuning [ 41], prompt-tuning [28], and adapters [25]. Further, it is demonstrated that the structural disentanglement also allows for flexible combination of various tuners, leading to the discovery of stronger tuning strategies. On top of that, we show that such an unbinding formulation also allows for the detachment of the tuners from the backbone as in Fig. 1c, which further improves the memory efficiency. In this memory-efficient version, i.e., Res-Tuning-Bypass, not only is the training cost reduced because the gradient computation through the massive parameters is avoided in the base model, but it also reduces the number of forward passes of the backbone model to only once during multi-task inference. We evaluateRes-Tuning framework on both discriminative and generative tasks. On discriminative tasks, we show that our unbinding formulation leads to a tuning strategy that achieves state-of-the-art performance on VTAB-1K with similar learnable parameters. Our Res-Tuning-Bypass framework also performs favorably against the fully-fine-tuned variant, while reducing the training memory consumption by 49.7% and the multi-task inference time by 80.9%. In addition, it also obtains better performance in few-shot learning and domain generalization scenarios. On generative tasks, apart from the strong performance achieved by Res-Tuning framework in terms of both FID scores and qualitative results, we further show that our Res-Tuning-Bypass can reduce the memory consumption by 70.7%, training time by 58.6%, and multi-task inference time by 83.6% when maintaining a competitive FID score and generation quality compared to the fully-fine-tuned variant. 2 Unbinding parameter-efficient tuning In order to reduce the entanglement between the base model and the tuners as well as to increase the flexibility of the tuning strategies, we set out to unbind the tuners from the pre-trained structures. In this section, we provide our unbinding formulation to existing parameter-efficient transfer learning strategies. We start by revisiting the basic building blocks of the foundation models, before diving into unbinding existing tuners from the backbone. In the last part of this section, we further provide empirical proof that our unbinding formulation could seamlessly encompass existing PETL methods like prompt tuning [28], prefix tuning [41], and adapters [25]. 2.1 Basic building blocks of foundation models Existing foundation models in both natural language processing, vision, and vision-language applications mostly adopt Transformers [ 70] as the backbone. The major building blocks of the Transformers that one usually adapts for the downstream tasks are the multi-head attention (MHA) and the feed-forward network (FFN). Formally, the standard MHA and FFN could be expressed as: Attn(Q, K, V ) = softmax \u0012QKT √ d \u0013 V FFN(x) = ϕ(xW1 + b1)W2 + b2 , (1) 2MHA Input FFN Adapter Input FFN Adapter Input MHA Input Pro. MHA Pro. Input MHA Input MHA MHA (a) Prefix Tuning (b) Prompt Tuning (c) Adapter Tuning Original Parallel  Original Parallel Original Parallel Figure 2: The original and the unbinding form of (a) prefix tuning [41], (b) prompt tuning [28], and (c) adapter tuning [25] for parameter-efficient transfer learning. where Q, K and V denote the query, key and value, respectively. W1 and W2 are the projection weights, b1 and b2 are the bias terms, and ϕ is the non-linear activation between fully-connected layers. Usually, given input tokensx, the query, key, and value are obtained through a linear projection Q = xWq, K = xWk, and V = xWv, where Wq, Wk and Wv are learnable projection weights. 2.2 Unbinding tuners from foundation models For the adaptation of the foundation models to downstream applications, the existing PETL approaches mostly resort to adjusting the output of MHA, FFN, or the Transformer block composed of MHA and FFN in various ways. We choose popular and exemplary approaches and unbind their structures from foundation models. Here, we provide the unbinding formulations of prefix tuning [41] and prompt tuning [28] for MHA adaptation, as well as adapter tuning [25] for FFN adaptation. Prefix tuning [41] prepends learnable parameters, i.e., prefix tokens, to the projected keys and values: MHApre = Attn(xWq, [Kpre; xWk], [Vpre; xWv]), (2) where Kpre and Vpre are prefix tokens. Essentially, if we view this as performing MHA separately between (Q, K, V ) and between (Q, Kpre, Vpre), we unbind prefix tuning as follows: MHApre = (1 − λ) Attn (Q, K, V )| {z } original attention +λ Attn (Q, Kpre, Vpre)| {z } prefix attention in parallel , (3) where λ weighs between the original and prefix attention. Detailed value for λ and the derivation process are included in appendix A. In this way, the original MHA in the foundation model Attn(Q, K, V ) and the prefix attention Attn(Q, Kpre, Vpre) can be computed independently. The unbinding formulation of prefix tuning can be seen in Fig. 2a. Prompt tuning [28] appends latent tokens to the input token before performing MHA in the backbone: MHApro = Attn ([x; xpro]Wq, [x; xpro]Wk, [x; xpro]Wv) , (4) where xpro are prompt tokens concatenated to the input token x in the first layer or between multiple layers. Similar to prefix tuning, the unbinding formulation of prompt tuning is as follows: MHApro = [(1 − λ) Attn (Q, K, V )| {z } original attention +λ Attn (Q, Kpro, Vpro)| {z } prompt attention in parallel ; D], (5) where Kpro = xproWk and Vpro = xproWv. D denotes disposable parts that would not affect the output of MHA, where D = (1−β) Attn (Qpro, Kpro, Vpro) +β Attn (Qpro, K, V ). λ and β are individual weights. More details can be seen in appendix A. The unbinding formulation of prompt tuning can be seen in Fig. 2b. Adapter tuning [25] typically inserts a multi-layer perceptron (MLP) after FFN. Since the MLP could be performed independently, we simply re-route the adapter and connect it in parallel to the FFN as in Fig. 2c. The resultant unbinding formulation of the adapters is as follows: FFNadapter = FFN( x)| {z } original module + ϕ(FFN(x)Wdown)Wup| {z } adapter module in parallel , (6) where Wdown and Wup denote the weights for the down- and up-projection layers, respectively. 3MHA FFN Input Attention Feed Forward Block Norm Norm x L Residual Adapter Nonlinear OP Scale Residual Prompt OP MHA Pro. Scale Residual Preﬁx OP MHA Scale Trainable Frozen Ada. Pre. Pro. OP MHA FFN Block … … Res- Tuner Res- Tuner Res- Tuner Res-Tuner Start Layer MHA FFN Input End Layer MHA FFN × (b)  Horizontal Vertical Res- Tuner Res- Tuner Res- Tuner (c) (a) × Res- Tuner … … Figure 3: Structure illustration of (a) various Res-Tuners in our unbinding form, (b) our Res-Tuning framework that is flexible and efficient, and (c) Res-Tuning-Bypass, the memory- efficient version of our framework. Table 1: Empirical equivalence of PETL methods and their counterparts in our unbinding form. ViT/B-16 (IN-21K) ViT/L-14 (CLIP) Method Original Unbinding form ∆ Original Unbinding form ∆ Adapter [25] 92.34 92.34 0.00 92.43 92.44 +0.01 Prefix [41] 91.90 91.88 -0.02 90.99 91.01 +0.02 Prompt [28] 92.21 92.24 +0.03 91.86 91.83 -0.03 2.3 Empirical equivalency of the unbinding formulation After we have obtained the unbinding formulation of popular PETL methods with theoretical derivation, Tab. 1 shows empirical evidence of the equivalency between the original formulation and our unbinding formulation. We carry out the comparison between two formulations on CIFAR-100, using ViT [13] pre-trained on ImageNet-21K and by CLIP [59]. For all cases, we observe that the performance discrepancy between the original and our unbinding form is within a reasonable range (± 0.03), which shows that our formulation encompasses existing PETL methods effortlessly. 3 Res-Tuning 3.1 Unified formulation of Res-Tuning Given the unbinding formulation of the existing PETL methods, we can derive a unified formulation as the combination of the frozen pre-trained operation and the tuner with learnable parameters: x′ = OP(x) +Res-Tuner(x), (7) where OP denotes existing operations in the pre-trained backbone such as MHA and FFN, while the Res-Tuner represents the learnable structures that are connected in parallel to the existing operations. With this unified unbinding formulation, the design of the tuner structure can now be independent of the original operation in the base model. This leads to unprecedented flexibility in parameter-efficient tuning, which enables us to explore various instantiations of the tuner (as in Fig. 3a) for the base structures. For example, we found in Tab. 2a that, instantiating the Res-Tuner with prompt tuning for FFN results in a better performance compared to adapters. Further, the structural disentanglement also allows for the flexible combination of various tuning strategies. In Fig. 3b, we instantiate our Res-Tuning framework by associating one Res-Tuner with every operation in the base model, including MHA, FFN, and the whole Transformer block. 3.2 Res-Tuning-Bypass: Towards memory-efficient PETL The unified formulation of Res-Tuning provides a viable solution for flexible and parameter-efficient transfer learning. However, since it is directly derived from existing PETL methods, it shares the 4same vulnerability as existing PETL solutions. Despite the parameter efficiency, they require back- propagation through the massive parameters in the pre-trained backbone, which leads to unnecessary extra consumption of memory and computation. To avoid this, we further present a memory-efficient version of our Res-Tuning framework, dubbed as Res-Tuning-Bypass, as in Fig. 3c. Specifically, we remove the data flow from the Res-Tuner to the backbone, such that the Res-Tuner is detached from the pre-trained architectures. In this way, we form a bypass network constructed by Res-Tuners in parallel with the backbone. Formally, given the tokenized feature x0 and the output feature of the l-th layer xl from the pre-trained model, our bypass network is formulated as: xbypass 0 = x0, xbypass l = λRes-Tuner(xl) + (1− λ)Res-Tuner(xbypass l−1 ), l≥ 1, (8) where λ is a learnable parameter followed by a sigmoid function, which is initialized to 0.5. As demonstrated in Fig. 3c, we group the Res-Tuners in the bypass network into horizontal ones and vertical ones, respectively processing the output feature of the l-th layer from the backbone and the (l − 1)-th feature from the bypass network. Within each group, we keep the structure identical. Thanks to the flexibility of our unbinding formulation, we can also explore different instantiations of the Res-Tuner and various combinations of the existing tuners. 4 Empirical evaluations on discriminative tasks 4.1 Experimental setups Evaluation scenarios. We mainly analyze the flexibility and efficiency of our proposedRes-Tuning framework and evaluate the discriminative capabilities on three different scenarios: transfer learning, few-shot learning, and domain generalization. Baselines. Apart from the traditional downstream adaptation methods like fully fine-tuning and linear probing, we divide existing tuning approaches into two categories: (i) methods focusing on parameter-efficiency, including adapter tuning [25], prefix tuning [ 41], VPT [28], LoRA [27], AdaptFormer [7], SSF [42], and NOAH[83]; (ii) methods focusing on memory-efficiency, including Side-Tuning [82], and LST [68]. Since these two categories have distinct characteristics with respect to the parameters, memory, and performance, we mainly compare ourRes-Tuning framework within the former category, while the Res-Tuning-Bypass is mainly compared in the latter one. Implementation details. For most experiments, we adopt ViT-B/16 [13] pre-trained on ImageNet- 21K [11] as the backbone model, following VPT [ 28]. Unless otherwise specified, the middle of the adapter, as well as the number of prefix and prompt tokens in our Res-Tuning are set to 10 for parameter efficiency. We include the training details in appendix C. For all the tasks, we use top-1 accuracy as the main evaluation metric. 4.2 Analysis on flexibility Flexible combination between backbone structures and tuners. Since our unbinding formulation allows for the structural disentanglement between the frozen structure in the backbone and the tuner with learnable parameters, it enables us to explore various combinations of the operation and the tuners. Here, we experiment with various instantiations of OP and Res-Tuner in our Res-Tuning framework, and the number of tuners is limited to one tuner per block (Single-Res-Tuner). The results are presented in Tab. 2a. We found that the default combination in existing approaches (prefix and prompt for MHA adaptation and adapter for FFN adaptation) is far from optimal, and connecting prompt tuning to FFN results in the best performance. Flexible combination of multiple tuning strategies.Next, we show that the flexibility brought by our unbinding formulation could also effortlessly lead to stronger tuning strategies by combining various tuners in our unbinding formulation. Here, we consider two tuners per block ( Dual-Res-Tuner), respectively connected to MHA and FFN. As in Tab. 2b, employing two tuners for each block brings notable improvements over the Single-Res-Tuner variants, with employing two adapters respectively for MHA and FFN achieving the strongest performance of 93.25. On top of the best performing Dual-Res-Tuner model, we further attach tuners at the block level in Tab. 2c. With 5Table 2: Exploration of various combinations of operations in the pre-trained backbone and various Res-Tuners achieves a stronger performance compared to the existing tuning strategies on CIFAR- 100. Adapter, prefix, and prompt are abbeviated as Ada., Pre. and Pro., respectively. (a) Single-Res-Tuner. Tuner\\OP MHA FFN Block Res-Ada. 92.46 92.3492.49 Res-Pre. 91.8892.33 92.39 Res-Pro. 92.2492.6892.16 (b) Dual-Res-Tuner. MHA\\FFN Res-Ada. Res-Pre. Res-Pro. Res-Ada. 93.25 92.95 92.62 Res-Pre. 93.22 92.38 92.87 Res-Pro. 93.03 92.92 92.91 (c) Tri-Res-Tuner. Block Dual-Res-Tuner Res-Ada. 93.28 Res-Pre. 93.20 Res-Pro. 93.16 Table 3: In-depth analysis of our Res-Tuning and Res-Tuning-Bypass in terms of performance, parameter-efficiency and memory efficiency on CIFAR-100. (a) Parameter efficiency of Res-Tuning on CIFAR-100. Method Full Linear Probing Single- Dual- Tri- Acc. 89.12 85.95 92.68 93.25 93.28 Param. 85.9M 0.07M 0.17M 0.48M 0.67M (b) Parameter and memory efficiency of Res-Tuning- Bypass on CIFAR-100. Method BypassRes-TunerAcc. Param. Mem. Linear probing ✘ - 85.95 0.07M 2.72G ↓ ✔ None 86.34 0.07M 3.48G ✔ Hori. 88.08 0.27M 3.66G ✔ Vert. 87.26 0.27M 3.64G Res-Tuning-Bypass✔ Both 89.330.46M 4.72G Fully fine-tuning ✘ - 89.12 85.9M 9.02G (c) Performance and efficiency comparison with existing tuning strategies on CIFAR-100. Method Acc. Param. (M) Mem. Full 89.12 85.9 (100%) 9.02G Linear 85.95 0.07 (0.08%) 2.72G Parameter-efficient tuning methods MAM-Adapter† [19] 91.70 10.08 (11.72%) 9.57G AdaptFormer [7] 91.86 1.26 (1.46%) 6.32G Res-Tuning 93.25 0.48 (0.55%) 6.85G Memory-efficient tuning methods Side-Tuning [82] 87.16 9.62 (11.18%) 3.48G LST† [68] 88.72 0.93 (1.08%) 5.26G Res-Tuning-Bypass89.33 0.46 (0.53%) 4.72G †denotes our own implementation since the original approach is presented for natural langauge processing. adapters on top of the Dual-Res-Tuner model, we observe further slight improvements on the classification performance. Compared to existing tuning strategies of underlining in Tab. 2a, without bells and whistles, the Tri-Res-Tuner version of our Res-Tuning framework achieves at least 0.94% performance improvement. 4.3 Analysis on parameter-, memory- and multi-task inference-efficiency Parameter-efficiency. We analyze the parameter efficiency of ourRes-Tuning framework in Tab. 3a. Our Single-Res-Tuner version surpasses the performance of the fully-fine-tuned variant with less than 0.2% learnable parameters. Combining all three tuners on MHA, FFN, and block-level, we manage to outperform the fully-fine-tuned and linear evaluated variants by respectively 4.16% and 7.33%, while only using 0.67M parameters. Memory-efficiency. Here, we present the evolution from linear probing to ourRes-Tuning-Bypass in Tab. 3b. It is observed that introducing a bypass network without any tuners can help ensemble the original features obtained from different layers, thereby mildly improving the classification accuracy as compared to the linear probing approach where only the classifiers are trained. On top of that, both horizontal and vertical Res-Tuner bring notable performance improvement with limited parameter and memory overhead. With both horizontal and vertical Res-Tuners in place, our Res-Tuning-Bypass framework achieves stronger performance with only 52% of the memory consumption when compared with the fully-fine-tuned variant. Multi-task inference-efficiency. In Fig. 4, our Res-Tuning-Bypass demonstrates superior multi- task inference-efficiency on both discriminative and generative tasks. For discriminative multi-task inference, we combine the validation set of 19 tasks in VTAB-1K, perform 19 tasks on every image, and obtain the overall process time for the whole validation set. For generation multi-task inference, we take one image and provide the model with 10 fixed-length prompts for generation and record the overall generation time for the 10 prompts. For existing parameter-efficient methods, the inference time grows linearly when the number of tasks grows. Compared to the fully fine-tuned variant, all existing parameter-efficient tuning strategies increase the inference time to various extents. In contrast, our Res-Tuning-Bypass framework significantly reduces the inference time on 19 discriminative tasks and 10 generative tasks by respectively 80.9% and 83.6% when compared with the fully-fine-tuned variant. 6Full Linear VPT SSF NOAH Res-Tuning Adapter Linear Res-Tuning-Bypass FullLST Side-Tuning Figure 4: Comparisons of the parameter-, memory-, and multi-task inference-efficiency . For multi-task inference-efficiency, we evaluateRes-Tuning-Bypass on both discriminative and generative tasks. For parameter- and memory-efficiency, here, we show the comparisons on VTAB-1K between our approach and existing tuning strategies. Table 4: Performance and efficiency comparison on the VTAB-1K benchmark with ViT-B/16 models pre-trained on ImageNet-21K. “Group Mean” denotes the average accuracy of the three subgroups. “All Mean” denotes the average accuracy of 19 downstream tasks. Natural Specialized Structured CIFAR-100 Caltech101 DTD Flowers102 Pets SVHN Sun397 Camelyon EuroSAT Resisc45 Retinopathy Clevr-Count Clevr-Dist DMLab KITTI-Dist dSpr-Loc dSpr-Ori sNORB-Azim sNORB-Elev Group Mean All Mean Param. (M) Mem. (GB) Traditional methods Full 68.9 87.7 64.3 97.2 86.9 87.4 38.8 79.7 95.7 84.2 73.9 56.3 58.6 41.7 65.5 57.5 46.7 25.7 29.1 68.96 65.57 85.84 9.40 Linear 63.4 85.0 63.2 97.0 86.3 36.6 51.0 78.5 87.5 68.6 74.0 34.3 30.6 33.2 55.4 12.5 20.0 9.6 19.2 57.64 52.94 0.04 3.09 Parameter-efficient tuning methods Adapter [25] 74.2 85.7 62.7 97.8 87.2 36.4 50.7 76.9 89.2 73.5 71.6 45.2 41.8 31.1 56.4 30.4 24.6 13.2 22.0 60.52 56.35 1.82 6.53 LoRA [27] 67.1 91.4 69.4 98.8 90.4 85.3 54.0 84.9 95.3 84.4 73.6 82.9 69.2 49.8 78.5 75.7 47.1 31.0 44.0 74.60 72.30 0.29 6.88 VPT-Deep [28] 78.8 90.8 65.8 98.0 88.3 78.1 49.6 81.8 96.1 83.4 68.4 68.5 60.0 46.5 72.8 73.6 47.9 32.9 37.8 71.96 69.43 0.60 8.13 SSF [42] 69.0 92.6 75.1 99.4 91.8 90.2 52.9 87.4 95.9 87.4 75.5 75.9 62.3 53.3 80.6 77.3 54.9 29.5 37.9 75.69 73.10 0.24 7.47 NOAH [83] 69.6 92.7 70.2 99.1 90.4 86.1 53.7 84.4 95.4 83.9 75.8 82.8 68.9 49.9 81.7 81.8 48.3 32.8 44.2 75.48 73.25 0.42 7.27 Res-Tuning 75.2 92.7 71.9 99.3 91.9 86.7 58.5 86.7 95.6 85.0 74.6 80.2 63.6 50.6 80.2 85.4 55.7 31.9 42.0 76.32 74.10 0.55 8.95 Memory-efficient tuning methods Side-Tuning [82] 60.7 60.8 53.6 95.5 66.7 34.9 35.3 58.5 87.7 65.2 61.0 27.6 22.6 31.3 51.7 8.2 14.4 9.8 21.8 49.91 45.65 9.59 3.48 LST† [68] 58.0 87.1 66.2 99.1 89.7 63.2 52.6 81.9 92.2 78.5 69.4 68.6 56.1 38.8 73.4 72.9 30.5 16.6 31.0 67.56 64.52 0.89 5.13 Res-Tuning-Bypass 64.5 88.8 73.2 99.4 90.6 63.5 57.2 85.5 95.2 82.4 75.2 70.4 61.0 40.2 66.8 79.2 52.6 26.0 49.3 72.32 69.51 0.42 4.73 † denotes our own implementation since the original approach is proposed for natural language processing. 4.4 Comparisons with existing tuning strategies on different scenarios Transfer Learning. We mainly evaluate our approach on the basic transfer learning scenario, where pre-trained models are fine-tuned on different downstream tasks. CIFAR-100 [ 35] is a standard general-purpose image classification dataset. VTAB-1K [ 80] is composed of 19 various visual classification tasks falling into three categorizations, i.e., natural, specialized, and structured. We compare the performance of our approach and other baseline methods on the following: CIFAR-100. We present the comparisons to existing tuning strategies in Tab. 3c. For parameter- efficient methods, our Res-Tuning framework notably improves over AdaptFormer [7] by 1.39%, using only 0.55% extra parameters, which is 0.91% less than AdaptFormer [7]. For memory-efficient methods, we outperform LST [68] by 0.61% with memory reduced by 0.54G (10%). VTAB-1K. In Tab. 4, we present comprehensive evaluation on the 19 datasets on the VTAB-1K benchmark. Both our Res-Tuning and Res-Tuning-Bypass outperform existing approaches respectively within the group of parameter- and memory-efficient tuning methods. Specifically, our Res-Tuning framework achieves a 0.85% improvement in terms of the average performance on 19 datasets, compared to the previous best performance. For our Res-Tuning-Bypass, we outperform LST [68] in 18 out of 19 tasks and overall by 4.99% in terms of average performance. This is achieved with even fewer parameters and memory consumption. We further visualize the parameter vs. performance curve and memory vs. performance curve in Fig. 4 to show the advantage of our Res-Tuning and Res-Tuning-Bypass framework on VTAB-1K. Few-Shot Learning. To evaluate the ability of our approach to adapt with only a few training samples, we follow the few-shot evaluation protocol in NOAH [83], using {1, 2, 4, 8, 16}-shots for training and full test data. We conduct experiments on five fine-grained datasets, including FGVC-Aircraft [51], Food-101 [4], Oxford Flowers [55], Oxford Pets [56], Stanford Cars [15]. 7Figure 5: Results of few-shot learning on five fine-grained visual recognition datasets. The solid line represents the comparison of parameter-efficient tuning methods, and the dashed line represents the comparison of memory-efficient tuning methods. All results are averaged over 3 random seeds. Table 5: Results on domain generalization. “Mean” denotes the average accuracy of four variants of ImageNet. All results are averaged over 3 random seeds. Source Target ImageNet IN-V2 IN-Sketch IN-A IN-R Mean Parameter-efficient tuning methods Adapter [25] 70.5 59.1 16.4 5.5 22.1 25.8 VPT [28] 70.5 58.0 18.3 4.6 23.2 26.0 LoRA [27] 70.8 59.3 20.0 6.9 23.3 27.4 NOAH [83] 71.5 66.1 24.8 11.9 28.5 32.8 Res-Tuning 78.04 66.58 29.23 13.15 29.01 34.50 Memory-efficient tuning methods Side-Tuning [82] 74.57 62.52 23.55 10.37 25.06 30.38 LST [68] 70.00 57.04 14.39 7.21 17.02 23.92 Res-Tuning-Bypass 77.30 65.23 27.39 10.66 26.45 32.43 As the results are shown in Fig. 5, in terms of the overall performance (top-left), both Res-Tuning and Res-Tuning-Bypass demonstrate clear advantages over other corresponding parameter-efficient and memory-efficient tuning strategies of few-shot learning on five FGVC datasets. We also observe that Res-Tuning-Bypass performs as well as or even better than the non-memory-efficient methods on one or two shots with low training samples on serveral datasets. Domain Generalization. To evaluate the robustness of our approach to distribution shift, we train a model on the source domain using 16 shots per category and test it on both the source and target domain. The source domain uses ImageNet-1K [11]) and the target domains use four other variants of ImageNet, including ImageNet-V2 [62], ImageNet-Sketch [73], ImageNet-A [24], ImageNet-R [23]. The results in Tab. 5 prove that our approach is robust under domain shift. Our Res-Tuning goes beyond NOAH [83] by 6.54% on ImageNet and 1.7% on the average accuracy of four variants of ImageNet. Furthermore, Res-Tuning-Bypass also demonstrates stronger robustness than the memory-efficient baselines and outperforms most existing parameter-efficient tuning methods. 5 Empirical evaluations on generative task 5.1 Experimental setups Downstream tasks. To provide a more comprehensive evaluation of ourRes-Tuning framework, we further apply it to the text-to-image generation task. Following [63], we evaluate the text-to-image 8Table 6: Comparison of FID and efficiency on COCO2017. Following the default settings of stable diffusion1, we sample 10k captions from the validation set for generating images of size 5122 using 50 PLMS steps with classifier-free guidance scale 3.0 and compare against the full validation set. Method FID Param. (M) Mem. (GB) Train (Hour/Epoch) SD v1.5 15.48 - - - + Full 14.85 862 (100%) 72.77 1.98 + LoRA 14.50 9.96 (1.15%) 61.03 1.42 + Adapter 14.73 2.51 (0.29%) 54.30 1.30 + Prefix 15.36 4.99 (0.58%) 64.91 2.20 + Prompt 14.90 1.25 (0.14%) 63.70 2.17 + Res-Tuning 13.96 2.54 (0.29%) 54.49 1.38 + Res-Tuning Bypass 14.89 3.76 (0.44%) 21.35 0.82 A surfboard  propped up in  the sand on a  beech. SD v1.5 Full LoRA Adapter Prefix Prompt Res-Tuning BypassText A black bird on  top of berry  branches. Figure 6: Qualitative results of SD v1.5, existing tuning strategies and our Res-Tuning on COCO2017 validation set [43]. We frame our results in green and others in red. generation performance on COCO2017 dataset [43]. The main quantitative evaluation metric is the FID score and we also perform instance-level fine-tuning transfer on the fine-grained datasets [4, 55]. Baselines. We experiment with the version 1.5 of stable diffusion [63] (SD) model. On COCO2017, we compare our Res-Tuning framework with zero-shot SD, SD with fully fine-tuning as well as SD with other existing tuning methods. On the fine-grained datasets, we employ DreamBooth [65] as our baseline and employ various tuning methods including our own for comparison. 5.2 Main results Text-to-image generation on COCO.We show the comparison between ourRes-Tuning framework and other approaches both quantitatively and qualitatively. The quantitative comparison is presented in Tab. 6. Compared with the fully fine-tuned baseline, our Res-Tuning framework improves the FID score by 0.89, using only 0.29% of the parameters. It is noteworthy that our Res-Tuning framework is the only approach that reaches a FID score below 14, while the best existing tuning strategy on the task is LoRA [27] achieving 14.50 with 4x the number of parameters in Res-Tuning. Hence, employing Res-Tuning could greatly facilitate the adaptation of pre-trained text-to-image generation models to downstream tasks. A highlight is observed that our Res-Tuning-Bypass framework reduces the memory consumption for tuning the SD v1.5 model to only 29% while maintaining a similar performance (14.89 vs 14.85). Meanwhile, the time consumption for training the model is reduced to 41%. In terms of the reduction in the memory and time consumption, our Res-Tuning-Bypass framework outperforms the best tuning strategy by 3.3x (70.7% memory reduction ofRes-Tuning-Bypass vs. 25.3% that of adapter) and 1.7x (58.6% reduction in time consumption of Res-Tuning-Bypass vs. 34.3% that of adapter), respectively. The qualitative results are presented in Fig. 6. Both Res-Tuning and Res-Tuning-Bypass show a better alignment between the text and the generated image, where the surfboard is propped up in our generated images. Res-Tuning also demonstrates a better fidelity where the feather texture is realistic in the generated black bird. 1https://huggingface.co/runwayml/stable-diffusion-v1-5 9SD v1.5 DreamBooth + LoRA + Adapter + Prefix + Prompt + Res-Tuning + Bypass Balloon Flower five petals bell-shaped Hotdog mayonnaise mustard ketchup Figure 7: Qualitative results of SD v1.5, DreamBooth, existing tuning strategies, and our Res-Tuning on Oxford Flowers and Food-101 fine-grained dataset with the same generated seed. We frame our results in green and others in red. Text-to-image generation on Oxford Flowers and Food-101. Here, we evaluate the transfer ability of existing tuning strategies on specific fine-grained categories. During every evaluation process, we select data from one specific category to train the model. The qualitative results are demonstrated in Fig. 7. It is observed that Res-Tuning presents a better view in terms of fidelity and the correct understanding of ambiguous categories. For example, the balloon flower is bell-shaped and has five petals, while existing approaches generate flowers with the wrong number of petals or even balloons rather than real flowers. Instead, both our Res-Tuning and our Res-Tuning-Bypass retain correct semantics and fine-grained features. 6 Related work Transformers in computer vision. Transformers [70] have demonstrated strong capabilities in various fields [5, 61, 12, 59, 2, 17, 48, 53]. In computer vision, Vision Transformers (ViT) [13] are widely applied in various applications, such as visual classification [48, 39], object detection [67, 6], segmentation [84, 74] and generation [63, 57]. Owing to the strong scalability of the Transformer backbone [31, 10], recent endeavors either focus on expanding the size of the ViT [81] or training on a larger corpus of data in an unsupervised manner [75, 14]. Parameter-efficient tuning. Despite the strong performance and generalization ability brought by scaling up the Transformers, it also makes the adaptation to the downstream tasks expensive and almost infeasible. Hence, parameter-efficient transfer learning (PETL) emerged [ 26, 25, 76, 41, 85, 28]. Existing PETL methods could be generally categorized into three types: (i) MHA-based tuning embeds tunable parameters in the multi-head self-attention layers [27, 76, 28, 41, 45, 46]. (ii) FFN-based tuning methods represented by adapters [25] and its generalized versions [58, 33, 32, 19] introduce a multi-layer perceptron to the FFN layer. (iii) Other tuning methods adapt certain existing parameters [79, 42]. Closely related to our work, some recent efforts are devoted to finding out the optimal design paradigm of the tuning modules by neural architecture search [83]. Instead, we show that through our Res-Tuning framework, a stronger tuning strategy can be easily found by the flexible combination of several existing tuning strategies in our unbinding form. Memory-efficient tuning. Since the structures of the existing PETL methods are deeply embedded in the backbone structure, back-propagation is required through the massive parameters of the pre- trained models, leading to unnecessary extra consumption of memory and computation. Hence, Side-Tuning [82] and LST [68] in natural language processing connect a side network in parallel with the pre-trained model to avoid data flow from the trainable parameters to the frozen ones. Our Res-Tuning-Bypass is inspired by the conceptual idea of these approaches. Compared to Side- Tuning and LST, we show that ourRes-Tuning-Bypass is more flexible and memory-efficient. 7 Conclusion In this work, we unbind the tuners from the backbone and form a flexible and efficient tuning paradigm Res-Tuning. With Res-Tuning, we are able to find stronger tuning strategies compared to existing ones. On top of Res-Tuning, we further extend a memory-efficient Res-Tuning-Bypass, which significantly reduces the memory consumption and multi-task inference cost. We hope our discoveries can facilitate further research in the flexible and efficient tuning of large foundation models. 10References [1] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y . Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al. Flamingo: A visual language model for few-shot learning. arXiv preprint arXiv:2204.14198, 2022. [2] H. Bao, L. Dong, S. Piao, and F. Wei. BEiT: BERT pre-training of image Transformers. arXiv preprint arXiv:2106.08254, 2021. [3] C. Beattie, J. Z. Leibo, D. Teplyashin, T. Ward, M. Wainwright, H. Küttler, A. Lefrancq, S. Green, V . Valdés, A. Sadik, et al. Deepmind lab. arXiv preprint arXiv:1612.03801, 2016. [4] L. Bossard, M. Guillaumin, and L. Van Gool. Food-101 – mining discriminative components with random forests. In Eur. Conf. Comput. Vis., 2014. [5] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. In Adv. Neural Inform. Process. Syst., 2020. [6] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko. End-to-end object detection with Transformers. In Eur. Conf. Comput. Vis., pages 213–229, 2020. [7] S. Chen, C. Ge, Z. Tong, J. Wang, Y . Song, J. Wang, and P. Luo. AdaptFormer: Adapting vision Transformers for scalable visual recognition. In Adv. Neural Inform. Process. Syst., 2022. [8] G. Cheng, J. Han, and X. Lu. Remote sensing image scene classification: Benchmark and state of the art. Proceedings of the IEEE, 2017. [9] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi. Describing textures in the wild. In IEEE Conf. Comput. Vis. Pattern Recog., 2014. [10] M. Dehghani, J. Djolonga, B. Mustafa, P. Padlewski, J. Heek, J. Gilmer, A. Steiner, M. Caron, R. Geirhos, I. Alabdulmohsin, et al. Scaling Vision Transformers to 22 billion parameters. arXiv preprint arXiv:2302.05442, 2023. [11] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A large-scale hierarchical image database. In IEEE Conf. Comput. Vis. Pattern Recog., pages 248–255, 2009. [12] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional Transformers for language understanding. North Am. Chap. Assoc. Comput. Linguist., 2018. [13] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In Int. Conf. Learn. Represent., 2020. [14] Y . Fang, W. Wang, B. Xie, Q. Sun, L. Wu, X. Wang, T. Huang, X. Wang, and Y . Cao. Eva: Exploring the limits of masked visual representation learning at scale. arXiv preprint arXiv:2211.07636, 2022. [15] T. Gebru, J. Krause, Y . Wang, D. Chen, J. Deng, and L. Fei-Fei. Fine-grained car detection for visual census estimation. In Assoc. Adv. Artif. Intell., 2017. [16] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets robotics: The KITTI dataset. Int. J. Robotics Research, 2013. [17] Y . Gong, Y .-A. Chung, and J. Glass. Ast: Audio spectrogram transformer.arXiv preprint arXiv:2104.01778, 2021. [18] B. Graham. Kaggle diabetic retinopathy detection competition report., 2015. [19] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, and G. Neubig. Towards a unified view of parameter-efficient transfer learning. In Int. Conf. Learn. Represent., 2022. [20] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In IEEE Conf. Comput. Vis. Pattern Recog., pages 770–778, 2016. [21] K. He, X. Chen, S. Xie, Y . Li, P. Dollár, and R. Girshick. Masked autoencoders are scalable vision learners. In IEEE Conf. Comput. Vis. Pattern Recog., pages 16000–16009, 2022. [22] P. Helber, B. Bischke, A. Dengel, and D. Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2019. 11[23] D. Hendrycks, S. Basart, N. Mu, S. Kadavath, F. Wang, E. Dorundo, R. Desai, T. Zhu, S. Parajuli, M. Guo, D. Song, J. Steinhardt, and J. Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Int. Conf. Comput. Vis., 2021. [24] D. Hendrycks, K. Zhao, S. Basart, J. Steinhardt, and D. Song. Natural adversarial examples. In IEEE Conf. Comput. Vis. Pattern Recog., 2021. [25] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly. Parameter-efficient transfer learning for NLP. In Int. Conf. Mach. Learn., pages 2790–2799, 2019. [26] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly. Parameter-efficient transfer learning for nlp. In Int. Conf. Mach. Learn., pages 2790–2799, 2019. [27] E. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, L. Wang, and W. Chen. LoRA: Low-rank adaptation of large language models. In Int. Conf. Learn. Represent., 2021. [28] M. Jia, L. Tang, B.-C. Chen, C. Cardie, S. Belongie, B. Hariharan, and S.-N. Lim. Visual prompt tuning. In Eur. Conf. Comput. Vis., 2022. [29] S. Jie and Z.-H. Deng. Convolutional bypasses are better vision Transformer adapters. arXiv preprint arXiv:2207.07039, 2022. [30] J. Johnson, B. Hariharan, L. van der Maaten, L. Fei-Fei, C. Lawrence Zitnick, and R. Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In IEEE Conf. Comput. Vis. Pattern Recog., 2017. [31] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [32] R. Karimi Mahabadi, J. Henderson, and S. Ruder. Compacter: Efficient low-rank hypercomplex adapter layers. In Adv. Neural Inform. Process. Syst., volume 34, pages 1022–1035, 2021. [33] R. Karimi Mahabadi, S. Ruder, M. Dehghani, and J. Henderson. Parameter-efficient multi-task fine-tuning for Transformers via shared hypernetworks. In Assoc. Comput. Linguist., pages 565–576, 2021. [34] A. Khosla, N. Jayadevaprakash, B. Yao, and F.-F. Li. Novel dataset for fine-grained image categorization: Stanford dogs. In IEEE Conf. Comput. Vis. Pattern Recog. Worksh., 2011. [35] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. Master’s thesis, Department of Computer Science, University of Toronto, 2009. [36] Y . LeCun, F. J. Huang, and L. Bottou. Learning methods for generic object recognition with invariance to pose and lighting. In IEEE Conf. Comput. Vis. Pattern Recog., 2004. [37] B. Lester, R. Al-Rfou, and N. Constant. The power of scale for parameter-efficient prompt tuning. In Conf. Empirical Methods NLP, 2021. [38] F.-F. Li, R. Fergus, and P. Perona. One-shot learning of object categories. IEEE Trans. Pattern Anal. Mach. Intell., 2006. [39] K. Li, Y . Wang, P. Gao, G. Song, Y . Liu, H. Li, and Y . Qiao. UniFormer: Unified Transformer for efficient spatiotemporal representation learning. In Int. Conf. Learn. Represent., 2022. [40] L. H. Li, M. Yatskar, D. Yin, C.-J. Hsieh, and K.-W. Chang. VisualBERT: A simple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557, 2019. [41] X. L. Li and P. Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Assoc. Comput. Linguist., pages 4582–4597, 2021. [42] D. Lian, D. Zhou, J. Feng, and X. Wang. Scaling & shifting your features: A new baseline for efficient model tuning. In Adv. Neural Inform. Process. Syst., 2022. [43] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft COCO: Common objects in context. In Eur. Conf. Comput. Vis., pages 740–755, 2014. [44] L. Liu, Y . Ren, Z. Lin, and Z. Zhao. Pseudo numerical methods for diffusion models on manifolds. InInt. Conf. Learn. Represent., 2022. 12[45] X. Liu, K. Ji, Y . Fu, Z. Du, Z. Yang, and J. Tang. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. arXiv preprint arXiv:2110.07602, 2021. [46] X. Liu, Y . Zheng, Z. Du, M. Ding, Y . Qian, Z. Yang, and J. Tang. GPT understands, too.arXiv preprint arXiv:2103.10385, 2021. [47] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V . Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692, 2019. [48] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and B. Guo. Swin Transformer: Hierarchical Vision Transformer using shifted windows. In Int. Conf. Comput. Vis., 2021. [49] Z. Liu, H. Mao, C.-Y . Wu, C. Feichtenhofer, T. Darrell, and S. Xie. A convnet for the 2020s. InIEEE Conf. Comput. Vis. Pattern Recog., pages 11976–11986, 2022. [50] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In Int. Conf. Learn. Represent., 2019. [51] S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013. [52] L. Matthey, I. Higgins, D. Hassabis, and A. Lerchner. dSprites: Disentanglement testing sprites dataset, 2017. [53] T. Meinhardt, A. Kirillov, L. Leal-Taixe, and C. Feichtenhofer. Trackformer: Multi-object tracking with transformers. In IEEE Conf. Comput. Vis. Pattern Recog., pages 8844–8854, 2022. [54] Y . Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y . Ng. Reading digits in natural images with unsupervised feature learning. In Adv. Neural Inform. Process. Syst. Worksh., 2011. [55] M.-E. Nilsback and A. Zisserman. Automated flower classification over a large number of classes. In Ind. Conf. Comput. Vis. Graph. Image Process., pages 722–729, 2008. [56] O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. V . Jawahar. Cats and dogs. InIEEE Conf. Comput. Vis. Pattern Recog., 2012. [57] W. Peebles and S. Xie. Scalable diffusion models with Transformers. arXiv preprint arXiv:2212.09748, 2022. [58] J. Pfeiffer, A. Kamath, A. Rücklé, K. Cho, and I. Gurevych. AdapterFusion: Non-destructive task composition for transfer learning. In Eur. Chap. Assoc. Comput. Linguist., pages 487–503, 2020. [59] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In Int. Conf. Mach. Learn., pages 8748–8763, 2021. [60] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with a unified text-to-text Transformer. J. Mach. Learn. Res., 21(140):1–67, 2020. [61] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional image generation with CLIP latents. arXiv preprint arXiv:2204.06125, 2022. [62] B. Recht, R. Roelofs, L. Schmidt, and V . Shankar. Do imagenet classifiers generalize to imagenet? InInt. Conf. Mach. Learn., 2019. [63] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In IEEE Conf. Comput. Vis. Pattern Recog., pages 10684–10695, 2022. [64] O. Ronneberger, P. Fischer, and T. Brox. U-Net: Convolutional networks for biomedical image segmentation. In Med. Image Comput. Computer-Assisted Interv., pages 234–241, 2015. [65] N. Ruiz, Y . Li, V . Jampani, Y . Pritch, M. Rubinstein, and K. Aberman. Dreambooth: Fine tuning text- to-image diffusion models for subject-driven generation. In IEEE Conf. Comput. Vis. Pattern Recog., 2022. [66] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng, and C. Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Conf. Empirical Methods NLP, pages 1631–1642, 2013. 13[67] H. Song, D. Sun, S. Chun, V . Jampani, D. Han, B. Heo, W. Kim, and M.-H. Yang. ViDT: An efficient and effective fully Transformer-based object detector. In Int. Conf. Learn. Represent., 2022. [68] Y .-L. Sung, J. Cho, and M. Bansal. LST: Ladder side-tuning for parameter and memory efficient transfer learning. In Adv. Neural Inform. Process. Syst., 2022. [69] G. Van Horn, S. Branson, R. Farrell, S. Haber, J. Barry, P. Ipeirotis, P. Perona, and S. Belongie. Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection. In IEEE Conf. Comput. Vis. Pattern Recog., pages 595–604, 2015. [70] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. In Adv. Neural Inform. Process. Syst., volume 30, 2017. [71] B. S. Veeling, J. Linmans, J. Winkens, T. Cohen, and M. Welling. Rotation equivariant cnns for digital pathology. In Med. Image Comput. Computer-Assisted Interv., 2018. [72] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011 dataset. California Institute of Technology, 2011. [73] H. Wang, S. Ge, Z. C. Lipton, and E. P. Xing. Learning robust global representations by penalizing local predictive power. In Adv. Neural Inform. Process. Syst., 2019. [74] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and L. Shao. Pyramid vision Transformer: A versatile backbone for dense prediction without convolutions. In Int. Conf. Comput. Vis., pages 568–578, 2021. [75] W. Wang, H. Bao, L. Dong, J. Bjorck, Z. Peng, Q. Liu, K. Aggarwal, O. K. Mohammed, S. Singhal, S. Som, et al. Image as a foreign language: Beit pretraining for all vision and vision-language tasks. arXiv preprint arXiv:2208.10442, 2022. [76] Z. Wang, Z. Zhang, S. Ebrahimi, R. Sun, H. Zhang, C.-Y . Lee, X. Ren, G. Su, V . Perot, J. Dy, et al. DualPrompt: Complementary prompting for rehearsal-free continual learning. In Eur. Conf. Comput. Vis., 2022. [77] A. Williams, N. Nangia, and S. Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In North Am. Chap. Assoc. Comput. Linguist., pages 1112–1122, 2018. [78] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. SUN database: Large-scale scene recognition from abbey to zoo. In IEEE Conf. Comput. Vis. Pattern Recog., pages 3485–3492, 2010. [79] E. B. Zaken, S. Ravfogel, and Y . Goldberg. BitFit: Simple parameter-efficient fine-tuning for Transformer- based masked language-models. Assoc. Comput. Linguist., 2021. [80] X. Zhai, J. Puigcerver, A. Kolesnikov, P. Ruyssen, C. Riquelme, M. Lucic, J. Djolonga, A. S. Pinto, M. Neumann, A. Dosovitskiy, et al. A large-scale study of representation learning with the visual task adaptation benchmark. arXiv preprint arXiv:1910.04867, 2019. [81] X. Zhai, A. Kolesnikov, N. Houlsby, and L. Beyer. Scaling Vision Transformers. In IEEE Conf. Comput. Vis. Pattern Recog., pages 12104–12113, 2022. [82] J. O. Zhang, A. Sax, A. Zamir, L. J. Guibas, and J. Malik. Side-Tuning: Network adaptation via additive side networks. In Eur. Conf. Comput. Vis., 2019. [83] Y . Zhang, K. Zhou, and Z. Liu. Neural prompt search. arXiv preprint arXiv:2206.04673, 2022. [84] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y . Wang, Y . Fu, J. Feng, T. Xiang, P. H. Torr, and L. Zhang. Rethinking semantic segmentation from a sequence-to-sequence perspective with Transformers. In IEEE Conf. Comput. Vis. Pattern Recog., 2021. [85] K. Zhou, J. Yang, C. C. Loy, and Z. Liu. Conditional prompt learning for vision-language models. In IEEE Conf. Comput. Vis. Pattern Recog., pages 16816–16825, 2022. 14In the appendix, we provide a detailed derivation process for turning existing PETL methods into our unbinding formulation (appendix A), demonstration of our unbinding formulation as a unified formulation for existing PETL methods (appendix B), more implementation details (appendix C) including the dataset used, architectures for discriminative and generative tasks, and hyperparameters used in training, additional results on discriminative tasks (appendix D) and generative tasks (appendix E). A Detailed derivations In this section, we provide the detailed deriving process for the existing prefix [41] and prompt [28] tuning to be turned into our unbinding formulation. Prefix tuning [41]: The following is the detailed derivation of Eq. (3). MHApre = Attn(xWq, [Kpre; xWk], [Vpre; xWv]) = softmax \u0000 xWq[Kpre; xWk]⊤\u0001 \" Vpre xWv # = (1− λ(x)) softmax \u0000 xWqW⊤ k x⊤\u0001 xWv + λ(x) softmax \u0000 xWqK⊤ pre \u0001 Vpre = (1− λ(x)) Attn (xWq, xWk, xWv) +λ(x) Attn (xWq, Kpre, Vpre) = (1− λ(Q, K, Kpre)) Attn (Q, K, V )| {z } standard attention +λ(Q, K, Kpre) Attn (Q, Kpre, Vpre)| {z } independent of Kpre,Vpre (9) where Q, K denote the original query and original key, Kpre and Vpre are prefix key tokens and prefix value tokens, and λ(Q, K, Kpre) = P i exp \u0000 QK⊤ pre \u0001 iP i exp (QK⊤)i + P j exp \u0000 QK⊤pre \u0001 j , (10) Prompt tuning [28]: The following is the detailed derivation of Eq. (5). MHApro = Attn ([x; xpro]Wq, [x; xpro]Wk, [x; xpro]Wv) = concat \u0012 softmax \u0000 xWq[xWk; xproWk]⊤\u0001\u0014 xWv xproWv \u0015 , softmax \u0000 xproWq[xWk; xproWk]⊤\u0001\u0014 xWv xproWv \u0015\u0013 = concat ((1− λ(Q, K, Kpro)) Attn (Q, K, V ) +λ(Q, K, Kpro) Attn (Q, Kpro, Vpro) , (1 − β(Qpro, Kpro, K)) Attn (Qpro, Kpro, Vpro) +β(Qpro, Kpro, K) Attn (Qpro, K, V )) (11) where Kpro and Vpro are prompt key tokens and prompt value tokens, and λ(Q, K, Kpro) = P i exp \u0000 QK⊤ pro \u0001 iP i exp (QK⊤)i + P j exp \u0000 QK⊤pro \u0001 j , (12) β(Qpro, Kpro, K) = P i exp \u0000 QproK⊤\u0001 iP i exp \u0000 QproK⊤pro \u0001 i + P j exp (QproK⊤)j , (13) 15B Unified formulation From Eq. (7), we derive a unified formulation for existing PETL methods, which is the combination of the frozen pre-trained operation (OP) and the tuner with learnable parameters (Res-Tuner). The following is the detailed instantiation of this unified formulation (as in Tab. 7). Thanks to the form of unbinding, tuners can be treated as individual and flexibly combined. Our framework can effectively encompass existing tuning methods in a residual form and is not limited to the OP and Res-Tuner mentioned in our work. For example, for MAM-Adapter, we are free to attach Res-Prefix and Res-Adapter to MHA and FFN modules, respectively. In particular, when new PETL methods are proposed, they can be quickly applied (abbreviated as New-Tuning), and different methods can be arbitrarily combined (abbreviated as Mix-Tuning). Table 7: The combination of Res-Tuning. Method OP Res-Tuner Adapter [25] FFN Res-Adapter Prefix [41] MHA Res-Prefix Prompt [28] MHA Res-Prompt LoRA [27] Weight MLP BitFit [79] Bias Parameter AdaptFormer [7] FFN Res-Adapter MAM-Adapter [19] MHA + FFN Res-Prefix + Res-Adapter Side-Tuning [82] All blocks Side model LST [68] Block ViT New-Tuning Any where New-Tuner Mix-Tuning Any combination Any combination C Implementation details C.1 Dataset description In Tab. 8 and Tab. 9, we list the description of each dataset in our experiments, which includes the number of classes and the amount of images in training set and test set for discriminative and generative tasks, respectively. Table 8: Datasets used for generative tasks. ⋆ denotes only part of the data is used. Dataset Description Classes Train Test image prompt image prompt Common Objects in Context (COCO) COCO2017 Captions common objects - 118287 591753 5000 25014 Fine-grained Image Generation Aircraft [51]⋆ Fine-grained aircraft 100 3334 - 3333 - Food-101 [4]⋆ Fine-grained food 101 75750 - 25250 - NABirds [69]⋆ Fine-grained bird 555 23929 - 24633 - Stanford Cars [15]⋆ Fine-grained car 196 8144 - 8144 - Stanford Dogs [34]⋆ Fine-grained dog 120 12000 - 8580 - SUN397 [78]⋆ Fine-grained scene 397 76044 - 16295 - 16Table 9: Datasets used for discriminative tasks. Dataset Description Classes Train Test General Image Classification CIFAR-100 [35] General 100 50000 10000 Fine-grained Visual Classification (FGVC) CUB-200-2011 [72] Bird 200 5994 5794 NABirds [69] Bird 555 23929 24633 Oxford Flowers [55] Flower 102 1020 6149 Stanford Cars [15] Car 196 8144 8041 Stanford Dogs [34] Dog 120 12000 8580 Visual Task Adaptation Benchmark (VTAB-1K) CIFAR-100 [35] Natural 100 1000 10000 Caltech101 [38] 102 6084 DTD [9] 47 1880 Flower102 [55] 102 6149 Pets [56] 37 3669 SVHN [54] 10 26032 SUN397 [78] 397 21750 Camelyon [71] Specialized 2 1000 32768 EuroSAT [22] 10 5400 Resisc45 [8] 45 6300 Retinopathy [18] 5 42670 Clevr-Count [30] Structured 8 1000 15000 Clevr-Dist [30] 6 15000 DMLab [3] 6 22735 KITTI-Dist [16] 4 711 dSpr-Loc [52] 16 73728 dSpr-Ori [52] 16 73728 sNORB-Azim [36] 18 12150 sNORB-Ele [36] 9 12150 Few-Shot Learning FGVC-Aircraft [51] Aircraft 100 (1/2/4/8/16) * class 3333 Food-101 [4] Food 101 (1/2/4/8/16) * class 30300 Oxford Flowers [55] Flower 102 (1/2/4/8/16) * class 2463 Oxford Pets [56] Pet 37 (1/2/4/8/16) * class 3669 Stanford Cars [15] Car 196 (1/2/4/8/16) * class 8041 Domain Generalization ImageNet-1K [11] General 1000 16 * class 50000 ImageNet-V2 [62] General 1000 / 10000 ImageNet-Sketch [73] General 1000 / 50889 ImageNet-A [24] General 200 / 7500 ImageNet-R [23] General 200 / 30000 17C.2 Architectures design We show the complete Res-Tuning-Bypass structural design according to the different task frameworks. For discriminative tasks, we design based on the architecture of Vision Transformers [13], mainly based on main blocks and unbound bypass (as in Fig. 8). For generative tasks, we design based on the stable diffusion [ 63] framework and innovatively apply Res-Tuner to U-Net [ 64] structure (as in Fig. 9). We attach Res-Tuner to the U-Net intermediate module and decoder module for more efficient training. Block_1 Block_2 Block_N Horizontal Vertical Res- Tuner Res- Tuner Res- Tuner Res- Tuner Res-Tuning-Bypass Froward Backward × Detach Trainable Frozen ViT × …… Res- Tuner× Head Figure 8: Architecture design for discriminative tasks. 18Encoder 64×64 Encoder 32×32 Encoder 16×16 Encoder 8×8 Middle 8×8 Decoder 8×8 Decoder 16×16 Decoder 32×32 Decoder 64×64 × Horizontal Vertical Res- Tuner Res- Tuner × Res- Tuner × Res- Tuner × Res- Tuner Res- Tuner Res- Tuner Res- Tuner Conditional 2D U-Net Res-Tuning-Bypass Froward Backward × Detach Trainable Frozen Decoder Figure 9: Architecture design for generative tasks. 19C.3 Hyperparameters We list all the hyperparameters for discriminative (see in Tab. 10) and generative (see in Tab. 11) tasks. Table 10: Hyperparameter selection for discriminative tasks. Config Value Batch size 32 Optimizer AdamW [50] Weight decay 0.05 Base learning rate range {0.05, 0.01, 0.005, 0.001} Learning rate schedule Cosine decay Training epochs range {50, 100} Warmup epochs 10 Augmentation RandomResizedCrop, RandomHorizontalFlip OP MHA [70] FFN [70] Block [70] Res-Tuner Res-Adapter Res-Prefix Res-Prompt Architecture ViT/B-16 [13] ViT/L-14 [13] Pre-trained ImageNet-21K [11] CLIP [59] Device A100×1 Table 11: Hyperparameter selection for generative tasks. Config Value Batch size 8 Optimizer AdamW [50] Base learning rate 1e-4 Learning rate schedule Constant Training epochs 10 Augmentation CenterCrop, RandomHorizontalFlip Resolution 512 × 512 Sampler PLMS [44] Guidance 3.0 OP MHA [70] FFN [70] Block [70] Conditional 2D U-Net [64] Res-Tuner Res-Adapter Res-Prefix Res-Prompt Architecture Stable Diffusion [63] Pre-trained Stable Diffusion v1.5 [63] CLIP [59] Device A100 × 8 Library Diffusers2 2https://github.com/huggingface/diffusers 20D Additional experiments on discriminative tasks D.1 Comparisons on FGVC datasets We compare the transfer ability of our Res-Tuning framework with different existing approaches for parameter- and memory-efficient transfer learning on FGVC datasets. As shown in Tab. 12, our Res-Tuning outperforms other tuning methods on the average accuracy of five FGVC datasets. For memory-efficient methods, Res-Tuning-Bypass achieves a 2.89% improvement compared to LST [68] with less memory consumption. Table 12: Performance and efficiency comparison on FGVC. † denotes our own implementation. Method Datasets CUB- 200-2011 NABirds Oxford Flowers Stanford Cars Stanford Dogs Mean Param. (M) Mem. (GB) Full 87.3 82.7 98.8 84.5 89.4 88.54 85.98 9.40 Linear 85.3 75.9 97.9 51.3 86.2 79.32 0.18 3.09 Parameter-efficient tuning methods Adapter [25] 87.3 84.3 98.4 68.4 88.8 85.46 1.96 6.55 Prefix† [41] 85.4 78.8 99.2 76.4 89.5 85.86 0.36 6.60 VPT-Shallow [28] 86.7 78.8 98.4 68.7 90.7 84.62 0.25 8.14 VPT-Deep [28] 88.5 84.2 99.0 83.6 90.2 89.11 0.85 8.16 LoRA† [27] 86.0 80.2 99.2 85.2 88.6 87.84 0.55 6.78 Convpass† [29] 86.9 81.4 99.3 85.7 89.9 88.66 0.51 7.44 SSF [42] 89.5 85.7 99.6 89.2 89.6 90.72 0.39 7.47 Res-Tuning 89.66 85.87 99.45 87.58 92.21 90.95 0.68 8.98 Memory-efficient tuning methods Side-Tuning [82] 84.7 75.8 96.9 48.6 85.8 78.35 9.73 3.48 LST† [68] 82.98 76.11 98.83 78.46 87.89 84.85 1.04 5.28 Res-Tuning-Bypass 88.75 83.00 99.61 75.41 92.40 87.83 0.56 4.73 D.2 More ablation studies Varying the length of dimensions.The length of dimensions represents the hidden dimension for Res- Adapter and the number of tokens of Res-Prompt and Res-Prefix. Altering the length of dimensions affects performances, the number of parameters, and memory consumption simultaneously (see in Tab. 13). By default, we use the length that balances the number of parameters and memory, although there is a slight increase as the length grows to a certain extent. Table 13: Ablation studies on the length of dimension for Res-Tuning on CIFAR-100. Default setting is marked in gray. Dim Res-Tuning Res-Tuning-Bypass Acc. Param. Mem. Acc. Param. Mem. 5 93.03 0.30M 6.84G 89.16 0.37M 4.65G 10 93.25 0.48M 6.85G 89.33 0.46M 4.72G 20 92.98 0.85M 6.87G 89.28 0.64M 4.83G 50 92.69 1.96M 6.90G 89.22 1.19M 5.08G 100 92.48 3.80M 6.96G 89.52 2.11M 5.68G 21Different number of block layers. Based on the ViT/B-16 structure, we change the number of block layers in two ways: (i) increase from head to toe gradually (see in Tab. 14), (ii) discard several intermediate layers (see in Tab. 15), and evaluate the impact of using a different number of layers. It can be seen that the best performance can be achieved by using the full number of layers, but it requires the use of relatively more parameters and memory. Table 14: Ablation studies on the tuning block layers for Res-Tuning on CIFAR-100. The right arrow denotes use the number of all layers from left to right. Default setting is marked in gray. Blocks Res-Tuning Res-Tuning-Bypass Acc. Param. Mem. Acc. Param. Mem. 1 → 1 92.23 0.11M 6.43G 88.50 0.11M 3.58M 1 → 3 92.78 0.18M 6.52G 88.60 0.17M 3.73G 1 → 6 92.93 0.28M 6.64G 89.05 0.27M 4.09G 1 → 9 93.17 0.38M 6.84G 89.32 0.36M 4.53G 1 → 12 93.25 0.48M 6.85G 89.33 0.46M 4.72G Table 15: Ablation studies on the drop block layers for Res-Tuning on CIFAR-100. Inside the braces indicate the layer number that is discarded. The default setting is marked in gray. Drop block layers Res-Tuning Res-Tuning-Bypass Acc. Param. Mem. Acc. Param. Mem. {} 93.25 0.48M 6.85G 89.33 0.46M 4.72G {3, 6, 9} 92.88 0.38M 6.73G 89.30 0.36M 4.36G {2, 3, 5, 7, 9, 11} 92.96 0.28M 6.68G 88.89 0.27M 3.96G {2, 3, 4, 5, 6, 8, 9, 10 ,11} 92.71 0.18M 6.56G 88.79 0.17M 3.65G {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11} 87.77 0.11M 3.31G 88.49 0.11M 3.48G Different CNN backbones. We present the results for ConvNeXt [49] and ResNet-101 [20] pre- trained on ImageNet-21K in Tab. 16. We observe that two convolutional models have different characteristics, where the performance variation of ConvNeXt is small and that of ResNet is large. In terms of the effectiveness of Res-Tuning-Bypass, it is observed that it outperforms the fully- finetuned version of ConvNeXt and notably improves the performance of linear probing for ResNet. Table 16: Ablation studies on the CNN-based backbones on CIFAR-100. ConvNext ResNet-101 Method Acc. Param. Mem. Acc. Param. Mem. Full 90.15 87.67 11.16 77.40 42.70 7.30 Linear 90.06 0.11 3.36 54.96 0.20 2.83 Res-Tuning 90.86 0.87 9.45 86.80 0.92 7.20 Res-Tuning-Bypass 90.51 1.13 3.63 72.27 3.63 4.05 Experiments on NLP downstream task. We also conduct experiments on downstream tasks beyond vision. For NLP, we perform experiments on the SST2 [66] and MNLI [77] datasets for text classification tasks. It is observed that the performance of our Res-Tuning framework is on par with or better than MAM-Adapter [19] in text classification, with slightly longer training time but lower memory consumption. Our Res-Tuning-Bypass significantly reduces the training time and memory consumption and achieves a mildly lower performance. 22Table 17: Performance comparison on text classification task. SST2 MNLI Train Param. Test Param. Mem.Method Acc. Train Time Acc. Train Time MAM Adapter [19] 94.2 7.2 87.4 41.4 46.78 (37.4%) 0.61 (0.5%) 22.4G Res-Tuning 94.56 7.9 87.45 47.3 0.97 (0.77%) 0.97 (0.77%) 19.3G Res-Tuning-Bypass 92.94 4.2 82.01 24.2 0.98 (0.78%) 0.98 (0.78%) 4.3G E Additional Experiments on generative tasks E.1 More visualizations on COCO We present more visualization results, comparing real images, stable diffusion [63] (SD) v1.5, fully fine-tuning, LoRA [ 27], Res-Tuning and Res-Tuning-Bypass in Fig. 10 and more detailed generated image in Fig. 11. Specifically, the text conditions of text-to-image task are sampled from COCO Captions. To better demonstrate the advantages of our approach in understanding the concepts of text, the main elements are marked in blue, and the main modifiers or prepositions are marked in green. E.2 More visualizations on fine-grained datasets We present more visualization results on fine-grained datasets (see in Fig. 12, Fig. 13, and Fig. 14), including NABirds [69], Stanford Dogs [34], Stanford Cars [15], Aircraft [51], SUN397 [78] and Food-101 [4]. F Limitations and Societal Impacts This work is a tuning paradigm that is fine-tuned based on the pre-trained foundation models while freezing the backbone network, so its transfer ability depends to a large extent on the performance of the upstream model. However, when the upstream pre-training model contains illegal content training, it will also lead to the illegal use of tuning methods. 23Figure 10: Qualitative results of existing tuning strategies and our Res-Tuning on COCO2017 validation set. 24Bananas, marshmellows, chocolate  chips and sprinkles in a bowl. The rain is pouring on  the white car on the street. A close up of glazed donuts  that are plain or with chocolate. A landscape with water,  a boat, trees and mountains. Two elephants standing  next to each other in a field. A single white rose  in a glass vase. Figure 11: Visualization of our Res-Tuning on COCO2017 validation set. 25Figure 12: Visualization of our Res-Tuning on NABirds and Stanford Dogs. 26Figure 13: Visualization of Res-Tuning on Stanford Cars and Aircraft. 27Figure 14: Visualization of Res-Tuning on SUN397 and Food-101. 28",
      "meta_data": {
        "arxiv_id": "2310.19859v1",
        "authors": [
          "Zeyinzi Jiang",
          "Chaojie Mao",
          "Ziyuan Huang",
          "Ao Ma",
          "Yiliang Lv",
          "Yujun Shen",
          "Deli Zhao",
          "Jingren Zhou"
        ],
        "published_date": "2023-10-30T17:58:19Z",
        "pdf_url": "https://arxiv.org/pdf/2310.19859v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Res-Tuning, a novel paradigm that unbinds tuners from the backbone of large foundation models to address restrictions in design flexibility and high computational costs of existing parameter-efficient tuning (PETL) methods. It proposes a unified unbinding formulation for popular PETL methods (prefix, prompt, adapters) and demonstrates its seamless integration. The framework allows for flexible combination of various tuning strategies, leading to stronger performance. A memory-efficient variant, Res-Tuning-Bypass, detaches tuners from the backbone to enable gradient propagation only to tuners, reducing training memory and facilitating one-time backbone forward for multi-task inference, achieving state-of-the-art efficacy and efficiency across discriminative and generative tasks.",
        "methodology": "Res-Tuning unbinds tuners from the pre-trained backbone by re-formulating existing PETL methods (prefix tuning, prompt tuning, adapter tuning) into a parallel structure, expressed as x' = OP(x) + Res-Tuner(x). OP is a frozen backbone operation, and Res-Tuner is a learnable parallel structure. This allows flexible design and combination of tuners (e.g., Single, Dual, Tri-Res-Tuner). Res-Tuning-Bypass, a memory-efficient variant, physically detaches the Res-Tuners from the backbone during training, creating a bypass network. This prevents backpropagation through the massive backbone parameters, only updating the tuners, and enables a single backbone forward pass for multi-task inference.",
        "experimental_setup": "The framework was evaluated on discriminative tasks (transfer learning, few-shot learning, domain generalization) and generative tasks (text-to-image generation). Backbones included ViT-B/16 (ImageNet-21K) and ViT-L/14 (CLIP) for discriminative tasks, and Stable Diffusion v1.5 (U-Net architecture) for generative tasks. Discriminative datasets used were CIFAR-100, VTAB-1K (19 diverse tasks), five fine-grained datasets for few-shot learning, and ImageNet variants for domain generalization. Generative datasets included COCO2017, Oxford Flowers, and Food-101. Metrics were Top-1 accuracy for discriminative tasks and FID score plus qualitative results for generative tasks. Baselines included fully fine-tuning, linear probing, and various parameter-efficient and memory-efficient tuning methods (e.g., Adapter, LoRA, VPT, SSF, Side-Tuning, LST). Hyperparameters involved AdamW optimizer, cosine decay learning rate schedule, and specific tuner dimensions.",
        "limitations": "The transfer ability of Res-Tuning largely depends on the performance of the upstream pre-trained foundation model. Additionally, if the upstream pre-training model contains illegal content, this could lead to the illegal use of tuning methods with Res-Tuning.",
        "future_research_directions": "The authors express hope that their discoveries will facilitate further research in the flexible and efficient tuning of large foundation models, implying future exploration of more sophisticated tuner designs, combinations, and application to an even wider range of foundation models or tasks, leveraging the unbinding formulation and memory efficiency."
      }
    },
    {
      "title": "Conditionally Adaptive Multi-Task Learning: Improving Transfer Learning in NLP Using Fewer Parameters & Less Data",
      "abstract": "Multi-Task Learning (MTL) networks have emerged as a promising method for\ntransferring learned knowledge across different tasks. However, MTL must deal\nwith challenges such as: overfitting to low resource tasks, catastrophic\nforgetting, and negative task transfer, or learning interference. Often, in\nNatural Language Processing (NLP), a separate model per task is needed to\nobtain the best performance. However, many fine-tuning approaches are both\nparameter inefficient, i.e., potentially involving one new model per task, and\nhighly susceptible to losing knowledge acquired during pretraining. We propose\na novel Transformer architecture consisting of a new conditional attention\nmechanism as well as a set of task-conditioned modules that facilitate weight\nsharing. Through this construction (a hypernetwork adapter), we achieve more\nefficient parameter sharing and mitigate forgetting by keeping half of the\nweights of a pretrained model fixed. We also use a new multi-task data sampling\nstrategy to mitigate the negative effects of data imbalance across tasks. Using\nthis approach, we are able to surpass single task fine-tuning methods while\nbeing parameter and data efficient (using around 66% of the data for weight\nupdates). Compared to other BERT Large methods on GLUE, our 8-task model\nsurpasses other Adapter methods by 2.8% and our 24-task model outperforms by\n0.7-1.0% models that use MTL and single task fine-tuning. We show that a larger\nvariant of our single multi-task model approach performs competitively across\n26 NLP tasks and yields state-of-the-art results on a number of test and\ndevelopment sets. Our code is publicly available at\nhttps://github.com/CAMTL/CA-MTL.",
      "full_text": "Published as a conference paper at ICLR 2021 CONDITIONALLY ADAPTIVE MULTI -TASK LEARNING : IMPROVING TRANSFER LEARNING IN NLP USING FEWER PARAMETERS & LESS DATA Jonathan Pilault1∗, Amine El hattami1∗, Christopher Pal1,2,3 1Polytechnique Montreal & Mila, 2Element AI, 3Canada CIFAR AI Chair {jonathan.pilault,amine.elhattami,christopher.pal}@polymtl.ca ABSTRACT Multi-Task Learning (MTL) networks have emerged as a promising method for transferring learned knowledge across different tasks. However, MTL must deal with challenges such as: overﬁtting to low resource tasks, catastrophic forgetting, and negative task transfer, or learning interference. Often, in Natural Language Processing (NLP), a separate model per task is needed to obtain the best perfor- mance. However, many ﬁne-tuning approaches are both parameter inefﬁcient, i.e., potentially involving one new model per task, and highly susceptible to losing knowledge acquired during pretraining. We propose a novel Transformer based Adapter consisting of a new conditional attention mechanism as well as a set of task-conditioned modules that facilitate weight sharing. Through this construction, we achieve more efﬁcient parameter sharing and mitigate forgetting by keeping half of the weights of a pretrained model ﬁxed. We also use a new multi-task data sam- pling strategy to mitigate the negative effects of data imbalance across tasks. Using this approach, we are able to surpass single task ﬁne-tuning methods while being parameter and data efﬁcient (using around 66% of the data for weight updates). Compared to other BERT Large methods on GLUE, our 8-task model surpasses other Adapter methods by 2.8% and our 24-task model outperforms by 0.7-1.0% models that use MTL and single task ﬁne-tuning. We show that a larger variant of our single multi-task model approach performs competitively across 26 NLP tasks and yields state-of-the-art results on a number of test and development sets. Our code is publicly available at https://github.com/CAMTL/CA-MTL. 1 I NTRODUCTION The introduction of deep, contextualized Masked Language Models (MLM) 1 trained on massive amounts of unlabeled data has led to signiﬁcant advances across many different Natural Language Processing (NLP) tasks (Peters et al., 2018; Liu et al., 2019a). Much of these recent advances can be attributed to the now well-known BERT approach (Devlin et al., 2018). Substantial improvements over previous state-of-the-art results on the GLUE benchmark (Wang et al., 2018) have been obtained by multiple groups using BERT models with task speciﬁc ﬁne-tuning. The “BERT-variant + ﬁne-tuning” formula has continued to improve over time with newer work constantly pushing the state-of-the-art forward on the GLUE benchmark. The use of a single neural architecture for multiple NLP tasks has shown promise long before the current wave of BERT inspired methods (Collobert & Weston, 2008) and recent work has argued that autoregressive language models (ARLMs) trained on large-scale datasets – such as the GPT family of models (Radford et al., 2018), are in practice multi-task learners (Brown et al., 2020). However, even with MLMs and ARLMs trained for multi-tasking, single task ﬁne-tuning is usually also employed to achieve state-of-the-art performance on speciﬁc tasks of interest. Typically this ﬁne-tuning process may entail: creating a task-speciﬁc ﬁne-tuned model (Devlin et al., 2018), training specialized model components for task-speciﬁc predictions (Houlsby et al., 2019) or ﬁne-tuning a single multi-task architecture (Liu et al., 2019b). ∗Joint ﬁrst-authors 1For reader convenience, all acronyms in this paper are summarized in section A.1 of the Appendix. 1 arXiv:2009.09139v3  [cs.LG]  21 Apr 2022Published as a conference paper at ICLR 2021 Figure 1: CA-MTL base architecture with our uncertainty-based sampling algorithm. Each task has its own decoder. The input embedding layer and the lower Transformer layers are frozen. The up- per Transformer layer and Conditional Alignment module are modulated with the task embedding. Single-task ﬁne-tuning overall pretrained model parameters may have other issues. Recent analy- ses of such MLM have shed light on the linguistic knowledge that is captured in the hidden states and attention maps (Clark et al., 2019b; Tenney et al., 2019a; Merchant et al., 2020). Particularly, BERT has middle Transformer (Vaswani et al., 2017) lay- ers that are typically the most transferable to a downstream task (Liu et al., 2019a). The model proxies the steps of the traditional NLP pipeline in a localizable way (Tenney et al., 2019a) — with basic syntactic information appearing earlier in the network, while high-level semantic information ap- pearing in higher-level layers. Since pretraining is usually done on large-scale datasets, it may be use- ful, for a variety of downstream tasks, to conserve that knowledge. However, single task ﬁne-tuning causes catastrophic forgetting of the knowledge learned during MLM (Howard & Ruder, 2018). To preserve knowledge, freezing part of a pretrained network and using Adapters for new tasks have shown promising results (Houlsby et al., 2019). Inspired by the human ability to transfer learned knowledge from one task to another new task, Multi-Task Learning (MTL) in a general sense (Caruana, 1997; Rajpurkar et al., 2016b; Ruder, 2017) has been applied in many ﬁelds outside of NLP. Caruana (1993) showed that a model trained in a multi-task manner can take advantage of the inductive transfer between tasks, achieving a better generalization performance. MTL has the advantage of computational/storage efﬁciency (Zhang & Yang, 2017), but training models in a multi-task setting is a balancing act; particularly with datasets that have different: (a) dataset sizes, (b) task difﬁculty levels, and (c) different types of loss functions. In practice, learning multiple tasks at once is challenging since negative transfer (Wang et al., 2019a), task interference (Wu et al., 2020; Yu et al., 2020) and catastrophic forgetting (Serrà et al., 2018) can lead to worse data efﬁciency, training stability and generalization compared to single task ﬁne-tuning. Using Conditionally Adaptive Learning, we seek to improve pretraining knowledge retention and multi-task inductive knowledge transfer. Our contributions are the following: • A new task conditioned Transformer that adapts and modulates pretrained weights (Section 2.1). • A novel way to prioritize tasks with an uncertainty based multi-task data sampling method that helps balance the sampling of tasks to avoid catastrophic forgetting (Section 2.2). Our Conditionally Adaptive Multi-Task Learning (CA-MTL) approach is illustrated in Figure 1. To the best of our knowledge, our work is the ﬁrst to explore the use of a latent representation of tasks to modularize and adapt pretrained architectures. Further, we believe our work is also the ﬁrst to examine uncertainty sampling for large-scale multi-task learning in NLP. We show the efﬁcacy of CA-MTL by: (a) testing on 26 different tasks and (b) presenting state-of-the-art results on a number of test sets as well as superior performance against both single-task and MTL baselines. Moreover, we further demonstrate that our method has advantages over (c) other adapter networks, and (d) other MTL sampling methods. Finally, we provide ablations and separate analysis of the MT-Uncertainty Sampling technique in section 4.1 and of each component of the adapter in 4.2. 2 M ETHODOLOGY This section is organized according to the two main MTL problems that we will tackle: (1) How to modularize a pretrained network with latent task representations? (2) How to balance different tasks in MTL? We deﬁne each task as: Ti ≜ {pi(yi|xi,zi),Li,˜pi(xi)}, where zi is task i’s learnable shallow embedding, Li is the task loss, and ˜pi(xi) is the empirical distribution of the training data pair {xi,yi}, for i∈{1,...,T }and T the number of supervised tasks. The MTL objective is: min φ(z),θ1,...,θT T∑ i=1 Li(fφ(zi),θi(xi),yi) (1) 2Published as a conference paper at ICLR 2021 where f is the predictor function (includes encoder model and decoder heads), φ(z) are learnable generated weights conditioned on z, and θi are task-speciﬁc parameters for the output decoder heads. z is constructed using an embedding lookup table. 2.1 T ASK CONDITIONED TRANSFORMER Our task conditioned Transformer architecture is based on one simple concept. We either add conditional layers or modulate existing pretrained weights using a task representation by extending Feature Wise Linear Modulation (Perez et al., 2018) functions in several ways depending on the Transformer layer. We deﬁne our framework below. Deﬁnition 1 (Conditional Weight Transformations). Given a neural network weight matrixW, we compute transformations of the formφ(W|zi) = γi(zi)W + βi(zi), whereγi and βi are learned functions that transform the weights based on a learned vector embeddingzi, for taski. Deﬁnition 2 (Conditionally Adaptive Learning). In our setting, Conditionally Adaptive Learning is the process of learning a set ofφs for the conditionally adaptive modules presented below along with a set of task embedding vectorszi for T tasks, using a multi-task loss (see equation 1). In the subsections that follow: We introduce a new Transformer Attention Module using block- diagonal Conditional Attention that allows the original query-key based attention to account for task-speciﬁc biases (section 2.1.1). We propose a new Conditional Alignment method that aligns the data of diverse tasks and that performs better than its unconditioned and higher capacity predecessor (section 2.1.2). We adapt layer normalization statistics to speciﬁc tasks using a new Conditional Layer Normalization module (section 2.1.3). We add a Conditional Bottleneck that facilitates weight sharing and task-speciﬁc information ﬂow from lower layers (section 2.1.4). In our experiments we provide an ablation study of these components (Table 1) examining performance in terms of GLUE scores. 2.1.1 C ONDITIONAL ATTENTION Figure 2: Conditional Attention Module Given d, the input dimensions, the query Q, the key K, and the value V as deﬁned in Vaswani et al. (2017), we redeﬁne the attention operation: Attention(Q,K,V,zi)) = softmax [ M(zi) + QKT √ d ] V M(zi) = N⨁ n=1 A′ n(zi), A ′ n(zi) = Anγi(zi) + βi(zi) where ⨁is the direct sum operator (see section A.6), N is the number of block matrices An ∈R(L/N)×(L/N) along the diagonal of the attention matrix, Lis the input sequence, M(zi) = diag(A′ 1,...,A ′ N) is a block diagonal conditional matrix. Note that An is constructed using L/N trainable and randomly initialized L/N dimensional vectors. While the original attention matrix depends on the hidden states h, M(zi) is a learnable weight matrix that only depends on the task embedding zi ∈Rd. γi,βi : Rd ↦→RL2/N2 are Feature Wise Linear Modulation (Perez et al., 2018) functions. We also experimented with full-block Conditional Attention ∈RL×L. Not only did it have N2 more parameters compared to the block-diagonal variant, but it also performed signiﬁcantly worse on the GLUE development set (see FBA variant in Table 10). It is possible that GLUE tasks derive a certain beneﬁt from localized attention that is a consequence of M(zi). With M(zi), each element in a sequence can only attend to other elements in its subsequence of length L/N. In our experiments we used N = d/L. The full Conditional Attention mechanism used in our experiments is illustrated in Figure 2. 2.1.2 C ONDITIONAL ALIGNMENT Wu et al. (2020) showed that in MTL havingT separate alignment modules R1,...,R T increases BERTLARGE avg. scores on ﬁve GLUE tasks (CoLA, MRPC, QNLI, RTE, SST-2) by 2.35%. Inspired by this work, we found that adding a task conditioned alignment layer between the input embedding 3Published as a conference paper at ICLR 2021 layer and the ﬁrst BERT Transformer layer improved multi-task model performance. However, instead of having T separate alignment matrices Ri for each T task, one alignment matrix ˆR is generated as a function of the task embedding zi. As in Wu et al. (2020), we tested this module on the same ﬁve GLUE tasks and with BERTLARGE. Enabling task conditioned weight sharing across covariance alignment modules allows us to outperformsBERTLARGE by 3.61%. This is 1.26 % higher than having T separate alignment matrices. Inserting ˆRinto BERT, yields the following encoder function ˆf: ˆf = T∑ t=1 gθi(E(xi) ˆR(zi)B), ˆR(zi) = Rγi(zi) + βi(zi) (2) where xi ∈Rd is the layer input, gθi is the decoder head function for task iwith weights θi, Ethe frozen BERT embedding layer, Bthe BERT Transformer layers and Rthe linear weight matrix of a single task conditioned alignment matrix. γi,βi : Rd ↦→Rd are Feature Wise Linear Modulation functions. 2.1.3 C ONDITIONAL LAYER NORMALIZATION (CLN) We extend the Conditional Batch Normalization idea from de Vries et al. (2017) to Layer Normaliza- tion (Ba et al., 2016). For task Ti, i∈{1,...,T }: hi = 1 σ ⊙(ai −µ) ∗ˆγi(zi) + βi(zi), ˆγi(zi) = γ′γi(zi) + β′ (3) where hi is the CLN output vector, ai are the preceding layer activations associated with task i, µ and σare the mean and the variance of the summed inputs within each layer as deﬁned in Ba et al. (2016). Conditional Layer Normalization is initialized with BERT’s Layer Normalization afﬁne transformation weights and bias γ′and β′from the original formulation: h = 1 σ ⊙(a −µ) ∗γ′+ β′. During training, the weight and bias functions of γi(∗) and βi(∗) are always trained, while the original Layer Normalization weight may be kept ﬁxed. This module was added to account for task speciﬁc rescaling of individual training cases. Layer Normalization normalizes the inputs across features. The conditioning introduced in equation 2.1.3 allows us to modulate the normalization’s output based on a task’s latent representation. 2.1.4 C ONDITIONAL BOTTLENECK Figure 3: a) Conditional Bottleneck for CA-MTLBASE. b) Conditional Bottleneck for CA-MTLLARGE. We created a task conditioned two layer feed-forward bot- tleneck layer (CFF up/down in Figure 3). The conditional bottleneck layer follows the same transformation as in equation 2. The module in Figure 3a is added to the top most Transformer layers ofCA-MTLBASE and uses a CLN. For CA-MTLLARGE this module is the main building block of the skip connection added alongside all Transformer layers seen in Figure 3b. The connection at layer jtakes in the matrix sum of the Transformer layer output at j and the previous connection’s output atj−1. The Con- ditional bottleneck allows lower layer information to ﬂow upwards depending on the task. Our intuition for intro- ducing this component is related to recent studies (Tenney et al., 2019a) that showed that the “most important layers for a given task appear at speciﬁc positions”. As with the other modules described so far, each task adaptation is created from the weights of a single shared adapter that is modulated by the task embedding. 2.2 M ULTI -TASK UNCERTAINTY SAMPLING MT-Uncertainty Sampling is a task selection strategy that is inspired by Active Learning techniques. Our algorithm 1 is outlined in the Appendix, Section A.2. Similar to Active Learning, our algorithm ﬁrst evaluates the model uncertainty. MT-Uncertainty Sampling uses Shannon Entropy, an uncertainty measure, to choose training examples by ﬁrst doing forward pass through the model with b×T input samples. For an output classiﬁcation prediction with Ci possible classes and probabilities 4Published as a conference paper at ICLR 2021 (pi,1,...,p i,Ci), the Shannon Entropy Hi, for task Ti and i∈{1,...,T }, our uncertainty measure U(x) are given by: Hi = Hi(fφ(zi),θi(x)) = − Ci∑ c=1 pc log pc, U(xi) = Hi(fφ(zi),θi(x)) ˆH×H′ i (4) ˆH = max i∈{1,...,T} ¯Hi = max [ 1 b ∑ x∈xi Hi ] , H ′ i = − Ci∑ c=1 1 Ci log [ 1 Ci ] (5) where ¯Hi is the average Shannon Entropy across bsamples of task t, H′ i, the Shannon entropy of choosing classes with uniform distribution and ˆH, the maximum of each task’s average entropy over bsamples. H′ i is normalizing factor that accounts for differing number of prediction classes (without the normalizing factor H′ i, tasks with a binary classiﬁcation Ci = 1 were rarely chosen). Further, to limit high entropy outliers and to favor tasks with highest uncertainty, we normalize with ˆH. The measure in eq. 4 allows Algorithm 1 to choose bsamples from b×T candidates to train the model. 3 R ELATED WORK Multi-Tasking in NLP. To take advantage of the potential positive transfer of knowledge from one task to another, several works have proposed carefully choosing which tasks to train as an intermediate step in NLP before single task ﬁne-tuning (Bingel & Søgaard, 2017; Kerinec et al., 2018; Wang et al., 2019a; Standley et al., 2019; Pruksachatkun et al., 2020; Phang et al., 2018). The intermediate tasks are not required to perform well and are not typically evaluated jointly. In this work, all tasks are trained jointly and all tasks usedare evaluated from a single model. In Natural Language Understanding (NLU), it is still the case that to get the best task performance one often needs a separate model per task (Clark et al., 2019c; McCann et al., 2018). At scale, Multilingual NMT systems (Aharoni et al., 2019) have also found that MTL model performance degrades as the number of tasks increases. We notice a similar trend in NLU with our baseline MTL model. Recently, approaches in MTL have tackled the problem by designing task speciﬁc decoders on top of a shared model (Liu et al., 2019b) or distilling multiple single-task models into one (Clark et al., 2019c). Nonetheless, such MTL approaches still involves single task ﬁne-tuning. In this paper, we show that it is possible to achieve high performance in NLU without single task ﬁne-tuning. Adapters. Adapters are trainable modules that are attached in speciﬁc locations of a pretrained network. They provide another promising avenue to limit the number of parameters needed when confronted with a large number of tasks. This approach is useful with pretrained MLM models that have rich linguistic information (Tenney et al., 2019b; Clark et al., 2019b; Liu et al., 2019a; Tenney et al., 2019a). Recently, Houlsby et al. (2019) added an adapter to a pretrained BERT model by ﬁne-tuning the layer norms and adding feed forward bottlenecks in every Transformer layer. However, such methods adapt each task individually during the ﬁne-tuning process. Unlike prior work, our method harnesses the vectorized representations of tasks to modularize a single pretrained model across all tasks. Stickland et al. (2019) and Tay et al. (2020) also mix both MTL and adapters with BERT and T5 encoder-decoder (Raffel et al., 2019) respectively by creating local task modules that are controlled by a global task agnostic module. The main drawback is that a new set of non-shared parameters must be added when a new task is introduced. CA-MTL shares all parameters and is able to re-modulate existing weights with a new task embedding vector. Active Learning, Task Selection and Sampling. Our sampling technique is similar to the ones found in several active learning algorithms (Chen et al., 2006) that are based on Shannon entropy estimations. Reichart et al. (2008) and Ikhwantri et al. (2018) examined Multi-Task Active Learning (MTAL), a technique that chooses one informative sample forT different learners (or models) for each T tasks. Instead we choose T tasks samples for one model. Moreover, the algorithm weights each sample by the corresponding task score, and the Shannon entropy is normalized to account for various losses (see equation 5). Also, our algorithm is used in a large scale MTL setup (≫2 tasks). Recently, Glover & Hokamp (2019) explored task selection in MTL using learning policies based on counterfactual estimations (Charles et al., 2013). However, such method considers only ﬁxed stochastic parameterized policies while our method adapts its selection criterion based on model uncertainty throughout the training process. 5Published as a conference paper at ICLR 2021 Hypernetworks. CA-MTL is a hypernetwork adapter. The method to generate task-conditioned adapter weights is inspired by von Oswald et al. (2020). Hypernetwork layers have also been ﬁnetuned along with pretrained models. For example, Ponti et al. (2021) uses stochastic variational inference Hoffman et al. (2013) to produce language and task latent codes that conditionally generates the weights of a BERT prediction head, a single hypernetwork linear layer shared across multiple languages and tasks. Unlike previous methods however, CA-MTL conditionally modulates pretrained weights and biases, attention matrices, hidden representations and normalization statistics with task embeddings. Further, CA-MTL can preserve the pretraining knowledge by freezing the underlying Transformer model. Finally, we show a synergy between our hypernetwork adapter and our active task sampling technique (see section 2.2) that allows CA-MTL to continue surpassing fully tuned models as we scale the number of tasks (see ﬁgure 7). 4 E XPERIMENTS AND RESULTS We show that our adapter of section 2 achieve parameter efﬁcient transfer for 26 NLP tasks. Our implementation of CA-MTL is based on HuggingFace (Wolf et al., 2019). Hyperparameters and our experimental set-up are outlined in A.5. To preserve the weights of the pretrained model, CA-MTL’s bottom half Transformer layers are frozen in all experiments (except in section 4.4). We also tested different layer freezing conﬁgurations and found that freezing half the layers worked best on average (see Section A.8). 4.1 M ULTI -TASK UNCERTAINTY SAMPLING 0 250005000075000100000125000150000175000200000Training iteration 0.66 0.68 0.70 0.72 0.74 0.76 0.78 0.80 0.82Average score MT-UncertaintyCouterfactualTask sizeRandom Figure 4: MT-Uncertainty vs. other task sam- pling strategies: median dev set scores on 8 GLUE tasks and using BERTBASE. Data for the Counter- factual and Task Size policyπ|task|(eq. 6) is from Glover & Hokamp (2019). Our MT-Uncertainty sampling strategy, from section 2.2, is compared to 3 other task selection schemes: a) Counterfactual b) Task size c) Random. We used a BERTBASE (no adapters) on 200k iterations and with the same hyperparameters as in Glover & Hokamp (2019). For more information on Counterfactual task selection, we invite the reader to consult the full expla- nation in Glover & Hokamp (2019). For T tasks and the dataset Di for tasks i ∈{1,...,T }, we rewrite the deﬁnitions of Randomπrand and Task sizeπ|task| sampling: πrand = 1/T, π|task|= |Di| [ T∑ i=1 |Di| ]−1 (6) 500 5000 10000Train iteration 0.0 0.2 0.4 0.6 0.8 (a) Random 500 5000 10000Train iteration 0.0 0.2 0.4 0.6 0.8 (b) MT-Uncertainty MNLI-mm dev scoreCoLA dev score MNLI-mm train entropyCoLA train entropy Figure 5: CoLA/MNLI Dev set scores and Entropy for πrand (left) and MT-Uncertainty (right). In Figure 4, we see from the results that MT- Uncertainty converges faster by reaching the 80% average GLUE score line before other task sampling methods. Further, MT-Uncertainty maximum score on 200k iterations is at 82.2, which is 1.7% higher than Counterfactual sam- pling. The datasets in the GLUE benchmark offers a wide range of dataset sizes. This is useful to test how MT-Uncertainty manages a jointly trained low resource task (CoLA) and high resource task (MNLI). Figure 5 explains how catastrophic forgetting is curtailed by sam- pling tasks before performance drops. With πrand, all of CoLA’s tasks are sampled by it- eration 500, at which point the larger MNLI dataset overtakes the learning process and CoLA’s dev set performance starts to diminish. On the other hand, with MT-Uncertainty sampling, CoLA is sampled whenever Shannon entropy is higher than MNLI’s. The model ﬁrst assesses uncertain samples using Shannon Entropy then decides what data is necessary to train on. This process allows lower resource tasks to keep performance steady. 6Published as a conference paper at ICLR 2021 We provide evidence in Figure 8 of A.2 that MT-Uncertainty is able to manage task difﬁculty — by choosing the most difﬁcult tasks ﬁrst. 4.2 A BLATION AND MODULE ANALYSIS Table 1: Model ablation studya on the GLUE dev set. All models have the bottom half layers frozen. Model changes Avg Task σ % data GLUE GLUE used BERTBASE MTL (πrand) 80.61 14.41 100 + Conditional Attention 82.41 10.67 100 + Conditional Adapter 82.90 11.27 100 + CA and CLN 83.12 10.91 100 + MT-Uncertainty 84.03 10.02 66.3(CA-MTLBERT-BASE) aCA=Conditional Alignment, CLN=Conditional Layer Normal- ization, Task σ=scores standard deviation across tasks. In Table 1, we present the results of an ablation study to determine which elements of CA-MTLBERT-BASE had the largest positive gain on average GLUE scores. Starting from a MTL BERTBASE baseline trained us- ing random task sampling ( πrand). Apart for the Conditional Adapter, each module as well as MT- Uncertainty lift overall performance and reduce vari- ance across tasks. Please note that we also included accuracy/F1 scores for QQP, MRPC and Pearson/ Spearman correlation for STS-B to calculate score standard deviation Task σ. Intuitively, when negative task transfer occurs between two tasks, either (1) task interference is bidirectional and scores are both impacted, or (2) interference is unidirectional and only one score is impacted. We calculate Task σ to characterize changes in the dynamic range of performance across multiple tasks. We do this to asses the degree to which performance im- provements are distributed across all tasks or only subsets of tasks. As we can see from Table 1, Conditional Attention, Conditional Alignment, Conditional Layer Normalization, MT-Uncertainty play roles in reducing Task σand increasing performance across tasks. This provides partial evidence of CA-MTL’s ability to mitigating negative task transfer. Figure 6: Task performance vs. avg. covariance similarity scores (eq. 7) for MTL and CA-MTL. We show that Conditional Alignment can learn to capture covariate distribution differences with task embeddings co-learned from other adapter compo- nents of CA-MTL. In Figure 6, we arrive at similar conclusions as Wu et al. (2020), who proved that neg- ative task transfer is reduced when task covariances are aligned. The authors provided a “covariance simi- larity score” to gauge covariance alignment. For task iand j with mi and mj data samples respectively, and given d dimensional inputs to the ﬁrst Trans- former layer Xi ∈Rmi×d and Xj ∈Rmj×d, we rewrite the steps to calculate the covariance similarity score between task iand j: (a) Take the covariance matrix X⊤ i Xi, (b) Find its best rank- ri approxima- tion Ui,riDi,riU⊤ i,ri, where ri is chosen to contain 99% of the singular values. (c) Apply steps (a), (b) to Xj, and compute the covariance similarity score CovSimi,j: CovSimi,j := ∥(Ui,riD1/2 i,ri )⊤Uj,rj D1/2 j,rj ∥F ∥Ui,riD1/2 i,ri ∥F ·∥Uj,rj D1/2 j,rj ∥F . CovSimi = 1 T −1 ∑ j̸=i CovSimi,j (7) Since we are training models with T tasks, we take the average covariance similarity score CovSimi between task iand all other tasks. We measure CovSimi using equation 7 between 9 single-task models trained on individual GLUE tasks. For each task in Figure 6, we measure the similarity score on the MTL trained BERTBASE baseline, e.g., CoLA (MTL), or CA-MTLBERT-BASE model, e.g., MNLI (CA-MTL). Our score improvement measure is the % difference between a single task model and MTL or CA-MTL on the particular task. We ﬁnd that covariance similarity increases for 9 tasks and that performance increases for 7 out 9 tasks. These measurements conﬁrm that the Conditional Alignment is able to align task covariance, thereby helping alleviate task interference. 4.3 J OINTLY TRAINING ON 8 TASKS : GLUE In Table 2, we evaluate the performance of CA-MTL against single task ﬁne-tuned models, MTL as well as the other BERT-based adapters on GLUE. As in Houlsby et al. (2019), MNLIm and MNLImm are treated as separate tasks. Our results indicate that CA-MTL outperforms both the BASE adapter, 7Published as a conference paper at ICLR 2021 Table 2: Adapters with layer freezing vs. ST/MT on GLUE test set. F1 scores are reported for QQP/MRPC, Spearman’s correlation for STS-B, accuracy on the matched/mismatch sets for MNLI, Matthew’s correlation for CoLA and accuracy for other tasks. * Individual scores not available. ST=Single Task, MTL=Multitask, g.e.= greater or equal to. Results from: 1Devlin et al. (2018) 2Stickland et al. (2019). 3Houlsby et al. (2019) . Method Type Total Trained # tasks GLUE params params/task g.e. ST CoLA MNLI MRPC QNLI QQP RTE SST-2 STS-B Avg Base Models — Test Server Results BERTBASE 1 ST 9.0× 100% — 52.1 84.6/83.4 88.9 90.5 71.2 66.4 93.5 85.8 79.6 BERTBASE 2 MTL 1.0× 11.1% 2 51.2 84.0/83.4 86.7 89.3 70.8 76.6 93.4 83.6 79.9 PALs+Anneal Samp.2 MTL 1.13× 12.5% 4 51.2 84.3/83.5 88.7 90.0 71.5 76.0 92.6 85.8 80.4 CA-MTLBERT-BASE (ours) MTL 1.12× 5.6 % 5 53.1 85.9/85.8 88.6 90.5 69.2 76.4 93.2 85.3 80.9 Large Models — Test Server Results BERTLARGE 1 ST 9.0× 100% — 60.5 86.7/85.9 89.3 92.7 72.1 70.1 94.9 86.5 82.1 Adapters-2563 ST 1.3× 3.6% 3 59.5 84.9/85.1 89.5 90.7 71.8 71.5 94.0 86.9 80.0 CA-MTLBERT-LARGE (ours) MTL 1.12× 5.6% 3 59.5 85.9/85.4 89.3 92.6 71.4 79.0 94.7 87.7 82.8 PALS+Anneal Sampling (Stickland et al., 2019), and the LARGE adapter, Adapters-256 (Houlsby et al., 2019). Against single task (ST) models, CA-MTL is 1.3% higher than BERTBASE, with 5 out 9 tasks equal or greater performance, and 0.7% higher than BERTLARGE, with 3 out 9 tasks equal or greater performance. ST models, however, need 9 models or close to 9×more parameters for all 9 tasks. We noted that CA-MTLBERT-LARGE’s average score is driven by strong RTE scores. While RTE beneﬁts from MTL, this behavior may also be a side effect of layer freezing. In Table 10, we see that CA-MTL has gains over ST on more and more tasks as we gradually unfreeze layers. 4.4 T RANSFER TO NEW TASKS Table 3: Domain adaptation results on dev. sets for BASE models. 1Liu et al. (2019b), 2Jiang et al. (2020) % data used SciTail SNLI 0.1% 1% 10% 100% 0.1% 1% 10% 100% BERTBASE 1 51.2 82.2 90.5 94.3 52.5 78.1 86.7 91.0 MT-DNN1 81.9 88.3 91.1 95.7 81.9 88.3 91.1 95.7 MT-DNNSMART 2 82.3 88.6 91.3 96.1 82.7 86.0 88.7 91.6 CA-MTLBERT 83.2 88.7 91.4 95.6 82.8 86.2 88.0 91.5 In Table 3 we examine the ability of our method to quickly adapt to new tasks. We performed domain adaptation on SciTail (Khot et al., 2018) and SNLI (Bowman et al., 2015) datasets, using a CA-MTLBASE model trained on GLUE and a new linear decoder head. We tested several pretrained and randomly initialized task embeddings in a zero-shot setting. The com- plete set of experiments with all task embeddings can be found in the Appendix, Section A.4. We then selected the best task embedding for our results in Table 3. STS-B and MRPC MTL-trained task embeddings performed best on SciTail and SNLI respectively. CA-MTLBERT-BASE has faster adapta- tion than MT-DNNSMART (Jiang et al., 2020) as evidenced by higher performances in low-resource regimes (0.1% and 1% of the data). When trained on the complete dataset, CA-MTLBERT-BASE is on par with MT-DNNSMART. Unlike MT-DNNSMART however, we do not add context from a semantic similarity model – MT-DNNSMART is built off HNN (He et al., 2019). Nonetheless, with a larger model, CA-MTL surpasses MT-DNNSMART on the full SNLI and SciTail datasets in Table 6. 4.5 J OINTLY TRAINING ON 24 TASKS : GLUE/S UPER -GLUE, MRQA AND WNUT2017 Figure 7: Effects of adding more datasets on avg GLUE scores. Experiments conducted on 3 epochs. When 23 tasks are trained jointly, performance of CA-MTLBERT-BASE continues to improve. Effects of Scaling Task Count. In Figure 7 we continue to test if CA-MTL mitigates task in- terference by measuring GLUE average scores when progressively adding 9 GLUE tasks, 8 Super-GLUE tasks (Wang et al., 2019b), 6 MRQA tasks (Fisch et al., 2019). Tasks are described in Appendix section A.9. The results show that adding 23 tasks drops the performance of our baseline MTL BERTBASE (πrand). MTL BERT increases by 4.3% when adding MRQA but, with 23 tasks, the model performance drops by 1.8%. The opposite is true when CA-MTL modules are integrated into the model. CA-MTL continues to show gains with a large number of tasks and surpasses the baseline MTL model by close to 4% when trained on 23 tasks. 8Published as a conference paper at ICLR 2021 Table 4: 24-task CA-MTL vs. ST and vs. 24-task MTL with frozen layers on GLUE, SuperGLUE, MRQA and NER development sets. ST=Single Task, MTL=Multitask, g.e.= greater or equal to. Details in section A.5. Model Task Grouping Avg # tasks Total GLUE SuperGLUE MRQA NER e.g. ST Params BERT-LARGE models STReImp 84.5 68.9 79.7 54.1 76.8 — 24× MTLReImp 83.2 72.1 77.8 42.2 76.4 9/24 1× CA-MTL 86.6 74.1 79.5 49.0 79.1 17/24 1.12× RoBERTa-LARGE models STReImp 88.2 76.5 83.6 57.8 81.9 — 24× MTLReImp 86.0 78.6 80.7 49.3 80.7 7/24 1× CA-MTL 89.4 80.0 82.4 55.2 83.1 15/24 1.12× 24-task CA-MTL. We jointly trained large MTL baselines and CA-MTL models on GLUE/Super-GLUE/MRQA and Named En- tity Recognition (NER) WNUT2017 (Derczyn- ski et al., 2017). Since some dev. set scores are not provided and since RoBERTa results were reported with a median score over 5 random seeds, we ran our own single seed ST/MTL baselines (marked “ReImp”) for a fair compar- ison. The dev. set numbers reported in Liu et al. (2019c) are displayed with our baselines in Table 9. Results are presented in Table 4. We notice in Table 4 that even for large models, CA-MTL provides large gains in performance on average over both ST and MTL models. For the BERT based models, CA-MTL provides 2.3% gain over ST and higher scores on 17 out 24 tasks. For RoBERTa based models, CA-MTL provides 1.2% gain over ST and higher scores on 15 out 24 tasks. We remind the reader that this is achieved with a single model. Even when trained with 16 other tasks, it is interesting to note that the MTL baseline perform better than the ST baseline on Super GLUE where most tasks have a small number of samples. Also, we used NER to test if we could still outperform the ST baseline on a token-level task, signiﬁcantly different from other tasks. Unfortunately, while CA-MTL performs signiﬁcantly better than the MTL baseline model, CA-MTL had not yet overﬁt on this particular task and could have closed the gap with the ST baselines with more training cycles. Table 5: Our 24-task CA-MTL vs. other large models on GLUE. F1 is reported for QQP/MRPC, Spearman’s corr. for STS-B, Matthew’s corr. for CoLA and accuracy for other tasks. *Split not available. **Uses intermediate task ﬁne-tuning + ST. Model GLUE tasks AvgCoLA MNLI MRPC QNLI QQP RTE SST-2 STS-B BERT-LARGE based models on Dev set. MT-DNN 63.5 87.1 /86.7 91.0 92.9 89.2 83.4 94.3 90.6 85.6 STILTS ** 62.1 86.1* 92.3 90.5 88.5 83.4 93.2 90.8 85.9 BAM! 61.8 87.0* – 92.5 – 82.8 93.6 89.7 – 24-task CA-MTL 63.8 86.3/86.0 92.9 93.4 88.1 84.5 94.5 90.3 86.6 RoBERTa-LARGE based models on Test set. RoBERTA** with 67.8 91.0/90.8 91.6 95.4 74.0 87.9 97.5 92.5 87.3Ensemble 24-task CA-MTL 62.2 89.0/88.4 92.0 94.7 72.3 86.2 96.3 89.8 85.7 Comparisons with other methods. In Table 5, CA-MTLBERT is com- pared to other Large BERT based methods that either use MTL + ST, such as MT-DNN (Liu et al., 2019b), intermediate tasks + ST, such as STILTS (Phang et al., 2018) or MTL model distillation + ST, such as BAM! (Clark et al., 2019c). Our method scores higher than MT-DNN on 5 of 9 tasks and by 1.0 % on avg. Against STILTS, CA-MTL realizes a 0.7 % avg. score gain, surpassing scores on 6 of 9 tasks. We also show that CA-MTLRoBERTa is within only 1.6 % of a RoBERTa ensemble of 5 to 7 models per task and that uses intermediate tasks. Using our 24-task CA-MTL large RoBERTa-based model, we report NER F1 scores on the WNUT2017 test set in Table 6a. We compare our result with RoBERTaLARGE and XLM-RLARGE (Nguyen et al., 2020) the current state-of-the-art (SOTA). Our model outperforms XLM-RLARGE by 1.6%, reaching a new state-of-the-art. Using domain adaptation as described in Section 4.4, we report results on the SciTail test set in Table 6b and SNLI test set in Table 6b. For SciTail, our model matches the current SOTA2 ALUM (Liu et al., 2020), a RoBERTa large based model that additionally uses the SMART (Jiang et al., 2020) ﬁne-tuning method. For SNLI, our model outperforms SemBert, the current SOTA3. Table 6: CA-MTL test performance vs. SOTA. (a) WNUT2017 F1 RoBERTaLARGE 56.9 XLM-RLARGE 57.1 CA-MTLRoBERTa (ours) 58.0 (b) SciTail % Acc MT-DNN 94.1 ALUMRoBERTa 96.3 ALUMRoBERTa-SMART 96.8 CA-MTLRoBERTa (ours) 96.8 (c) SNLI % Acc MT-DNN 91.6 MT-DNNSMART 91.7 SemBERT 91.9 CA-MTLRoBERTa (ours) 92.1 2https://leaderboard.allenai.org/scitail/submissions/public on 09/27/2020 3https://nlp.stanford.edu/projects/snli/ on 09/27/2020 9Published as a conference paper at ICLR 2021 5 C ONCLUSION We believe that our experiments here have helped demonstrate the potential of task conditioned adaptive learning within a single model that performs multiple tasks. In a large-scale 24-task NLP experiment, CA-MTL outperforms fully tuned single task models by 2.3% for BERT Large and by 1.2% for RoBERTa Large using 1.12 times the number of parameters, while single task ﬁne-tuning approach requires 24 separately tuned single task models or 24 times the number of parameters. When a BERT vanilla MTL model sees its performance drop as the number of tasks increases, CA-MTL scores continue to climb. Performance gains are not driven by a single task as it is often the case in MTL. Each CA-MTL module that adapts a Transformer model is able to reduce performance variances between tasks, increasing average scores and aligning task covariances. This evidence shows that CA-MTL is able to mitigate task interference and promote more efﬁcient parameter sharing. We showed that MT-Uncertainty is able to avoid degrading performances of low resource tasks. Tasks are sampled whenever the model sees entropy increase, helping avoid catastrophic forgetting. Overall, CA-MTL offers a promising avenue to dynamically adapt and modularize knowledge embedded in large monolithic pretrained models. Extending such ideas will be an objective for future work. 10Published as a conference paper at ICLR 2021 ACKNOWLEDGMENTS This research was supported by the Canada CIFAR AI Chairs Program, NSERC and PROMPT. Exper- iments in this article were conducted with Compute Canada and MILA computational infrastructure and we thank them for their support. We would like to thank Colin Raffel, Sandeep Subramanian, and Nicolas Gontier for their useful feedback and the anonymous reviewers for helpful comments, discussions and suggestions. REFERENCES Roee Aharoni, Melvin Johnson, and Orhan Firat. Massively multilingual neural machine translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 3874–3884, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1388. URL https://www.aclweb.org/anthology/N19-1388. Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR, abs/1607.06450, 2016. URL http://arxiv.org/abs/1607.06450. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pp. 41–48, 2009. Joachim Bingel and Anders Søgaard. Identifying beneﬁcial task relations for multi-task learning in deep neural networks. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pp. 164–169, Valencia, Spain, April 2017. Association for Computational Linguistics. URL https://www.aclweb.org/ anthology/E17-2026. Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus for learning natural language inference. InProceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, 2015. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv, pp. arXiv–2005, 2020. Rich Caruana. Multitask learning. Mach. Learn., 28(1):41–75, July 1997. ISSN 0885-6125. doi: 10.1023/A:1007379606734. URL https://doi.org/10.1023/A:1007379606734. Richard Caruana. Multitask learning: A knowledge-based source of inductive bias. In Proceedings of the Tenth International Conference on Machine Learning, pp. 41–48. Morgan Kaufmann, 1993. Daniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-Gazpio, and Lucia Specia. SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pp. 1–14, Vancouver, Canada, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/S17-2001. URL https://www.aclweb.org/anthology/S17-2001. Denis Charles, Max Chickering, and Patrice Simard. Counterfactual reasoning and learning systems: The example of computational advertising. Journal of Machine Learning Research, 14:3207–3260, November 2013. Jinying Chen, Andrew Schein, Lyle Ungar, and Martha Palmer. An empirical study of the behavior of active learning for word sense disambiguation. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pp. 120–127, New York City, USA, June 2006. Association for Computational Linguistics. URL https://www.aclweb.org/ anthology/N06-1016. Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks. CoRR, abs/1711.02257, 2017. URL http://arxiv.org/abs/1711.02257. 11Published as a conference paper at ICLR 2021 Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difﬁculty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2924–2936, Minneapolis, Minnesota, June 2019a. Association for Computational Linguistics. doi: 10.18653/ v1/N19-1300. URL https://www.aclweb.org/anthology/N19-1300. Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. What does BERT look at? an analysis of BERT’s attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 276–286, Florence, Italy, August 2019b. Association for Computational Linguistics. doi: 10.18653/v1/W19-4828. URL https: //www.aclweb.org/anthology/W19-4828. Kevin Clark, Minh-Thang Luong, Urvashi Khandelwal, Christopher D. Manning, and Quoc V . Le. Bam! born-again multi-task networks for natural language understanding. CoRR, abs/1907.04829, 2019c. URL http://arxiv.org/abs/1907.04829. Edward Collins, Nikolai Rozanov, and Bingbing Zhang. Evolutionary data measures: Understanding the difﬁculty of text classiﬁcation tasks. In Proceedings of the 22nd Conference on Computational Natural Language Learning, pp. 380–391, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/K18-1037. URL https://www.aclweb.org/ anthology/K18-1037. Ronan Collobert and Jason Weston. A uniﬁed architecture for natural language processing: deep neural networks with multitask learning. In ICML, pp. 160–167, 2008. URL https://doi. org/10.1145/1390156.1390177. Marie-Catherine de Marneffe, Mandy Simons, and Judith Tonhauser. The commitmentbank: Investi- gating projection in naturally occurring discourse. Proceedings of Sinn und Bedeutung, 23(2):107– 124, Jul. 2019. doi: 10.18148/sub/2019.v23i2.601. URL https://ojs.ub.uni-konstanz. de/sub/index.php/sub/article/view/601. Harm de Vries, Florian Strub, Jeremie Mary, Hugo Larochelle, Olivier Pietquin, and Aaron C Courville. Modulating early visual processing by language. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar- nett (eds.), Advances in Neural Information Processing Systems 30 , pp. 6594– 6604. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/ 7237-modulating-early-visual-processing-by-language.pdf . Leon Derczynski, Eric Nichols, Marieke van Erp, and Nut Limsopatham. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Workshop on Noisy User-generated Text, pp. 140–147, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-4418. URL https://www.aclweb.org/ anthology/W17-4418. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018. URL http://arxiv.org/abs/1810.04805. William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005. URL https://www.aclweb.org/anthology/I05-5002. Matthew Dunn, Levent Sagun, Mike Higgins, V . Ugur Güney, V olkan Cirik, and Kyunghyun Cho. Searchqa: A new q&a dataset augmented with context from a search engine.CoRR, abs/1704.05179, 2017. URL http://arxiv.org/abs/1704.05179. Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, and Danqi Chen. MRQA 2019 shared task: Evaluating generalization in reading comprehension. In Proceedings of the 2nd Workshop on Machine Reading for Question Answering, pp. 1–13, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-5801. URL https: //www.aclweb.org/anthology/D19-5801. 12Published as a conference paper at ICLR 2021 John Glover and Chris Hokamp. Task selection policies for multitask learning. CoRR, 2019. URL http://arxiv.org/abs/1907.06214. Andrew Gordon, Zornitsa Kozareva, and Melissa Roemmele. SemEval-2012 task 7: Choice of plausi- ble alternatives: An evaluation of commonsense causal reasoning. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main confer- ence and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Se- mantic Evaluation (SemEval 2012), pp. 394–398, Montréal, Canada, 7-8 June 2012. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/S12-1052. Michelle Guo, Albert Haque, De-An Huang, Serena Yeung, and Li Fei-Fei. Dynamic task prioriti- zation for multitask learning. In Proceedings of the European Conference on Computer Vision (ECCV), September 2018. Pengcheng He, Xiaodong Liu, Weizhu Chen, and Jianfeng Gao. A hybrid neural network model for commonsense reasoning. In Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing, pp. 13–21, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-6002. URL https://www.aclweb.org/ anthology/D19-6002. Matthew D. Hoffman, David M. Blei, Chong Wang, and John Paisley. Stochastic variational inference. Journal of Machine Learning Research, 14(4):1303–1347, 2013. URL http://jmlr.org/ papers/v14/hoffman13a.html. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efﬁcient transfer learning for NLP. CoRR, abs/1902.00751, 2019. URL http://arxiv.org/abs/1902.00751. Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classiﬁcation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 328–339, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1031. URL https://www.aclweb.org/anthology/ P18-1031. Fariz Ikhwantri, Samuel Louvan, Kemal Kurniawan, Bagas Abisena, Valdi Rachman, Alfan Farizki Wicaksono, and Rahmad Mahendra. Multi-task active learning for neural semantic role labeling on low resource conversational corpus. InProceedings of the Workshop on Deep Learning Approaches for Low-Resource NLP, pp. 43–50, 2018. Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Tuo Zhao. SMART: Robust and efﬁcient ﬁne-tuning for pre-trained natural language models through principled regular- ized optimization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 2177–2190, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.197. URL https://www.aclweb.org/anthology/2020. acl-main.197. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1601– 1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/ v1/P17-1147. URL https://www.aclweb.org/anthology/P17-1147. Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. Spanbert: Improving pre-training by representing and predicting spans. CoRR, abs/1907.10529, 2019. URL http://arxiv.org/abs/1907.10529. Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh losses for scene geometry and semantics. CoRR, abs/1705.07115, 2017. URL http://arxiv.org/ abs/1705.07115. Emma Kerinec, Chloé Braud, and Anders Søgaard. When does deep multi-task learning work for loosely related document classiﬁcation tasks? In Proceedings of the 2018 EMNLP Workshop 13Published as a conference paper at ICLR 2021 BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 1–8, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5401. URL https://www.aclweb.org/anthology/W18-5401. Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. InProceed- ings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 252–262, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1023. URL https://www.aclweb.org/anthology/N18-1023. Tushar Khot, A. Sabharwal, and Peter Clark. Scitail: A textual entailment dataset from science question answering. In AAAI, 2018. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2015. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics, 2019. Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020. Hector J. Levesque. The winograd schema challenge. In AAAI Spring Symposium: Logical Formal- izations of Commonsense Reasoning. AAAI, 2011. URL http://dblp.uni-trier.de/ db/conf/aaaiss/aaaiss2011-6.html#Levesque11. Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. Linguistic knowledge and transferability of contextual representations. CoRR, abs/1903.08855, 2019a. URL http://arxiv.org/abs/1903.08855. Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks for natural language understanding. CoRR, abs/1901.11504, 2019b. URL http://arxiv.org/ abs/1901.11504. Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu Wang, Hoifung Poon, and Jianfeng Gao. Adversarial training for large neural language models, 2020. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019c. URL http://arxiv.org/abs/1907.11692. Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language decathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730, 2018. Amil Merchant, Elahe Rahimtoroghi, Ellie Pavlick, and Ian Tenney. What happens to bert embeddings during ﬁne-tuning? arXiv preprint arXiv:2004.14448, 2020. Dat Quoc Nguyen, Thanh Vu, and Anh Tuan Nguyen. Bertweet: A pre-trained language model for english tweets. arXiv preprint arXiv:2005.10200, 2020. Hao Peng, Roy Schwartz, Dianqi Li, and Noah A. Smith. A mixture of h - 1 heads is better than h heads. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 6566–6577, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.587. URL https://www.aclweb.org/anthology/2020. acl-main.587. Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron C. Courville. Film: Visual reasoning with a general conditioning layer. In AAAI, 2018. 14Published as a conference paper at ICLR 2021 Matthew E. Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. Dissecting contextual word embeddings: Architecture and representation. CoRR, abs/1808.08949, 2018. URL http: //arxiv.org/abs/1808.08949. Jason Phang, Thibault Févry, and Samuel R. Bowman. Sentence encoders on STILTs: Supplementary training on intermediate labeled-data tasks. CoRR, abs/1811.01088, 2018. URL http://arxiv. org/abs/1811.01088. Adam Poliak, Aparajita Haldar, Rachel Rudinger, J. Edward Hu, Ellie Pavlick, Aaron Steven White, and Benjamin Van Durme. Collecting diverse natural language inference problems for sentence representation evaluation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 67–81, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1007. URL https://www.aclweb.org/ anthology/D18-1007. Edoardo M. Ponti, Ivan Vuli´c, Ryan Cotterell, Marinela Parovic, Roi Reichart, and Anna Korhonen. Parameter space factorization for zero-shot learning across tasks and languages. Transactions of the Association for Computational Linguistics, 9:410–428, 2021. doi: 10.1162/tacl_a_00374. URL https://aclanthology.org/2021.tacl-1.25. Yada Pruksachatkun, Jason Phang, Haokun Liu, Phu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe Pang, Clara Vania, Katharina Kann, and Samuel R Bowman. Intermediate-task transfer learning with pretrained models for natural language understanding: When and why does it work? arXiv preprint arXiv:2005.00628, 2020. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under- standing by generative pre-training. 2018. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer, 2019. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2383–2392, Austin, Texas, November 2016a. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. URL https://www.aclweb.org/ anthology/D16-1264. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2383–2392, Austin, Texas, November 2016b. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. Roi Reichart, Katrin Tomanek, Udo Hahn, and Ari Rappoport. Multi-task active learning for linguistic annotations. In Proceedings of ACL-08: HLT, pp. 861–869, 2008. Sebastian Ruder. An overview of multi-task learning in deep neural networks.ArXiv, abs/1706.05098, 2017. Ozan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. CoRR, abs/1810.04650, 2018. URL http://arxiv.org/abs/1810.04650. Joan Serrà, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. In ICML, pp. 4555–4564, 2018. URL http:// proceedings.mlr.press/v80/serra18a.html. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1631–1642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/D13-1170. 15Published as a conference paper at ICLR 2021 Trevor Standley, Amir Roshan Zamir, Dawn Chen, Leonidas J. Guibas, Jitendra Malik, and Silvio Savarese. Which tasks should be learned together in multi-task learning? CoRR, abs/1905.07553, 2019. URL http://arxiv.org/abs/1905.07553. Asa Cooper Stickland, Iain Murray, someone, and someone. BERT and PALs: Projected attention layers for efﬁcient adaptation in multi-task learning. volume 97 of Proceedings of Machine Learning Research, pp. 5986–5995, Long Beach, California, USA, 09–15 Jun 2019. PMLR. URL http://proceedings.mlr.press/v97/stickland19a.html. Yi Tay, Zhe Zhao, Dara Bahri, Donald Metzler, and Da-Cheng Juan. Hypergrid: Efﬁcient multi-task transformers with grid-wise decomposable hyper projections. arXiv preprint arXiv:2007.05891, 2020. Ian Tenney, Dipanjan Das, and Ellie Pavlick. BERT rediscovers the classical NLP pipeline. CoRR, abs/1905.05950, 2019a. URL http://arxiv.org/abs/1905.05950. Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R. Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R. Bowman, Dipanjan Das, and Ellie Pavlick. What do you learn from context? probing for sentence structure in contextualized word representations. CoRR, abs/1905.06316, 2019b. URL http://arxiv.org/abs/1905.06316. Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman. NewsQA: A machine comprehension dataset. In Proceedings of the 2nd Workshop on Representation Learning for NLP, pp. 191–200, Vancouver, Canada, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-2623. URL https: //www.aclweb.org/anthology/W17-2623. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. CoRR, abs/1706.03762, 2017. URL http://arxiv.org/abs/1706.03762. Johannes von Oswald, Christian Henning, Benjamin F. Grewe, and João Sacramento. Continual learning with hypernetworks. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=SJgwNerKvB. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 353–355, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/anthology/W18-5446. Alex Wang, Jan Hula, Patrick Xia, Raghavendra Pappagari, R. Thomas McCoy, Roma Patel, Najoung Kim, Ian Tenney, Yinghui Huang, Katherin Yu, Shuning Jin, Berlin Chen, Benjamin Van Durme, Edouard Grave, Ellie Pavlick, and Samuel R. Bowman. Can you tell me how to get past sesame street? sentence-level pretraining beyond language modeling. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 2019a. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. CoRR, abs/1905.00537, 2019b. URL http://arxiv.org/abs/ 1905.00537. Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. Neural network acceptability judgments. CoRR, abs/1805.12471, 2018. URL http://arxiv.org/abs/1805.12471. Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan- guage Technologies, Volume 1 (Long Papers), pp. 1112–1122, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1101. URL https: //www.aclweb.org/anthology/N18-1101. 16Published as a conference paper at ICLR 2021 Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. Huggingface’s transformers: State-of-the-art natural language processing. CoRR, abs/1910.03771, 2019. URL http://arxiv.org/abs/1910.03771. Sen Wu, Hongyang R. Zhang, and Christopher Ré. Understanding and improving information transfer in multi-task learning. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=SylzhkBtDB. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 2369–2380, Brussels, Belgium, October-November 2018. Association for Com- putational Linguistics. doi: 10.18653/v1/D18-1259. URL https://www.aclweb.org/ anthology/D18-1259. Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning. arXiv preprint arXiv:2001.06782, 2020. Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. Record: Bridging the gap between human and machine commonsense reading comprehension. CoRR, abs/1810.12885, 2018. URL http://arxiv.org/abs/1810.12885. Yu Zhang and Qiang Yang. A survey on multi-task learning. CoRR, abs/1707.08114, 2017. URL http://arxiv.org/abs/1707.08114. 17Published as a conference paper at ICLR 2021 A A PPENDIX A.1 S UMMARY OF ACRONYMS Acronyms of datasets and descriptions can be found below in section A.9. Table 7: List of acronyms used in this paper. Acronym Description ARLM Autoregressive Language Models CA-MTL Conditional Adaptive Multi-Task Learning: our architecture CFF Conditional Feed-Forward: a feed-forward layer modulated by a conditioning vector CLN Conditional Layer Normalization in section 2.1.3 EDM Evolutionary Data Measures (Collins et al., 2018): a task difﬁculty estimate GLUE General Language Understanding Evaluation Wang et al. (2018): a benchmark with multiple datasets QA Question Answering MT Multi-Task MTAL Multi-Task Active Learning: ﬁnding the most informative instance for multiple learners (or models) MLM Masked Language Model: BERT Devlin et al. (2018) is an example of an MLM MTL Multi-Task Learning: \"learning tasks in parallel while using a shared representation\" (Caruana, 1997) MRQA Machine Reading for Question Answering Fisch et al. (2019): a benchmark with multiple datasets NER Named Entity Recognition NLP Natural Language Processing SOTA State of the art ST Single Task ﬁne-tuning: all weights are typically updated ST-A ST with Adapter modules: one adapter per task is trained and pretrained weights are optionally updated A.2 U NCERTAINTY SAMPLING : A LGORITHM AND ADDITIONAL RESULTS Algorithm 1: Multi-task Uncertainty Sampling Input: Training data Dt for task t∈[1,...,T ]; batch size b; Ct possible output classes for task t; f := fφ(zi),θi our model with weights φ,θi; Output: B′- multi-task batch of size b 1 B←∅ 2 for t←1 to T do 3 Generate xt := {xt,1,...,x t,b} i.i.d. ∼Dt 4 for i←1 to bdo 5 Ht,i ←−∑Ci c=1 pc(f(xt,i)) log pc(f(xt,i)) ⊿Entropy of each sample 6 7 end 8 Compute ¯Ht ←1 b ∑ x∈xi Ht,i ⊿Average entropy for task t 9 10 Compute H′ t ←−∑Ct c=1 1 Ct log [ 1 Ct ] ⊿Max entropy (uniform distribution) 11 12 B←B∪ xt and Dt ←Dt \\xt 13 if Dt = ∅then 14 Reload Dt 15 end 16 for i←1 to bdo 17 Compute: Ut,i ←Ht,i/H′ t ⊿Uncertainty normalized with max entropy 18 end 19 end 20 Compute ˆH← maxi∈{1,...,T}[ ¯Ht] ⊿Entropy of task with highest average entropy 21 Update Ut,i ←Ut,i/ˆH ⊿Normalize each sample’s uncertainty measure 22 B′←top_b({Ut,i|t∈[1,...,T ],i ∈[1,...,b ]}) ⊿b samples w/ highest uncertainty Return: With B′, solve eq. 1 with gradient descent; updated model f 18Published as a conference paper at ICLR 2021 An advantage of our MT-Uncertainty Sampling approach is its ability to manage task difﬁculty. This is highlighted in Figure 8. In this experiment, we estimated task difﬁculty using the Evolutionary Data Measures (EDM)4 proposed by Collins et al. (2018). The task difﬁculty estimate relies on multiple dataset statistics such as the data size, class diversity, class balance and class interference. Interestingly, estimated task difﬁculty correlates with the ﬁrst instance that the selection of a speciﬁc task occurs. Supposing that QNLI is an outlier, we notice that peaks in the data occur whenever tasks are ﬁrst selected by MT Uncertainty sampling. This process follows the following order: 1. MNLI 2. CoLA 3. RTE 4. QQP 5. MRPC 6.SST-2, which is the order from highest task difﬁculty to lowest task difﬁculty using EDM. As opposed to Curriculum Learning (Bengio et al., 2009), MT-Uncertainty dynamically prioritizes the most difﬁcult tasks. As also discovered in MTL vision work (Guo et al., 2018), this type of prioritization on more difﬁcult tasks may explain MT-Uncertainty’s improved performance over other task selection methods. In MTL, heuristics to balance tasks during training is typically done by weighting each task’s loss differently. We see here how MT-Uncertainty is able to prioritize task difﬁculty. 0 500 1000 1500 2000 2500 3000 Train iteration 0 4 8 12 16 20 24 28 32Number of samples Task Difﬁculty MNLI 4.2 QNLI 3.8 CoLA 3.7 RTE 3.6 MRPC 3.5 QQP. 3.5 SST-2 3.2 Figure 8: Task composition of MT-Uncertainty sampling and estimated task difﬁculty using EDM: number of training samples per task at each iteration for batch size of 32. The occurrence of ﬁrst peaks and estimated difﬁculty follow the same order: From highest to lowest: MNLI >CoLA >RTE >QQP = MRPC >SST-2. While the EDM difﬁculty measure is shown to correlate well with model performance, it lacks precision. As reported in Collins et al. (2018), the average score achieved on the Yahoo Answers dataset is 69.9% and its difﬁculty is 4.51. The average score achieved on Yelp Full is 56.8%, 13.1% less than Yahoo Answers and its difﬁculty is 4.42. The authors mention that “This indicates that the difﬁculty measure in its current incarnation may be more effective at assigning a class of difﬁculty to datasets, rather than a regression-like value”. A.3 O THER RELATED WORK Multi-Tasking in NLP and other ﬁelds.MTL weight sharing algorithms such as Mixture-of-Experts (MoE) have found success in NLP (Lepikhin et al., 2020). CA-MTL can complement MoE since the Transformers multi-headed attention can be seen as a form of MoE (Peng et al., 2020). In Vision, MTL can also improve with optimization (Sener & Koltun, 2018) or gradient-based approaches (Chen et al., 2017; Yu et al., 2020). Active Learning, Task Selection and Sampling. Ikhwantri et al. (2018) examined multi-task active learning for neural semantic role labeling in a low resource setting, using entity recognition as the sole auxiliary task. They used uncertainty sampling for active learning and found that 12% less data could be used compared to passive learning. Reichart et al. (2008) has examined different active learning techniques for the two task annotation scenario, focusing on named entity recognition and syntactic parse tree annotations. In contrast, here we examine the larger scale data regime, the modularization of a multi-task neural architecture, and the many task (≫2) setting among other differences. Other than MTAL (Reichart et al., 2008; Ikhwantri et al., 2018), Kendall et al. (2017) leveraged model uncertainty to balance MTL losses but not to select tasks as is proposed here. 4https://github.com/Wluper/edm 19Published as a conference paper at ICLR 2021 A.4 Z ERO -SHOT RESULTS ON SCITAIL AND SNLI Before testing models on domain adaptation in section 4.4, we ran zero-shot evaluations on the development set of SciTail and SNLI. Table 8 outlines8-task CA-MTLBERT-BASE’s zero-shot transfer abilities when pretrained on GLUE with our MTL approach. We expand the task embedding layer to accommodate an extra task and explore various embedding initialization. We found that reusing STS-B and MRPC task embeddings worked best for SciTail and SNLI respectively. Table 8: CA-MTL is ﬂexible and extensible to new tasks. However, CA-MTL is sensitive to the new task’s embedding. We tested multiple task embeddings that worked best on either SciTail or SNLI by checking performance in a zero shot setting or using 0% of the data. Initialization of new SciTail SNLI task embedding layer 0% of data 0% of data CoLA’s embeddings 43.0 34.0 MNLI’s embeddings 24.2 33.0 MRPC’s embeddings 34.5 45.5 STS-B’s embeddings 46.9 33.2 SST-2’s embeddings 25.8 34.2 QQP’s embeddings 31.7 37.3 QNLI’s embeddings 32.0 38.0 RTE’s embeddings 32.3 40.6 WNLI’s embeddings 29.0 30.4 Average 28.7 37.7 Random initialization 46.8 34.0 Xavier initialization 29.8 37.6 A.5 M ORE EXPERIMENTAL DETAILS We used a batch size of 32 and a seed of 12 in all experiments. We used Adam (Kingma & Ba, 2015) as the optimizer with a learning rate of 2e-5. We applied a learning rate decay with warm up over the ﬁrst 10% of the training steps. Unless otherwise speciﬁed, we used 5 epochs, a seed of 12 and a sequence length of 128. Additional details are outlined in section . Our data prepossessing and linear decoder heads are the same as in Devlin et al. (2018). We used the same dropout rate of 0.1 in all layers. To run our experiments, we used either four NVIDIA P100 GPU for base models or four NVIDIA V100 GPU for larger ones. We did not perform parameter search. We do not use ensemble of models or task-speciﬁc tricks (Devlin et al., 2018; Liu et al., 2019b; Clark et al., 2019c). All models are either 12 Transformer layers for BASE and 24 Transformer layers for LARGE. Apart from CA-MTL, models trained in multi-task learning (BERT or RoBERTa without adapters) used random task sampling. For Table 1 and Figure 7, all BERT-based model have half their layers frozen (untrained) for a fair comparison of ablation results. For the 24-task MTL and CA-MTL models in Tables 4 and 5, we increased the input sequence length to 256 and used 8 epochs. A.6 T HE DIRECT SUM OPERATOR In section 2.1.1, we used the direct sum operator ⊕. This operation allows us to create a block diagonal matrix. The direct sum of a matrix A∈Rn×m and B ∈Rp×q results in a matrix of size (m+ p) ×(n+ q), deﬁned as: A ⊕B = [ A 0 0 B ] =   a11 ··· a1n 0 ··· 0 ... ... ... ... ... ... am1 ··· amn 0 ··· 0 0 ··· 0 b11 ··· b1q ... ... ... ... ... ... 0 ··· 0 bp1 ··· bpq   20Published as a conference paper at ICLR 2021 A.7 B ASELINES AND OTHER EXPERIMENTAL RESULTS In this section, we present our baseline results for BERT, RoBERTa, CA-MTL as well as other models. Our single task results (ST) that we ran ourselves surpass other paper’s reported scores in Table 9. Liu et al. (2019c) reports random seed median scores for RoBERTa. However, our RoBERTa ST baseline matches or surpasses the original paper’s scores4 out 7 times on the development set when scores are comparable (QQP F1 and STS-B spearman are not reported). Table 9: F1 scores are reported for QQP/MRPC, Spearman’s correlation for STS-B, accuracy on the matched/mismatch sets for MNLI, Matthew’s correlation for CoLA and accuracy for other tasks. ST=Single Task, MTL=Multitask. *QNLI v1 (we report v2) **F1 score or Spearman’s correlation is not reported. ***Unknown random seeds. Results from: 1Stickland et al. (2019) 2Liu et al. (2019b) 3Phang et al. (2018) 4Liu et al. (2019c). Method Total Trained GLUE params params/task CoLA MNLI MRPC QNLI QQP RTE SST-2 STS-B Avg Base Models — Dev set Results PALs+Anneal Samp.1 1.13× 12.5% – – – – – – – – 81.70 8-task CA-MTLBERT-BASE (ours) 1.12× 5.6% 60.9 82.7/83.1 88.9 90.7 90.3 79.1 91.9 88.8 84.03 BERT LARGE Models — Dev set Results ST BERT-LARGE2 9× 100% 60.5 86.7/85.9 89.3 92.7* 89.3 70.1 94.9 86.5 84.0 ST BERT-LARGE3 9× 100% 62.1 86.2/86.2 92.3 89.4 88.5 70.0 92.5 90.1 84.1 ST BERT-LARGE (ours) 9× 100% 63.6 86.5/86.0 91.4 91.0 88.5 70.2 94.7 88.2 84.5 24-task CA-MTLBERT-LARGE (ours) 1.12× 5.6% 63.8 86.3/86.0 92.9 93.4 88.1 84.5 94.5 90.3 86.6 RoBERTa LARGE Models — Dev set Results RoBERTa-LARGE4 9× 100% 68.0 90.2 90.9 94.7 ** 86.6 96.4 ** –(Median 5 runs)*** ST RoBERTa-LARGE (ours) 9× 100% 68.3 89.2/88.9 92.6 94.8 84.6 87.0 96.4 91.7 88.2 24-task CA-MTLRoBERTa-LARGE (ours) 1.12× 5.6% 69.7 89.4/89.3 93.9 94.9 88.8 91.0 96.2 91.0 89.4 A.8 S OME RESULTS ON LAYER FREEZING AND WITH FULL BLOCK ATTENTION . All experiments in this section were run for only 5 epochs, exclusively on the GLUE dataset for the large BERT-based 8-task CA-MTL model. Results in Table 10 reveal that as we freeze more layers, performance tends to decrease. However, since we wanted to preserve as much pretrained knowledge as possible, we chose to keep at least 50% of layers frozen. While this has slightly lowered our performance on 9 GLUE tasks, we believe that keeping as much of the original pretrained weights is beneﬁcial when increasing the total number of tasks in MTL to 24 or more tasks. However, we did not explore this hypothesis more. Table 10: 8-task CA-MTLBERT-LARGE (see section 4.3) for various layer freezing conﬁgurations. F1 scores are reported for QQP/MRPC, Spearman’s correlation for STS-B, accuracy on the matched/mismatch sets for MNLI, Matthew’s correlation for CoLA and accuracy for other tasks. FBA = Full Block Attention Method % frozen # tasks GLUE layers g.e ST CoLA MNLI MRPC QNLI QQP RTE SST-2 STS-B Avg LARGE Models — Dev set Results ST BERT-LARGE (ours) 0% — 63.6 86.5/86.0 91.4 91.0 88.5 70.2 93.1 88.2 84.3 CA-MTL 0% 7 60.2 86.2/86.0 92.0 91.5 88.7 76.3 93.3 89.5 84.9 CA-MTL 25% 6 63.7 86.1/85.8 89.1 91.2 88.6 79.7 92.9 88.5 85.1 CA-MTL 50% 3 63.2 85.5/85.5 91.8 90.9 88.3 81.4 93.0 90.1 85.5 CA-MTL FBA 50% 0 60.2 81.7/81.1 88.0 85.8 85.7 78.7 88.6 87.1 81.8 A.9 D ATASET DESCRIPTION The datasets that were used for the domain adaptation experiments were SciTail 5 and SNLI6. We jointly trained a CA-MTLRoBERTa-LARGE model on 9 GLUE tasks, 8 Super-GLUE7 tasks, 6 MRQA8 tasks, and on WNUT20179 (Derczynski et al., 2017). All GLUE tasks are binary classiﬁcation, except STS-B (regression) and MNLI (three classes). We used the same GLUE data preprocessing as in Devlin et al. (2018). 5https://allenai.org/data/scitail; Leaderboard can be found at: https://leaderboard.allenai.org/scitail/submissions/public 6https://nlp.stanford.edu/projects/snli/ 7https://super.gluebenchmark.com/tasks 8https://github.com/mrqa/MRQA-Shared-Task-2019 9https://github.com/leondz/emerging_entities_17 21Published as a conference paper at ICLR 2021 Table 11: GLUE (Wang et al., 2018) dataset description. References: 1Warstadt et al. (2018), 2Socher et al. (2013), 3Dolan & Brockett (2005), 4Cer et al. (2017), 5Williams et al. (2018), 6Wang et al. (2018), 7Levesque (2011) Acronym Corpus |Train| Task Domain CoLA1 Corpus of Linguistic Acceptability 8.5K acceptability miscellaneous SST-22 Stanford Sentiment Treebank 67K sentiment detection movie reviews MRPC3 Microsoft Research Paraphrase Corpus 3.7K paraphrase detection news STS-B4 Semantic Textual Similarity Benchmark 7K textual similarity miscellaneous QQP Quora Question Pairs 364K paraphrase detection online QA MNLI5 Multi-Genre NLI 393K inference miscellaneous RTE6 Recognition Textual Entailment 2.5K inference/entailment news, Wikipedia WNLI7 Winograd NLI 634 coreference ﬁction books Table 12: Super-GLUE (Wang et al., 2019b) dataset description. References:1Clark et al. (2019a),2de Marneffe et al. (2019), 3Gordon et al. (2012), 4Khashabi et al. (2018), 5Zhang et al. (2018), 6Wang et al. (2019b), 7Poliak et al. (2018), 8Levesque (2011) Acronym Corpus |Train| Task Domain BoolQ1 Boolean Questions 9.4K acceptability Google queries, Wikipedia CB2 CommitmentBank 250 sentiment detection miscellaneous COPA3 Choice of Plausible Alternatives 400 paraphrase detection blogs, encyclopedia MultiRC4 Multi-Sentence Reading Comprehension5.1K textual similarity miscellaneous ReCoRD5 Reading Comprehension 101K paraphrase detection news and Commonsense Reasoning RTE6 Recognition Textual Entailment 2.5K inference news, Wikipedia WiC7 Word-in-Context 6K word sense disambiguationWordNet, VerbNet WSC8 Winograd Schema Challenge 554 coreference resolution ﬁction books Table 13: MRQA (Fisch et al., 2019) dataset description. References: 1Rajpurkar et al. (2016a), 2Trischler et al. (2017), 3Joshi et al. (2017), 4Dunn et al. (2017), 5Yang et al. (2018), 6Kwiatkowski et al. (2019) Acronym Corpus |Train| Task Domain SQuAD1 Stanford QA Dataset 86.6K crowdsourced questions Wikipedia NewsQA2 NewsQA 74.2K crowdsourced questions news TriviaQA3 TriviaQA 61.7K trivia QA web snippets SearchQA4 SearchQA 117.4K Jeopardy QA web snippets HotpotQA5 HotpotQA 72.9K crowdsourced questions Wikipedia Natural Questions6 Natural Questions 104.7K search logs Wikipedia SuperGLUE has a more diverse task format than GLUE, which is mostly limited to sentence and sentence-pair classiﬁcation. We follow the same preprocessing procedure as in Wang et al. (2019b). All tasks are binary classiﬁcation tasks, except CB (three classes). Also, WiC and WSC are span based classiﬁcation tasks. We used the same modiﬁed MRQA dataset and preprocessing steps that were used in Joshi et al. (2019). All MRQA tasks are span prediction tasks which seeks to identify start and end tokens of an answer span in the input text. Table 14: SNLI (Bowman et al., 2015) and SciTail (Khot et al., 2018) datasets description. Acronym Corpus |Train| Task Domain SNLI1 Stanford Natural Language Inference 550.2k inference human-written English sentence pairs SciTail2 Science and Entailment 23.5K entailment Science question answering SNLI is a natural inference task where we predict three classes. Examples of three target labels are: Entailment, Contradiction, and Neutral (irrelevant). SciTail is a textual entailment dataset. The hypotheses in SciTail are created from multiple-choice science exams and the answer candidates (premise) are extracted from the web using information retrieval tools. SciTail is a binary true/false classiﬁcation tasks that seeks to predict whether the premise entails the hypothesis. The two datasets are used only for domain adaptation in this study (see section A.4 for the details of our approach). 22",
      "meta_data": {
        "arxiv_id": "2009.09139v3",
        "authors": [
          "Jonathan Pilault",
          "Amine Elhattami",
          "Christopher Pal"
        ],
        "published_date": "2020-09-19T02:04:34Z",
        "pdf_url": "https://arxiv.org/pdf/2009.09139v3.pdf",
        "github_url": "https://github.com/CAMTL/CA-MTL"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper addresses challenges in Multi-Task Learning (MTL) for Natural Language Processing (NLP), such as overfitting to low-resource tasks, catastrophic forgetting, and negative task transfer, which often necessitate a separate model per task. The authors propose Conditionally Adaptive Multi-Task Learning (CA-MTL), a novel Transformer-based Adapter that incorporates a new conditional attention mechanism and task-conditioned modules to facilitate efficient weight sharing. A key contribution is fixing half of the pretrained model weights to mitigate forgetting and introducing a new uncertainty-based multi-task data sampling strategy (MT-Uncertainty Sampling) to address data imbalance across tasks. CA-MTL surpasses single-task fine-tuning methods while being parameter and data efficient (using 66% of data for weight updates). Empirically, their 8-task model outperforms other Adapter methods by 2.8% on GLUE, and their 24-task model surpasses MTL and single-task fine-tuning models by 0.7-1.0%. The approach achieves competitive performance across 26 NLP tasks and state-of-the-art results on several test and development sets, being the first to explore latent representations for modularizing pretrained architectures and uncertainty sampling for large-scale NLP MTL.",
        "methodology": "The CA-MTL framework is built upon a task-conditioned Transformer architecture that either adds conditional layers or modulates existing pretrained weights using a task embedding (zi). This involves Conditional Weight Transformations of the form φ(W|zi) = γi(zi)W + βi(zi), where γi and βi are learned functions. The architecture integrates several novel components: (1) Conditional Attention, a block-diagonal attention mechanism that incorporates task-specific biases via a learnable weight matrix M(zi). (2) Conditional Alignment, a task-conditioned layer between the input embedding and the first Transformer layer, where a single alignment matrix ˆR is generated as a function of the task embedding. (3) Conditional Layer Normalization (CLN), which extends Conditional Batch Normalization to adapt layer normalization statistics to specific tasks. (4) A Conditional Bottleneck, a task-conditioned two-layer feed-forward bottleneck that facilitates weight sharing and information flow. To balance different tasks in MTL, the paper introduces Multi-Task Uncertainty Sampling, an active learning-inspired task selection strategy. It evaluates model uncertainty using Shannon Entropy (Hi) and normalizes it to account for differing class numbers and high entropy outliers. The algorithm prioritizes tasks with higher uncertainty by choosing 'b' samples with the highest uncertainty from 'b × T' candidates to train the model.",
        "experimental_setup": "The CA-MTL implementation is based on HuggingFace and uses Adam optimizer with a learning rate of 2e-5, a batch size of 32, and a fixed seed of 12. Learning rate decay with warm-up is applied over the first 10% of training steps. For 8-task models, 5 epochs and a sequence length of 128 were used, while for 24-task models, 8 epochs and a sequence length of 256 were employed. A dropout rate of 0.1 was applied across all layers. The bottom half of the Transformer layers were frozen in most experiments to preserve pretrained knowledge, with ablation studies confirming this choice. Experiments were conducted using either four NVIDIA P100 GPUs for base models or four NVIDIA V100 GPUs for larger models. No parameter search, model ensembles, or task-specific tricks were used. The evaluation involved 26 NLP tasks across various benchmarks: GLUE (9 tasks including MNLIm/MNLImm), SuperGLUE (8 tasks), MRQA (6 tasks), and WNUT2017 (Named Entity Recognition). Domain adaptation capabilities were tested on SciTail and SNLI datasets. Performance was measured using GLUE average scores (accuracy, F1, Matthew's correlation, Spearman's correlation as appropriate), Task σ (standard deviation of scores across tasks), and covariance similarity scores. Baselines included single-task fine-tuned BERT/RoBERTa models, MTL models (BERT, RoBERTa, MT-DNN, PALs+Anneal Samp.), and other adapter networks (Adapters-256). Comparisons were also made against MT-DNNSMART, STILTS, BAM!, ALUM, and SemBert.",
        "limitations": "The paper notes that the performance gain on RTE, while positive for MTL, might be a side effect of the chosen layer freezing configuration. An ablation study indicated that the Conditional Bottleneck module did not consistently improve overall performance. Experiments with a full-block Conditional Attention variant showed it performed significantly worse and required more parameters than the block-diagonal variant. For the WNUT2017 Named Entity Recognition task, CA-MTL, despite significantly outperforming the MTL baseline, did not fully close the gap with the single-task baselines, suggesting it had not yet overfit and could have benefited from more training cycles. The Evolutionary Data Measures (EDM) used to estimate task difficulty for analyzing MT-Uncertainty sampling behavior were acknowledged to lack precision, being more effective at classifying task difficulty rather than providing a regression-like value. Furthermore, CA-MTL's zero-shot transfer ability to new tasks was found to be sensitive to the initialization of the new task's embedding. The hypothesis that preserving more pretrained weights by freezing layers is beneficial when scaling to 24 or more tasks was stated but not further explored.",
        "future_research_directions": "The authors state that extending the ideas of dynamically adapting and modularizing knowledge embedded in large monolithic pretrained models will be a primary objective for future work. Additionally, they implicitly suggest further exploration into the hypothesis regarding the benefits of preserving more pretrained weights when increasing the total number of tasks (to 24 or more), as this aspect was not fully investigated in the current study.",
        "experimental_code": "class FiLM(nn.Module):\n    \"\"\" Feature-wise Linear Modulation (FiLM) layer\"\"\"\n    def __init__(self, input_size, output_size, num_film_layers=1, layer_norm=False):\n        \"\"\"\n        :param input_size: feature size of x_cond\n        :param output_size: feature size of x_to_film\n        :param layer_norm: true or false\n        \"\"\"\n        super(FiLM, self).__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n        self.num_film_layers = num_film_layers\n        self.layer_norm = nn.LayerNorm(output_size) if layer_norm else None\n        film_output_size = self.output_size * num_film_layers * 2\n        self.gb_weights = nn.Linear(self.input_size, film_output_size)\n        self.gb_weights.bias.data.fill_(0)\n\n    def forward(self, x_cond, x_to_film):\n        gb = self.gb_weights(x_cond).unsqueeze(1)\n        gamma, beta = torch.chunk(gb, 2, dim=-1)\n        out = (1 + gamma) * x_to_film + beta\n        if self.layer_norm is not None:\n            out = self.layer_norm(out)\n        return out\n\n\nclass CBDA(nn.Module):\n    \"\"\" Conditional Block Diagonal Attention (CBDA) layer\"\"\"\n    def __init__(self, input_size, output_size, blocks=1, num_film_layers=1, layer_norm=False):\n        \"\"\"\n        :param input_size: feature size of x_cond\n        :param output_size: feature size of x_to_film\n        :param layer_norm: true or false\n        \"\"\"\n        super(CBDA, self).__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n        self.num_film_layers = num_film_layers\n        self.layer_norm = nn.LayerNorm(output_size) if layer_norm else None\n        self.blocks = blocks\n        film_output_size = self.output_size * num_film_layers * 2\n        self.gb_weights = nn.Linear(self.input_size, film_output_size)\n        self.gb_weights.bias.data.fill_(0)\n\n    def forward(self, x_cond, x_to_film):\n        gb = self.gb_weights(x_cond).unsqueeze(1)\n        gamma, beta = torch.chunk(gb, 2, dim=-1)\n        out = (1 + gamma) * x_to_film + beta\n        if self.layer_norm is not None:\n            out = self.layer_norm(out)\n        out = [torch.block_diag(*list(out_b.chunk(self.blocks, 0))) for out_b in out]\n        out = torch.stack(out)\n        return out[:, :, :out.size(1)]\n\n# Usage of CBDA in attention (e.g., from MyBertSelfAttention9.forward in ca_mtl_base.py):\n# attention_scores2 = self.cond_block_diag_attn(\n#     x_cond=task_embedding,\n#     x_to_film=self.random_weight_matrix,\n# )\n# attention_scores = attention_scores1 + attention_scores2.unsqueeze(1)\n\n\nclass ConditionalLayerNorm(nn.Module):\n    r\"\"\"Applies Conditional Layer Normalization over a mini-batch of inputs.\"\"\"\n    __constants__ = ['normalized_shape', 'condition_size', 'weight', 'bias', 'eps']\n\n    def __init__(self, normalized_shape, condition_size, eps=1e-5):\n        super(ConditionalLayerNorm, self).__init__()\n        if isinstance(normalized_shape, numbers.Integral):\n            normalized_shape = (normalized_shape,)\n        self.normalized_shape = tuple(normalized_shape)\n\n        self.condition_size = condition_size\n        self.eps = eps\n\n        self.weight = nn.Parameter(torch.Tensor(*normalized_shape))\n        self.ln_weight_modulation = FiLM(condition_size, sum(normalized_shape))\n        self.bias = nn.Parameter(torch.Tensor(*normalized_shape))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.ones_(self.weight)\n        nn.init.zeros_(self.bias)\n\n    def forward(self, input_, condition, task_id):\n        unique_task_ids = torch.unique(task_id)\n        cln_output = torch.zeros_like(input_)\n        for unique_task_id in unique_task_ids:\n            task_id_filter = task_id == unique_task_id\n            task_emb = condition[task_id_filter][0].unsqueeze(0)\n            weight = self.ln_weight_modulation(task_emb, self.weight).view(-1)\n            cln_output[task_id_filter] = F.layer_norm(input_[task_id_filter], self.normalized_shape, weight, self.bias, self.eps)\n        return cln_output\n\n    def extra_repr(self):\n        return '{normalized_shape}, {condition_size}, eps={eps}'.format(**self.__dict__)\n\n\nclass ConditionalBottleNeck(nn.Module):\n    \"\"\"Down projection and up projection with FiLM layers within Transformer layer.\"\"\"\n    def __init__(self, config):\n        super(ConditionalBottleNeck, self).__init__()\n        self.emb_transf = nn.Linear(config.hidden_size, config.hidden_size)\n        self.hidden_modulation = FiLM(config.hidden_size, config.hidden_size)\n        self.down_proj_layer = nn.Linear(config.hidden_size, config.hidden_size//3)\n        self.up_proj_layer = nn.Linear(config.hidden_size//3, config.hidden_size)\n\n    def forward(self, x_cond, hidden_states):\n        x_cond = self.emb_transf(x_cond)\n        hidden_states = self.hidden_modulation(x_cond=x_cond, x_to_film=hidden_states)\n        hidden_states = self.down_proj_layer(hidden_states)\n        hidden_states = self.up_proj_layer(hidden_states)\n        return hidden_states\n\n\n# Conditional Alignment (FiLM applied after embeddings, e.g., from CaMtlBaseEncoder.forward):\n# embedding_output = self.embeddings(\n#     input_ids=input_ids,\n#     position_ids=position_ids,\n#     token_type_ids=token_type_ids,\n#     inputs_embeds=inputs_embeds,\n# )\n# embedding_output = self.conditional_alignment(\n#     x_cond=task_embedding,\n#     x_to_film=embedding_output,\n# )\n\n\n# Multi-Task Uncertainty Sampling - Entropy Calculation (from Decoder.py):\n# class Decoder(torch.nn.Module):\n#     # ... other methods ...\n#     def calculate_entropy(self, logits):\n#         probas = Softmax(dim=1)(logits.detach())\n#         samples_entropy = entropy(probas.transpose(0, 1).cpu())\n#         even_preds = numpy.array(\n#             [[1 / self.num_labels for _ in range(self.num_labels)]]\n#         )\n#         max_entropy = entropy(even_preds.T)\n#         epsilon = 1e-5\n#         samples_entropy = samples_entropy / (max_entropy.item() + epsilon)\n#         return torch.tensor(samples_entropy)\n\n\n# Multi-Task Uncertainty Sampling - Sampling Logic (from MtUcertaintyIterator.__next__ in mtl_trainer.py):\n# test_batch = {}\n# for idx, loader_iter in enumerate(self.loader_iters):\n#     try:\n#         batch = loader_iter.__next__()\n#     except StopIteration:\n#         new_loader_iter = iter(self.my_loader.loaders[idx])\n#         self.loader_iters[idx] = new_loader_iter\n#         batch = new_loader_iter.__next__()\n#\n#     test_batch = self.batchify_data(batch, test_batch)\n#\n# inputs = {}\n# for k, v in test_batch.items():\n#     if k not in [\"labels\"]:\n#         inputs[k] = v.detach().to(self.my_loader.args.device)\n#\n# with torch.no_grad():\n#     model.select_batch_mode = True # Placeholder for context, actual code might not have this line\n#     outputs = model(**inputs)\n#     model.select_batch_mode = False # Placeholder for context, actual code might not have this line\n#\n# (\n#     test_batch_entropy,\n#     test_batch_entropy_mean,\n#     max_mean_batch_entropy,\n# ) = outputs[-3:]\n#\n# # ... del inputs ...\n#\n# test_batch_entropy_mean = (\n#     test_batch_entropy_mean / max_mean_batch_entropy\n# )\n# test_batch_entropy = test_batch_entropy * test_batch_entropy_mean\n#\n# # ... handling specific task types (sts-b, mrpc) for entropy adjustment ...\n#\n# select_size = min(\n#     self.my_loader.args.train_batch_size,\n#     test_batch[\"input_ids\"].shape[0],\n# )\n# top_entropy = torch.topk(test_batch_entropy, select_size)\n#\n# for k, v in test_batch.items():\n#     test_batch[k] = torch.index_select(v, 0, top_entropy.indices)",
        "experimental_info": "CA-MTL Model Variants and Architectural Details: CA-MTL-base implements Conditional Attention (CBDA) and Conditional Layer Normalization (CLN) in its attention and output layers, respectively. It applies a Conditional Bottleneck only in its top encoder layer, and Conditional Alignment (FiLM) after the embedding layer. CA-MTL-large, on the other hand, also uses CBDA and CLN in its attention and output layers, but applies the Conditional Bottleneck to *all encoder layers*. Both variants utilize task embeddings for conditioning various modules. Weight Freezing Strategy: The framework allows freezing a range of encoder layers (e.g., '0-5'). Critically, when layers are frozen, specific task-conditioned components are *explicitly unfrozen* to enable adaptation. These include `random_weight_matrix` (for CBDA), `film.gb_weights` (for FiLM in alignment and bottleneck), `ln_weight_modulation.gb_weights` (for CLN), and `adapter` (for the Conditional Bottleneck). Multi-Task Uncertainty Sampling: This active learning-inspired strategy is activated by the `use_mt_uncertainty` argument. It calculates model uncertainty using Shannon Entropy on softmax probabilities, which is then normalized by the maximum possible entropy. This sample-wise entropy is further scaled by a normalized mean batch entropy. `torch.topk` selects the samples with the highest uncertainty for training. The `percent_of_max_data_size` argument controls the number of candidate samples. An alternative, `uniform_mt_sampling`, ensures equal samples per task per epoch. General Data Settings: The `data_dir` specifies the input data location. `tasks` defines the list of GLUE benchmarks used (e.g., CoLA, MNLI, RTE, WNLI, QQP, STS-B, SST-2, QNLI, MRPC). `max_seq_length` sets the maximum sequence length (default 128). Training Configuration: The `warmup_proportion` (default 0.1) dictates the fraction of total training steps for linear learning rate warmup. The optimizer used is AdamW."
      }
    },
    {
      "title": "Towards Modular LLMs by Building and Reusing a Library of LoRAs",
      "abstract": "The growing number of parameter-efficient adaptations of a base large\nlanguage model (LLM) calls for studying whether we can reuse such trained\nadapters to improve performance for new tasks. We study how to best build a\nlibrary of adapters given multi-task data and devise techniques for both\nzero-shot and supervised task generalization through routing in such library.\nWe benchmark existing approaches to build this library and introduce\nmodel-based clustering, MBC, a method that groups tasks based on the similarity\nof their adapter parameters, indirectly optimizing for transfer across the\nmulti-task dataset. To re-use the library, we present a novel zero-shot routing\nmechanism, Arrow, which enables dynamic selection of the most relevant adapters\nfor new inputs without the need for retraining. We experiment with several\nLLMs, such as Phi-2 and Mistral, on a wide array of held-out tasks, verifying\nthat MBC-based adapters and Arrow routing lead to superior generalization to\nnew tasks. We make steps towards creating modular, adaptable LLMs that can\nmatch or outperform traditional joint training.",
      "full_text": "Towards Modular LLMs by Building and Reusing a Library of LoRAs Oleksiy Ostapenko* 1 2 3 Zhan Su* 2 4 Edoardo Maria Ponti5 Laurent Charlin2 6 7 Nicolas Le Roux1 2 3 7 Matheus Pereira1 Lucas Caccia* 1 Alessandro Sordoni* 1 2 3 Abstract The growing number of parameter-efficient adap- tations of a base large language model (LLM) calls for studying whether we can reuse such trained adapters to improve performance for new tasks. We study how to best build a library of adapters given multi-task data and devise tech- niques for both zero-shot and supervised task gen- eralization through routing in such library. We benchmark existing approaches to build this li- brary and introduce model-based clustering, MBC, a method that groups tasks based on the similarity of their adapter parameters, indirectly optimiz- ing for transfer across the multi-task dataset. To re-use the library, we present a novel zero-shot routing mechanism, Arrow, which enables dy- namic selection of the most relevant adapters for new inputs without the need for retraining. We experiment with several LLMs, such as Phi-2 and Mistral, on a wide array of held-out tasks, verify- ing that MBC-based adapters and Arrow routing lead to superior generalization to new tasks. We make steps towards creating modular, adaptable LLMs that can match or outperform traditional joint training. 1. Introduction Tailoring large language models (LLMs) towards down- stream tasks, domains, or user profiles is of paramount importance given the recent democratization of their usage, catalyzed by the release of open-source LLMs (Zhang et al., 2023b; Microsoft Research, 2023, inter alia). This process often relies on an adapter, such as LoRA (Hu et al., 2022), a parameter-efficient fine-tuning (PEFT) of a pre-trained LLM (Hu et al., 2022; Liu et al., 2022; Li & Liang, 2021). *Equal contribution 1Microsoft Research 2Mila — Que- bec AI Institute 3Universit´e de Montr ´eal 4University of Copen- hagen 5University of Edinburgh 6HEC Montr ´eal 7Canada CI- FAR AI Chair. Correspondence to: A. Sordoni <alsor- don@microsoft.com>. Preprint. Figure 1.How to coordinate a library of adapters (e.g., LoRAs) for zero-shot generalization to new tasks? To build this library (top), we propose MBC, a novel method that clusters tasks based on the similarity of the parameters of corresponding LoRAs. To reuse a library (either private or MBC, bottom), we route hidden states to trained LoRAs via Arrow, which leverages the SVD decomposition of each LoRA. LLM adapters are increasingly available as part of online hubs (Beck et al., 2021; Mangrulkar et al., 2022). These adapters are developed independently and asynchronously by users across the globe. Hence, they implicitly consti- tute a library built on top of multi-task data (Pfeiffer et al., 2023). Prior works show that mixtures of pretrained trained adapters can facilitate few-shot adaptation of LLMs to un- seen tasks (Ponti et al., 2023; Vu et al., 2021; Huang et al., 2024). Reusing pre-existing adapters in a zero-shot fashion remains less explored (Jang et al., 2023; Belofsky, 2023). In contrast to standard mixture-of-experts approaches (Fedus et al., 2022), in this setting, new inputs must be routed to in- dependently trained experts without requiring joint training of the routing mechanism and expert parameters. This leads to the question: how to create a modular LLM end-to-end by first building and then reusing a library of adapters for supervised adaptation and zero-shot generaliza- tion? First, given a base LLM, such as Phi-2 (Microsoft Re- search, 2023) or Mistral (Jiang et al., 2023), we investigate 1 arXiv:2405.11157v1  [cs.LG]  18 May 2024Towards Modular LMs by Building and Reusing a Library of LoRAs building a library of adapters by leveraging 256 tasks from Flan-v2 (Longpre et al., 2023). 1 We focus on LoRA (Hu et al., 2022) and leave the extension to other adapter types for future work. Once the adapter library has been built, we devise routing strategies to evaluatezero-shot generalization on 10 downstream tasks comprising common-sense reason- ing and coding (ARC (Clark et al., 2018), MBPP (Austin et al., 2021), inter alia) and supervised adaptation on 12 SuperNatural Instructions (SNI) tasks (Wang et al., 2022b). How to build the adapter library?One straightforward approach is to operate in a private scenario, in which one trains one adapter per task on the multi-task data and mix those adapters for unseen tasks (Chronopoulou et al., 2023a; Vu et al., 2021; Huang et al., 2024). This method is useful when the multi-task data cannot be shared for joint training (Mireshghallah et al., 2020) but trained adapters can. To favour transfer between training tasks, recent approaches compress the multi-task data into a smaller set of reusable, composable adapters (Ponti et al., 2023; Caccia et al., 2023). In this shared data setting, we propose model-based cluster- ing (MBC), a simple two-stage approach to build a library of adapters. We find a positive correlation between the similar- ity of the LoRA weights of a pair of tasks and the transfer between the two tasks. Building on this intuition, we first exploit LoRA similarity in weight space between privately trained adapters as a proxy for detecting clusters of similar tasks, then train one adapter per cluster. Our approach em- pirically improves performance while matching the compute budget. How to reuse the library for new scenarios?Given a library of trained LoRAs, we examine strategies of how to reuse the library in two settings: zero-shot generaliza- tion and parameter-efficient supervised adaptation to new tasks. Reusing LoRAs in a zero-shot manner is challenging because there is no labelled data to learn a routing mecha- nism. We propose Arrow (↗↗), a routing mechanism that automatically selects relevant LoRAs without requiring i) joint training and ii) access to the data used to train each LoRA. This facilitates the vision of a decentralized system where LoRAs can be trained asynchronously and be read- ily reused with minimal assumptions. Arrow computes a representation for each LoRA as the direction of maximum variance induced by the LoRA parameters. At inference time, Arrow routes per token and per layer, i.e. each hidden state is routed by computing its alignment with each LoRA representation. In summary, our contributions are:i) we study how to create LoRA-based modular multi-task LLM in a setting where experts are trained independently and the router is created after the training of the experts; ii) assuming shared multi- task data, we propose a clustering approach (MBC) to train a 1We held out SNI tasks to test supervised adaptation. library of adapters; and, iii) we propose Arrow, a zero-shot routing method to select which adapters to use from a library of LoRAs. This allows for routing to independently trained experts without accessing their training data. 2. Preliminaries We are given a set of tasks T = {t1, . . . , tT }, where each task ti is associated with a dataset containing a set of sam- ples Di = {(x1, y1), ...,(xn, yn)}. The union of the train- ing sets constitutes our multi-task dataset D; in our case, it is Flan (Longpre et al., 2023). In order to create our library of task adapters, we use LoRA (Hu et al., 2022). LoRA achieves competitive trade-offs between performance and parameter efficiency (Karimi Mahabadi et al., 2021) by modifying the linear transformations in a base LM. For each linear transformation in the base LM, LoRA modifies the base model parameters as follows: h = Wx + s · AB⊤x, (LoRA) where W are the (frozen) weights of the base LM, A, B∈ Rd×r are low-rank learnable parameters and s ≥ 1 is a tunable scalar hyperparameter. LoRA achieves parameter efficiency because of the reduced rank r (≪ d). 3. Building the LoRA Library We propose different alternatives for building a libraryL of adapters that perform well on the tasks they were trained on and are versatile enough to be effective on other unseen downstream tasks. To do so, we seek methods that enhance multi-task transfer while reducing task interference (Wang et al., 2021; Chen et al., 2022). Private Adapters One straightforward solution is to train separate adapters on each training task, i.e. the library will be composed ofT adapters (see Fig. 1). Several existing methods operate in this setting, such as LoraHub (Huang et al., 2024), AdapterSoup (Chronopoulou et al., 2023a) and SPoT (Vu et al., 2021). Although this solution does not exploit multi-task training, it is required in settings where the task data is private, e.g., user data, and cannot be shared. Moreover, this setting reflects well the scenario in which adapters are trained by end users in a decentralized fashion and added asynchronously to the library. Shared Adapter To encourage transfer, another solution is to train a single adapter on all the multi-task training data. One possible shortcoming is the reduced capacity to fit the multi-task training data and the possibility of interference between the multitude of training tasks (Ponti et al., 2023). Training a single adapter may result in negative transfer because task gradients are misaligned (Wang et al., 2021). An obvious solution to reduce the amount of interference 2Towards Modular LMs by Building and Reusing a Library of LoRAs 0.2 0.4 0.6 0.8 1.0 Adapter Similarity 0.1 0.0 0.1 0.2 Performance Delta Phi 2 (r = 0.51) GPT-Neo (r = 0.75) Figure 2. For any pair of tasks, we report the cosine similarity between the corresponding LoRA weights (x-axis) against the delta in performance between LoRAs trained on them individually and jointly (y-axis). The positive correlation indicates that if LoRAs are dissimilar, we should abstain from multi-task training. is to increase the number of trainable parameters, e.g. to fine-tune the whole base LM on the multi-task data (Liu et al., 2022). Poly / MHR Adapters Polytropon (Poly) and Multi-Head Routing (MHR) (Ponti et al., 2023; Caccia et al., 2023) explore intermediate approaches between private and shared, where K < T“basis” adapters are trained on the multi-task training data. These K adapters can be considered “latent skills”, as each task adapter in the multi-task training set can be expressed as a linear combination of these basis adapters. If private training for all the tasks learns a matrix of parameters Φ ∈ RT×D, where D is the dimensionality of the LoRA adapters, Poly decomposes Φ = Z ˆΦ, where Z ∈ RT×K, ˆΦ ∈ RK×D, ˆΦ storing the latent skills and Z the linear combination coefficients for each task which specify the task-specific routing w.r.t. the latent skills. Both Z and ˆΦ are trained jointly on the multi-task training set by gradient descent. Note that the skills ˆΦ do not correspond to specific tasks and therefore it is not clear how to reuse them for zero-shot generalization (Caccia et al., 2023). Model-Based Clustering (MBC) While Polytropon and MHR reduce the inventory size, they require joint training of experts and the router on the combined dataset of all tasks. Here, we propose another approach to compress multi-task data into a set of reusable adapters; we cluster tasks based on their similarity and then train one adapter per task cluster. Ideally, the similarity between two tasks should correlate with the benefit of training a single model on both tasks compared to having two independent models (Fifty et al., 2021; Vu et al., 2020a). Motivated by (Zhou et al., 2022), we rely on the intuition that LoRA parameter vectors similarity can approximate the amount of transfer between a pair of tasks. To confirm this, we devise the following Algorithm 1Model-Based Clustering (MBC) Input: Multi-task data D1, . . . ,DT , base model LLMθ, number of library adapters K Output: Library L L = {}, A = {} ▷ LoRA params for t = 1to T do At, Bt = train(Dt, LLMθ) ▷ Train LoRA on task t A = A ∪ {cat(flatten(At), flatten(Bt))} end for U = SVD(A) ▷ Reduce LoRA dim S = cosine-similarity(U, U) ▷ T× T similarities c1, . . . , cK = k-means(S, K) ▷ Cluster similarities for k = 1to K do Dk = SDt, ∀t ∈ ck ▷ Join datasets in cluster Ak, Bk = train(Dk, LLMθ) L = L ∪ {(Ak, Bk)} end for Returns L experiment: we sample pairs of tasks (ti, tj), t∈ Tfrom the multi-task dataset, and we train both a) a LoRA on each task independently b) a LoRA on the union of the training datasets for the two tasks. We then compute the cosine similarity between the flattened LoRA parameters. We quantify transfer as the difference in the average log- likelihood induced by the joint and private models when evaluated on the test set of the two tasks. In Fig. 2, we observe that, for two different base models (GPT-Neo and Phi-2), the closer the tasks are in LoRA parameter space, the more performance delta is when we train on the joint dataset. The previous observation warrants our simple two-stage training procedure illustrated in Fig. 1 (top). Given a fixed computation training budget of N training steps per task, we use the first n steps to train private LoRAs. We then use these LoRA parameters to group tasks into K clusters by running a standard clustering algorithm (K-Means). In the second stage of training, we train one adapter per cluster for an additional N − n training steps, which keeps the total amount of computation similar to other approaches. We refer to this method as Model-Based Clustering (MBC) as it uses the model-based information encoded in the weights to determine a similarity metric between tasks (see Alg. 1). 4. Reusing the LoRA Library Next, we study the reuse of a trained library L in two sce- narios: for new inputs x∗, i.e. zero-shot, and in a super- vised adaptation setting, where new tasks t∗ come equipped with their training data Dt∗ . While the latter has been ad- dressed in recent works (Huang et al., 2024; Caccia et al., 2023; Vu et al., 2021), the former scenario remains less explored (Jang et al., 2023; Belofsky, 2023). We first devise routing strategies in the zero-shot and supervised settings and then describe how to aggregate the contributions of 3Towards Modular LMs by Building and Reusing a Library of LoRAs adapters selected by the routing strategies. 4.1. Routing We denote the hidden state for any token at a given trans- former layer produced by the input token x∗ as h∗. Similar to MoE approaches, we seek to parameterize a layer-specific routing distribution that prescribes which adapters to use. We denote this categorical distribution over|L| outcomes as p(· |h∗, x∗), where we drop the dependence on the layer for simplicity. For example, in standard MoE approaches (Fe- dus et al., 2022), p(· |h∗, x∗) = softmax(Wh∗). Given that we relax the assumption that the routing and the library should be trained together, we must devise ways to learn such routing distribution a posteriori. 4.1.1. Z ERO -SHOT ROUTING µ Routing One straightforward method to route to exist- ing experts is to set the routing distribution to uniform for all layers, p(· |h∗, x∗) = [1 /|L|, . . . ,1/|L|]. Despite its simplicity, µ routing was shown to be quite effective in re- cent work (Caccia et al., 2023; Chronopoulou et al., 2023a) and, due to the linearity of the LoRA adapters, effectively boils down to averaging the weights of the trained adapters uniformly. TP Routing Another variant treats routing as an |L|-way classification problem. Specifically, given an input x be- longing to task t in our multi-task training set, we train a task predictor f by minimizing the categorical cross-entropy loss −log f(x)[t], where f(x) is a probability distribution obtained by learning a classifier on top of a T5 encoder (Raf- fel et al., 2020). We then set p(· |h∗, x∗) = f(x∗) at inference time. Note that the routing decisions are not de- pendent on the hidden state h∗, so this is a router dependent on the whole input but independent of the particular token or layer in the Transformer. We call this predictor TP (Task Predictor). CM Routing Centroid Matching (CM) computes a prototype for every expert (and for each layer) by averaging the hidden representations obtained by a forward pass of the LLM on each expert dataset. These prototypes can be stored in the columns of the routing matrix W. Once the prototypes for each expert have been obtained, the routing distribution is calculated by taking the cosine similarity between h∗ and each expert prototype and finally applying softmax. This routing is similar in spirit to Jang et al. (2023) and Belofsky (2023). Arrow Routing↗↗ The rows of every routing matrix W of standard MoE routing can be interpreted as expert “proto- types”. Arrow prescribes a way to estimate such routing ma- trix in a 0-shot fashion without requiring data access. Let’s denote by {Ai, Bi} the parameters for expert i at layer ℓ, Algorithm 2Arrow Routing ↗↗ Weight Initialization Input: LoRA library L, layer ℓ Output: Routing parameters for layer ℓ: Wℓ for i = 1to L do Ai, Bi = L[i, ℓ] ▷ Get weights for expert i U, D, V= SVD(AiBT i ) Wℓ[i] =V [:, 0] ▷ First right singular vector end for Returns Wℓ Routing Input: Routing parameters for layer ℓ: Wℓ ∈ R|L|×d, token in layer ℓ: hℓ ∈ Rd, top-k routing: k Output: Routing probabilities for layer ℓ: pℓ logits = abs(Wℓhℓ) pℓ[i] = ( logits[i] if i ∈ arg top-k(logits) −∞ else Returns softmax(pℓ) where we drop the dependency on ℓ. The i-th LoRA expert transforms each token’s hidden state h∗ as h∗ i = AiBT i h∗. Arrow finds a prototype for the expert i by decomposing the outer product AiBT i with SVD and taking the right first singular vector of this transformation (see Alg. 2). The pro- totype determines the direction of most variance induced by expert i in the space of hidden states h. If the LoRA adapters are of rank 1, i.e. Ai, Bi ∈ DD×1 the prototype for the expert i will be equal to the normalized Bi vector, i.e. argmaxv,∥v∥2=1∥AiBT i v∥2 = Bi/∥Bi∥2. In Section 10.1, we provide empirical evidence that indeed, ∥AiBT i v∥2 is larger when v belongs to task i, thus motivating this routing approach. Given that both v and −v are valid singular vec- tors, we compute expert logits as the absolute value of the dot product between prototypes and inputs. Alg. 2 details the prototype initialization and the routing step of Arrow. Arrow offers several advantages: a) it doesn’t require access to training data; b) it routes differently in every layer and token, increasing the overall model expressivity, and c) it is compute efficient since it requires no further training and SVD decomposition can be computed efficiently for low-rank matrices (Elhage et al., 2021; Nakatsukasa, 2019). 4.1.2. S UPERVISED TASK ROUTING When generalizing to a new task, we can learn the optimal routing given the task data D∗. This setting is similar to previous task generalization works (Ponti et al., 2023; Cac- cia et al., 2023; Huang et al., 2024). We compare results in this supervised setting to both Poly (Ponti et al., 2023) and LoraHub (Huang et al., 2024). Poly Routing treats the distribution over experts at each layer as an |L|-dimensional parameter that is learned by minimizing the cross-entropy on the new task data D∗. It 4Towards Modular LMs by Building and Reusing a Library of LoRAs optimizes the merging coefficients of LoRAs for the new task, i.e. A∗ = P|L| i=1 wiAi and B∗ = P|L| i=1 wiBi. Here p(·|h∗, x) = (w1, . . . , wn) is the (input-independent) learn- able routing distribution for a given layer. LoraHub Routing (Huang et al., 2024) is similar to Poly with the exception that a) it resorts to gradient-free optimiza- tion to learn routing coefficients and b) it doesn’t fine-tune the experts’ parameters, making it less expressive thanPoly. π-tuning Routing uses Fisher Information to create an embedding for each task-specific expert. In the fine-tuning process, π-tuning first trains an expert for the next task, then it retrieves a subset of experts most similar to the target task’s expert using FIM embeddings. Finally, both the inter- polation coefficients and experts’ parameters are tuned on the target task’s data (Wu et al., 2023). 4.2. LoRA Composition Given a routing distribution w = p(· |h∗, x) obtained ei- ther using the previously presented zero-shot or supervised routing, we linearly combine adapters in the library, i.e. A∗ = P|L| i=1 wiAi, B∗ = P|L| i=1 wiBi and use the result- ing adapter to perform inference at every layer of the base LLM (Ponti et al., 2023; Huang et al., 2024). For 0-shot task generalization, we employ top-k routing, composing the k experts with the highest routing logits. 5. Experiments Our experimental evaluation aims to answer the following questions: 1) How does building a LoRA library compare to non-modular methods (e.g. full fine-tuning)? 2) How large is the gap between privately trained libraries (similar to online hubs) and libraries which assume access to multi- task data? 3) To what extent does routing facilitate reusing a library of LoRA adapters? Multi-Task DatasetWe train expert modules on 256 tasks from the original Flan v2 dataset (Longpre et al., 2023). We exclude the SNI tasks (> 1000 tasks) (Wang et al., 2022b) from training for computational reasons. We reserved 12 SNI tasks for downstream out-of-domain evaluation. Simi- larly to Wang et al. (2023), we sub-sampled 10,000 exam- ples per task to ensure computational feasibility. Within these samples, 1,000 are allocated for validation and early- stopping. We will release our dataset for reproducibility. Evaluation For our supervised adaptation study, we use 12 held-out SNI tasks, each corresponding to a different SNI category. We threshold the number of training ex- amples to 10,000 examples per task. We evaluate perfor- mance with Rouge-L scores (Lin & Hovy, 2003). For zero- shot evaluation, we mainly use ten tasks widespread in the literature, including 1) common-sense reasoning: Wino- Grande (Sakaguchi et al., 2021), HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020); 2) question answering: boolQ (Clark et al., 2019), OpenbookQA (Mihaylov et al., 2018), ARC-easy and hard (Clark et al., 2018); 3) cod- ing: (HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021); 4) general-purpose reasoning: BBH. (Suzgun et al., 2022)2 We remove overlaps between the evaluation tasks and the Flan multi-task training set (boolQ, ARC, Wino- Grande, HellaSwag, OpenbookQA and PIQA). We also include zero-shot results on the 12 held-out SNI tasks in the appendix. Models and TrainingThis work focuses on augmenting LLMs with a library of adapters to transform them into mod- ular architectures. Our primary focus is on Phi-2 (Microsoft Research, 2023), a state-of-the-art model (as of March 2024) with 2.8 billion parameters, leading its class of models with parameter counts below 3 billion, according to the open leaderboard (Beeching et al., 2023). Additionally, we con- ducted experiments using the larger Mistral 7B (Jiang et al., 2023) model, given its widespread use in the community. For all models, we only patch attention layers with LoRA adapters. Unless stated otherwise, for all our multi-task training and single-task adaptation scenarios, we use LoRA rank of 4, dropout of 0.05 and learning rate of 1e-4. Unless specified, we set the number of clusters for MBC to 10, re- sulting in the best upstream validation loss and downstream performance for Phi-2, as demonstrated in Fig. 4. Methods We consider the following methods in both zero- shot and supervised scenarios (except for FullFT): • Base: the base model tested without any adaptation; • Shared: a single expert LoRA finetuned on the joint training set of all tasks (256 tasks unless stated other- wise) on top of the base model with multi-task learning; • FullFT: like Shared but the full model is finetuned. We adopt the following naming convention for the models using a library of experts: <library>–<routing>. For the library type, we consider Poly, MHR, Private and MBC libraries described in Sec. 3. For MBC, we match the total amount of compute, meaning that we use 40% of the training steps to compute the LoRA clustering and the other 60% to compute the final cluster adapters. For routing, we use µ, TP, CM and Arrow in the zero-shot scenario and Poly and LoraHub3 for the supervised scenario, described in Sec. 4. 2We test on a subset of 1000 randomly sampled examples to reduce evaluation costs. 3For LoraHub, we match the amount of compute used by SGD. Assuming the backward pass is twice the compute of a forward pass, and since nevergrad (NG; Rapin & Teytaud, 2018) only does forward passes, to match the compute of 5 SGD training epochs, we perform 30 epochs of NG with 1/2 of the training data used by SGD methods. 5Towards Modular LMs by Building and Reusing a Library of LoRAs Library Route |L| PIQA BOOLQ WG HSWAG ARC E ARC C HE OQA BBH MBPP Acc. Phi-2 (2.8B) Base - - 79.2 82.7 75.7 72.5 77.5 52.9 45.1 49.8 48.0 56.0 63.8 FullFT - - 80.3 80.8 77.0 73.2 83.5 57.9 50.0 48.0 47.7 57.2 65.6 Shared - 1 80.4 82.4 76.6 73.4 83.2 55.8 46.3 50.4 48.4 58.4 65.5 Poly µ 8 80.6 82.3 76.7 71.7 82.7 55.3 48.2 50.4 49.8 59.1 65.7 MHR µ 8 80.1 83.0 77.1 70.4 83.2 55.5 46.3 53.4 52.0 58.0 65.9 Private µ 256 79.5 83.2 76.0 73.1 81.4 54.5 43.9 47.8 48.5 59.9 64.8 Private ↗↗ 256 80.2 84.3 77.6 72.6 84.2 56.4 50.6 52.2 47.7 59.9 66.6 MBC µ 10 80.3 85.1 77.3 73.1 84.3 57.7 48.8 50.2 51.6 62.3 67.1 MBC ↗↗ 10 79.9 84.7 77.7 72.9 84.8 57.9 51.8 50.2 52.2 62.3 67.4 Mistral (7B) Base - - 81.1 82.2 66.5 78.8 68.9 49.6 28.0 44.6 47.9 47.5 59.5 Shared - 1 50.4 84.6 68.6 79.5 84.8 60.0 24.4 50.4 49.2 47.5 63.1 Private µ 256 82.1 82.7 67.2 79.6 78.7 54.8 29.9 45.2 49.0 49.4 61.9 Private ↗↗ 256 82.8 86.6 66.6 81.1 85.7 60.8 30.5 50.6 49.5 49.4 64.4 MBC µ 10 83.0 87.6 68.5 80.8 86.2 60.9 28.7 48.6 51.5 50.2 64.6 MBC ↗↗ 10 82.8 87.3 70.6 80.9 84.5 59.6 28.0 52.8 45.5 47.1 63.9 Table 1.Downstream zero-shot results for Phi-2 and Mistral backbones. |L| denotes the library size. For comparison with other routing baselines, see Fig. 3. Figure 3.Comparison of routing approaches with both Private and MBC libraries. Left & Middle. Downstream zero-shot performance on two backbones; Arrow outperforms other routing approaches in the case of private libraries, while in the case of MBC libraries, routing is less important. Right. Upstream performance on the held-out sets of each of the 256 training tasks. Arrow nearly matches Oracle routing (which uses information about the task identity) in the case of Private library and noticeably improves for MBC. 5.1. Zero-Shot Results In the zero-shot scenario, downstream tasks are evaluated without further fine-tuning. Tab. 1 presents the mean down- stream accuracy for 10 held-out tasks. First, we analyze Phi-2 results. We observe that MHR-µ achieves strong zero- shot performance, competitive with Shared and FullFT, in line with the results of Caccia et al. (2023). Interest- ingly, training one adapter per task and then taking the average, Private-µ, still achieves gains w.r.t.Base, albeit falling short of multi-task training ( FullFT and Shared), highlighting the competitiveness of uniform ( µ) adapter routing (Chronopoulou et al., 2023a). Comparing the perfor- mance of our proposed MBC approach for library construc- tion (MBC-µ) to previous approaches, we notice a sizable bump in performance of 1.2% absolute accuracy points over the strongest baseline (MHR). Similarly, when studying the zero-shot performance of Phi-2 on 12 SNI tasks in Tab. 5 we observe that MBC-µ strongly outperforms other baselines. Importantly, both Shared and FullFT methods, as well as Poly and MHR libraries assume simultaneous access to the full dataset of all tasks. In contrast, Private and MBC libraries can be trained in an embarrassingly parallel man- ner and therefore do not require any distributed training infrastructure (Li et al., 2022). Next, we analyze whether more informed routing strategies can improve performance beyond the µ−routing. The full results are reported in Figure 3 (Left & Middle). We see that TP, CM and Arrow routing improve the performance over µ routing for the Private Phi-2 library, gaining 0.9%, 0.9% and 1.8% points respectively. This highlights the importance of routing for larger libraries. Notably, Arrow (66.6%) can surpass the performance of FullFT (65.5%) when applied to the Private library. On the MBC library, TP routing decreases performance when 6Towards Modular LMs by Building and Reusing a Library of LoRAs 100 101 102 # of Clusters 65 66 67Downstream Acc. (%) 100 101 102 # of Clusters 0.9 0.8 0.7 Upstream Log Lik. Figure 4.Phi-2 zero-shot accuracy on the 10 held-out tasks (left) and validation log-likelihood on the training tasks ( right) as a function of the number of MBC clusters. compared to uniform routing, while MBC-↗↗ improves over MBC-µ by 0.3% points and proves itself as a more robust routing method for bothPrivate and MBC libraries. Overall, MBC-↗↗ improves 3.6 points over the base model and 1.8% absolute over FullFT. For Mistral, we find a similar trend withMBC libraries achiev- ing the best performance. Arrow routing results in a 2.5% increase in average performance over µ routing when used with the Private library (Private-↗↗ vs. Private-µ). Arrow is able to narrow the performance gap with MBC, without requiring simultaneous data access across tasks. We do not see any gains from using other routing methods for 10 experts in the MBC library in this case. We make simi- lar observations analyzing 0-shot SNI-12 results presented in Table 5, where Private-↗↗ attains notable gains of 10 Rouge-L points over Private-µ while MBC-µ strongly out- performs all other baselines. MBC Analysis Overall, MBC enhances the performance of the library across all our results. To investigate this further, we compare different clustering techniques. First, we com- pare to clusters obtained by randomly selecting examples (RandomExamples). This is equivalent to randomly parti- tioning the joint multi-task dataset. Then, we compare to clusters obtained by randomly choosing tasks from the entire set of training tasks (RandomTask). Finally, we cluster task embeddings, which are obtained by forwarding task-specific examples through the model and averaging their represen- tation at the model’s penultimate layer (Embeddings). For all these methods, we set the number of clusters to 10. The results are shown in Table 2. RandomTask surpasses RandomExamples by 1.6%, which indicates that grouping tasks rather than task examples is crucial for positive trans- fer. Embeddings underperforms MBC and supports our ob- servation that the cosine similarity between the weights of privately-trained LoRA correlates better than using repre- sentation similarity for 0-shot generalization. Additionally, we also report average pairwise cluster “similarity” (as mea- sured by the cosine similarity of the LoRA weights for each cluster) and observe a tendency that expert clusters with Clustering Mean Acc. Similarity RandExamples∗-µ 64.8 0.82 RandTask∗-µ 66.4 0.58 RandTask-µ 66.4 0.58 Embeddings∗-µ 66.1 0.37 MBC∗-µ 66.7 0.37 MBC-µ 67.1 0.27 Table 2.Ablation of task clustering: RandTask clusters tasks ran- domly, RandExamples clusters examples randomly, Embeddings clusters examples based on their embedding similarity. ‘*’ denotes one epoch of training to save computation. We also report average cosine similarity between cluster adapters. lower similarity, i.e. higher diversity, tend to result in higher performance. We conjecture that this stems from different clusters contributing distinct features to the joint model; however, we leave further investigation in this direction to future work (Jolicoeur-Martineau et al., 2023). 5.2. Upstream Performance We further assess the efficacy of Arrow routing by looking at the upstream in-distribution performance, measured as the average of the Rouge-L on the validation sets of the 256 training tasks. Within this setting, we can compute the performance of the Oracle routing, which selects for each task the corresponding expert. In Fig. 3 (Right) we report the results for Arrow and µ routing with both MBC and Private libraries. For both libraries, ↗↗ increases performance w.r.t. µ and almost matches Oracle performance in the Private setting. This demonstrates Arrow’s ability to correctly select the most relevant modules from a large library of experts. 5.3. Supervised Adaptation In Table 3, we present the supervised adaptation results for Phi-2 on the full (100% of training data) and limited (10% of training data) data regimes. The detailed per-task performance as well as the adaptation results for the Mis- tral model are presented in Table 8 and 7. First, for all models (Phi-2, Mistral) we observe a notable performance boost coming from using Private and MBC libraries com- pared to No Library, which optimizes a LoRA for each downstream task by starting from a random initialization, and Shared, which starts from the multi-task trained LoRA solution. Secondly, similarly to zero-shot results, we ob- serve that MBC can boost the performance with both Poly and µ routing: for Phi-2 the performance of MBC-µ tops Private-µ. Additionally, we see that randomly grouping tasks RandomTask-Poly outperforms the non-library base- lines but does not quite match MBC-based clustering for all the models. The low performance of LoraHub can be attributed to the fact that LoraHub does not fine-tune the LoRA experts’ weights but only their routing coefficients 7Towards Modular LMs by Building and Reusing a Library of LoRAs Method 100% Data 10% Data Base 22.2 22.2 No Library 75.5 53.9 Shared 75.8 56.4 Poly 73.4 61.7 MHR 74.8 64.5 π-tuning 76.7 64.6 Private-µ 76.9 62.5 RandTask-Poly 76.7 67.6 MBC-µ 78.8 67.0 MBC-Poly 78.8 68.2 Table 3.Supervised adaptation results on 12 SNI held-out tasks for Phi-2 obtained in the full (100% of training data) and limited (10% of the training data) data settings. (due to gradient free optimization). Refer to App. 10.2 for more insights onto this point. Finally, MBC-µ performs simi- larly to MBC-Poly, echoing results in (Caccia et al., 2023). 5.4. Summary of Results Mirroring the questions at the start of this section, we list our main takeaway messages below: 1. When appropriately routed, independently trained ex- perts (Private-↗↗) can match and surpass the zero- shot performance of full fine-tuning (for Phi-2) and shared tuning (for Mistral 7B). This is a rather sur- prising result given that experts are independently trained and routing is learned post-hoc. These results show promise for building collaboratively and asyn- chronously trained LMs. 2. If data sharing is possible, then clustering tasks by their similarity with MBC constitutes a very effective strategy. In this case, simply averaging the LoRA adapters ob- tained through MBC (MBC-µ) is sufficient compared to more sophisticated routing. Our zero-shot and super- vised adaptation results underscore the superiority of task-based over example-based clustering. 3. Arrow appears to be a very performant zero-shot rout- ing strategy while requiring minimal information about the trained LoRAs and none about the training data. For supervised adaptation, training both adapters and the routing coefficients appears to be crucial. Over- all, if routing seems beneficial for large libraries of adapters, the gains for smaller libraries are diminish- ing. This appears to stand in contrast with sparse MoE models, where (non-uniform) routing is crucial (Jiang et al., 2024). This may be due to the linearity of LoRA experts, which stands in contrast with MLP experts in sparse MoEs (Fedus et al., 2022); however, we leave this investigation for future work. Our main finding is that adapter parameters are suitable both to inform task clustering, and thus guide library building, and to route new inputs, thus facilitating library reuse. 6. Related Work Multi-task learninginvolves training on a joint set of all tasks (Caruana, 1997), potentially leading to performance degradation due to task interference (Zhao et al., 2018). An extensive literature studies how to partition learnable param- eters into shared and task-specific ones (Ding et al., 2023; Strezoski et al., 2019; Bragman et al., 2019; Zaremoodi et al., 2018; Wallingford et al., 2022; Fifty et al., 2021). We operate in the parameter-efficient multi-task learning setting (Ponti et al., 2023; Vu et al., 2021; Chronopoulou et al., 2023a; Pfeiffer et al., 2021). Vu et al. (2021) train one prefix adapter (Li & Eisner, 2019) per task and learn to re-use them for other tasks based on the adapter similarities. MBC can be seen as an extension of this approach where we cluster tasks based on their weight similarity to ensure more transfer during multi-task pre-training. Mixture of experts(MoEs), when coupled with sparse rout- ing, are notable for augmenting model capacity with mini- mal computational overhead (Fedus et al., 2022). Among the most important differences in this work: i) adapter ex- perts are not trained during base model pre-training, ii) they are parameter-efficient and iii) they are tailored to specific tasks instead of being opaque computation units at the token level whose specialization is not easily interpretable (Jiang et al., 2024). Regarding ii), Wang et al. (2022a); Zadouri et al. (2023); Muqeeth et al. (2023) employs routing each ex- ample to a set of experts, showcasing enhanced performance on unseen tasks. Gupta et al. (2022) trains a separate router for each task and picks a router from a similar task based on domain knowledge. Ye et al. (2022) proposes task-level MoEs that treat a collection of transformer layers as experts and a router chooses from these experts dynamically. Recent work by Caccia et al. (2023); Ponti et al. (2023); Ostapenko et al. (2023) investigate the effectiveness of densely routed adapter experts trained end-to-end with an expert library for MTL fine-tuning. For expert aggregation, we employ parameter-space weighted averaging (Wortsman et al., 2022; Zhang et al., 2023a; Ram´e et al., 2023) with weights induced by a learned router, a technique akin to those in previous works (Ostapenko et al., 2023; Zadouri et al., 2023). Sev- eral recent works have also proposed techniques for learning how to route queries to specialized pretrained open-source LLMs (Lu et al., 2023; Shnitzer et al., 2023). Model ensemblingtechniques aim to enhance model ro- bustness and generalization by integrating multiple dis- tinct models (Frankle et al., 2020; Wortsman et al., 2022; Ram´e et al., 2023; Jin et al., 2022; Matena & Raffel, 2022; Chronopoulou et al., 2023b; Yang et al., 2023). Parameter space averaging of independent models serves as an efficient 8Towards Modular LMs by Building and Reusing a Library of LoRAs ensembling method for full models (Ilharco et al., 2022; Ainsworth et al., 2022; Jin et al., 2022) and adapters (Zhang et al., 2023a; Yadav et al., 2024), requiring only a single forward pass through the model, unlike output space ensem- bling (Dietterich, 2000; Breiman, 1996), that requires many forward passes. Efficient output ensembling techniques that can be applied in conjunction with our work are in (Wen et al., 2020). Similarly, Pfeiffer et al. (2021) proposes en- sembling bottleneck style adapters with the subsequent fine- tuning step. Tam et al. (2023) presents a merging framework called MaTs using the conjugate gradient method. Yadav et al. (2024) proposes Ties-Merging to mitigate interference due to redundant parameter values. Daheim et al. (2024) merge models by reducing their individual gradient mis- match with an ideal joint model, weighting their parameters with normalized Fisher Information. Data Clustering for LMs have been proposed to im- prove performance and decrease task interference (Fifty et al., 2021; Gururangan et al., 2023; Gou et al., 2023). These methods include clustering using similarities com- puted by tf-idf and neural embeddings, K-means clustering with balanced linear assignment, and soft clustering with GMMs (Gross et al., 2017; Chronopoulou et al., 2023a; 2021; Gururangan et al., 2023; Duan et al., 2021; Caron et al., 2018). Recent work by Zhou et al. (2022) observes the potential of adapter parameters as effective task embed- dings for clustering purposes, a concept we leverage in this work. A similar observation, but regarding task gradients, has been made by Vu et al. (2020b). Building libraries of composable expertshas been envi- sioned in several previous works (Pfeiffer et al., 2021; Wu et al., 2023; Huang et al., 2023; Shah et al., 2023; Xun Wu, 2024). Beck et al. (2021); Poth et al. (2023) orchestrated a framework for assembling diverse adapters, offering flex- ibility in both training and inference. Most related to this work, Huang et al. (2023) build LoRAHub, a library of task-specific LoRAs that can be combined for few-shot generalization. Pfeiffer et al. (2021) introduce a two-stage learning algorithm that leverages knowledge from multiple tasks. They first learn task-specific experts and then com- bine the experts in a separate knowledge composition step. Xun Wu (2024) introduces a learnable gating function to combine multiple LoRAs, called Mixture of LoRA Experts (MoLE). Wu et al. (2023) presents π-tuning for vision, lan- guage, and vision-language few-shot tasks. π-tuning trains task-specific experts and then uses task embedding based on the diagonal of the Fisher information matrix to retrieve the top-k most similar tasks to a target task. We extend and complement these works by i) proposing novel methods to build a library, and ii) proposing techniques for zero-shot post-hoc routing independently trained adapters. Related to ii), in a concurrent work, Muqeeth et al. (2024) learns a sigmoid gate for each expert, which is later used as expert prototype for zero-shot transfer. Notably, this method is applicable to the same setting as Arrow, and generalizes beyond linear adapters. However, in contrast to Arrow, ob- taining the expert prototypes requires additional training after the experts are learned. 7. Conclusions and Future Work We investigate how to build and reuse a library of adapters “end-to-end”. We show the potential of reusing indepen- dently (or partially independently) trained adapters with a zero-shot routing strategy. Overall, we strategically in- vestigate the modular augmentation of smaller (language) models, offering a promising direction for research that prioritizes efficiency, flexibility, and performance. The current investigation focuses on LoRA adapters. For future work, we are excited by the exploration of a het- erogeneous “universe” of adapters—including soft and hard prompts (Lester et al., 2021; Wen et al., 2023), MLPs (Houlsby et al., 2019), etc.—and combinations thereof. Whether our approach can result in encouraging re- sults at a greater scale (both in terms of data and model size) remains open to further investigation. Using the proposed routing strategy for modular continual learning (Ostapenko et al., 2021; Ermis et al., 2022; Wang et al., 2022c) is an- other promising direction for future work, especially given the fact that the Arrow router is local to each expert. In prin- ciple, it may be less susceptible to catastrophic forgetting as no gradient-based training is required to incorporate new experts into the library. 8. Broader Impact This work sheds light on different ways of extending the capabilities of language models by surrounding them with a universe of lightweight adapters that can be trained on conventional hardware. Allowing the reuse of adapters might enable systems that are trained in a collaborative and distributed fashion and that use less total energy, with positive ramifications for the environment, but still attain the performance of vanilla systems. Further, this might allow users with smaller computational resources to more easily use and customize LLMs. There are also many potential societal consequences of improving LLMs, some being less desirable and even undesirable, but none of which we feel must be specifically highlighted here. 9. Summary of Contributions • OO led the effort on library compression and adapta- tion baselines. conceptualized and implemented MBC clustering, designed and implemented various experi- ments, and contributed to paper writing and codebase. 9Towards Modular LMs by Building and Reusing a Library of LoRAs • ZS worked on 0-shot and supervised adaptation, de- signed the task predictor routing, implemented the π- tuning baseline, and contributed to the codebase and writing. • EP, LCh, NLR were involved in the project after its start, and contributed to the general vision and to proof- writing. • MP maintained and optimized the code, and prepared the code release. • LCa led the efforts on 0-shot routing; designed, con- ceptualized, and implemented Arrow routing; imple- mented the CM baseline and contributed to the code- base. • AS led the project and conceived its idea, worked on the codebase, data generation and evaluations, and wrote the paper. References Ainsworth, S. K., Hayase, J., and Srinivasa, S. Git re-basin: Merging models modulo permutation symmetries. arXiv preprint arXiv:2209.04836, 2022. Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Beck, T., Bohlender, B., Viehmann, C., Hane, V ., Adamson, Y ., Khuri, J., Brossmann, J., Pfeiffer, J., and Gurevych, I. Adapterhub playground: Simple and flexible few-shot learning with adapters. arXiv preprint arXiv:2108.08103, 2021. Beeching, E., Fourrier, C., Habib, N., Han, S., Lambert, N., Rajani, N., Sanseviero, O., Tunstall, L., and Wolf, T. Open llm leaderboard. https://huggingface.co/ spaces/HuggingFaceH4/open_llm_leaderboard, 2023. Belofsky, J. Token-level adaptation of lora adapters for downstream task generalization, 2023. Bisk, Y ., Zellers, R., Gao, J., Choi, Y ., et al. Piqa: Reasoning about physical commonsense in natural language. In Pro- ceedings of the AAAI conference on artificial intelligence, pp. 7432–7439, 2020. Bragman, F. J., Tanno, R., Ourselin, S., Alexander, D. C., and Cardoso, J. Stochastic filter groups for multi-task cnns: Learning specialist and generalist convolution ker- nels. In Proceedings of the IEEE/CVF International Con- ference on Computer Vision, pp. 1385–1394, 2019. Breiman, L. Bagging predictors. Machine learning, 24: 123–140, 1996. Caccia, L., Ponti, E., Su, Z., Pereira, M., Roux, N. L., and Sordoni, A. Multi-head adapter routing for cross-task generalization, 2023. Caron, M., Bojanowski, P., Joulin, A., and Douze, M. Deep clustering for unsupervised learning of visual features. In Proceedings of the European conference on computer vision (ECCV), pp. 132–149, 2018. Caruana, R. Multitask learning. Machine learning, 28: 41–75, 1997. Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y ., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Chen, Z., Shen, Y ., Ding, M., Chen, Z., Zhao, H., Learned- Miller, E., and Gan, C. Mod-squad: Designing mixture of experts as modular multi-task learners, 2022. Chronopoulou, A., Peters, M. E., and Dodge, J. Efficient hierarchical domain adaptation for pretrained language models. arXiv preprint arXiv:2112.08786, 2021. Chronopoulou, A., Peters, M. E., Fraser, A., and Dodge, J. Adaptersoup: Weight averaging to improve general- ization of pretrained language models. arXiv preprint arXiv:2302.07027, 2023a. Chronopoulou, A., Pfeiffer, J., Maynez, J., Wang, X., Ruder, S., and Agrawal, P. Language and task arithmetic with parameter-efficient layers for zero-shot summarization. arXiv preprint arXiv:2311.09344, 2023b. Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova, K. Boolq: Exploring the surprising difficulty of natural yes/no questions. In NAACL, 2019. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Daheim, N., M ¨ollenhoff, T., Ponti, E., Gurevych, I., and Khan, M. E. Model merging by uncertainty-based gra- dient matching. In The Twelfth International Confer- ence on Learning Representations, 2024. URL https: //openreview.net/forum?id=D7KJmfEDQP. Dietterich, T. G. Ensemble methods in machine learning. In International workshop on multiple classifier systems, pp. 1–15. Springer, 2000. Ding, C., Lu, Z., Wang, S., Cheng, R., and Boddeti, V . N. Mitigating task interference in multi-task learning via explicit task routing with non-learnable primitives. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7756–7765, 2023. 10Towards Modular LMs by Building and Reusing a Library of LoRAs Duan, Z., Zhang, H., Wang, C., Wang, Z., Chen, B., and Zhou, M. Enslm: Ensemble language model for data di- versity by semantic clustering. In Proceedings of the 59th Annual Meeting of the Association for Computational Lin- guistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 2954–2967, 2021. EleutherAI. Multiple-choice normaliza- tion. https://blog.eleuther.ai/ multiple-choice-normalization/, 2021. Ac- cessed: 2024-05-12. Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Bai, Y ., Chen, A., Conerly, T., et al. A mathematical framework for transformer circuits. Transformer Circuits Thread, 1, 2021. Ermis, B., Zappella, G., Wistuba, M., Rawal, A., and Ar- chambeau, C. Memory efficient continual learning with transformers. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Pro- cessing Systems , 2022. URL https://openreview. net/forum?id=U07d1Y-x2E. Fedus, W., Zoph, B., and Shazeer, N. Switch transformers: Scaling to trillion parameter models with simple and ef- ficient sparsity. Journal of Machine Learning Research, 23(120):1–39, 2022. URL http://jmlr.org/papers/ v23/21-0998.html. Fifty, C., Amid, E., Zhao, Z., Yu, T., Anil, R., and Finn, C. Efficiently identifying task groupings for multi-task learning. Advances in Neural Information Processing Systems, 34:27503–27516, 2021. Frankle, J., Dziugaite, G. K., Roy, D., and Carbin, M. Linear mode connectivity and the lottery ticket hypothesis. In International Conference on Machine Learning, pp. 3259– 3269. PMLR, 2020. Gou, Y ., Liu, Z., Chen, K., Hong, L., Xu, H., Li, A., Yeung, D.-Y ., Kwok, J. T., and Zhang, Y . Mixture of cluster- conditional lora experts for vision-language instruction tuning. arXiv preprint arXiv:2312.12379, 2023. Gross, S., Ranzato, M., and Szlam, A. Hard mixtures of experts for large scale weakly supervised vision. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6865–6873, 2017. Gupta, S., Mukherjee, S., Subudhi, K., Gonzalez, E., Jose, D., Awadallah, A. H., and Gao, J. Sparsely activated mixture-of-experts are robust multi-task learners. arXiv preprint arXiv:2204.07689, 2022. Gururangan, S., Li, M., Lewis, M., Shi, W., Althoff, T., Smith, N. A., and Zettlemoyer, L. Scaling expert lan- guage models with unsupervised domain discovery.arXiv preprint arXiv:2303.14177, 2023. Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. Parameter-efficient transfer learning for NLP. In International Conference on Machine Learning, pp. 2790– 2799, 2019. URL http://proceedings.mlr.press/ v97/houlsby19a/houlsby19a.pdf. Hu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang, S., Wang, L., and Chen, W. LoRA: Low-rank adapta- tion of large language models. In International Confer- ence on Learning Representations, 2022. URL https: //openreview.net/forum?id=nZeVKeeFYf9. Huang, C., Liu, Q., Lin, B. Y ., Pang, T., Du, C., and Lin, M. Lorahub: Efficient cross-task generalization via dynamic lora composition. arXiv preprint arXiv:2307.13269 , 2023. Huang, C., Liu, Q., Lin, B. Y ., Pang, T., Du, C., and Lin, M. Lorahub: Efficient cross-task generalization via dynamic lora composition, 2024. Ilharco, G., Ribeiro, M. T., Wortsman, M., Gururangan, S., Schmidt, L., Hajishirzi, H., and Farhadi, A. Editing mod- els with task arithmetic.arXiv preprint arXiv:2212.04089, 2022. Jang, J., Kim, S., Ye, S., Kim, D., Logeswaran, L., Lee, M., Lee, K., and Seo, M. Exploring the benefits of training expert language models over instruction tuning, 2023. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., de las Casas, D., Hanna, E. B., Bressand, F., Lengyel, G., Bour, G., Lample, G., Lavaud, L. R., Saulnier, L., Lachaux, M.-A., Stock, P., Subramanian, S., Yang, S., Antoniak, S., Scao, T. L., Gervet, T., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mixtral of experts, 2024. Jin, X., Ren, X., Preotiuc-Pietro, D., and Cheng, P. Data- less knowledge fusion by merging weights of language models. arXiv preprint arXiv:2212.09849, 2022. Jolicoeur-Martineau, A., Gervais, E., Fatras, K., Zhang, Y ., and Lacoste-Julien, S. Population parameter averaging (papa), 2023. 11Towards Modular LMs by Building and Reusing a Library of LoRAs Karimi Mahabadi, R., Ruder, S., Dehghani, M., and Hen- derson, J. Parameter-efficient multi-task fine-tuning for Transformers via shared hypernetworks. In Proceed- ings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pp. 565–576, August 2021. URL https://aclanthology. org/2021.acl-long.47. Lester, B., Al-Rfou, R., and Constant, N. The power of scale for parameter-efficient prompt tuning, 2021. URL https://arxiv.org/pdf/2104.08691.pdf. Li, M., Gururangan, S., Dettmers, T., Lewis, M., Althoff, T., Smith, N. A., and Zettlemoyer, L. Branch-train-merge: Embarrassingly parallel training of expert language mod- els. arXiv preprint arXiv:2208.03306, 2022. Li, X. L. and Eisner, J. Specializing word embeddings (for parsing) by information bottleneck. In Proceedings of the 2019 Conference on Empirical Methods in Natu- ral Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP- IJCNLP), pp. 2744–2754, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1276. URL https://www.aclweb. org/anthology/D19-1276. Li, X. L. and Liang, P. Prefix-tuning: Optimizing con- tinuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computa- tional Linguistics and the 11th International Joint Confer- ence on Natural Language Processing (Volume 1: Long Papers), pp. 4582–4597, Online, August 2021. Asso- ciation for Computational Linguistics. doi: 10.18653/ v1/2021.acl-long.353. URL https://aclanthology. org/2021.acl-long.353. Lin, C.-Y . and Hovy, E. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proceedings of the 2003 human language technology conference of the North American chapter of the association for computa- tional linguistics, pp. 150–157, 2003. Liu, H., Tam, D., Muqeeth, M., Mohta, J., Huang, T., Bansal, M., and Raffel, C. Few-shot parameter-efficient fine- tuning is better and cheaper than in-context learning, 2022. URL https://arxiv.org/abs/2205.05638. Longpre, S., Hou, L., Vu, T., Webson, A., Chung, H. W., Tay, Y ., Zhou, D., Le, Q. V ., Zoph, B., Wei, J., et al. The flan collection: Designing data and methods for effec- tive instruction tuning. arXiv preprint arXiv:2301.13688, 2023. Lu, K., Yuan, H., Lin, R., Lin, J., Yuan, Z., Zhou, C., and Zhou, J. Routing to the expert: Efficient reward- guided ensemble of large language models.arXiv preprint arXiv:2311.08692, 2023. Mangrulkar, S., Gugger, S., Debut, L., Belkada, Y ., Paul, S., and Bossan, B. Peft: State-of-the-art parameter- efficient fine-tuning methods. https://github.com/ huggingface/peft, 2022. Matena, M. S. and Raffel, C. A. Merging models with fisher- weighted averaging. Advances in Neural Information Processing Systems, 35:17703–17716, 2022. Microsoft Research. Phi-2: The Surprising Power of Small Language Models, 2023. Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can a suit of armor conduct electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018. Mireshghallah, F., Taram, M., Vepakomma, P., Singh, A., Raskar, R., and Esmaeilzadeh, H. Privacy in deep learn- ing: A survey. arXiv preprint arXiv:2004.12254, 2020. Muqeeth, M., Liu, H., and Raffel, C. Soft merging of experts with adaptive routing. arXiv preprint arXiv:2306.03745, 2023. Muqeeth, M., Liu, H., Liu, Y ., and Raffel, C. Learning to route among specialized experts for zero-shot generaliza- tion. arXiv preprint arXiv: 2402.05859, 2024. Nakatsukasa, Y . The low-rank eigenvalue problem.arXiv preprint arXiv:1905.11490, 2019. Ostapenko, O., Rodriguez, P., Caccia, M., and Charlin, L. Continual learning via local mod- ule composition. Advances in Neural Informa- tion Processing Systems , 34, 2021. URL https: //proceedings.neurips.cc/paper/2021/file/ fe5e7cb609bdbe6d62449d61849c38b0-Paper.pdf. Ostapenko, O., Caccia, L., Su, Z., Le Roux, N., Charlin, L., and Sordoni, A. A case study of instruction tuning with mixture of parameter-efficient experts. In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023. Pfeiffer, J., Kamath, A., R¨uckl´e, A., Cho, K., and Gurevych, I. AdapterFusion: Non-destructive task composition for transfer learning. In Proceedings of the 16th Conference of the European Chapter of the Association for Com- putational Linguistics, pp. 487–503, April 2021. URL https://aclanthology.org/2021.eacl-main.39. Pfeiffer, J., Ruder, S., Vuli´c, I., and Ponti, E. M. Modular deep learning. arXiv preprint arXiv:2302.11529, 2023. URL https://arxiv.org/pdf/2302.11529.pdf. 12Towards Modular LMs by Building and Reusing a Library of LoRAs Ponti, E. M., Sordoni, A., Bengio, Y ., and Reddy, S. Com- bining parameter-efficient modules for task-level gen- eralisation. In Proceedings of the 17th Conference of the European Chapter of the Association for Computa- tional Linguistics, pp. 687–702, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.eacl-main.49. Poth, C., Sterz, H., Paul, I., Purkayastha, S., Engl ¨ander, L., Imhof, T., Vuli ´c, I., Ruder, S., Gurevych, I., and Pfeiffer, J. Adapters: A unified library for parameter- efficient and modular transfer learning. arXiv preprint arXiv:2311.11077, 2023. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21: 1–67, 2020. URL https://www.jmlr.org/papers/ volume21/20-074/20-074.pdf. Ram´e, A., Ahuja, K., Zhang, J., Cord, M., Bottou, L., and Lopez-Paz, D. Model ratatouille: Recycling diverse models for out-of-distribution generalization. In Inter- national Conference on Machine Learning, pp. 28656– 28679. PMLR, 2023. Rapin, J. and Teytaud, O. Nevergrad - A gradient- free optimization platform. https://GitHub.com/ FacebookResearch/Nevergrad, 2018. Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y . Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM , 64(9):99–106, 2021. Shah, V ., Ruiz, N., Cole, F., Lu, E., Lazebnik, S., Li, Y ., and Jampani, V . Ziplora: Any subject in any style by effec- tively merging loras. arXiv preprint arXiv:2311.13600, 2023. Shnitzer, T., Ou, A., Silva, M., Soule, K., Sun, Y ., Solomon, J., Thompson, N., and Yurochkin, M. Large language model routing with benchmark datasets. arXiv preprint arXiv:2309.15789, 2023. Strezoski, G., Noord, N. v., and Worring, M. Many task learning with task routing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1375–1384, 2019. Suzgun, M., Scales, N., Sch ¨arli, N., Gehrmann, S., Tay, Y ., Chung, H. W., Chowdhery, A., Le, Q. V ., Chi, E. H., Zhou, D., et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. Tam, D., Bansal, M., and Raffel, C. Merging by matching models in task subspaces. arXiv preprint arXiv:2312.04339, 2023. Vu, T., Wang, T., Munkhdalai, T., Sordoni, A., Trischler, A., Mattarella-Micke, A., Maji, S., and Iyyer, M. Ex- ploring and predicting transferability across NLP tasks. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pp. 7882–7926, Online, November 2020a. Association for Computational Linguistics. doi: 10.18653/v1/2020. emnlp-main.635. URL https://aclanthology.org/ 2020.emnlp-main.635. Vu, T., Wang, T., Munkhdalai, T., Sordoni, A., Trischler, A., Mattarella-Micke, A., Maji, S., and Iyyer, M. Exploring and predicting transferability across nlp tasks. arXiv preprint arXiv:2005.00770, 2020b. Vu, T., Lester, B., Constant, N., Al-Rfou, R., and Cer, D. Spot: Better frozen model adaptation through soft prompt transfer. arXiv preprint arXiv:2110.07904, 2021. Wallingford, M., Li, H., Achille, A., Ravichandran, A., Fowlkes, C., Bhotika, R., and Soatto, S. Task adaptive parameter sharing for multi-task learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7561–7570, 2022. Wang, Y ., Agarwal, S., Mukherjee, S., Liu, X., Gao, J., Awadallah, A. H., and Gao, J. Adamix: Mixture-of- adaptations for parameter-efficient model tuning. arXiv preprint arXiv:2205.12410, 2022a. Wang, Y ., Mishra, S., Alipoormolabashi, P., Kordi, Y ., Mirzaei, A., Arunkumar, A., Ashok, A., Dhanasekaran, A. S., Naik, A., Stap, D., et al. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. arXiv preprint arXiv:2204.07705, 2022b. Wang, Y ., Ivison, H., Dasigi, P., Hessel, J., Khot, T., Chandu, K. R., Wadden, D., MacMillan, K., Smith, N. A., Beltagy, I., et al. How far can camels go? exploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751, 2023. Wang, Z., Tsvetkov, Y ., Firat, O., and Cao, Y . Gradient vac- cine: Investigating and improving multi-task optimization in massively multilingual models. In International Con- ference on Learning Representations, 2021. URL https: //openreview.net/forum?id=F1vEjWK-lH_. Wang, Z., Zhang, Z., Lee, C.-Y ., Zhang, H., Sun, R., Ren, X., Su, G., Perot, V ., Dy, J., and Pfister, T. Learning to prompt for continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 139–149, 2022c. 13Towards Modular LMs by Building and Reusing a Library of LoRAs Wen, Y ., Tran, D., and Ba, J. Batchensemble: An alterna- tive approach to efficient ensemble and lifelong learning, 2020. Wen, Y ., Jain, N., Kirchenbauer, J., Goldblum, M., Geiping, J., and Goldstein, T. Hard prompts made easy: Gradient- based discrete optimization for prompt tuning and discov- ery. arXiv preprint arXiv:2302.03668, 2023. Wortsman, M., Ilharco, G., Gadre, S. Y ., Roelofs, R., Gontijo-Lopes, R., Morcos, A. S., Namkoong, H., Farhadi, A., Carmon, Y ., Kornblith, S., et al. Model soups: averaging weights of multiple fine-tuned mod- els improves accuracy without increasing inference time. In International Conference on Machine Learning , pp. 23965–23998. PMLR, 2022. Wu, C., Wang, T., Ge, Y ., Lu, Z., Zhou, R., Shan, Y ., and Luo, P. π-tuning: Transferring multimodal foundation models with optimal multi-task interpolation. In Inter- national Conference on Machine Learning, pp. 37713– 37727. PMLR, 2023. Xun Wu, Shaohan Huang, F. W. Mole: Mixture of lora ex- perts. In International Conference on Learning Represen- tations, ICLR 2024, 2024. URL https://openreview. net/forum?id=uWvKBCYh4S. Yadav, P., Tam, D., Choshen, L., Raffel, C. A., and Bansal, M. Ties-merging: Resolving interference when merging models. Advances in Neural Information Processing Systems, 36, 2024. Yang, E., Wang, Z., Shen, L., Liu, S., Guo, G., Wang, X., and Tao, D. Adamerging: Adaptive model merging for multi-task learning. arXiv preprint arXiv:2310.02575, 2023. Ye, Q., Zha, J., and Ren, X. Eliciting and understanding cross-task skills with task-level mixture-of-experts. arXiv preprint arXiv:2205.12701, 2022. Zadouri, T., ¨Ust¨un, A., Ahmadian, A., Ermis ¸, B., Locatelli, A., and Hooker, S. Pushing mixture of experts to the limit: Extremely parameter efficient moe for instruction tuning. arXiv preprint arXiv:2309.05444, 2023. Zaremoodi, P., Buntine, W., and Haffari, G. Adaptive knowledge sharing in multi-task learning: Improving low- resource neural machine translation. In Proceedings of the 56th Annual Meeting of the Association for Computa- tional Linguistics (Volume 2: Short Papers), pp. 656–661, 2018. Zellers, R., Holtzman, A., Bisk, Y ., Farhadi, A., and Choi, Y . Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. Zhang, J., Chen, S., Liu, J., and He, J. Composing parameter- efficient modules with arithmetic operations. arXiv preprint arXiv:2306.14870, 2023a. Zhang, R., Han, J., Zhou, A., Hu, X., Yan, S., Lu, P., Li, H., Gao, P., and Qiao, Y . Llama-adapter: Efficient fine- tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023b. Zhao, X., Li, H., Shen, X., Liang, X., and Wu, Y . A modu- lation module for multi-task learning with applications in image retrieval. In Proceedings of the European Confer- ence on Computer Vision (ECCV), pp. 401–416, 2018. Zhou, W., Xu, C., and McAuley, J. Efficiently tuned parameters are task embeddings. arXiv preprint arXiv:2210.11705, 2022. 14Towards Modular LMs by Building and Reusing a Library of LoRAs Library L piqa boolq wgrande hswag arcE arcC HE oqa bbh mbpp Acc . StableLM (3B) Base - - 78.2 73.1 66.6 73.7 59.6 41.5 18.3 37.6 34.7 32.3 51.6 Shared - 1 79.4 80.3 68.0 71.3 74.7 42.1 11.6 38.0 38.3 21.0 52.5 Private µ 100 79.4 76.8 67.3 74.4 72.4 44.0 16.5 42.6 37.0 34.6 54.5 Private ↗↗ 100 80.1 72.1 70.8 74.8 73.4 45.3 16.5 43.6 36.1 33.5 54.6 MBC µ 10 80.4 80.4 68.2 74.7 76.7 47.4 14.6 43.0 35.4 36.2 55.7 MBC ↗↗ 10 80.5 79.0 68.2 73.6 75.2 46.4 13.4 43.0 32.0 27.6 53.9 Table 4.Out-of-distribution zero-shot results: Accuracy on held-out tasks for StableLM. The best results are underlined. 10. Appendix 10.1. Analyzing∥ABT v∥2 for in-distribution and out-of-distribution samples In this section, we analyze whether the motivation behind Arrow routing holds in practice. Recall that at each layer, Arrow routing initializes prototypes in the linear router for expert i with the unit vector vi maximizing ∥ABT v∥2. Concretely, we hypothesize that for a hidden activation h computed from x ∈ Di, we have ∥AiBT i v∥2 > ∥AjBT j v∥2, for experts i, j. In other words, the norm of the linearly transformed prototype will be higher under the expert belonging to the same task as the input h. To test this hypothesis, we run the following experiment. Let hl denote the input to the expert at layer l, and (ABT )i l denote the linear transformation of expert i at layer l. We first sample 5000 examples from the multitask dataset. Then, for a given input x ∈ Di at each layer l, we compute both ∥(ABT )i l · hl∥2 and ∥(ABT )j l · hl∥2 where j is another randomly sampled expert such that i ̸= j. We then compute the average norm ratio r across all layers, i.e. r = LX l 1 L ∥(ABT )i l · hi l∥2 ∥(ABT )j l · hi l∥2 . Note that the random expert j is sampled at every layer, and the output of the in-distribution expert is propagated to the next layer. As such, r > 1 indicates that on average, the in-distribution expert produces a higher norm output, which would validate the use of the norm-maximizing initialization that Arrow routing uses. In figure 5, we see that for all the points considered, this ratio is positive, indicating that in-distribution experts tend to be maximize the norm of the linearly transformed input. 1.000 1.002 1.004 1.006 1.008 1.010 1.012 1.014 1.016 average norm ratio r 0 50 100 150 200 250 300probability density Norm Ratio r of in-distribution over out-of-distribution experts Figure 5.Histogram of the ratios r computed over 5000 samples. 10.2. Few-shot adaptation We apply some of the proposed methods to a data scarce setting with up to only 0.5% of the original training data per task (approx. 40 examples per task). We show the results in Table 6. Even in this setting gradient based method MBC-Poly considerably outperforms LoraHub, where the LoraHub is given compute equivalent to training gradient based methods 15Towards Modular LMs by Building and Reusing a Library of LoRAs Method L SNI Tasks Rouge-L 202 304 614 613 362 242 1728 1557 035 1356 039 1153 Phi-2 (2.8B) Base - 4.0 3.3 26.4 3.5 16.2 32.5 35.2 62.5 54.2 12.8 8.2 7.6 22.2 Shared 1 38.3 17.9 36.4 11.5 77.2 39.4 45.8 84.5 40.7 21.5 34.3 24.1 39.3 Private-µ 256 10.6 16.1 35.6 9.6 64.8 58.2 42.6 72.2 61.7 17.5 25.1 24.0 36.6 Private-↗↗ 256 20.4 18.8 31.8 10.5 76.3 36.4 46.8 84.2 41.8 19.2 33.4 28.7 37.4 MBC-µ 10 31.8 26.9 33.9 12.7 77.6 77.9 47.2 86.0 49.2 22.4 37.0 29.8 44.4 MBC-↗↗ 10 32.6 15.9 31.3 7.6 79.6 36.6 41.7 80.2 33.1 21.5 32.0 28.5 36.7 Mistral (7B) Base - 13.7 10.7 31.8 5.6 37.4 22.0 35.6 49.1 58.6 13.7 22.2 14.2 26.4 Shared 1 50.8 18.0 37.5 8.8 67.4 80.0 54.1 81.9 59.6 30.0 32.0 27.0 45.6 Private-µ 256 30.1 17.2 10.2 7.7 70.4 37.7 38.6 63.0 63.0 20.7 25.4 23.2 36.4 Private-↗↗ 256 38.5 23.7 43.7 12.7 78.0 76.7 54.6 83.3 57.9 25.2 35.4 33.4 46.9 MBC-µ 10 54.0 26.1 46.4 15.0 80.8 80.1 46.0 82.7 66.5 28.1 46.1 36.0 50.6 MBC-↗↗ 10 38.1 24.3 35.9 12.8 85.5 77.1 43.3 82.1 57.7 29.0 33.9 31.2 45.9 Table 5.Out-of-distribution zero-shot results on 12 held-out SNI tasksfor library built for the Phi-2 and Mistral base models. Applying ↗↗ routing to the Private libraries results in performance improvements over the µ routing for both models, with a notable improvement of over 10 Rouge-L points in case of Mistral. It is worth noticing that µ routing performed better than ↗↗ in case of MBC library for both models. We note that ↗↗ only selects top-4 experts for routing, whereas µ averages full libraries. Best results are underlined. on full dataset. Additionally, we observe that MBC-PolyZ, a method similar to MBC-Poly that only updates the routings and not the expert’s weights, performs similarly to LoraHub. Interestingly, when data amount is lowered, the perfromance of MBC-PolyZ is reduced by a relatively smaller margin than MBC-Poly which can be explained by a smaller amount of updated parameters. Method L SNI Tasks Rouge-L 202 304 614 613 362 242 1728 1557 035 1356 039 1153 Full data MBC-LoraHub 10 41.5 21.9 37.4 17.5 78.1 68.3 48.0 82.0 62.6 21.2 33.5 31.1 45.3 MBC-Poly 10 96.9 84.4 67.2 53.9 96.4 97.8 60.2 87.9 91.3 29.4 81.7 99.7 78.9 MBC-PolyZ 10 37.7 27.9 36.2 12.6 75.9 74.4 48.7 81.3 58.9 22.5 36.1 31.1 45.3 10% MBC-Poly 10 89.6 53.3 64.5 44.5 93.5 98.5 58.5 75.7 87.3 27.2 65.6 66.8 68.8 MBC-PolyZ 10 34.3 27.5 36.2 12.4 76.5 74.3 47.5 86.2 57.9 22.7 35.3 31.4 45.2 5% MBC-Poly 10 87.0 43.0 61.3 41.7 92.0 95.2 55.3 77.3 89.0 25.4 59.1 47.8 64.5 MBC-PolyZ 10 32.6 28.6 36.0 13.0 76.4 73.9 47.3 86.3 57.7 22.7 36.6 31.0 45.2 0.5% MBC-Poly 10 49.7 30.4 43.7 20.3 77.3 78.4 48.2 86.5 72.2 23.2 43.0 29.1 50.2 MBC-PolyZ 10 32.0 27.4 34.3 12.6 77.6 78.1 47.2 86.5 53.2 22.2 37.0 29.1 44.8 Table 6.Rouge-L score for Phi-2 model after adaptation with different portions of data per task ranging from full dataset down to 10% and 5% of data per task. Note, MBC-PolyZ only tunes the routing weights, whereas MBC-Poly trains both the routing weights and the experts parameters. 11. Implementation details and hyperparameters In this section we provide some technical details about the experiments conducted in this paper. Training hyperparameters. For all LoRA experts trained in this paper we employ LoRA rank of 4, LoRA dropout probability of 0.05, LoRA α of 16, and a learning rate of 1e-4 with a learning rate warm-up and annealing phases. We experimented with only patching fully connected layers (FC), only attention layers + attention output projection (ATT+O) or both (BOTH). For the preliminary experiments in Figure 2 we modify only the MLP (FC) layers of the transformer (.*fc[12].*). We found that patching FC layers severely underperform ATT+O layers. Patching BOTH gives marginal gains over ATT+O while significantly increasing computation cost and memory usage due to the wide projection (4 * hidden 16Towards Modular LMs by Building and Reusing a Library of LoRAs Method L SNI Tasks Rouge-L 202 304 614 613 362 242 1728 1557 035 1356 039 1153 Phi-2 (2.8B) No Library - 93.2 74.2 64.9 51.4 95.9 96.2 59.3 81.4 90.5 26.9 73 99.1 75.5 Shared 1 93.1 73.4 65.0 48.9 96.0 95.9 58.4 86.8 91.2 29.0 73.4 98.4 75.8 MHR 1 94.5 66.9 63.0 47.8 94.9 95.6 59.5 86.6 91.0 27.9 70.7 98.2 74.8 Poly 10 92.1 66.4 63.0 45.9 94.9 96.4 56.0 85.3 90.2 27.6 70.0 93.2 73.4 Private-µ 256 93.6 78.1 65.0 50.7 94.8 97.8 59.7 87.9 90.7 28.1 76.4 99.7 76.9 MBC-µ 10 96.4 83.2 67.6 53.5 96.2 98.0 60.5 88.2 90.7 29.8 82.3 99.5 78.8 MBC-LoraHub 10 41.5 21.9 37.4 17.5 78.1 68.3 48.0 82.0 62.6 21.2 33.5 31.1 45.3 RandTask-Poly 10 96.4 77.1 66.5 48.6 96.7 98.9 59.9 85.1 90.7 28.6 73.9 97.5 76.7 MBC-Poly 10 96.9 84.4 67.2 53.9 96.4 97.8 60.2 87.9 91.3 29.4 81.7 99.7 78.9 Mistral (7B) No Library - 97.6 88.3 68.9 59.9 98.8 98.8 62.9 87.3 91.8 37.5 80.5 100 81.0 Shared 1 95.8 87.4 69.9 52.7 98.7 99.2 63.5 87.6 91.6 37.2 78.1 100 80.1 Private-µ 256 98.5 87.2 70.6 54.1 98.3 99.1 64.0 89.1 92.0 37.3 81.0 100 80.9 MBC-µ 10 98.1 84.8 70.1 54.2 98.7 95.7 62.8 82.9 92.0 38.2 82.1 99.5 79.9 MBC-LoRAHub 10 47.8 23.1 45.9 14.0 81.4 79.6 49.7 84.9 69.6 28.6 42.2 34.5 50.1 RandTask-Poly 10 98.4 86.9 69.3 53.8 96.1 93.0 64.7 84.5 92.1 39.3 80.1 99.5 79.8 MBC-Poly 10 98.7 88.7 69.2 56.1 97.4 99.5 64.1 82.2 92.2 38.7 81.1 99.0 80.6 Table 7.Supervised adaptation results (100% training data per task): Rouge-L on 12 held-out SNI for Phi-2 and Mistral 7B models for different libraries. LoraHub follows the original implementation and optimizes the weighting coefficients for the adapters in the library with a non-gradient based optimizer. Best results are underlined. Method L SNI Tasks (10%) Rouge-L 202 304 614 613 362 242 1728 1557 035 1356 039 1153 Phi-2 (2.8B) No Library - 71.5 36.1 53.6 36.9 80.0 85.5 46.7 62.3 84.0 21.7 41.3 27.2 53.9 Shared 1 76.8 35.5 55.5 39.4 82.6 89.3 47.6 61.8 86.0 23.3 47.9 31.2 56.4 MHR 1 83.6 45.1 58.2 40.0 91.3 94.3 54.0 84.1 85.7 25.7 58.0 54.2 64.5 Poly 1 74.4 38.3 57.8 39.5 82.5 92.2 50.8 85.1 85.1 25.5 54.8 54.1 61.7 Private-µ 256 81.7 41.2 60.6 40.4 89.8 96.0 49.6 75.1 87.1 23.8 57.2 47.9 62.5 MBC-µ 10 86.2 52.3 64.3 43.8 93.8 97.3 53.3 75.0 87.5 26.3 61.1 63.8 67.0 MBC-LoraHub 10 43.6 22.2 36.5 13.5 77.0 68.8 45.5 82.2 63.2 21.2 34.6 27.6 44.7 RandTask-Poly 10 87.9 51.0 63.5 41.4 94.1 95.8 55.6 79.6 89.0 27.1 61.1 65.3 67.6 MBC-Poly 10 88.9 52.0 64.4 45.6 94.3 96.9 56.7 75.2 87.5 27.1 64.4 66.0 68.2 Mistral (7B) No Library - 91.7 66.8 66.2 47.8 95.2 98.3 59.5 69.9 90.7 33.9 65.8 68.1 71.2 Shared 1 94.6 64.9 65.1 45.3 90.7 91.0 60.3 82.7 89.6 33.4 66.3 91.3 72.9 Private-µ 256 89.0 64.3 66.0 47.2 94.7 98.8 59.4 84.1 90.6 33.5 67.6 92.9 74.0 MBC-µ 10 94.2 62.6 66.3 47.9 95.9 96.6 59.9 83.6 90.7 33.7 69.3 92.8 74.5 MBC-LoRAHub 10 47.7 23.2 47.0 15.4 85.8 72.4 49.0 81.9 71.7 27.3 27.7 30.6 48.3 RandTask-Poly 10 95.0 64.8 66.0 48.8 94.3 92.1 59.6 87.1 90.6 34.4 66.4 92.4 74.3 MBC-Poly 10 95.1 66.3 66.0 48.3 96.4 98.4 60.2 84.9 90.5 33.7 67.8 92.7 75.0 Table 8.Supervised few-shot adaptation results (10% training data per task): Rouge-L on 12 held-out SNI for Phi-2 and Mistral 7B models for different libraries. LoraHub follows the original implementation and optimizes the weighting coefficients for the adapters in the library with a non-gradient based optimizer. Best results are underlined. 17Towards Modular LMs by Building and Reusing a Library of LoRAs size) of the first FC layer in the transformer residual block. Therefore, for the rest of the experiments, we modified attention layers + attention output projection (e.g. .*Wqkv.* |.*out proj.* for Phi-2). Downstream zero-shot results.All library-bases downstream zero-shot results are reported using top-4 routing with temperature 1. Unless stated otherwise, for all MBC libraries we use 10 experts. Additionally, in our implementation of the downstream evaluation we append an EOS token to the target options to mark the end of a sentence. We use token-length normalized scores for selecting continuations for multiple-choice tasks evaluation (EleutherAI, 2021). Adaptation experiments.For the adaptation experiments we also use the learning rate of 1e-4, with the same learning rate schedule as stated above. For both MHR and Poly adaptation, we tune both the experts and the routing weights. 18Towards Modular LMs by Building and Reusing a Library of LoRAs c0 ”ropes background new situation answer”, ”ropes prompt bottom no hint”, ”ropes plain background situation”, ”ropes new situation background answer”, ”ropes given background situation”, ”ropes prompt bottom hint beginning”, ”ropes prompt beginning”, ”ropes read background situation”, ”ropes plain bottom hint”, ”ropes plain no background”, ”ropes prompt mix”, ”ropes background situation middle” c1 ”glue sst2 2 0 0”, ”adversarial qa droberta generate question”, ”true case”, ”stream qed”, ”huggingface xsum”, ”cot esnli”, ”cot gsm8k”, ”trec 1 0 0”, ”yelp polarity reviews 0 2 0”, ”lambada 1 0 0”, ”glue cola 2 0 0”, ”ag news subset 1 0 0”, ”gem dart 1 1 0”, ”math dataset algebra linear 1d 1 0 0”, ”cnn dailymail 3 4 0”, ”wiki hop original explain relation”, ”dbpedia 14 given list what category does the paragraph belong to”, ”gem wiki lingua english en 1 1 0”, ”fix punct”, ”imdb reviews plain text 1 0 0”, ”race middle Write a multi choice question for the following article”, ”gigaword 1 2 0”, ”dbpedia 14 given a list of category what does the title belong to”, ”gem web nlg en 1 1 0”, ”word segment”, ”race high Write a multi choice question for the following article”, ”wmt16 translate de en 1 0 0”, ”cot ecqa”, ”aeslc 1 0 0”, ”dream generate first utterance”, ”wmt16 translate fi en 1 0 0”, ”dream answer to dialogue”, ”para crawl enes”, ”adversarial qa dbert generate question”, ”race middle Write a multi choice question options given ”, ”wmt14 translate fr en 1 0 0” c2 ”adversarial qa dbidaf question context answer”, ”super glue record 1 0 2”, ”wiki hop original generate object”, ”adversarial qa droberta tell what it is”, ”dbpe- dia 14 given a choice of categories ”, ”wiki hop original choose best object affirmative 3”, ”quac 1 0 0”, ”wiki hop original choose best object interrogative 1”, ”wiki hop original choose best object affirmative 1”, ”adversarial qa dbert answer the following q”, ”wiki hop original choose best object interrogative 2”, ”adversarial qa droberta question context answer”, ”squad v2 0 3 0 0”, ”wiki hop original generate subject”, ”wiki bio guess person”, ”adversarial qa dbidaf answer the following q”, ”adversarial qa droberta answer the following q”, ”adversarial qa dbert tell what it is”, ”race high Write a multi choice question options given ”, ”wiki hop original choose best object affirmative 2”, ”wiki hop original generate subject and object”, ”drop 2 0 0”, ”adversarial qa dbert question context answer”, ”adversarial qa dbidaf tell what it is” c3 ”wiqa what might be the first step of the process”, ”wiqa what is the final step of the following process”, ”wmt16 translate ro en 1 0 0”, ”wiqa what might be the last step of the process”, ”wiki bio key content”, ”gem common gen 1 1 0”, ”duorc SelfRC build story around qa”, ”app reviews generate review”, ”wiki bio what content”, ”wiki bio who”, ”gem e2e nlg 1 1 0”, ”cot esnli ii”, ”wmt16 translate tr en 1 0 0”, ”wiqa what is the missing first step”, ”wiki bio comprehension”, ”coqa 1 0 0”, ”duorc ParaphraseRC build story around qa”, ”multi news 1 0 0” c4 ”wiki qa found on google”, ”app reviews categorize rating using review”, ”race middle Is this the right answer”, ”super glue cb 1 0 2”, ”wiki qa Topic Prediction Answer Only”, ”wiki qa Direct Answer to Question”, ”super glue wsc fixed 1 0 2”, ”cot gsm8k ii”, ”unified qa science inst”, ”race high Is this the right answer”, ”cot strategyqa”, ”cot ecqa ii”, ”quarel do not use”, ”wiki qa exercise”, ”wiki qa automatic system”, ”cot creak ii”, ”quarel heres a story”, ”quarel choose between”, ”stream qed ii”, ”wiki qa Topic Prediction Question Only”, ”glue qnli 2 0 0”, ”cot sensemaking ii”, ”super glue copa 1 0 2”, ”social i qa Generate the question from the answer”, ”social i qa Show choices and generate index”, ”quarel testing students”, ”wiki qa Topic Prediction Question and Answer Pair”, ”wiki qa Decide good answer”, ”wiki qa Jeopardy style”, ”wiki qa Generate Question from Topic”, ”definite pronoun resolution 1 1 0”, ”wiqa effect with label answer”, ”glue wnli 2 0 0”, ”cot qasc”, ”cot strategyqa ii”, ”quarel logic test”, ”stream aqua ii” c5 ”quoref Context Contains Answer”, ”duorc SelfRC generate question by answer”, ”quoref Find Answer”, ”duorc ParaphraseRC movie director”, ”duorc ParaphraseRC answer question”, ”quoref Found Context Online”, ”quoref Read And Extract ”, ”duorc ParaphraseRC title generation”, ”duorc ParaphraseRC decide worth it”, ”quoref What Is The Answer”, ”duorc ParaphraseRC generate question”, ”quoref Guess Title For Context”, ”quoref Answer Test”, ”duorc SelfRC question answering”, ”duorc SelfRC title generation”, ”duorc ParaphraseRC generate question by answer”, ”duorc ParaphraseRC extract answer”, ”duorc SelfRC answer question”, ”duorc SelfRC decide worth it”, ”duorc ParaphraseRC question answering”, ”quoref Answer Question Given Context”, ”duorc SelfRC extract answer”, ”quoref Guess Answer”, ”quoref Answer Friend Question”, ”duorc SelfRC movie director”, ”duorc SelfRC generate question”, ”quoref Given Context Answer Question” c6 ”super glue rte 1 0 2”, ”cot sensemaking”, ”super glue wic 1 0 2”, ”cos e v1 11 rationale”, ”anli r3 0 1 0”, ”dream generate last utterance”, ”paws wiki 1 1 0”, ”cos e v1 11 generate explanation given text”, ”cot creak”, ”stream aqua”, ”snli 1 1 0”, ”cos e v1 11 i think”, ”glue qqp 2 0 0”, ”cos e v1 11 explain why human”, ”anli r2 0 1 0”, ”anli r1 0 1 0”, ”glue stsb 2 0 0”, ”cos e v1 11 aligned with common sense”, ”glue mnli 2 0 0”, ”so- cial i qa I was wondering”, ”cosmos qa 1 0 0”, ”glue mrpc 2 0 0”, ”social i qa Generate answer” c7 ”dream read the following conversation and answer the question”, ”app reviews convert to star rating”, ”cos e v1 11 question option description text”, ”social i qa Show choices and generate answer”, ”quartz answer question based on”, ”sciq Direct Question Closed Book ”, ”qasc qa with separated facts 3”, ”quartz given the fact answer the q”, ”quartz answer question below”, ”kilt tasks hotpotqa final exam”, ”sciq Multiple Choice”, ”wiqa does the supposed perturbation have an effect”, ”cos e v1 11 question description option text”, ”wiki qa Is This True ”, ”quartz use info from question paragraph”, ”sciq Direct Question”, ”qasc qa with separated facts 2”, ”wiqa which of the following is the supposed perturbation”, ”app reviews convert to rating”, ”cos e v1 11 question option description id”, ”wiqa effect with string answer”, ”qasc qa with separated facts 5”, ”dream baseline”, ”quartz having read above passage”, ”cos e v1 11 question description option id”, ”qasc qa with separated facts 1”, ”cos e v1 11 description question option text”, ”qasc qa with combined facts 1”, ”qasc is correct 1”, ”cos e v1 11 description question option id”, ”so- cial i qa Check if a random answer is valid or not”, ”sciq Multiple Choice Closed Book ”, ”quartz use info from paragraph question”, ”qasc is correct 2”, ”qasc qa with separated facts 4”, ”quartz read passage below choose”, ”quartz paragraph question plain concat”, ”sciq Multiple Choice Question First” c8 ”race middle Read the article and answer the question no option ”, ”race high Select the best answer”, ”quail description context question answer id”, ”quail context question description text”, ”race high Read the article and answer the question no option ”, ”race high Select the best answer no instructions ”, ”quail context description question answer id”, ”race high Taking a test”, ”super glue multirc 1 0 2”, ”race middle Select the best answer”, ”quail context question description answer id”, ”quail description context question answer text”, ”quail context question answer description text”, ”race high Select the best answer generate span ”, ”race middle Select the best answer generate span ”, ”quail context question answer description id”, ”quail context description question answer text”, ”quail context description question text”, ”quail context question description answer text”, ”quail description context question text”, ”race middle Taking a test”, ”quail no prompt id”, ”quail no prompt text”, ”race middle Select the best answer no instructions ” c9 ”natural questions open 1 0 0”, ”web questions whats the answer”, ”web questions question answer”, ”dbpedia 14 pick one category for the following text”, ”kilt tasks hotpotqa combining facts”, ”web questions short general knowledge q”, ”kilt tasks hotpotqa straighforward qa”, ”adversar- ial qa dbidaf generate question”, ”adversarial qa droberta based on”, ”web questions get the answer”, ”kilt tasks hotpotqa complex question”, ”web questions potential correct answer”, ”trivia qa rc 1 1 0”, ”kilt tasks hotpotqa formulate”, ”adversarial qa dbert based on”, ”adversarial qa dbidaf based on”, ”squad v1 1 3 0 0” Table 9.Task names for each of the 10 clusters obtained by applying MBC clustering to Phi-2 private library with 256 experts, with each expert trained for 2 epochs. 19Towards Modular LMs by Building and Reusing a Library of LoRAs c0 ”adversarial qa dbert generate question”, ”adversarial qa dbidaf generate question”, ”adversarial qa droberta generate question”, ”app reviews generate review”, ”cot creak”, ”cot esnli”, ”cot esnli ii”, ”dream generate first utterance”, ”dream generate last utterance”, ”duorc ParaphraseRC title generation”, ”duorc SelfRC title generation”, ”fix punct”, ”gem common gen 1 1 0”, ”gem dart 1 1 0”, ”gigaword 1 2 0”, ”huggingface xsum”, ”lam- bada 1 0 0”, ”race high Write a multi choice question for the following article”, ”race high Write a multi choice question options given ”, ”race middle Write a multi choice question for the following article”, ”race middle Write a multi choice question options given ”, ”stream aqua”, ”stream qed”, ”wiqa what is the missing first step”, ”wmt16 translate fi en 1 0 0”, ”wmt16 translate ro en 1 0 0”, ”yelp polarity reviews 0 2 0” c1 ”ag news subset 1 0 0”, ”app reviews convert to rating”, ”app reviews convert to star rating”, ”cot creak ii”, ”cot ecqa ii”, ”cot gsm8k ii”, ”cot sensemaking ii”, ”cot strategyqa”, ”dbpedia 14 given a choice of categories ”, ”dbpedia 14 given a list of category what does the title belong to”, ”dbpedia 14 given list what category does the paragraph belong to”, ”glue mnli 2 0 0”, ”glue qnli 2 0 0”, ”glue qqp 2 0 0”, ”glue stsb 2 0 0”, ”glue wnli 2 0 0”, ”kilt tasks hotpotqa complex question”, ”paws wiki 1 1 0”, ”qasc is correct 1”, ”qasc is correct 2”, ”snli 1 1 0”, ”so- cial i qa Check if a random answer is valid or not”, ”social i qa Generate answer”, ”social i qa Generate the question from the answer”, ”so- cial i qa I was wondering”, ”squad v1 1 3 0 0”, ”squad v2 0 3 0 0”, ”stream qed ii”, ”super glue multirc 1 0 2”, ”super glue rte 1 0 2”, ”super glue wic 1 0 2”, ”super glue wsc fixed 1 0 2”, ”trec 1 0 0”, ”wiki bio guess person”, ”wiki qa Is This True ” c2 ”app reviews categorize rating using review”, ”cos e v1 11 question option description text”, ”cot qasc”, ”cot strategyqa ii”, ”dbpe- dia 14 pick one category for the following text”, ”definite pronoun resolution 1 1 0”, ”kilt tasks hotpotqa final exam”, ”math dataset algebra linear 1d 1 0 0”, ”qasc qa with separated facts 4”, ”quarel do not use”, ”quoref Context Contains Answer”, ”race high Is this the right answer”, ”race middle Is this the right answer”, ”sciq Direct Question”, ”sciq Multiple Choice”, ”sciq Multiple Choice Closed Book ”, ”sciq Multiple Choice Question First”, ”social i qa Show choices and generate index”, ”stream aqua ii”, ”super glue cb 1 0 2”, ”super glue copa 1 0 2”, ”uni- fied qa science inst”, ”wiki qa Decide good answer”, ”wiki qa Direct Answer to Question”, ”wiki qa Generate Question from Topic”, ”wiki qa Jeopardy style”, ”wiki qa Topic Prediction Answer Only”, ”wiki qa Topic Prediction Question Only”, ”wiki qa Topic Prediction Question and Answer Pair”, ”wiki qa automatic system”, ”wiki qa exercise”, ”wiki qa found on google” c3 ”adversarial qa dbert answer the following q”, ”adversarial qa dbert based on”, ”adversarial qa dbert question context answer”, ”adversar- ial qa dbert tell what it is”, ”adversarial qa dbidaf answer the following q”, ”adversarial qa dbidaf based on”, ”adversarial qa dbidaf question context answer”, ”adversarial qa dbidaf tell what it is”, ”adversarial qa droberta answer the following q”, ”adversarial qa droberta based on”, ”adversar- ial qa droberta question context answer”, ”adversarial qa droberta tell what it is”, ”cos e v1 11 aligned with common sense”, ”cos e v1 11 explain why human”, ”cos e v1 11 generate explanation given text”, ”cos e v1 11 i think”, ”cos e v1 11 rationale”, ”drop 2 0 0”, ”duorc ParaphraseRC generate question by answer”, ”duorc SelfRC generate question by answer”, ”kilt tasks hotpotqa combining facts”, ”kilt tasks hotpotqa formulate”, ”kilt tasks hotpotqa straighforward qa”, ”natural questions open 1 0 0”, ”trivia qa rc 1 1 0”, ”web questions get the answer”, ”web questions potential correct answer”, ”web questions question answer”, ”web questions short general knowledge q”, ”web questions whats the answer” c4 ”duorc ParaphraseRC answer question”, ”duorc ParaphraseRC decide worth it”, ”duorc ParaphraseRC extract answer”, ”duorc ParaphraseRC generate question”, ”duorc ParaphraseRC movie director”, ”duorc ParaphraseRC question answering”, ”duorc SelfRC answer question”, ”duorc SelfRC decide worth it”, ”duorc SelfRC extract answer”, ”duorc SelfRC generate question”, ”duorc SelfRC movie director”, ”duorc SelfRC question answering”, ”quac 1 0 0”, ”quoref Answer Friend Question”, ”quoref Answer Test”, ”quoref Find Answer”, ”quoref Found Context Online”, ”quoref Given Context Answer Question”, ”quoref Guess Answer”, ”quoref Guess Title For Context”, ”quoref Read And Extract ”, ”quoref What Is The Answer” c5 ”cos e v1 11 description question option id”, ”cos e v1 11 question description option id”, ”dream baseline”, ”dream read the following conversation and answer the question”, ”quail context description question answer id”, ”quail context description question answer text”, ”quail context description question text”, ”quail context question answer description id”, ”quail context question answer description text”, ”quail context question description answer id”, ”quail context question description answer text”, ”quail context question description text”, ”quail description context question answer id”, ”quail description context question answer text”, ”quail description context question text”, ”quail no prompt id”, ”quail no prompt text”, ”race high Read the article and answer the question no option ”, ”race high Select the best answer”, ”race high Select the best answer generate span ”, ”race high Select the best answer no instructions ”, ”race high Taking a test”, ”race middle Read the article and answer the question no option ”, ”race middle Select the best answer”, ”race middle Select the best answer generate span ”, ”race middle Select the best answer no instructions ”, ”race middle Taking a test” c6 ”cos e v1 11 description question option text”, ”cos e v1 11 question description option text”, ”cos e v1 11 question option description id”, ”qasc qa with combined facts 1”, ”qasc qa with separated facts 1”, ”qasc qa with separated facts 2”, ”qasc qa with separated facts 3”, ”qasc qa with separated facts 5”, ”quarel choose between”, ”quarel heres a story”, ”quarel logic test”, ”quarel testing students”, ”quartz answer question based on”, ”quartz answer question below”, ”quartz given the fact answer the q”, ”quartz having read above passage”, ”quartz paragraph question plain concat”, ”quartz read passage below choose”, ”quartz use info from paragraph question”, ”quartz use info from question paragraph”, ”quoref Answer Question Given Context”, ”ropes background new situation answer”, ”ropes background situation middle”, ”ropes given background situation”, ”ropes new situation background answer”, ”ropes plain background situation”, ”ropes plain bottom hint”, ”ropes plain no background”, ”ropes prompt beginning”, ”ropes prompt bottom hint beginning”, ”ropes prompt bottom no hint”, ”ropes prompt mix”, ”ropes read background situation”, ”sciq Direct Question Closed Book ”, ”so- cial i qa Show choices and generate answer”, ”wiqa does the supposed perturbation have an effect”, ”wiqa effect with label answer”, ”wiqa effect with string answer”, ”wiqa which of the following is the supposed perturbation” c7 ”aeslc 1 0 0”, ”cnn dailymail 3 4 0”, ”coqa 1 0 0”, ”cot gsm8k”, ”dream answer to dialogue”, ”duorc ParaphraseRC build story around qa”, ”duorc SelfRC build story around qa”, ”gem e2e nlg 1 1 0”, ”gem web nlg en 1 1 0”, ”gem wiki lingua english en 1 1 0”, ”multi news 1 0 0”, ”wiki bio comprehension”, ”wiki bio key content”, ”wiki bio what content”, ”wiki bio who”, ”wiqa what is the final step of the following process”, ”wiqa what might be the first step of the process”, ”wiqa what might be the last step of the process”, ”wmt16 translate tr en 1 0 0” c8 ”anli r1 0 1 0”, ”anli r2 0 1 0”, ”anli r3 0 1 0”, ”cosmos qa 1 0 0”, ”cot ecqa”, ”cot sensemaking”, ”glue cola 2 0 0”, ”glue mrpc 2 0 0”, ”glue sst2 2 0 0”, ”imdb reviews plain text 1 0 0”, ”para crawl enes”, ”super glue record 1 0 2”, ”true case”, ”wmt14 translate fr en 1 0 0”, ”wmt16 translate de en 1 0 0”, ”word segment” c9 ”wiki hop original choose best object affirmative 1”, ”wiki hop original choose best object affirmative 2”, ”wiki hop original choose best object affirmative 3”, ”wiki hop original choose best object interrogative 1”, ”wiki hop original choose best object interrogative 2”, ”wiki hop original explain relation”, ”wiki hop original generate object”, ”wiki hop original generate subject”, ”wiki hop original generate subject and object” Table 10.Task names for each of the 10 clusters obtained by applying MBC clustering to Mistral 7B private library with 256 experts. 20",
      "meta_data": {
        "arxiv_id": "2405.11157v1",
        "authors": [
          "Oleksiy Ostapenko",
          "Zhan Su",
          "Edoardo Maria Ponti",
          "Laurent Charlin",
          "Nicolas Le Roux",
          "Matheus Pereira",
          "Lucas Caccia",
          "Alessandro Sordoni"
        ],
        "published_date": "2024-05-18T03:02:23Z",
        "pdf_url": "https://arxiv.org/pdf/2405.11157v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the challenge of reusing trained parameter-efficient adaptations (LoRAs) to enhance Large Language Model (LLM) performance on new tasks, particularly in zero-shot settings. It introduces Model-Based Clustering (MBC) for building a library of LoRA adapters, which groups tasks based on the similarity of their privately trained LoRA parameters and then trains a single adapter per cluster. A novel zero-shot routing mechanism, Arrow (↗↗), is proposed, dynamically selecting relevant LoRAs by leveraging the Singular Value Decomposition (SVD) of adapter parameters to identify directions of maximum variance. The research demonstrates that MBC-based adapters combined with Arrow routing achieve superior generalization, matching or outperforming traditional joint training methods (like full fine-tuning) on various LLMs (Phi-2, Mistral 7B) across zero-shot and supervised adaptation.",
        "methodology": "The study utilizes LoRA for parameter-efficient fine-tuning. For library construction, it proposes Model-Based Clustering (MBC): initially training private LoRAs for each task, then computing cosine similarity between their flattened and SVD-reduced parameter vectors, clustering tasks using K-means, and finally training a single adapter for each cluster on the combined cluster data. For reusing the library, several routing strategies are explored. Zero-shot routing includes µ Routing (uniform averaging), TP Routing (a task predictor classifier), CM Routing (centroid matching with hidden state prototypes), and the novel Arrow (↗↗) Routing. Arrow computes a prototype for each LoRA expert (Ai, Bi) by taking the first right singular vector of AiBᵀi, then routes per token and per layer by calculating the absolute dot product between the hidden state and these prototypes, selecting top-k experts. Supervised task routing methods examined are Poly Routing (learning input-independent layer-specific weights and tuning expert parameters), LoraHub Routing (gradient-free optimization of routing coefficients only), and π-tuning (using Fisher Information for task embeddings, retrieving similar experts, and tuning coefficients and expert parameters). LoRA composition involves linearly combining selected adapters based on routing probabilities.",
        "experimental_setup": "Experiments were conducted using Phi-2 (2.8B parameters) and Mistral 7B as base LLMs, with LoRA adapters applied only to attention layers (rank 4, dropout 0.05, learning rate 1e-4, alpha 16). The multi-task training dataset comprised 256 tasks from Flan v2, with 10,000 examples per task. SuperNatural Instructions (SNI) tasks were held out from training. Evaluation for zero-shot generalization involved 10 diverse held-out tasks (common-sense reasoning, QA, coding, general reasoning), measured by accuracy, with overlaps removed from the training set. Supervised adaptation was evaluated on 12 held-out SNI tasks, using Rouge-L scores, in full-data and limited-data (10%, 5%, 0.5%) regimes. MBC was configured with 10 clusters, and its training budget was matched to other methods by allocating 40% for initial private LoRA training and 60% for cluster adapter training. Zero-shot routing used top-4 expert selection with temperature 1.",
        "limitations": "The research is currently limited to LoRA adapters, with other adapter types (e.g., soft prompts, MLPs) not yet explored. The scalability of the proposed MBC and Arrow methods to significantly larger datasets and models remains an open question for future investigation. The paper notes that the performance gains from advanced routing strategies like Arrow diminish for smaller adapter libraries, a phenomenon that contrasts with sparse Mixture-of-Experts models and warrants further study, possibly due to the linear nature of LoRA experts. Additionally, LoraHub's underperformance in supervised adaptation is attributed to its design, which optimizes only routing coefficients without fine-tuning the expert LoRA weights themselves.",
        "future_research_directions": "Future work includes exploring a more heterogeneous 'universe' of adapters beyond LoRA, such as soft and hard prompts, MLPs, and their combinations. Investigating the scalability of the proposed approach to larger datasets and models is also a key direction. The authors suggest applying the Arrow routing strategy to modular continual learning, anticipating that its local nature and independence from gradient-based training could make it less susceptible to catastrophic forgetting. Further research is also needed to understand why the benefits of sophisticated routing diminish for smaller libraries, potentially by contrasting the linearity of LoRA experts with the MLP experts in sparse Mixture-of-Experts models. Lastly, a deeper exploration into why expert clusters with lower similarity (higher diversity) lead to better performance, possibly by examining distinct feature contributions, is proposed."
      }
    },
    {
      "title": "On the Weight Dynamics of Deep Normalized Networks",
      "abstract": "Recent studies have shown that high disparities in effective learning rates\n(ELRs) across layers in deep neural networks can negatively affect\ntrainability. We formalize how these disparities evolve over time by modeling\nweight dynamics (evolution of expected gradient and weight norms) of networks\nwith normalization layers, predicting the evolution of layer-wise ELR ratios.\nWe prove that when training with any constant learning rate, ELR ratios\nconverge to 1, despite initial gradient explosion. We identify a ``critical\nlearning rate\" beyond which ELR disparities widen, which only depends on\ncurrent ELRs. To validate our findings, we devise a hyper-parameter-free\nwarm-up method that successfully minimizes ELR spread quickly in theory and\npractice. Our experiments link ELR spread with trainability, a relationship\nthat is most evident in very deep networks with significant gradient magnitude\nexcursions.",
      "full_text": "On the Weight Dynamics of Deep Normalized Networks Christian H.X. Ali Mehmeti-G¨opel 1 Michael Wand1 Abstract Recent studies have shown that high disparities in effective learning rates (ELRs) across layers in deep neural networks can negatively affect train- ability. We formalize how these disparities evolve over time by modeling weight dynamics (evolu- tion of expected gradient and weight norms) of networks with normalization layers, predicting the evolution of layer-wise ELR ratios. We prove that when training with any constant learning rate, ELR ratios converge to 1, despite initial gradient explosion. We identify a “critical learning rate” beyond which ELR disparities widen, which only depends on current ELRs. To validate our find- ings, we devise a hyper-parameter-free warm-up method that successfully minimizes ELR spread quickly in theory and practice. Our experiments link ELR spread with trainability, a relationship that is most evident in very deep networks with significant gradient magnitude excursions. 1. Introduction In the past decade, combining neural networks and big data has enabled dramatic breakthroughs (Krizhevsky et al., 2012; OpenAI, 2023), and network depth has been a key factor: Compositions of many individual layers provide rich function spaces that empirically appear to be better-aligned with real-world data distributions than any other inductive biases we are aware of today. A fundamental problem of deep networks, maybe easily brushed over as technicality at first sight, is the problem of vanishing and exploding gradi- ents. Propagating signals through a multi-layer networks is not easy: In the forward pass, the magnitude input signals easily increases or decreases, thus leading to an exponential excursion of signal magnitude. Similarly, during the back- ward pass, we easily obtain similar excursion of gradient *Equal contribution 1Department of Computer Science, Johannes-Gutenberg University, Mainz, Germany. Correspon- dence to: Christian H.X. Ali Mehmeti-G ¨opel <chalimeh@uni- mainz.de>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). magnitudes (Yang et al., 2019). Further, using deep stacks of layers also easily increase correlations, thereby causing vanishing dimensionality (Saxe et al., 2013). Proper Initial- ization (He et al., 2015a) can reduce the problem; trying to prevent it completely in a simple feed-forward network is challenging though (Pennington et al., 2017). Modern architectures (He et al., 2016a; Vaswani et al., 2017) thus usually address these issues by combining residual connections (He et al., 2016a) and variants of normalization layers such as batch normalization (BN) (Ioffe and Szegedy, 2015). The former implicitly performs a down-weighting of deep paths, exponentially with depth (Veit et al., 2016), and in combination with normalization layers, this effect is further increased (at initialization) by decreasing the weight of the residual branch (De and Smith, 2020). The central objective of our paper is to understand how the dynamics (over training time) of gradient magnitude excursions (we do not consider correlations) are affected by normalization layers (BN and the similar). 2. Related Work and Contributions Understanding of the benefits of BatchNorm standalone is not straightforward and still subject to debate. The ini- tial claim of reduced “internal covariate shift” was quickly refuted (Awais et al., 2021) and many alternative explana- tions were proposed, such as smoothing of the loss surface (Santurkar et al., 2018) or enabling bigger learning rates (Bjorck et al., 2018). Salimans and Kingma (2016) intro- duced WeightNorm, a method to decouple a layer’s length and direction by training them as independent network pa- rameters. They also demonstrated that in weight-normalized networks, gradients are orthogonal to layer weights, allow- ing update size calculation via the Pythagorean theorem. Hoffer et al. (2018) showed that the “effective step size” in normalized networks is approximately proportional to 1 ||W||2 2 ; this shows that scale invariance gives us an additional degree of freedom, as scaling a layer’s weights is equivalent to inversely scaling its gradients or learning rate. You et al. (2017) have observed that the ratio ||∇W||F ||W||F , which we call effective learning rates (ELR), can vary wildly (up to a factor ∼ 250 in AlexNet-BN) between layers after only one step of gradient descent. The authors conjecture that this can create instability in training, especially for large 1 arXiv:2306.00700v3  [cs.LG]  24 May 2024On the Weight Dynamics of Deep Normalized Networks batch training requiring high learning rates, and propose to re-scale gradients by their effective learning rate. You et al. (2020) have later proposed a modified version of this algorithm for increased performance with transformer mod- els. Brock et al. (2021) have combined a similar re-scaling of the gradients with gradient clipping and are able to train normalizer-free networks using this technique. Bernstein et al. (2020) supported this intuition by showing that in a perturbed gradient descent, an optimization step decreases the loss function if all layer-wise ELRs are bounded by a term that depends on the perturbation angle. Arora et al. (2019) described the auto rate-tuning effect, proving that gradient descent asymptotically converges to a stationary point without manual tuning of learning rates for specific layers, given certain assumptions. Wan et al. (2021) prove that the “angular update” (a measure similar to ELR) of a given normalized layer eventually converges to a constant limit value which does not depend on initial conditions, but rely on weight decay for their demonstration. Interestingly, Li and Arora (2020) show that using weight decay with a constant learning rate schedule is mathematically equivalent to using no weight decay and an exponentially increasing learning rate schedule. The above-mentioned works show that the learning speeds of different layers do eventually align, but glance over the importance of correct learning rate scheduling in the early training phase, which we believe to be crucial in practice. We find that while convergence is always guaranteed given simplifying assumptions and over an indefinite number of iterations, choosing an excessively high learning rate, espe- cially in the first steps of training, can drastically increase imbalances in layer-wise learning speeds (ELR spread) to a degree where recovery is impossible within a realistic time frame. Furthermore, Li and Arora show a connection be- tween weight decay and warm-up but do not demonstrate how these techniques affect ELR spread. In this work, we model the dynamic effects solely induced by normalization layers and assume that the layer-wise gra- dient magnitude excluding normalization effects (base gra- dient magnitude) remains constant over time. In this setting, we derive a model predicting the evolution of a network’s weight dynamics (expected layer-wise gradient and weight norms). In the gradient flow, this behavior reduces to a non-linear ODE with a closed-form solution, where all ELR ratios between layers smoothly converge to 1. When training with higher learning rates, the behavior changes fundamen- tally, as the layer with the highest ELR can flip even below the layer with the lowest ELR in a single step if a certain critical learning rate is exceeded, which in turn increases ELR spread of the network. When training with constant learning rates, ELR spread can increase only during the first step, slowing down convergence, but still eventually con- verging. From there, we derive a warm-up scheme that is guaranteed to converge in num layers steps. Empirically, we were able to show that high ELR spreads indeed seem to correlate with low trainability: by using techniques that control ELR spread (gradient normalization and warm-up), we are able to reduce the high (initially exponential in the number of layers) ELR spreads of a 110 layer feedforward network and render the previously un-trainable network trainable. In summary, we create a theoretical framework that shows how the dynamical effects of normalization lay- ers can help counter gradient magnitude excursions in deep neural networks. 3. Auto Rate-Tuning Effect and Its Dynamics The core observation is that for any layer N that is invariant to scaling in the forward pass N(γ · x) = N(x) (e.g. all normalization layers), its gradient scales inversely with its input: dN dγx (γx) = 1 γ dN dx (x). (1) This is a simple consequence of the chain rule and has been shown for BatchNorm by Wu et al. (2018) and for Layer- Norm by Xiong et al. (2020). Secondly, Arora et al. (2019) show that since normalization layers are scale-invariant, no gradient can flow in this direction. Hence, weight updates ∇W are orthogonal to the weights W themselves: ⟨∇W, W⟩ = 0. (2) We now explore how this affects a network’s weight dynam- ics. Intuitively, the weight norm of layers with high gradient norms grows fast and thus down-scales the gradient, lead- ing to auto-regulation: this effect is called auto rate-tuning. We would like to point out that in a realistic scenario, the data tensor is multi-dimensional and condition 1 is satisfied along a subset of its dimensions (e.g. the batch, height and width axis for BatchNorm); auto-rate tuning is therefore given along those dimensions. 3.1. Sufficient Conditions for Auto Rate-Tuning / Correct Placement of Normalization Layers A necessary condition for auto rate-tuning of a linear layer L is the invariance of the network’s output with respect to re-scaling the weights in L (Arora et al., 2019). We deduct that any type of normalization layer (e.g. BatchNorm, Lay- erNorm) induces auto-rate tuning and that placing a nor- malization layer directly after every linear layer, as it is the case in most convolutional networks, is sufficient to achieve scale-invariance. In Transformer models, this was initially not the case, and we conjecture that this could explain the improvements when adding additional normalization layers in the feedforward blocks (Shleifer et al., 2021) or query/key blocks (Henry et al., 2020). Arora et al. also note that the 2On the Weight Dynamics of Deep Normalized Networks scale invariance property is not disrupted by positive ho- mogeneous functions of degree 1. We infer the following classification of commonly used layers: Auto-tuning passes through Breaks auto-tuning Linear layers w/o bias Linear layers w. bias Homogeneous nonlin. of deg. 1 Other nonlinearities Dropout MaxPool If a residual connection is placed in-between a linear layer and the next normalization layer, it can break auto rate- tuning; this is the case e.g. in a ResNet v2 (He et al., 2016b). 3.2. Training Dynamics Induced by Auto Rate-Tuning To model training dynamics, we assume that weights of a given layer, as well as their gradients, are random matrices where entries are normally distributed with zero mean and a time-dependent standard deviation that is uniform in each layer. We parameterize training time t ∈ R such that ti = i · λ2 after i optimization steps with a constant learning rate λ > 0. In this notation, gradient descent updates can be written as W(ti+1) =W(ti + λ2) =W(ti) − λ∇W(ti). The updates preserve zero norm and uniform variance of all entries in W. Assuming the independence of all entries in the weight and gradient matrices, we can deduce the following update rule from the orthogonality condition (2): ||W(ti+1)||2 F = ||W(ti)||2 F + λ2||∇W(ti)||2 F . (3) Condition (1) implies that gradient updates are inversely proportional to the current layer weights. We now assume that the “base gradient” of a layer, meaning the gradient magnitude excluding normalization induced scaling effects, is constant during training i.e. E(||W(ti)||F · ||∇W(ti)||)F = c, (4) for a constant c ∈ R at all times-steps ti. We discuss the limitations of this assumption in Section 4.1.2. Using shorthand σ2(ti) := E(∥W(ti)∥2 F ) and σ(ti) = p σ2(ti), we obtain: σ2(ti+1) =σ2(ti) + λ2c2 σ2(ti), (5) for a constant base gradient c >0 depending only on layer depth and initial weights norm that we assume to be strictly positive σ2(0) > 0. We call this the discrete model. 3.3. Gradient Norms at Initialization Feed-forward networks: The dynamics of Eq. 12 apply to all normalized layers equally, but the initial gradient norms ∥∇Wi(0)∥F differ substantially across layers i ≤ L: Yang et al. (2019) show that in feedforward networks with Batch Normalization, the gradient norm at initialization grows as: ci ∼ αL−i, (6) with α := p π/(π − 1) ≈ 1.21 for ReLU activations and He. initialization. See also Luther (2020) for a simplified derivation. ResNets: When considering residual networks, as per the multivariate chain rule, the gradient of residual blocks is ad- ditive instead of multiplicative (He et al., 2016b). Addition- ally, frequency-dependent signal-averaging further dampens gradients in a ResNet (Ali Mehmeti-G¨opel et al., 2021). It follows from the consideration for fully-connected network above and He et al. (2016b) Eq. 5 that for a residual network using ReLU units: ci ∼ 1 +⌊L − i s ⌋αs, (7) where s is the number of ReLU units in a residual block. 3.4. Auto Rate-Tuning Affects Each Layer Separately In this section, we establish that the dynamic re-scaling of gradients explored above applies to each layer independently and does not affect layers above or below, showing that a simple layer-wise view is sufficient. Proposition 3.1(Every Layer Auto-Tunes Separately). Con- sider a concatenation of a linear layer L(x, W) = xT W followed by a normalization layer N. Then, the derivative wrt. the input remains the same when layer weights are scaled by a factor γ: dN dx (x, γW) =dN dx (x, W). (8) The proof can be found in the Appendix Section A. 3.5. Effective Learning Rates and Their Ratios To account for scale variance induced by normalization layers, we are interested in the update size of the weight direction cW := W ∥W∥2 . Similarly to van Laarhoven (2017a), by approximating ∥W(ti+1)∥2 ≈ ∥W(ti)∥2, we can write: cW(ti+1) − cW(ti) ≈ W(ti+1) − W(ti) ∥W(ti)∥2 ∼ ∇W(ti) ∥W(ti)∥2 . (9) It is therefore imperative to consider the ratio from gradient- to-weight norm as measure of change in the layer’s weights. Definition 3.2 (Effective Learning Rate). We define the effective learning rate E of a layer with weight norm σ2 and base gradient c as: E(ti) := E \u0012||∇W(ti)||F ||W(ti)||F \u0013 = c σ2(ti). (10) As all effective learning rates can simply be globally re- scaled by adjusting the learning rate, we are interested in the evolution of layer-wise ratios of effective learning rates. 3On the Weight Dynamics of Deep Normalized Networks Definition 3.3 (Effective Learning Rate Ratios). We define the effective learning rate ratio Rjk of two layers j and k with weight norms σ2 j , σ2 k and base gradients cj, ck at a given time step ti as: Rjk(ti) := Ej Ek (ti) = cjσ2 k ckσ2 j (ti). (11) 3.6. Analysis in the Gradient Flow In this section, we show that in the gradient flow, weight dynamics have a closed-form solution and all ELR ratios converge smoothly to 1. Theorem 3.4 (Closed-Form Solution). In the gradient flow (λ → 0), Eq. 5 has the following closed form solution: σ2(t) = p 2c2t + k0. (12) with k0 = 4, assuming He initialization (He et al., 2015b). We will further call this the continuous model. Proof. Starting from Eq. 5, we can utilize thatti+1 = t+λ2 to drop the index and solve for the difference quotient: σ2(t + λ2) − σ2(t) λ2 = c2 σ(t)2 . (13) In the limit λ2 → 0, this yields the gradient flow that can be expressed as a nonlinear first order differential equation : dσ2 dt = c2 σ2 (14) The exact positive solution to the differential equation is given by: σ2(t) = p 2c2t + k0. (15) Assuming He initialization, the expected initial squared weight norm is 2 for layer width n. Thus, 2 =σ2(0) =√k0 and therefore k0 = 4. Theorem 3.5 (Convergence to Fixed Point). In the gradient flow (λ → 0), all effective learning rate ratios eventually converge given enough time, i.e. for any layer pairj, k≤ L: lim i→∞ Rjk(ti) = 1. (16) Proof. We consider two arbitrary layers j and k with re- spective weight norms σ2 j , σ2 k > 0 and base gradients cj, ck > 0. Using the formulae for gradient norm (Eq. 14) and weight norm (Eq. 12) in the continuous model, we write: Ej Ek (t) = cj σ2 j (t) · σ2 k(t) ck = cj p 2c2 kt + k0 ck q 2c2 jt + k0 t→∞ −→ 1. (17) 3.7. Analysis for Bigger Learning Rates In this section, we characterize the evolution of ELR ratios for non-infinitesimal, scheduled learning rates λ(ti), now relying solely on the discrete model. If λ(ti) is constant, we find the asymptotic behavior to be the same as in the gradient flow, where ELRs ratios converge in the time limit. On the contrary to the continous model, ratios can (temporarily) widen when surpassing a certain critical learning rate. Theorem 3.6 (Convergence to Fixed Point) . In the time limit and for a constant learning rateλ(ti) =λ, all effective learning rate ratios converge. For any layer pairj, k≤ L: lim i→∞ Rjk(ti) = 1. (18) The proof can be found in Appendix Section A. The main idea is that by substituting xi := σ2 j (ti) cjλ and yi := σ2 k(ti) ckλ , we can rewrite Eq. 4 for two distinct layers j and k as two sequences obeying the same recurrence relation and consequently bound the expression. Proposition 3.7 (Ratios Shrink). Let j, k≤ L be any layer pair. 1. If Rjk(ti) > 1, the ratio Rjk is then strictly lower in the next time step, i.e. Rjk(ti+1) < Rjk(ti). 2. If Rjk(ti) < 1, the ratio Rjk is then strictly greater in the next time step, i.e. Rjk(ti+1) > Rjk(ti). Proof. We start by showing the first proposition. We can reformulate the expression Ej Ek (ti+1) < Ej Ek (ti) using the definition of the effective learning rate as the following equivalent expression: c2 jσ4 k σ4 j c2 k (ti+1) < c2 jσ4 k σ4 j c2 k (ti). (19) We simplify this expression and take the square root. Since σ are variances and thus non-negative, the following expres- sion is also equivalent: σ2 k σ2 j (ti) − σ2 k σ2 j (ti+1) > 0. (20) We expand the second term using Eq. 5: σ2 k σ2 j (ti+1) = σ2 k + c2 kλ2 σ2 k σ2 j + c2 jλ2 σ2 j (ti) =σ2 j σ4 k + σ2 j c2 kλ2 σ4 j σ2 k + σ2 kc2 jλ2 (ti). (21) Substituting this term on the left hand side of Eq. 20 and combining the terms yields: σ2 k σ2 j (ti) − σ2 j σ4 k + σ2 j c2 kλ2 σ4 j σ2 k + σ2 kc2 jλ2 (ti) = σ4 kc2 jλ2 − σ4 j c2 kλ2 σ2 j (σ4 j σ2 k + σ2 kc2 jλ2)(ti). (22) 4On the Weight Dynamics of Deep Normalized Networks Since the denominator is strictly positive. Eq. 20 is therefore equivalent to: σ4 kc2 jλ2(ti) > σ4 j c2 kλ2(ti), (23) which is in turn equivalent to Ej Ek (ti) > 1 by definition. The second proposition can be shown analogously. A consequence of this proposition is that a given ratio Rjk diminishes during every step, except for when it flips, i.e. Rjk(ti) > 1 and Rjk(ti+1) < 1. In Appendix Section A, we show that when training with constant learning rates, this can only happen during the first step. Now, we would like to find the precise learning rate where the ratio flips. Definition 3.8 (Flipping Ratio). We define the “flipping ratio” κjk of two layers j and k at a given time step ti as: κjk(ti) := σjσk √cjck (ti) = s 1 EjEk (ti). (24) Proposition 3.9 (Flipping Conditions). Let j, k≤ L be any layer pair with w.l.o.g. be Rjk(ti) > 1. 1. The effective learning rate ratio does not flip between time steps ti and ti+1 i.e. Rjk(ti+1) > 1 if and only if λ(ti) < κjk(ti). 2. The effective learning rate ratio does flip between time steps ti and ti+1 i.e. Rjk(ti+1) < 1 if and only if λ(ti) > κjk(ti). 3. The ratio Ej Ek has reached a stationary point at a given time step ti, i.e. Rjk(tj) =Rjk(tj+1) for all j ≥ i if and only if λ(ti) =κjk(ti). Proof. We start by showing the first proposition. Using the definition of Rjk and Eq. 21, we can write: Rjk(ti+1) = cjσ2 k ckσ2 j (ti+1) = σ2 j σ4 kcj + σ2 j cjc2 kλ2 σ4 j σ2 kck + σ2 kc2 jckλ2 (ti). (25) Thus, the condition Rjk(ti+1) > 1 is equivalent to: \u0000 σ2 j σ4 kcj + σ2 j cjc2 kλ2\u0001 (ti) > \u0000 σ4 j σ2 kck + σ2 kc2 jckλ2\u0001 (ti) (26) ⇔ \u0000 σ2 j cjc2 kλ2 − σ2 kc2 jckλ2\u0001 (ti) > \u0000 σ4 j σ2 kck − σ2 j σ4 kcj \u0001 (ti) (27) ⇔λ2(σ2 j cjc2 k − σ2 kc2 jck)(ti) > σ4 j σ2 kck − σ2 j σ4 kcj(ti) (28) ⇔cjckλ2(σ2 j ck − σ2 kcj)(ti) > σ2 j σ2 k(σ2 j ck − σ2 kcj)(ti) (29) Since we assumed Rjk(ti) = Ej Ek (ti) = σ2 kcj σ2 j ck (ti) > 1, it follows that \u0000 σ2 j ck − σ2 kcj \u0001 (ti) < 0 and thus we invert the sign of the inequality when dividing by this quantity and we obtain the following equivalent condition: λ2(ti) < σ2 j σ2 k cjck (ti). (30) All quantities are non-negative, therefore taking the square root preserves equivalence and we obtain the sought condi- tion. The other propositions can be shown analogously. Since we are interested in reducing the highest overall ratio Rhℓ(ti) where ℓ, hare the layers with the lowest respective highest effective learning rate, we call κℓh(ti) the critrical learning rate. When using higher learning rates than this value, Eh(ti) flips below Eℓ(ti) during the next step, thus (for high λ considerably) increasing total ELR spread. In the following we will come to understand that in practice, a more conservative choice is advisable; for this reason, we propose the subcritical, but still provably fast warm-up scheme below. Corollary 3.10 (Subcritical Warm-Up). Given a network with L > 0 layers, if we schedule the learning rate as λ(ti) =κhh′ (ti), where we chose h, h′ ≤ L at each step to be the two layers with the highest effective learning rates, then no ratio Rjk for any j, k≤ L ever flips between a time step and the next and all pairs of effective learning rate ratios Rjk for any j, k≤ L converge to 1 in L steps. Proof. Let h, h′ be the two layers with the highest effective learning rates at time step ti. By Proposition 3.9, if we chose λ(ti) =κhh′ (ti), we have Rhh′ = 1at the next time step ti+1 and for all further time steps. Since h, h′ are the two layers with the highest effective learning rate at time step ti and we chose λ(ti) to be equal to their flipping ratio κhh′ (ti), we have: λ(ti) =κhh′ (ti) ≤ κjk(ti) (31) for all other layers j, k≤ L. Therefore, by Lemma 3.9, no ratio Rjk will ever flip between a time step and the next for any j, k≤ L. If we repeat this process L times, all pairwise learning rate ratios converge to 1. 3.8. Simulating Warm-Up Schedulers and Criticality In Figure 1, in order to visualize the concept of criticality, we simulated the evolution of effective learning rates and weight norms for popular learning rate schedulers with our discrete model (ref. Eq. 5), assuming initially exponentially exploding gradients (ref. Eq. 6). We also indicated λ(t) along with the critical learning rate κℓh(t). As predicted by our analysis in Section 3.7, wheneverλ(t) > κℓh(t), we see 5On the Weight Dynamics of Deep Normalized Networks Figure 1.Simulated evolution of layer-wise effective learning rates(top), weight norms (middle), learning rates λ(t) and critical LRs κℓh(t) (bottom) for popular learning rate schedulers. All y-axes are in logarithmic scale. Blue color corresponds to the lower layers. Figure 2.Short term evolution (after 10 steps) of predicted vs. measured weight and gradient norms of the lowest layer of a ResNet56 NoShort trained with random gradients (left) and real gradients (right) for various λ. that the highest effective learning rate flips below the lowest, which in turn increases the ELR spread and consequently the convergence time. We conclude that whether a warm-up scheme succeeds in quickly reducing ELR spread highly depends on the chosen hyper-parameters. 4. Experimental Validation In this experimental Section, we will first check the lim- itations of the assumption about constant base gradients and validate the predictivity of our model. Then, we will compare the predicted critical learning rate to an empirical value extracted from real training runs. Finally, we confirm that high ELR spreads correlate with network trainability in practice. 4.1. Experimental Setup 4.1.1. A RCHITECTURES , DATASETS AND TRAINING PROTOCOLS We chose ResNet v1 (He et al., 2016a) with (“Short”) and without (“NoShort”) residual connections as examples of standard architectures. We chose a ResNet v1 as opposed to a v2 since in the former, the “correct” placement of normal- ization layers (ref. Section 3.1) is given without modifying the architecture. Theory predicts that a high number of lay- ers and not using residual connections increase the strength of the observed effect (ref. Section 3.3). We therefore use 56 and 110 layer networks: Without residual connections, the former is deep but still trainable and the latter is mostly un-trainable with basic constant LR training. The final layer of a ResNet v1 is not scale-invariant and we therefore ex- clude it from our analysis. For computer vision tasks, we work with standard image classification datasets of variable difficulty: CIFAR-10, CIFAR-100 (Krizhevsky, 2009) and ILSVRC 2012 (called ImageNet in the following) (Deng et al., 2009). We use the most basic training setting possible (vanilla SGD) and disable all possible factors that influence weight dynamics: momentum, weight decay, affine Batch- Norm parameters and bias on linear layers (for a discussion, please refer to Appendix Section C). We further use dif- ferent kinds of learning rate scheduling with and without warm-up; further details about the architectures and training process can be found in the Appendix. 4.1.2. M EASURING ELR S PREAD In our experiments, we need a measure for ELR spreads that is relative to the network’s mean ELR. Definition 4.1 (Relative Logarithmic ELR Spread) . We define the Relative Logarithmic ELR Spread as: Srel := std(ln(E)), (32) computed across the layers of the network and usually aver- aged over all channels and the entire training process. 4.1.3. R ANDOM WALK In the past section, we modeled exclusively the dynamics induced by normalization assuming constant base gradients (ref. Eq 4), meaning that the layer-wise expected gradi- ent magnitude excluding normalization effects is constant 6On the Weight Dynamics of Deep Normalized Networks Figure 3.Long term evolution of predicted vs. measured layer-wise effective learning rates for a ResNet56 NoShort trained with random gradients (left) and real gradients (right). Blue color corresponds to the lower layers. over time. This is obviously not strictly true in a practical setting: apart from the obvious factors mentioned above (momentum, affine parameters etc.), the derivative of non- linear layers and the objective function itself changes when varying inputs or weights. During training, gradient norms tend to shrink as the objective function saturates (Lee et al., 2019). To verify that mostly learning effects are responsible for fluctuations in base gradients, we observe how weight dynamics evolve during a random walk. Definition 4.2 (Random Walk). During each training step, before applying the gradients computed in the backward- pass, we replace every layer’s gradient by a random vector of similar norm which is also orthogonal to the layer’s weights. Please refer to Algorithm 1 for a formal description. Algorithm 1 Random Walk Let eℓ denote the number of elements of the weight vector Wℓ and ⟨·, ·⟩ the dot product. for each gradient descent step i do for each layer ℓ do Compute ∇Wℓ(ti) σ ← p ∥∇Wℓ(ti)∥2 2/eℓ R ← N(0, σ2)eℓ V ← W(ti) ∇Wℓ(ti) ← R − ⟨R,V ⟩ ⟨V,V ⟩ V // orthogonalize end for end for 4.2. Model Validation and Limitations To validate our theory, we measure the initial gradient and weight norms of a network and extrapolate their evolution using our discrete model (Eq. 5). We then compare the pre- dicted weight/gradient norms to the empirically measured values after a given number of steps. We will first see that for a feedforward network with ReLU activations, it is already enough to exclude learning effects (random walk scenario) for our model to be long-term predictive. When including them, as expected, gradients are lower than predicted but the main takeaway qualitatively still holds: ELR spreads diminish over time, given that a certain learning rate is not exceeded. Short-Term Validation : In Figure 2, we compare the measured weight/gradient norm of the lowest layer of a ResNet56 NoShort after training on Cifar10 for 10 steps to the values predicted using our discrete model on the ini- tial values. In a random walk (left), predictions are quite accurate up to λ ≈ 1 and get slightly inaccurate for higher λ, presumably due to numerical issues. As for real train- ing (right), we see that gradients are notably smaller than expected after 10 steps. For the following, it is crucial to note that the difference between the predicted and measured values is not a constant ratio but instead increases in λ. Long Term Validation : We conducted a similar experi- ment for only two different learning rates λ ∈ {0.001, 1} over 3000 epochs and visualize the measured/predicted ELR of all layers in Figure 3. We see that in the random walk scenario, our prediction is remarkably accurate. In real training and for λ = 1, our model predicts this learning rate to be critical, but in reality it is super-critical as the gradients of the lower layers (blue) significantly undershoot with regard to the prediction and their ELR flips below the highest layers (red); further training does not seem to be able to recover the high ELR spread. Since training with any subcritical learning rate reduces ELR spread, we will see that it is sufficient to use a slightly lower λ than the predicted critical value to avoid an increase in ELR spread. Predicting Criticality: In Figure 4, we train a ResNet110 NoShort for a single epoch using various constant learn- ing rates on Cifar10 in a random walk and a real training scenario, tracking the evolution of ELR spreads. We plot ELR spreads at initialization and after one epoch, averaging measurements over 10 runs for each datapoint. First, we note that as predicted, up to a certain learning rate, ELR spreads are always lowered by training. Next, we indicate 7On the Weight Dynamics of Deep Normalized Networks Figure 4.Relative spread after one epoch (solid blue), relative spread at initialization (dotted blue) and the critical (red) / subcritical (green) learning rate at initialization of a ResNet110 NoShort with random gradients (left) and real gradients (right). Figure 5.Test accuracies and relative spreads of a ResNet110 (No)Short trained on Cifar10 using regular, warm-up and con- strained ELR training protocols for different target (E)LRs. the predicted (sub)critical learning rate at initialization: as per Proposition A.1, if a learning rate is subcritical in step 0, it should also not increase spreads during later steps. Con- sequently, we expect the runs with λ ≈ κℓh(0) to have the lowest Srel value after training. In Figure 4, we see that this is indeed the case for the random walk. In real training, the qualitative behavior is similar but the curve is shifted to the left as gradients are smaller. We also note that in real train- ing, when using super-critical learning rates ( λ >10−3), ELRs do not seem to converge anymore, presumably to auto-rate tuning effects becoming too weak compared to fluctuations in base gradient magnitude caused by training. 4.3. ELR Spread and Trainability In this section, we want to show empirically that net- works with high ELR spreads correlate with low trainabil- ity and that lowering spreads using various methods can restore trainability. For this, we chose an experimental set- ting where ELR spreads are large: we train a ResNet110 (No)Short on Cifar10. For all runs, we use a simple multi- step learning rate decay. In Figure 5 (top), we see that for the “NoShort” networks in regular training without warm-up (“base”), spreads (averaged over the training run) are very high and trainability is very low. Using skip connections (bottom), spreads are much lower and the network is able to train. 4.3.1. S UBCRITICAL WARM -UP As we have seen in Figure 4 (right), because of learning effects present in real training, the more conservative choice of using the subcritical learning rate for warm-up seems like a more sensible value to avoid overshooting in practice but still guarantees fast convergence in theory (ref. Corollary 3.10). Further, since we are using BatchNorm, we obtain channel-wise ELR values and use the maximum of these values as our layer-wise value. 4.3.2. C ONSTRAINING LAYER -WISE ELR S Another possibility of controlling ELR spread is scaling each layer’s gradients before each step so that layer-wise effective learning rates are constrained to be constant: ∇W ← ∇W · Egoal E + ϵ, (33) for a given constant goal effective learning rate Egoal and a small ϵ we chose as ϵ = 10−5. A similar mechanic was used in the popular LARS optimizer (You et al., 2017). To prevent increasing weight norms W from overflowing, we additionally divide all layer weights by the maximum layer weight cW = max(||W||F ) over all layers before every step. This should not change the network function since normalization layers are scale invariant and gradients are normalized. Alternatively, one could re-scale the gradients by 1√ 1+λ2 after every optimization step, as described by Bernstein et al. (2020). 4.3.3. E VALUATION In Figure 5 (left), we see that both techniques lower ELR spread across layers which correlates with the previously untrainable ResNet110 NoShort becoming trainable, despite its initially exponentially exploding gradients. Although not a proof of a general causal connection between ELR spread and trainability, the fact that an untrainable network 8On the Weight Dynamics of Deep Normalized Networks becomes trainable with the intervention made yields some compelling evidence supporting such a hypothesis. For the network with skip-connections (right), we can observe the same effects but less pronounced, which is expected since the initial spread is linear and not exponential in the number of layers as shown in Section 3.3. In Appendix Section B, we repeat this experiment on the Cifar100 dataset and draw similar conclusions. Finally, we train a ResNet101 (No)Short on ImageNet for 50 epochs using three different warm-up schedulers: OneCy- cle (Smith and Topin, 2017), sub-critical warm-up and no warm-up; we use the exact same cool-down phase (cosine) for all schedulers. We use default hyper-parameters for the OneCycle scheduler that work well in training a ResNet101 Short in a short amount of epochs on this dataset. In Table 1, we see that indeed warm-up lowers Srel and correlates with increased performance, but the hyper-parameters used for OneCycle that work well with the residual network still result in significant spreads for the non-residual network with higher initial spreads. This confirms that warm-up should be scheduled as a function of current ELRs. Al- though subcritical warm-up uses very few warm-up steps, it yields comparable or better results than our preset OneCycle warm-up. Using the ELR-constrain method to prescribe a global ELR similar to the OneCycle run, we see that we are able to train the network without residual connections; us- ing warm-up additionally decreases performance, showing that warm-up presents no benefits in a setting with no ELR spread. Table 1.Test accuracies and relative spread of a ResNet101 (No)Short trained on ImageNet using different types of warm- up / ELR-constrain; RES indicates residual connections and CTN whether the ELR-constrain method was used. RES CTN W. TYPE W. STEPS ACC. Srel NO NO NONE 0 08.50 3.96 NO NO ONECYCLE 64060 22.82 1.96 NO NO SUBCRITICAL 9 41.83 0.70 NO YES NONE 0 47.99 - NO YES ONECYCLE 64060 45.61 - YES NO NONE 0 72.85 0.29 YES NO ONECYCLE 64060 72.83 0.31 YES NO SUBCRITICAL 3 73.06 0.27 5. Discussion and Future Work In past work, high spreads in effective learning rates have been conjectured to negatively affect trainability, but to our best knowledge, no formal model exists that describes their time-based evolution in early training phases for scheduled learning rates. Under the assumption of constant gradient magnitudes beyond normalization effects, we derived a simple model from first principles that describes the evolution of expected weight/gradient norms and consequently effective learning rates during training. Under our model’s assumption, we were able to prove that when training long enough using any constant learning rate, all ratios of layer-wise effective learning rates eventually converge to the same value. Problems can still arise in the first step(s) if the learning rate λ(ti) is bigger than the critical value κℓh(ti) (which depends on current weight/gradient norms), momentarily increasing the disparity between layer-wise effective learning rates. We consider this theoretical model of normalization-induced dynamic effects to be our main contribution. In a series of empirical experiments, we have shown that although we exclusively model norm-induced dynamics (scale-invariant linear layers) and assume that the expected gradient norm of other layers (objective function, nonlinear layers) does not change over time, our main takeaway still holds when training a deep convolutional ReLU network on real data: training reduces effective learning rate spread up to a certain critical learning rate . By using live gradient values at each step and using a slightly more conservative learning rate choice than predicted, we were able to design a hyper-parameter-free warm-up scheduler that is able to quickly reduce effective learning rate spreads in practice. In an (extreme) setting with exponentially exploding initial gradients, we show that reducing ELR spreads using warm-up or by normaliz- ing gradients to prescribe a constant effective learning rate correlates with the network’s trainability being restored. Our analysis applies to all normalized networks, i.e. archi- tectures where the network function is invariant wrt. scaling in weight matrices, which is usually the case in most nor- malized feedforward architectures. Unfortunately, unlike most other traditional MLPs/CNNs/ResNets, the weight ma- trices of attention blocks are not scale-invariant and thus the inverse scaling property (Eq. 1) and orthogonality (Eq. 2), which our model relies on, are violated. Moreover, modi- fying the architecture (i.e. adding additional normalization layers) would fundamentally impact its way of working (e.g. attention cannot be unlearnt anymore). Preliminary results show that for architectures containing higher degree non- linearities (e.g. Transformer models), base gradients can vary much more compared to simple feedforward ReLU networks, therefore limiting the applicability of our model as is. If the order of the fluctuations of the base gradient ex- ceeds that of the auto-rate tuning effects, the effect vanishes. We could envision extending our model to include an error analysis for non-constant base gradients, estimating when this is the case. 9On the Weight Dynamics of Deep Normalized Networks Acknowledgments The authors acknowledge funding from the Emergent AI Center funded by the Carl-Zeiss-Stiftung. The authors would like to thank Daniel Franzen and Jan Disselhoff for their helpful discussions. The authors would also like to express their gratitude to the HPC working group of the Johannes-Gutenberg University Mainz for sharing their com- pute power in times of need. Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. References Christian H. X. Ali Mehmeti-G¨opel, David Hartmann, and Michael Wand. Ringing relus: Harmonic distortion analysis of nonlinear feedforward networks. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https: //openreview.net/forum?id=TaYhv-q1Xit. Sanjeev Arora, Zhiyuan Li, and Kaifeng Lyu. Theoretical analysis of auto rate-tuning by batch normalization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https: //openreview.net/forum?id=rkxQ-nA9FX. Muhammad Awais, Md. Tauhid Bin Iqbal, and Sung-Ho Bae. Revisiting internal covariate shift for batch normalization. IEEE Trans. Neural Networks Learn. Syst., 32(11):5082–5092, 2021. doi: 10.1109/TNNLS.2020.3026784. URL https: //doi.org/10.1109/TNNLS.2020.3026784. Jeremy Bernstein, Arash Vahdat, Yisong Yue, and Ming-Yu Liu. On the distance between two neural networks and the stability of learning. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/ paper/2020/hash/ f4b31bee138ff5f7b84ce1575a738f95- Abstract.html. Johan Bjorck, Carla P. Gomes, Bart Selman, and Kilian Q. Weinberger. Understanding batch normalization. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol`o Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr´eal, Canada, pages 7705–7716, 2018. URL https://proceedings. neurips.cc/paper/2018/hash/ 36072923bfc3cf47745d704feb489480- Abstract.html. Andy Brock, Soham De, Samuel L. Smith, and Karen Simonyan. High-performance large-scale image recognition without normalization. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 1059–1071. PMLR, 2021. URL http://proceedings.mlr.press/v139/ brock21a.html. Soham De and Sam Smith. Batch normalization biases residual blocks towards the identity function in deep networks. Advances in Neural Information Processing Systems, 33:19964–19975, 2020. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami, Florida, USA, pages 248–255. IEEE Computer Society, 2009. doi: 10.1109/CVPR.2009.5206848. URL https: //doi.org/10.1109/CVPR.2009.5206848. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In 2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pages 1026–1034. IEEE Computer Society, 2015a. doi: 10.1109/ICCV .2015.123. URL https://doi.org/10.1109/ICCV.2015.123. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In 2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pages 1026–1034. IEEE Computer Society, 2015b. doi: 10.1109/ICCV .2015.123. URL https://doi.org/10.1109/ICCV.2015.123. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 10On the Weight Dynamics of Deep Normalized Networks IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV , USA, June 27-30, 2016, pages 770–778. IEEE Computer Society, 2016a. doi: 10.1109/CVPR.2016.90. URL https://doi.org/10.1109/CVPR.2016.90. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors, Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV, volume 9908 of Lecture Notes in Computer Science, pages 630–645. Springer, 2016b. doi: 10.1007/978-3-319-46493-0 \\ 38. Alex Henry, Prudhvi Raj Dachapally, Shubham Shantaram Pawar, and Yuxuan Chen. Query-key normalization for transformers. In Trevor Cohn, Yulan He, and Yang Liu, editors, Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020, volume EMNLP 2020 of Findings of ACL, pages 4246–4253. Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.findings-emnlp.379. URL https://doi.org/10.18653/v1/2020. findings-emnlp.379. Elad Hoffer, Ron Banner, Itay Golan, and Daniel Soudry. Norm matters: efficient and accurate normalization schemes in deep networks. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol`o Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr´eal, Canada, pages 2164–2174, 2018. URL https://proceedings.neurips.cc/paper/ 2018/hash/ a0160709701140704575d499c997b6ca- Abstract.html. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Francis R. Bach and David M. Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of JMLR Workshop and Conference Proceedings, pages 448–456. JMLR.org, 2015. Alex Krizhevsky. Learning multiple layers of features from tiny images. pages 32–33, 2009. URL https://www.cs.toronto.edu/˜kriz/ learning-features-2009-TR.pdf . Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, L´eon Bottou, and Kilian Q. Weinberger, editors,Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States, pages 1106–1114, 2012. URL https://proceedings.neurips.cc/ paper/2012/hash/ c399862d3b9d6b76c8436e924a68c45b- Abstract.html. Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alch´e-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 8570–8581, 2019. URL https://proceedings. neurips.cc/paper/2019/hash/ 0d1a9651497a38d8b1c3871c84528bd4- Abstract.html. Zhiyuan Li and Sanjeev Arora. An exponential learning rate schedule for deep learning. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https: //openreview.net/forum?id=rJg8TeSFDH. Kyle Luther. Why batch norm causes exploding gradients. Blog post, 2020. URL https://kyleluther. github.io/2020/02/18/batchnorm- exploding-gradients.html. OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. doi: 10.48550/arXiv.2303.08774. URL https: //doi.org/10.48550/arXiv.2303.08774. Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice. In I. Guyon, U. V on Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_ files/paper/2017/file/ d9fc0cdb67638d50f411432d0d41d0ba- Paper.pdf. B.T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computational 11On the Weight Dynamics of Deep Normalized Networks Mathematics and Mathematical Physics, 4(5):1–17, 1964. ISSN 0041-5553. doi: https://doi.org/10.1016/0041-5553(64)90137-5. URL https://www.sciencedirect.com/ science/article/pii/0041555364901375. PyTorch. BatchNorm2d; PyTorch 2.1 documentation — pytorch.org. https://pytorch.org/docs/stable/ generated/torch.nn.BatchNorm2d.html. [Accessed 23-11-2023]. Tim Salimans and Diederik P. Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, page 901, 2016. URL https://proceedings.neurips.cc/ paper/2016/hash/ ed265bc903a5a097f61d3ec064d96d2e- Abstract.html. Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normalization help optimization? In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol`o Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr´eal, Canada, pages 2488–2498, 2018. URL https://proceedings.neurips.cc/paper/ 2018/hash/ 905056c1ac1dad141560467e0a99e1cf- Abstract.html. Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013. Sam Shleifer, Jason Weston, and Myle Ott. Normformer: Improved transformer pretraining with extra normalization. CoRR, abs/2110.09456, 2021. URL https://arxiv.org/abs/2110.09456. Leslie N. Smith and Nicholay Topin. Super-convergence: Very fast training of residual networks using large learning rates. CoRR, abs/1708.07120, 2017. URL http://arxiv.org/abs/1708.07120. Ryszard Szwarc. Convergence analysis of a quotient of two sequences x2 n+1 = x2 n + c x2n . Mathematics Stack Exchange. URL https: //math.stackexchange.com/q/4820434. URL:https://math.stackexchange.com/q/4820434 (version: 2023-12-05). Twan van Laarhoven. L2 regularization versus batch and weight normalization. CoRR, abs/1706.05350, 2017a. URL http://arxiv.org/abs/1706.05350. Twan van Laarhoven. L2 regularization versus batch and weight normalization. CoRR, abs/1706.05350, 2017b. URL http://arxiv.org/abs/1706.05350. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V . N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998–6008, 2017. URL https://proceedings. neurips.cc/paper/2017/hash/ 3f5ee243547dee91fbd053c1c4a845aa- Abstract.html. Andreas Veit, Michael Wilber, and Serge Belongie. Residual networks behave like ensembles of relatively shallow networks. In Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS’16, page 550–558, Red Hook, NY , USA, 2016. Curran Associates Inc. ISBN 9781510838819. Ruosi Wan, Zhanxing Zhu, Xiangyu Zhang, and Jian Sun. Spherical motion dynamics: Learning dynamics of normalized neural network using SGD and weight decay. In Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 6380–6391, 2021. URL https://proceedings.neurips.cc/paper/ 2021/hash/ 326a8c055c0d04f5b06544665d8bb3ea- Abstract.html. Xiaoxia Wu, Rachel Ward, and L´eon Bottou. Wngrad: Learn the learning rate in gradient descent. CoRR, abs/1803.02865, 2018. URL http://arxiv.org/abs/1803.02865. Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie-Yan Liu. On layer normalization in the 12On the Weight Dynamics of Deep Normalized Networks transformer architecture. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 10524–10533. PMLR, 2020. URL http://proceedings.mlr.press/v119/ xiong20b.html. Greg Yang, Jeffrey Pennington, Vinay Rao, Jascha Sohl-Dickstein, and Samuel S. Schoenholz. A mean field theory of batch normalization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https: //openreview.net/forum?id=SyMDXnCcF7. Yang You, Igor Gitman, and Boris Ginsburg. Scaling SGD batch size to 32k for imagenet training. CoRR, abs/1708.03888, 2017. URL http://arxiv.org/abs/1708.03888. Yang You, Jing Li, Sashank J. Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep learning: Training BERT in 76 minutes. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https: //openreview.net/forum?id=Syx4wnEtvH. 13On the Weight Dynamics of Deep Normalized Networks A. Additional Proofs This section contains proofs of theorems as well as additional material complementary to the main section. Proof of Proposition 3.1. We compute the derivative of the output with regard to the input using the chain rule and relate it to the derivative with unscaled inputs: dN dx (x, γW) =dN dL (x, γW) · dL dx (x, γW) (34) = 1 γ dN dL (x, W) · γ dL dW (x, W) (35) = dN dx (x, W). (36) Where in Eq. 35, we used the inverse scaling property from Eq. 1. Proof of Theorem 3.6. We would like to credit the original author of this proof (Szwarc). By setting xi := σ2 j (ti) cjλ and yi := σ2 k(ti) ckλ , we can rewrite Eq. 4 for layers j and k as two sequences obeying the same recurrence relation: xi+1 = xi + 1 xi (37) yi+1 = yi + 1 yi . (38) Raising xi to the second power yields: x2 i+1 = x2 i + 2 +1 x2 i . (39) This allows us to unroll the recursion as follows : x2 i = x2 1 + 2(i − 1) + 1 u2 1 + . . .+ 1 u2 i−1 . (40) As xj ≥ 2(j − 1), we can write the following inequality: 2(i−1) ≤ x2 i ≤ 2(i−1)+x2 1 + 1 x2 1 + 1 2 + 1 4 +. . .+2(i−2). (41) By the integral test, it is clear that Pn−1 i=1 1 k ≤ ln(n) and therefore Pn−1 i=1 1 2k ≤ ln(n) 2 = ln(√n). Let be γ := u2 1 + 1 u2 1 − 2, we consider the square root of the expression above: √ 2i − 2 ≤ xi ≤ q 2i + log( √ i − 1) +γ. (42) Since γ is a constant and limi→∞ log(i) i = 0, it follows that: lim i→∞ xi√ 2i = 1. (43) and analogously lim i→∞ yi√ 2i = 1. (44) We therefore obtain: lim i→∞ xi yi = σ2 j ck σ2 kcj (ti) =Rkj(ti) = 1, (45) which is in turn also true for the inverse fraction. Proposition A.1 (Ratios Flip at Most Once). Let j, k≤ L be any layer pair with w.l.o.g. Rjk(ti) > 1 and assume a constant learning rate λ(ti) =λ. 1. If effective learning rate ratios do not flip between a given time step and the next, they will never flip at a later time step, i.e. if Rjk(ti+1) > 1 it follows that Rjk(ti+j) > 1 for all j ≥ 1. 2. If effective learning rate ratios do flip between a given time step and the next, they will never flip again at a later time step,, i.e. if Rjk(ti+1) < 1 it follows that Rjk(ti+j) < 1 for all j ≥ 1. Proof. We start by showing the first statement. Assuming that the effective learning rate ratio does not flip between time steps ti and ti+1, we know by Lemma 3.9 that λ < κjk(ti). We now just have to show thatλ < κjk for all successive time steps. Since cj and ck are constants and we know by the definition of the discrete process in Eq. 5 that all weight norms σ(ti) are strictly increasing over time, we can write: λ < κjk(ti) = σjσk √cjck (ti) < σjσk √cjck (ti+j) =κjk(ti+j) (46) for all j ≥ 1 and thus by Lemma 3.9 the ratio will never flip again. We now show the second statement. Assuming that the effective learning rate ratio does flip between time steps ti and ti+1, we know by Lemma 3.9 that λ > κjk(ti). We start by showing that the ratio will not flip for the next time step, which is in turn equivalent to λ < κjk(ti+1). We can 14On the Weight Dynamics of Deep Normalized Networks expand this term as follows: κ2 jk(ti+1) =σ2 j σ2 k cjck (ti+1) (47) = 1 cjck   σ2 j + c2 jλ2 σ2 j !\u0012 σ2 k + c2 kλ2 σ2 k \u0013 (ti) (48) =   σ2 j σ2 k cjck + σ2 j ckλ2 σ2 kcj + σ2 kcjλ2 σ2 j ck + cjckλ4 σ2 j σ2 k ! (ti) (49) =   κ2 jk + Ek Ej λ2 + Ej Ek λ2 + 1 κ2 jk λ4 ! (ti) (50) > \u0012 κ2 jk + Ek Ej λ2 + Ej Ek λ2 + 1 λ2 λ4 \u0013 (ti) (51) ≥ λ2. (52) We can write Eq. 51 because of the assumption that λ > κjk(ti) and Eq. 52 because all summands are non-negative. We have therefore shown thatλ < κjk(ti+1) and thus the ratio will not flip between time step ti+1 and ti+2. By the first proposition shown above, we know it will therefore never flip in future time steps, i.e. Rjk(ti+j) < 1 for all j ≥ 1. B. Additional Experiments In Figure 6, we repeated the experiment of Figure 5 on the Cifar100 dataset. Qualitatively, we observe the same effects. The ResNet110 NoShort does not train at all and has high ELR spreads. By using the ELR-constrain or critical-warmup method, we are able to train the network to a significant, but not very good performance. As for the ResNet110 Short, we start to see a difference between runs without warm-up our sub-critical scheduler for high learning rates λ >10) where again, a reduced spread results in increased trainability. We conclude that reducing ELR spread correlates with increased trainability, but other factors (e.g. vanishing dimensionalty) explain the gap between short and no-short architectures. C. Other Factors Influencing Weight Dynamics As mentioned in the main paper, some techniques commonly used in training influence the evolution of weight dynamics in a way that is not modeled by Eq. 5; in this section we will discuss them. Figure 6.Test accuracies and relative spread of a ResNet110 (No)Short trained on Cifar100 using regular, warm-up and con- strained ELR training protocols. C.1. Weight Decay The fact that weight decay influences weight dynamics in normalized networks is quite trivial and well-explored in recent literature: (Hoffer et al., 2018) (van Laarhoven, 2017b). In a normalized network, if all weights are reduced by a factor α, this corresponds to an increase of the global learning rate by a factor α, as per Eq 1. C.2. Momentum As momentum SGD (Polyak, 1964) modifies each gradient’s direction and length before it is applied, it is easy to see that it must influence weight dynamics. It is possible to compute weight dynamics of a network optimized with momentum SGD, but we consider this to be out of scope of this work. C.3. Affine Normalization Parameters Normalization layers are usually applied with learnable affine parameters γ · N(x) +β that are initialized to γ = 1 and β = 0(PyTorch). In the case of a network where normalization layers are followed by ReLUs (this is the case in our experiments), this means that we initialize in the “maximum curvature region” of the nonlinearity but drift away from it during training (Ali Mehmeti-G¨opel et al., 2021) leading to gradients dropping further than expected. In Figure 7, we repeated the experiment of Figure 3 using random gradients (a setting that produces a reliable prediction) but add affine BatchNorm parameters to our training protocol. For λ = 0.001, the prediction is still quite accurate but for λ = 1, we see that the real gradients are much smaller than predicted. 15On the Weight Dynamics of Deep Normalized Networks Figure 7.Long term evolution of simulated/real layer-wise effec- tive learning rates for a ResNet56 NoShort trained with random gradients and affine BatchNorm parameters. D. Architecture and Training Details As described in the main paper, we used ResNet variants with varying hyper-parameters, with and without skip connections. Architectural details can be found in the tables below. The experiments in the paper were made on computers running Arch Linux, Python 3.11.5, PyTorch Version 2.1.2+cu121. Various Nvidia GPUS were used ranging from GeForce GTX 1080TI, GeForce GTX 2080Ti RTX 4090. Table 2.Network architecture and training regime used for the Cifar10/100 task. ARCHITECTURE RESNET56/110 BLOCK BASIC BLOCK V 1 NUM. B LOCKS 9 9 9 / 18 18 18 NUM. P LANES 16 32 64 SHORTCUT TYPE A (PADDING ) TRAINING CIFAR-10 / CIFAR-100 EPOCHS 200 SCHEDULER MULTISTEP (γ = 0.1) MILESTONES 100, 150 LEARNING RATE VARIABLE BATCH SIZE 256 OPTIMIZER SGD MOMENTUM 0 WEIGHT DECAY 0 AUGMENTATION RANDOM FLIP NESTEROV FALSE Table 3.Network architecture and training regime used for the ImageNet task. ARCHITECTURE RESNET101 BLOCK BOTTLENECK BLOCK V 1 NUM. B LOCKS 3 4 32 3 NUM. P LANES 64 128 256 512 SHORTCUT TYPE B (1 X1-C ONV +BN) TRAINING IMAGE NET EPOCHS 50 SCHEDULER ONECYCLE / NO-WARMUP + COSINE / SUBCRITICAL + COSINE MAX. LR 0.4 BATCH SIZE 100 OPTIMIZER SGD NESTEROV FALSE MOMENTUM 0 WEIGHT DECAY 0 AUGMENTATION RANDOM FLIP ONECYCLE ANNEAL STRATEGY COSINE ONECYCLE BASE MOMENTUM 0 ONECYCLE CYCLE MOMENTUM TRUE ONECYCLE DIVFACTOR 20 ONECYCLE EPOCHS START 0.1 ONECYCLE FINAL DIVFACTOR 2000 ONECYCLE MAXMOMENTUM 0.0 16",
      "meta_data": {
        "arxiv_id": "2306.00700v3",
        "authors": [
          "Christian H. X. Ali Mehmeti-Göpel",
          "Michael Wand"
        ],
        "published_date": "2023-06-01T14:09:52Z",
        "pdf_url": "https://arxiv.org/pdf/2306.00700v3.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper formalizes how effective learning rate (ELR) disparities evolve over time in deep normalized networks, which can negatively affect trainability. It models weight dynamics (expected gradient and weight norms) and predicts the evolution of layer-wise ELR ratios. Key findings include proving that ELR ratios converge to 1 with any constant learning rate despite initial gradient explosion, identifying a 'critical learning rate' beyond which ELR disparities widen, and devising a hyper-parameter-free warm-up method to minimize ELR spread quickly. The research empirically links high ELR spread with low trainability, particularly in very deep networks.",
        "methodology": "The methodology involves formalizing weight dynamics by modeling the evolution of expected gradient and weight norms in networks with normalization layers, assuming that these layers are scale-invariant and that gradient updates are orthogonal to weights. This leads to a discrete model described by Eq. 5. In the gradient flow limit (continuous model), this behavior reduces to a non-linear ordinary differential equation with a closed-form solution (Eq. 12), predicting smooth convergence of ELR ratios to 1. For bigger learning rates, the discrete model is analyzed, revealing a 'critical learning rate' (κjk) at which ELR ratios can temporarily widen or 'flip'. A 'subcritical warm-up' scheme is derived by setting the learning rate to this critical value, ensuring convergence in L steps. An alternative method of 'constraining layer-wise ELRs' by scaling gradients to maintain constant effective learning rates is also proposed.",
        "experimental_setup": "Experiments were conducted on ResNet v1 architectures (56 and 110 layers), both with ('Short') and without ('NoShort') residual connections, chosen to amplify the observed effects. Standard image classification datasets were used: CIFAR-10, CIFAR-100, and ILSVRC 2012 (ImageNet). Training employed vanilla SGD with momentum, weight decay, affine BatchNorm parameters, and bias on linear layers disabled to isolate weight dynamics. Various learning rate schedulers, with and without warm-up, were tested. Validation involved comparing predicted vs. measured weight/gradient norms and ELR evolution, assessing the model's predictive accuracy under 'random walk' (randomized gradients) and 'real training' scenarios. The 'Relative Logarithmic ELR Spread' (Srel) was used to quantify ELR disparities. The effectiveness of the proposed warm-up and ELR-constraining methods was evaluated by observing their impact on ELR spread and network trainability (test accuracies).",
        "limitations": "The theoretical model primarily assumes that the 'base gradient' magnitude (excluding normalization effects) remains constant over time, which is not strictly true in practical settings where gradient norms typically shrink as the objective function saturates. The model exclusively focuses on norm-induced dynamics from scale-invariant linear layers and does not consider correlations in gradient magnitudes. Its applicability is limited for architectures with higher degree nonlinearities, such as Transformer models, where base gradients can vary significantly more, and attention blocks are not scale-invariant, violating the model's core assumptions (inverse scaling property and orthogonality). In real training with super-critical learning rates, auto-rate tuning effects can become too weak compared to base gradient fluctuations, causing ELRs to not converge. Numerical issues were also observed for very high learning rates during short-term validation.",
        "future_research_directions": "Future research could extend the current model to include an error analysis for non-constant base gradients, aiming to estimate when auto-rate tuning effects might vanish due to other dynamic factors. Another potential direction is to investigate the weight dynamics of networks optimized with momentum SGD, as the current work deliberately excludes its influence to simplify the analysis."
      }
    },
    {
      "title": "Temperature Balancing, Layer-wise Weight Analysis, and Neural Network Training",
      "abstract": "Regularization in modern machine learning is crucial, and it can take various\nforms in algorithmic design: training set, model family, error function,\nregularization terms, and optimizations. In particular, the learning rate,\nwhich can be interpreted as a temperature-like parameter within the statistical\nmechanics of learning, plays a crucial role in neural network training. Indeed,\nmany widely adopted training strategies basically just define the decay of the\nlearning rate over time. This process can be interpreted as decreasing a\ntemperature, using either a global learning rate (for the entire model) or a\nlearning rate that varies for each parameter. This paper proposes TempBalance,\na straightforward yet effective layer-wise learning rate method. TempBalance is\nbased on Heavy-Tailed Self-Regularization (HT-SR) Theory, an approach which\ncharacterizes the implicit self-regularization of different layers in trained\nmodels. We demonstrate the efficacy of using HT-SR-motivated metrics to guide\nthe scheduling and balancing of temperature across all network layers during\nmodel training, resulting in improved performance during testing. We implement\nTempBalance on CIFAR10, CIFAR100, SVHN, and TinyImageNet datasets using\nResNets, VGGs, and WideResNets with various depths and widths. Our results show\nthat TempBalance significantly outperforms ordinary SGD and carefully-tuned\nspectral norm regularization. We also show that TempBalance outperforms a\nnumber of state-of-the-art optimizers and learning rate schedulers.",
      "full_text": "Temperature Balancing, Layer-wise Weight Analysis, and Neural Network Training Yefan Zhou1∗, Tianyu Pang2,3∗, Keqin Liu2,3, Charles H. Martin4, Michael W. Mahoney5,6,7, Yaoqing Yang1 1 Department of Computer Science, Dartmouth College 2 Department of Mathematics, Nanjing University 3 National Center for Applied Mathematics 4 Calculation Consulting 5 Department of Statistics, University of California at Berkeley 6 International Computer Science Institute 7 Lawrence Berkeley National Laboratory Abstract Regularization in modern machine learning is crucial, and it can take various forms in algorithmic design: training set, model family, error function, regularization terms, and optimizations. In particular, the learning rate, which can be interpreted as a temperature-like parameter within the statistical mechanics of learning, plays a crucial role in neural network training. Indeed, many widely adopted training strategies basically just define the decay of the learning rate over time. This process can be interpreted as decreasing a temperature, using either a global learning rate (for the entire model) or a learning rate that varies for each parameter. This paper proposes TempBalance, a straightforward yet effective layer-wise learning rate method. TempBalance is based on Heavy-Tailed Self-Regularization (HT-SR) Theory, an approach which characterizes the implicit self-regularization of different layers in trained models. We demonstrate the efficacy of using HT-SR-motivated metrics to guide the scheduling and balancing of temperature across all network layers during model training, resulting in improved performance during testing. We implement TempBalance on CIFAR10, CIFAR100, SVHN, and TinyImageNet datasets using ResNets, VGGs and WideResNets with various depths and widths. Our results show that TempBalance significantly outperforms ordinary SGD and carefully-tuned spectral norm regularization. We also show that TempBalance outperforms a number of state-of-the-art optimizers and learning rate schedulers. 1 Introduction Having a learning rate schedule that gradually decreases over time is crucial for the convergence and perfor- mance of state-of-the-art machine learning algorithms. Indeed, many optimization algorithms essentially boil down to designing a progression of parameter updates, as realized by different learning rate schedules (Duchi et al., 2011; Kingma and Ba, 2014; Smith, 2017; Yao et al., 2021). Common schedules assign a global learning rate per epoch, where the same learning rate is used for all layers in the model. This includes the family of cyclical learning rates (Smith, 2017), and parameter-wise learning rate schedules like Adam (Kingma and Ba, 2014) and its variants (Liu et al., 2020; Zhuang et al., 2020). However, such a global learning rate schedule does not take into account the structural characteristics of neural networks (NNs). At the same time, parameter-wise learning rate schedules are sometimes used, but they have long been conjectured to have worse generalization performance than carefully tuned stochastic gradient descent ( SGD) optimizers (Wilson et al., 2017), and storing both first and second-order moments for each parameter can lead ∗First two authors contributed equally. 1 arXiv:2312.00359v1  [cs.LG]  1 Dec 2023to substantially increased memory consumption (Singh et al., 2015). As mentioned in Smith et al. (2022), storing the whole Megatron-Turing NLG requires 10 terabytes of aggregate memory, and the Adam optimizer’s first and second-order moments (Kingma and Ba, 2014) consume 40% of the memory. Nonetheless, improving parameter-wise learning rate schedules is an active field of study (Loshchilov and Hutter, 2017, 2019; Yao et al., 2021; Zhuang et al., 2020). A largely under-explored idea to reconcile the two extremes of setting a single global learning rate or assigning fine-grained parameter-level learning rates is to assign layer-wise learning rates. Such a learning rate assignment method does not require much storage cost, and it can assign very different training speeds to different layers. However, existing layer-wise schemes are often introduced as an additional part of hyperparameter sweeping, thus substantially increasing computational cost; and most lack a strong (or any) theoretical foundation. For instance, layer-wise learning rates can increase test accuracy in transfer learning (Howard and Ruder, 2018) and domain adaptation (Long et al., 2015), but these learning rates are often empirically tuned. More recently, motivated by the intuition that lower-level layers should be domain-specific and higher-level layers should be task-specific, Chen et al. (2023) automates the search for an optimal set of learning rates. However, the authors find the nested, bi-level optimization scheme to be too computationally expensive in practice (Franceschi et al., 2018). AutoLR also automatically tunes its layer-wise learning rates according to the “role” of each layer (Ro and Choi, 2021). The method is validated almost entirely by empirical results, further explained by layer-wise weight variations. While the authors attempt to assign a different initial learning rate to each layer, the learning rate for each layer continues to stay largely constant throughout training. LARS (You et al., 2017, 2018) is another method to assign layer-wise learning rate. It is based on the “trust ratio,” defined as the ratio of weight norm to gradient update norm of each layer, and it is specifically used in large batch training to avoid gradient divergence. In this paper, we propose TempBalance, a simple yet effective layer-wise learning rate assignment (and regularization) method, grounded in Heavy-Tail Self Regularization (HT-SR) Theory (Martin and Mahoney, 2017, 2019, 2020, 2021a,b; Martin et al., 2021). Our approach leverages HT-SR Theory to assess the quality of each network layer. This is achieved through an analysis of the heavy-tail (HT) structure present in the Empirical Spectral Density (ESD) of NN weight matrices. Given this information, TempBalance meticulously adjusts the temperature-like parameter to control each layer’s quality, with the objective of ensuring consistently high quality across all layers of the network. From the statistical physics viewpoint on learning and optimization (Engel and den Broeck, 2001; Haussler et al., 1994; Martin and Mahoney, 2017; Seung et al., 1992; Watkin et al., 1993), a temperature-like parameter refers to some quantity related to the empirical noise/stochasticity of the learning process. This is the noise scale described by Smith and Le (2018); Smith et al. (2018), and it can be written as a function of learning rate, batch size, and momentum. Prior research (Gurbuzbalaban et al., 2021; Martin and Mahoney, 2021b) has shown that temperature-like parameters significantly influence HT structure in the ESD. Our approach, TempBalance, focuses on the strategic adjustment of the learning rate as the temperature-like parameter, thereby facilitating accurate control of the quality across each network layer, as characterized by its HT ESD structure. The following paragraph will delve deeper into the importance of HT-SR, highlighting its connection to the concept of layer-wise temperature. HT-SR Theory. HT-SR Theory (Martin and Mahoney, 2017, 2019, 2020, 2021a,b; Martin et al., 2021) relies on the empirical fact that very well-trained models tend to exhibit strong correlations, resulting in HT structure in the ESD of each layer. To obtain this ESD, we take a NN with L layers and the corresponding weight matrices W1, W2, ··· , WL with shape n × m (where n ≤ m). For the i-th layer, we calculate the eigenvalues of its correlation matrix Xi = WT i Wi, and then we plot the ESD for that layer. Upon training, the ESD will typically gradually change to have an HT structure (Martin and Mahoney, 2021b; Martin et al., 2021). We can then fit a power law (PL) distribution to the HT part of the ESD, and extract its exponent as, namely, PL Alpha. The fitted PL will have the following formula: p(λ) ∝ λ−α, λ min < λ < λmax. (1) The PL Alpha metric measures the PL exponent of the weight matrices’ ESD. Its underlying motivation stems from random matrix theory and statistical physics, as well as the empirical observation that HT ESDs are ubiquitous in well trained NN models (Martin and Mahoney, 2019; Martin et al., 2021). 2Figure 1: Examples of PL fitting. Blue histograms depict the ESDs. Vertical black lines indicate the lower threshold λmin used to truncate the full ESDs and extract the tail portion. Solid red curves represent the tail part of the ESDs truncated by λmin, while dashed red curves represent the fitted HT distributions. The left shows a more HT ESD, which requires a relatively lower learning rate. The right one shows a less HT ESD, which requires a relatively higher learning rate. Unlike prior work, we do not aim to find the “optimal” PL exponent. (Thus, we are less interested in obtaining a precise estimate than in obtaining a robust estimate.) Instead, we use the PL exponent to rank ESDs to find layers that need higher/lower learning rates. These two ESDs correspond to two layers of a ResNet18 model trained on TinyImageNet. The PL Alpha metric has been shown to predict the trends in the test accuracy of state-of-the-art models in computer vision (CV) and natural language processing (NLP), without even the need for access to training or testing data (Martin et al., 2021; Yang et al., 2023). According to Martin et al. (2021), one can aggregate PL Alpha’s for different layers either by simple averaging or weighted averaging, and each can predict test accuracy in different cases (Martin et al., 2021; Yang et al., 2023). Furthermore, the layer-wise nature of PL Alpha makes it a fine-grained metric that can be used to assess the quality of individual layers of the network. Thus, in this paper, we extend and apply HT-SR Theory (originally designed as a predictive diagnostic for analyzing pre-trained NN models) to NN training, and we exploit the layer-wise information provided by PL Alpha to determine the layer-wise learning rates for better test accuracy. We note that, while it provides perhaps the most principled approach, the PL Alpha metric is not the only way to try to measure the HT structure in NN models. Several recent papers (Agrawal et al., 2022; Nassar et al., 2020; Xie et al., 2022a) use different HT metrics to try to measure the spectral properties of other matrices (such as input/output covariance matrices, Fisher Information Matrices, and the Hessian). We show in Appendix A that these HT phenomena, measured in different ways on different matrices, are closely related to each other. On the other hand, this also means that (for the problems considered in this paper) the absolute numerical value of PL Alpha is less important, as optimal PL exponents estimated by different algorithms can be different (Agrawal et al., 2022; Martin et al., 2021). What matters the most, as we show in this paper, is the layer-wise quality ranked by the PL exponent: layers with a smaller PL Alpha tend to be relatively more “overtrained,” and layers with a larger PL Alpha tend to be relatively more “undertrained.” (We emphasize that this is true for the training problem we consider in this paper—for prior HT-SR work, the actual numerical value of PL Alpha mattered a lot.) This observation leads to a simple and efficient way to balance layer-wise learning rates: assign a lower learning rate to more overtrained layers and a larger learning rate to more undertrained layers, using PL Alpha (see Figure 1). In implementing this learning rate balancing approach, we use a scale-free method to map the PL Alpha value of each layer to a predefined learning rate range. This range is established in relation to a global learning rate. Rather than depending on the absolute numerical values of PL Alpha for each layer, this method emphasizes the importance of their relative differences and quality ranking. As a result, the learning rates assigned to individual layers remain stable and unaffected by arbitrary linear scaling of PL Alpha estimates, whether they arise from the choice of the estimator or the presence of noisy measurements. On 3top of this, we can perform a grid search on the global learning rate. This is standard practice, and it is more efficient than grid-searching the layer-wise learning rates. We use this combination of assigning layer-wise learning rates using PL Alpha and grid-searching the base global learning rate to avoid having to decide the “optimal” PL exponent, as this can be tricky due to different ways of measuring HT properties. Indeed, there are different ways to measure PL Alpha (Martin and Mahoney, 2021b), and we use the Hill estimator (Hill, 1975). While not necessarily the best estimate (see Martin and Mahoney (2021b); Martin et al. (2021)), it shows stable performance in our experiments. We refer to our version of the PL Alpha metric as the PL Alpha Hill metric, and we use it for the remainder of the paper. Another popular way to change the ESD of weights is to constrain the spectral norm (i.e., the largest eigenvalue) using spectral norm regularization ( SNR) (Miyato et al., 2018; Yoshida and Miyato, 2017). SNR provides a different form of regularization, compared to HT-SR, because it regulates the largest eigenvalue instead of the ESD slope (i.e., the PL Alpha Hill metric). It has been demonstrated that the spectral norm and PL Alpha Hill serve distinct roles in evaluating model quality, and their combined form yields optimal predictions for test accuracy trends (Martin and Mahoney, 2021a,b; Martin et al., 2021; Yang et al., 2023). To complement this, our results demonstrate that TempBalance outperforms SNR in training deep NNs in most cases. Moreover, when these two regularization methods are combined during training, they result in optimal test accuracy, thereby confirming their complementary roles. As described in Martin and Mahoney (2021a); Yang et al. (2023), the spectral norm and PL Alpha Hill measure the scale and the shape of a ESD, respectively; and regulating both the scale and shape is crucial for achieving better ESD regularization. We provide ablation studies on several layer-wise metrics for assigning layer-wise learning rates, including spectral norm, and we show that PL Alpha Hill performs the best among them. Our main contributions. The following summarizes our main contributions. 1 • We propose a simple yet effective layer-wise learning rate schedule,TempBalance, which is motivated by HT- SR Theory. Based on our empirical results, we obtain two main high-level insights. First, the mapping from PL Alpha Hill to learning rates should be scale-free, meaning that arbitrary linear scaling on the estimated PL exponent should not change the learning rate assignment. Second, searching for the minimum eigenvalue λmin, a standard practice in PL fitting (Alstott et al., 2014; Clauset et al., 2009; Martin and Mahoney, 2021b), leads to unstable training. To improve stability, we instead fix λmin as the median of the ESD. • We compare TempBalance to ordinary SGD and SNR on various training tasks. This includes (1) different network architectures, such as ResNet, VGG, WideResNet, (2) different datasets, such as CIFAR10, CIFAR100, SVHN, TinyImageNet, and (3) ablation studies, such as varying widths, depths, initial learning rates, HT-SR layer-wise metrics, and PL fitting methods. Compared to ordinary SGD, TempBalance achieves higher test accuracy by setting layer-wise learning rates. Compared to SNR, TempBalance performs better by providing a more fine-grained regularization on shape/slope instead of norm. We also show that combining TempBalance and SNR leads to further improved accuracy, verifying their complementary roles in informing deep learning training. • We compare TempBalance to a range of state-of-the-art optimizers and learning rate schedulers, including SGDR (Loshchilov and Hutter, 2017), LARS (You et al., 2017, 2018), Lookahead (Zhang et al., 2019) and SGDP (Heo et al., 2021) on ResNet18 and ResNet34 trained on CIFAR100. We show that TempBalance achieves the highest test accuracy. We do careful hyperparameter tuning for all baselines. All results are obtained from five random seeds. • We use ablation studies to show that PL Alpha Hill provides the best test accuracy among several layer- wise metrics considered by HT-SR (Martin et al., 2021; Yang et al., 2023). We also show that TempBalance maintains stable performance over SGD baselines when the model size changes. Furthermore, we show visualization results in Appendix B, verifying that TempBalance controls ESDs during training. 2 Related Work Here, we give an overview of the statistical mechanics of learning and recent progress in theoretical and empirical studies on generalization metrics and their applications. 1Our code is open-sourced: https://github.com/YefanZhou/TempBalance. 4Statistical mechanics of learning and HT-SR. Our paper is motivated by statistical mechanics of learning (Hopfield, 1982; Rere et al., 2015; Sompolinsky, 1988), and especially by works that connect load-like (Barra and Guerra, 2008; Barra et al., 2012; Hopfield, 1982) and temperature-like parameters (Brush, 1967; Seung et al., 1992) to NNs. According to prior works in this area (Martin and Mahoney, 2017; Yang et al., 2021), a temperature-like parameter represents the amount of noise/variance in an iteration of SGD, such as learning rate, weight decay parameters, and batch size. A load-like parameter represents the quantity and/or quality of data relative to the size of the learning model. To measure the quality of publicly-available pre-trained NNs, Martin and Mahoney (2021b) introduce HT-SR Theory, showing that the weight matrices of deep NNs exhibit HT ESDs, and they show that a decay coefficient of ESD, PL Alpha, effectively gauges model quality. Subsequently, Gurbuzbalaban et al. (2021); Hodgkinson and Mahoney (2021); Hodgkinson et al. (2022); Raj et al. (2023); Simsekli et al. (2019, 2020) provide rigorous bounds for HT phenomenon and generalization, further adding support to HT-SR Theory. HT-SR has also been applied to predicting trends in test accuracy of large-scale NNs, in both CV and NLP (Martin and Mahoney, 2021a; Martin et al., 2021; Yang et al., 2023), but it has yet to be systematically incorporated to novel training algorithms. Recently, more papers realize the important connections between deep NNs and statistical mechanics of learning (Martin and Mahoney, 2017). To name a few, Yang et al. (2021) use load and temperature parameters to study a wide range of loss landscapes, providing a taxonomy from the perspective of global structure of a loss landscape. On the theory side, Baity-Jesi et al. (2018) investigates the glassy behavior of NNs, and Barbier et al. (2019) derives the optimal generalization error of generalized linear systems. More recently, Sorscher et al. (2022) studies easy versus hard samples used in training and design a “data-pruning” method; and Zhou et al. (2023) establishes a “three-regime model” in network pruning, unifying multiple practical hyperparameter tuning methods in a principled way. Generalization measures. The search for effective and robust generalization metrics (which, importantly, can be very different than model quality metrics (Yang et al., 2023)) has been the focus of several recent theoretical and empirical works (Bartlett et al., 2017; Dziugaite et al., 2020; Jiang et al., 2019; Martin et al., 2021; Yang et al., 2021, 2023). Several recent papers apply metric-informed training and architecture search, such as those based on the Hessian (Dong et al., 2019; Shen et al., 2020; Yang et al., 2022; Yao et al., 2021), spectral norm (Miyato et al., 2018; Yoshida and Miyato, 2017), stable rank (Sanyal et al., 2020), and the spectrum of the neural tangent kernel (Chen et al., 2021). However, most generalization metrics, such as those based on the PAC-Bayes bounds (Dziugaite and Roy, 2017; Langford and Shawe-Taylor, 2002; McAllester, 1998; Neyshabur et al., 2018), do not straightforwardly transfer to layer-wise quality metrics, because such generalization metrics often study the whole NN as an architecture-free function, and they lack the fine granularity to unveil the quality of each layer. Also, it has been mentioned in the literature (Jiang et al., 2019) that (1) directly regularizing generalization metrics can lead to difficulty in training, (2) evaluating these regularization methods may be hard due to the existence of implicit regularization in SGD, and (3) these metrics, especially norm-based metrics, cannot be expected to correlate with test accuracy causally (Dziugaite et al., 2020), making the link between these generalization metrics and practical training methods nuanced. It will be clear in the next section that we do not regularize ESD metrics directly. Instead, we change learning rates to modify ESDs. 3 The TempBalance Algorithm In this section, we introduce our simple yet effective method TempBalance, based on the PL Alpha Hill metric from HT-SR Theory. For a NN, different layers tend to have different values for PL Alpha Hill, (Martin and Mahoney, 2017, 2021b): a layer with a larger PL Alpha Hill indicates that layer is relatively undertrained, while a layer with a smaller PL Alpha Hill indicates that layer is relatively overtrained. A natural idea is to adjust the degree of learning among different layers to get a balance: for a layer whose PL Alpha Hill is too large, we could assign a larger learning rate to accelerate its learning, and vice versa. The intuition of our method is transferring one layer’s learning rate to another and hence, TempBalance. The pipeline is in Figure 2. We provide the details of TempBalance in Algorithm 1. Based on PL Alpha Hill in different layers, we use the learning rate schedule function ft to map the i-th layer to a particular learning rate ft(i) in epoch t. 5(b) Layerwise Learning Rate Assignment More heavy-tailed (a) ESD Analysis and PL fitting Less heavy-tailed ESD PL fitting More heavy-tailed,  smaller LR Less heavy-tailed,  larger LR ESD PL fitting  ESD PL fitting Figure 2: The pipeline diagram of TempBalance. In each epoch, TempBalance undergoes two steps: (a) Performing ESD analysis on all layers and employing PL fitting to derive the layer-wise PL Alpha Hill, and (b) Using the layer-wise PL Alpha Hill to assign learning rates to each layer using an assignment function. We adopt ft as a linear map between the layer-wise PL Alpha Hill and the final layer-wise learning rate, which has the following formula: ft(i) = ηt · \u0014 αi t − αmin t αmax t − αmin t (s2 − s1) + s1 \u0015 , (2) where ηt means the base global learning rate in epoch t, (s1, s2) are the minimum and maximum learning rate scaling ratio relative to ηt, αi t represents the layer i’s PL Alpha Hill at the beginning of epoch t, and (αmin t , αmax t ) denote the minimum and maximum PL Alpha Hill across all the layers in epoch t. Using (2), we ensure that the new learning rate ft(i) is a scaled version of the original base learning rate ηt and is always inside the interval [s1ηt, s2ηt]. Note that ( s1, s2) serves as tunable hyperparameters in our method. We conducted ablation studies on it, which are detailed in Appendix C. The hyperparameter values used across all experiments can be found in Appendix D. Our studies reveal that the optimal results are usually achieved around (0.5, 1.5). To fit the PL distribution p(λ) defined in (1), we use the Hill estimator (Hill, 1975; Xiao et al., 2023). (It is not the best estimator for fine-scale diagnostics based on HT-SR Theory (Martin and Mahoney, 2021b; Martin et al., 2021), but it is robust, and it suffices for our purposes.) For the i-th layer, suppose the weight matrix is Wi and the correlation matrix W⊤ i Wi has ascending eigenvalues {λi}n i=1. Then, the Hill estimator calculates PL Alpha Hill using the following: PL Alpha Hill = 1 + k (Pk i=1 lnλn−i+1 λn−k ) , (3) where k is the adjustable parameter. We adopt k = n 2 in our experiments. Note that changing k essentially 6Algorithm 1: TempBalance Input: M: Deep NN, T: Total training epoch, t: Current epoch, αi t: ith layer’s PL Alpha Hill at epoch t, ηt: Baseline global learning rate at epoch t, s1, s2: Minimum and maximum scaling ratio, ft: Learning rate schedule function 1 Initialize model M; 2 for t ← 0 to T do 3 Compute αi t for all layers using the Hill estimator; 4 Leverage all αi t and adopt ft in (2) to assign per-layer learning rate ft(i) between s1ηt and s2ηt for the next epoch; 5 Update the optimizer for the next epoch; 6 end changes the lower eigenvalue threshold λmin for (truncated) PL estimation, as shown by the vertical black line in Figure 1. Choosing k = n 2 means using the largest half of the eigenvalues to estimate the slope. We empirically find that fixing k for all layers leads to more stable performance than searching k for different layers (e.g., optimizing k using the Kolmogorov–Smirnov test (Alstott et al., 2014), as is needed for other applications of HT-SR Theory (Martin and Mahoney, 2021b; Martin et al., 2021)). One advantage of mapping PL Alpha Hill to learning rates using (2) is that the scale of PL Alpha Hill is unimportant, i.e., linearly scaling PL Alpha Hill arbitrarily does not change the learning rate assignment because the linear scaling cancels each other in (2). This can maximally reduce the artifact of estimating the ESD PL exponent/slope due to estimation noise, which has been found to be a tricky issue in practice (Martin and Mahoney, 2021a,b). 4 Empirical results In this section, we give full details of the experimental setup (Section 4.1) and compare our method TempBalance to a few baselines (Section 4.2), and then (Section 4.3) we perform ablation studies on varied initial learning rates, model widths, HT-SR layer-wise metrics, and PL fitting methods. 4.1 Experimental setup Datasets. We consider CIFAR100, CIFAR10, SVHN and Tiny ImageNet (TIN) (Deng et al., 2009; Krizhevsky et al., 2009; Le and Yang, 2015; Sermanet et al., 2011). CIFAR100 consists of 50K pictures for training and 10K pictures for testing with 100 categories. CIFAR10 consists of 50K pictures for training and 10K pictures for testing with 10 categories. SVHN consists of around 73K pictures for training and around 26K pictures for testing with 10 categories. Tiny ImageNet consists of 10K pictures for training and 10K images for testing with 200 classes. Models. We mainly consider three types of NNs: VGG, ResNet, and WideResNet (WRN) (He et al., 2016; Simonyan and Zisserman, 2014; Zagoruyko and Komodakis, 2016). For each network, we consider two different size options. For VGG, we consider VGG16 and VGG19. For ResNet, we consider ResNet18 and ResNet34. For WideResNet, we consider WRN16-8 and WRN28-6. Also, for ResNet and VGG, we consider three different widths for ablation studies. Hyperparameters. One baseline is ordinary SGD training with a cosine annealing learning rate schedule (CAL), which follows the formula: ηt = η0 2 \u0000 1 + cos \u0000t·π T \u0001\u0001 , where t is the current epoch, T represents the total training epochs, and η0 is the initial learning rate. We grid search the optimal initial (base) learning rate η0 for the CAL baseline, using the grid {0.05, 0.1, 0.15} for ResNet and {0.025, 0.05, 0.1} for VGG. The momentum and weight decay are 0 .9 and 5 × 10−4, respectively, which are both standard choices. Another baseline is spectral norm regularization ( SNR). Prior work (Yoshida and Miyato, 2017) uses the 7(a) ResNet18, CIFAR100  (b) ResNet34, CIFAR100  (c) VGG16, CIFAR100  (d) VGG19, CIFAR100 (e) ResNet18, TIN  (f) ResNet34, TIN  (g) WRN16-8, TIN  (h) WRN28-6, TIN (i) ResNet18, CIFAR10  (j) VGG16, CIFAR10  (k) ResNet18, SVHN  (l) VGG16, SVHN Figure 3: (Main result). Comparing our method TempBalance (TB) to CAL and SNR. Our method TempBalance outperforms CAL and SNR in almost all the settings except for VGG19 and ResNet 34 on CIFAR 100. For all experiments, combining TempBalance and SNR (TB+SNR) yields the best performance. All baselines are carefully tuned. All results are obtained by running five random seeds. See Appendix D for the details in all hyperparameters. SNR objective: min Θ 1 n nX i=1 l (fΘ (xi) , yi) + λsr 2 LX l=1 σ (Wl)2 , (4) where λsr is the SNR coefficient, σ(Wl) is the largest eigenvalue, i.e., spectral norm of weight matrix Wl, and L is the number of layers. We use the power iteration method to calculate σ(Wl) in our experiments. For SNR, we grid search the optimal regularization coefficient λsr, and we again adopt the CAL schedule for SNR, similar to the CAL baseline. To make our results fully reproducible, we report in Appendix D all hyperparameters, random seeds, and all numerical values of experimental results shown in the figures. 4.2 Comparing TempBalance and multiple baseline methods. First, we compare TempBalance to two baseline training methods. See results in Figure 3. In the figure, CAL means SGD training with a CAL learning rate schedule, and SNR means SGD trained with spectral norm regularization. TB means our method TempBalance, and TB + SNR means TempBalance combined with SNR. All error bars are obtained from five random seeds. From Figure 3, we see that TempBalance outperforms the CAL baseline in all settings. In almost all cases, it performs better than SNR baseline. When TempBalance does not outperform SNR, combining SNR with TempBalance leads to better test accuracy. Second, we compare our method to a number of optimizers and learning rate schedulers that are not necessarily related to ESD of weights. These include SGDR (Loshchilov and Hutter, 2017), SGDP (Heo et al., 2021), Lookahead (Zhang et al., 2019) and LARS (You et al., 2017, 2018), and we compare these baselines with TempBalance for ResNet18 and ResNet34 trained on CIFAR100. SGDR is stochastic gradient descent with warm restarts. SGDP modifies the ordinary SGD to compensate for the effect of increasing weight norm. Lookahead (Zhang et al., 2019) modifies SGD by letting each gradient update approximate the future 8(a) ResNet18, CIFAR100  (b) ResNet34, CIFAR100 Figure 4: (More baseline optimizers). Comparing our method TempBalance (TB) to cosine annealing (CAL) baseline and other state-of-the-art optimizers and learning rate schedulers for ResNet18 and ResNet34 trained on CIFAR100. Crosses for the same method represent different hyperparameter settings. Each cross represents the mean test accuracy of five random seeds. The best performing model thus far is TB combined with SGDP. trajectory of multiple updates. LARS assigns layer-wise learning rates based on the so-called “trust-ratio” and is the closest to our method. Results in Figure 4 show that TempBalance outperforms these baselines, and TempBalance combined with SGDP is the best-performing method. The crosses on each column represent training runs with different hyperparameters. Note that there are several other methods based on modifying the Adam optimizer (Kingma and Ba, 2014), such as AdamW (Loshchilov and Hutter, 2019), AdamP (Heo et al., 2021) and LAMB (You et al., 2020). However, we do not find them to provide better results than the SGD baseline with cosine annealing ( CAL in Figure 4). The results are detailed in Appendix E. 4.3 Corroborating results and ablation studies. In addition to the main results (Figures 3 and 4), we provide corroborating results and ablation studies. Experiment one: tuning initial learning rate η0. We train models from scratch using TempBalance versus CAL with various initial learning rates, comparing TempBalance and the CAL baseline when both methods are allowed to search for the optimal hyperparameters. We again use ResNet18, ResNet34, VGG16 and VGG19 as our architectures and show results on CIFAR100. Results in Figure 5 show that TempBalance achieves a higher test accuracy than CAL for both ResNet and VGG. Experiment two: varying channel width. We view the fraction of model width in Experiment one as “100%,” and we experiment with models with varied widths in [50% , 100%, 150%]. We again used ResNet18, ResNet34, VGG16 and VGG19, and trained on CIFAR100, and we grid search for the optimal learning rate for each width to get the best accuracy. Results in Figure 6 show we find that TempBalance outperforms the baseline for all widths. Experiment three: varying HT-SR metric . We use different HT-SR metrics to assign layer-wise learning rates. That is, we replace the layer-wise PL Alpha Hill in (2) with other HT-SR metrics including SpectralNorm and AlphaWeighted (Martin et al., 2021). Results in Figure 7 show that PL Alpha Hill achieves the optimal test accuracy. Experiment four: varying PL fitting methods. The HT-SR metric PL Alpha Hill is derived through PL fitting, which is influenced by the choice of hyperparameter λmin. More specifically, this involves determining the adjustable parameter k as per Equation 3. Past research has employed various methods to select λmin based on the task, such as performance prediction. For instance, Clauset et al. (2009); Martin et al. (2021) choose λmin that aligns with the best fit according to the Kolmogorov-Smirnov statistic (Alstott et al., 2014), a method termed Goodness-of-fit. Meanwhile, Yang et al. (2023) adopted the Fix-finger 9(a) ResNet18, CIFAR100  (b) ResNet34, CIFAR100  (c) VGG16, CIFAR100  (d) VGG19, CIFAR100 Figure 5: (Tuning initial learning rate). Comparing the test accuracy of TempBalance (red) and CAL baseline (blue) for varying initial learning rate. Our method TempBalance outperforms CAL for both ResNet and VGG trained on CIFAR100. All results are obtained by running five random seeds. (a) ResNet18, CIFAR100  (b) ResNet34, CIFAR100  (c) VGG16, CIFAR100  (d) VGG19, CIFAR100 Figure 6: (Different widths). Comparing TempBalance and the CAL baseline for different network widths. Our method TempBalance consistently outperforms the CAL baseline across various network widths for both ResNet and VGG trained on CIFAR100. All results are obtained by running five random seeds. (a) ResNet18, CIFAR100  (b) ResNet34, CIFAR100  (c) VGG16, CIFAR100  (d) VGG19, CIFAR100 Figure 7: (Different HT-SR metrics). Comparing PL Alpha Hill with multiple HT-SR metrics. PL Alpha Hill achieves the best test accuracy among these metrics. All results are obtained by run- ning five random seeds. approach, which identifies λmin at the peak of the ESD. In our study, we designate λmin as the median of all eigenvalues present in the ESD for TempBalance. As depicted in Figure 8, our fitting method, termed Median, not only ensures optimal test accuracy but also notably decreases computation time. This shows that this PL fitting method is suited for the design of learning rate schedulers that demand low computation overhead. Empirical analysis results. We conduct an empirical analysis of TempBalance to discuss why it 10(a) ResNet18, CIFAR100  (b) ResNet34, CIFAR100  (c) VGG16, CIFAR100  (d) VGG19, CIFAR100 Figure 8: (Varying PL fitting method to determine the λmin). Results of using different PL fitting methods. The blue bar plot and the left y-axis label denote the test accuracy (higher the better), and the red line plots and the right y-axis label denote the time in seconds of using TempBalance once (lower the better). Our design ( Median) used in the proposed method achieves higher test accuracy and takes lower computation times compared to Goodness-of-fit and Fix-finger. The test accuracy is averaged over five random seeds and computation time is averaged over ten times. provides improvement. Our first analysis involves visualization to demonstrate how TempBalance effectively regularizes ESDs by scheduling the learning rate (see Appendix B). The second analysis strengthens the connections between TempBalance and HT structure, illustrating that the observed improvements are not due to indirectly addressing other training issues, such as gradient excursions (Pascanu et al., 2013) (see Appendix F). Corroborating results on other tasks. We extend our evaluation of TempBalance to two additional tasks: object detection and language modeling, the details of which can be found in Appendix G. Across these tasks, TempBalance consistently outperforms the baseline CAL in terms of generalization. 5 Conclusion Our extensive empirical evaluations demonstrate that TempBalance offers a straightforward yet effective layer-wise learning rate schedule. Our approach for balancing layer-wise temperature confirms the following: (i) HT-SR-motivated metric PL Alpha Hill helps layers achieve temperature balance during training, exhibits strong correlations with model quality, and yields improved performance during testing; (ii) temperature balancing is a novel and essential aspect of NN training, and HT-SR Theory provides a strong theoretical support for balancing temperatures; and (iii) layer-wise learning rate schedules are cheap and effective to apply, and it is useful to study these layer-wise learning rate schedules further. Our method provides insights into the study of layer-wise tuning approaches and load-temperature balancing in deep NN training, as it serves both as a layer-wise learning rate schedule and an effective regularization technique based on HT-SR Theory. Future directions, limitations, and societal impacts. Our paper leaves many future directions to explore, of which we mention just a few. • Can HT-SR metrics be extended to parameter-wise learning rate schedules, global learning rate schedules, or other hyperparameters? It would be of interest to observe how HT-SR can assist in acquiring a comprehensive set of hyperparameter tuning tools. • Is it possible to accelerate the computation of ESDs and PL Alpha Hill to achieve a more adaptive learning rate scheduler? Currently, we calculate layer-wise PL Alpha Hill once per epoch, resulting in a minimal increase in computational cost. Consider the example of training ResNet18 for 200 epochs on CIFAR100. Calculating layer-wise PL Alpha Hill takes 1.14 seconds for each epoch, leading to 3.8 minutes in total. Training CIFAR100 on 1 Quadro RTX 6000 takes 59 minutes, and thus using TB increases 6% of training time. However, if we can significantly decrease the expense of computing ESDs, it might enable an optimizer that adjusts the learning rate every few gradient updates. A study on computation overhead is detailed in Appendix H. Our research centers around developing a generic algorithm for optimizing NNs. Although TempBalance 11could be applied to learning models with adverse applications, we do not see any immediate negative societal impacts stemming from the algorithm itself. Indeed, we see a lot of societal value in using a practical, predictive, and quantitative theory, such as HT-SR Theory, as opposed to developing a method that relies on a theory that provides vacuous upper bounds and then relies on extremely expensive hyperparameter tuning to obtain good results. Acknowledgements. WeightWatcher is a publicly-available tool distributed under Apache License 2.0 with copyright held by Calculation Consulting. Our conclusions do not necessarily reflect the position or the policy of our sponsors, and no official endorsement should be inferred. References Kumar K Agrawal, Arnab Kumar Mondal, Arna Ghosh, and Blake Richards. $\\alpha$-req : Assessing representation quality in self-supervised learning by measuring eigenspectrum decay. In Advances in Neural Information Processing Systems, 2022. Jeff Alstott, Ed Bullmore, and Dietmar Plenz. Powerlaw: a python package for analysis of heavy-tailed distributions. PloS one, 9(1):e85777, 2014. Marco Baity-Jesi, Levent Sagun, Mario Geiger, Stefano Spigler, G´ erard Ben Arous, Chiara Cammarota, Yann LeCun, Matthieu Wyart, and Giulio Biroli. Comparing dynamics: Deep neural networks versus glassy systems. In International Conference on Machine Learning , pages 314–323, 2018. Jean Barbier, Florent Krzakala, Nicolas Macris, L´ eo Miolane, and Lenka Zdeborov´ a. Optimal errors and phase transitions in high-dimensional generalized linear models. Proceedings of the National Academy of Sciences, 116(12):5451–5460, 2019. Adriano Barra and Francesco Guerra. About the ergodic regime in the analogical hopfield neural networks: moments of the partition function. Journal of Mathematical Physics , 49(12):125217, 2008. Adriano Barra, Alberto Bernacchia, Enrica Santucci, and Pierluigi Contucci. On the equivalence of hopfield networks and boltzmann machines. Neural Networks, 34:1–9, 2012. Peter Bartlett, Dylan Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems , 2017. Peter L Bartlett, Philip M Long, G´ abor Lugosi, and Alexander Tsigler. Benign overfitting in linear regression. In Proceedings of the National Academy of Sciences , 2020. Stephen G. Brush. History of the lenz-ising model. Reviews of Modern Physics , 39:883–893, 1967. Wuyang Chen, Xinyu Gong, and Zhangyang Wang. Neural architecture search on imagenet in four gpu hours: A theoretically inspired perspective. In International Conference on Learning Representations, 2021. Yixiong Chen, Jingxian Li, Hua Jiang, Li Liu, and Chris Ding. Metalr: Layer-wise learning rate based on meta-learning for adaptively fine-tuning medical pre-trained models. In Medical Image Computing and Computer Assisted Intervention , 2023. Aaron Clauset, Cosma Rohilla Shalizi, and Mark EJ Newman. Power-law distributions in empirical data. SIAM review, 51(4):661–703, 2009. Romain Couillet and Zhenyu Liao. Random Matrix Methods for Machine Learning . Cambridge University Press, 2022. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition , 2009. 12Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine, 29(6):141–142, 2012. Zhen Dong, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. HAWQ: Hessian aware quantization of neural networks with mixed-precision. In IEEE/CVF International Conference on Computer Vision, 2019. John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research , 12(61):2121–2159, 2011. Ethan Dyer and Guy Gur-Ari. Asymptotics of wide networks from feynman diagrams. In International Conference on Learning Representations, 2020. Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. In Annual Conference on Uncertainty in Artificial Intelligence , 2017. Gintare Karolina Dziugaite, Alexandre Drouin, Brady Neal, Nitarshan Rajkumar, Ethan Caballero, Linbo Wang, Ioannis Mitliagkas, and Daniel M Roy. In search of robust measures of generalization. In Advances in Neural Information Processing Systems , 2020. Andreas Engel and Christian Van den Broeck. Statistical mechanics of learning . Cambridge University Press, 2001. Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International Journal of Computer Vision , 88:303–338, 2010. Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil. Bilevel programming for hyperparameter optimization and meta-learning. In International Conference on Machine Learning , 2018. Lingyu Gu, Yongqi Du, Yuan Zhang, Di Xie, Shiliang Pu, Robert Qiu, and Zhenyu Liao. ”lossless” compression of deep neural networks: A high-dimensional neural tangent kernel approach. In Advances in Neural Information Processing Systems, 2022. Mert Gurbuzbalaban, Umut Simsekli, and Lingjiong Zhu. The heavy-tail phenomenon in sgd. In International Conference on Machine Learning, 2021. David Haussler, H. Sebastian Seung, Michael Kearns, and Naftali Tishby. Rigorous learning curve bounds from statistical mechanics. In Proceedings of the Seventh Annual Conference on Computational Learning Theory, page 76–87, 1994. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition , 2016. Byeongho Heo, Sanghyuk Chun, Seong Joon Oh, Dongyoon Han, Sangdoo Yun, Gyuwan Kim, Youngjung Uh, and Jung-Woo Ha. Adamp: Slowing down the slowdown for momentum optimizers on scale-invariant weights. In International Conference on Learning Representations, 2021. Bruce M Hill. A simple general approach to inference about the tail of a distribution. The Annals of Statistics , pages 1163–1174, 1975. Liam Hodgkinson and Michael W Mahoney. Multiplicative noise and heavy tails in stochastic optimization. In International Conference on Machine Learning , pages 4262–4274, 2021. Liam Hodgkinson, Umut Simsekli, Rajiv Khanna, and Michael Mahoney. Generalization bounds using lower tail exponents in stochastic optimizers. In International Conference on Machine Learning, pages 8774–8795, 2022. 13John J Hopfield. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the National Academy of Sciences , 1982. Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics , pages 328–339, 2018. Arthur Jacot, Franck Gabriel, and Cl´ ement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems , 2018. Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic generalization measures and where to find them. In International Conference on Learning Representations, 2019. Ryo Karakida, Shotaro Akaho, and Shun-ichi Amari. Universal statistics of fisher information in deep neural networks: Mean field approach. In the 22nd International Conference on Artificial Intelligence and Statistics, 2019a. Ryo Karakida, Shotaro Akaho, and Shun-ichi Amari. Pathological spectra of the fisher information metric and its variants in deep neural networks. Neural Computation, 33:2274–2307, 2019b. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2014. Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 and cifar-100 datasets. 2009. John Langford and John Shawe-Taylor. Pac-bayes & margins. In Advances in Neural Information Processing Systems, 2002. Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015. Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari. The large learning rate phase of deep learning: the catapult mechanism. arXiv preprint arXiv:2003.02218 , 2020. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13 , pages 740–755, 2014. Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han. On the variance of the adaptive learning rate and beyond. In International Conference on Learning Representations, 2020. Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features with deep adaptation networks. In International Conference on Machine Learning , 2015. Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. In International Conference on Learning Representations, 2017. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou, Ming Zhou, and Dawei Song. A tensorized transformer for language modeling. Advances in neural information processing systems , 32, 2019. Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330, 1993. 14Charles H Martin and Michael W Mahoney. Rethinking generalization requires revisiting old ideas: statistical mechanics approaches and complex learning behavior. Technical Report Preprint: arXiv:1710.09553, 2017. Charles H Martin and Michael W Mahoney. Traditional and heavy tailed self regularization in neural network models. In International Conference on Machine Learning , 2019. Charles H Martin and Michael W Mahoney. Heavy-tailed universality predicts trends in test accuracies for very large pre-trained deep neural networks. In SIAM International Conference on Data Mining , 2020. Charles H Martin and Michael W Mahoney. Post-mortem on a deep learning contest: a Simpson’s paradox and the complementary roles of scale metrics versus shape metrics. Technical Report Preprint: arXiv:2106.00734, 2021a. Charles H Martin and Michael W Mahoney. Implicit self-regularization in deep neural networks: Evidence from random matrix theory and implications for learning. Journal of Machine Learning Research , 22(165): 1–73, 2021b. Charles H Martin, Tongsu Serena Peng, and Michael W Mahoney. Predicting trends in the quality of state-of-the-art neural networks without access to training or testing data. Nature Communications, 12(1): 1–13, 2021. David A McAllester. Some pac-bayesian theorems. In Proceedings of the Eleventh Annual Conference on Computational Learning Theory, 1998. Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. In International Conference on Learning Representations, 2018. Josue Nassar, Piotr Sokol, SueYeon Chung, Kenneth D Harris, and Il Memming Park. On 1/n neural representation and robustness. In Advances in Neural Information Processing Systems , 2020. Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-Bayesian approach to spectrally- normalized margin bounds for neural networks. In International Conference on Learning Representations, 2018. Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In International Conference on Machine Learning , pages 1310–1318, 2013. Yudi Pawitan. In all likelihood: statistical modelling and inference using likelihood . Oxford University Press, 2001. Anant Raj, Lingjiong Zhu, Mert Gurbuzbalaban, and Umut Simsekli. Algorithmic stability of heavy-tailed sgd with general loss functions. In International Conference on Machine Learning , pages 28578–28597, 2023. Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 779–788, 2016. LM Rasdi Rere, Mohamad Ivan Fanany, and Aniati Murni Arymurthy. Simulated annealing algorithm for deep learning. Procedia Computer Science, 72:137–144, 2015. Youngmin Ro and Jin Young Choi. Autolr: Layer-wise pruning and auto-tuning of learning rates in fine-tuning of deep networks. In Proceedings of the AAAI Conference on Artificial Intelligence , 2021. Amartya Sanyal, Philip H. Torr, and Puneet K. Dokania. Stable rank normalization for improved generalization in neural networks and gans. In International Conference on Learning Representations, 2020. 15Pierre Sermanet, Koray Kavukcuoglu, and Yann LeCun. Traffic signs and pedestrians vision with multi-scale convolutional networks. In Snowbird Machine Learning Workshop , 2011. Hyunjune Sebastian Seung, Haim Sompolinsky, and Naftali Tishby. Statistical mechanics of learning from examples. Physical Review A, 45(8):6056–6091, 1992. Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Q-BERT: Hessian based ultra low precision quantization of bert. In AAAI Conference on Artificial Intelligence, 2020. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 , 2014. Umut Simsekli, Levent Sagun, and Mert Gurbuzbalaban. A tail-index analysis of stochastic gradient noise in deep neural networks. In International Conference on Machine Learning , pages 5827–5837, 2019. Umut Simsekli, Ozan Sener, George Deligiannidis, and Murat A Erdogdu. Hausdorff dimension, heavy tails, and generalization in neural networks. Advances in Neural Information Processing Systems , 33:5138–5151, 2020. Bharat Singh, Soham De, Yangmuzi Zhang, Thomas Goldstein, and Gavin Taylor. Layer-specific adaptive learning rates for deep networks. In IEEE 14th International Conference on Machine Learning and Applications, 2015. Leslie N Smith. Cyclical learning rates for training neural networks. In 2017 IEEE Winter Conference on Applications of Computer Vision , pages 464–472, 2017. Samuel L Smith and Quoc V Le. A bayesian perspective on generalization and stochastic gradient descent. In International Conference on Learning Representations, 2018. Samuel L. Smith, Pieter-Jan Kindermans, and Quoc V. Le. Don’t decay the learning rate, increase the batch size. In International Conference on Learning Representations, 2018. Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yaz- dani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan Catanzaro. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990 , 2022. Haim Sompolinsky. Statistical mechanics of neural networks. Physics Today, 41(21):70–80, 1988. Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos. Beyond neural scaling laws: beating power law scaling via data pruning. In Advances in Neural Information Processing Systems , 2022. Timothy L. H. Watkin, Albrecht Rau, and Michael Biehl. The statistical mechanics of learning a rule. Rev. Mod. Phys., 65:499–556, Apr 1993. Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal value of adaptive gradient methods in machine learning. In Advances in Neural Information Processing Systems , 2017. Xuanzhe Xiao, Zeng Li, Chuanlong Xie, and Fengwei Zhou. Heavy-tailed regularization of weight matrices in deep neural networks. arXiv preprint arXiv:2304.02911 , 2023. Zeke Xie, Qian-Yuan Tang, Yunfeng Cai, Mingming Sun, and Ping Li. On the power-law hessian spectrums in deep learning. arXiv preprint arXiv:2201.13011 , 2022a. 16Zeke Xie, Qian-Yuan Tang, Zheng He, Mingming Sun, and Ping Li. Rethinking the structure of stochastic gradients: Empirical and statistical evidence. arXiv preprint arXiv:2212.02083 , 2022b. Huanrui Yang, Xiaoxuan Yang, Neil Zhenqiang Gong, and Yiran Chen. Hero: Hessian-enhanced robust optimization for unifying and improving generalization and quantization performance. In Proceedings of the 59th ACM/IEEE Design Automation Conference , 2022. Yaoqing Yang, Liam Hodgkinson, Ryan Theisen, Joe Zou, Joseph E Gonzalez, Kannan Ramchandran, and Michael W Mahoney. Taxonomizing local versus global structure in neural network loss landscapes. In Advances in Neural Information Processing Systems , 2021. Yaoqing Yang, Ryan Theisen, Liam Hodgkinson, Joseph E Gonzalez, Kannan Ramchandran, Charles H Martin, and Michael W Mahoney. Test accuracy vs. generalization gap: Model selection in nlp without accessing training or testing data. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining , pages 3011–3021, 2023. Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael W Mahoney. PyHessian: Neural networks through the lens of the hessian. In IEEE International Conference on Big Data , pages 581–590, 2020. Zhewei Yao, Amir Gholami, Sheng Shen, Mustafa Mustafa, Kurt Keutzer, and Michael Mahoney. Adahessian: An adaptive second order optimizer for machine learning. In Proceedings of the AAAI Conference on Artificial Intelligence, 2021. Yuichi Yoshida and Takeru Miyato. Spectral norm regularization for improving the generalizability of deep learning. arXiv preprint arXiv:1705.10941 , 2017. Yang You, Igor Gitman, and Boris Ginsburg. Scaling SGD batch size to 32k for ImageNet training. arXiv preprint arXiv:1708.03888, 6(12):6, 2017. Yang You, Zhao Zhang, Cho-Jui Hsieh, James Demmel, and Kurt Keutzer. Imagenet training in minutes. In Proceedings of the 47th International Conference on Parallel Processing , 2018. Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes. In International Conference on Learning Representations, 2020. Ka-Veng Yuen. Bayesian methods for structural dynamics and civil engineering . John Wiley & Sons, 2010. Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In Proceedings of the British Machine Vision Conference, 2016. Bohang Zhang, Du Jiang, Di He, and Liwei Wang. Rethinking lipschitz neural networks and certified robustness: A boolean function perspective. In Advances in Neural Information Processing Systems , 2022. Michael Zhang, James Lucas, Jimmy Ba, and Geoffrey E Hinton. Lookahead optimizer: k steps forward, 1 step back. In Advances in Neural Information Processing Systems , 2019. Yefan Zhou, Yaoqing Yang, Arin Chang, and Mahoney W Michael. A three-regime model of network pruning. In International Conference on Machine Learning , 2023. Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar C Tatikonda, Nicha Dvornek, Xenophon Papademetris, and James Duncan. Adabelief optimizer: Adapting stepsizes by the belief in observed gradients. Advances in Neural Information Processing Systems , 2020. 17Appendix A Heavy-tail phenomena in different DNN matrices are closely related Recently, several papers have separately studied HT structures in different types of matrices, including the Hessian, the Fisher Information Matrix (FIM), and input/output covariance matrices (Karakida et al., 2019a,b; Xie et al., 2022a). The results confirm that when NNs are well-trained, various matrices have HT properties. Among these works, there are two major ways to characterize the HT spectrum, namely the HT-shaped ESDs (such as PL Alpha Hill), or HT-shaped decaying eigenvalues (Agrawal et al., 2022; Nassar et al., 2020; Xie et al., 2022a). Our paper mainly uses the first way of characterizing the HT structure. On the other hand, the second way is to sort eigenvalues from largest to smallest and study the PL phenomena between the ordered eigenvalues and their index. Our experiments show fruitful connections between the PL phenomena manifested in different DNN matrices; if one matrix shows a PL spectrum, the other matrices often show something similar (Xie et al., 2022a). Thus, it is meaningful to ask why and how the PL phenomena in different prior works correlate. This section first establishes the connections between input/output covariance matrices, the FIM and the Hessian in subsection A.1. We find that if one of these matrices shows the PL phenomenon, the other two matrices have a high chance to exhibit a similar PL phenomenon. Then, in subsection A.2, we derive the connection between our metric PL Alpha Hill and the PL exponent on decaying eigenvalues, showing a simple reciprocal relationship between these two. A.1 Connections between different matrices Consider a NN fθ : Rd → RC, where θ ∈ RP is the vectorized weights, d is the input dimension, and C is the output dimension. When the NN is used for a classifying task, C is also the number of classes. We denote the input data as {(xi, yi)}n i=1, where xi ∈ Rd, and the number of samples is n. We denote the loss function as L(θ) = 1 n Pn i=1 l(yi, fθ(xi)). Covariance matrices. We denote the output covariance matrix as E[fθ(x)f⊤ θ (x)], where the expectation is taken over the input distribution. We tend to consider the following empirical covariance matrix: C(θ) := 1 n nX i=1 fθ(xi)f⊤ θ (xi) ∈ RC×C. (5) Fisher Information Matrices. We denote the (output) FIM as E[∇θfθ(x)∇θfθ(x)⊤] = CX k=1 E[∇θf(k) θ (x)∇θf(k) θ (x)⊤], (6) where f(k) θ (x) is the k-th entry of the vector function f(x). We also consider the empirical version of the FIM: F(θ) := CX k=1 1 n nX i=1 ∇θf(k) θ (xi)∇θf(k) θ (xi)⊤ ∈ RP×P . (7) Note that (7) can be equally written as F(θ) := 1 n∇θ ˜fθ(x)∇θ ˜fθ(x)⊤, (8) 18where ∇θ ˜fθ(x) has the following form:   ∂f (1) θ (x1) ∂θ1 ··· ∂f (1) θ (xn) ∂θ1 ··· ∂f (C) θ (x1) ∂θ1 ··· ∂f (C) θ (xn) ∂θ1 ... ... ... ∂f (1) θ (x1) ∂θP ··· ∂f (1) θ (xn) ∂θP ··· ∂f (C) θ (x1) ∂θP ··· ∂f (C) θ (xn) ∂θP   ∈ RP×Cn. Hessian Matrices. We denote the Hessian as E h ∂2l(y,fθ(x)) ∂θ2 i , and we tend to consider the empirical Hessian Matrices: H(θ) := ∂2L(θ) ∂θ2 ∈ RP×P , (9) where L(θ) is the empirical loss function L(θ) = 1 n Pn i=1 l(yi, fθ(xi)). Hessian and FIM are equivalent under certain conditions. FIM can be defined in alternative ways different from (6). For instance, from classic statistical knowledge, we have the standard FIM (sFIM) in the following form: sF IM:= E[∇θ log P(y|x; θ)∇θ log P(y|x; θ)T ], (10) where P(y|x; θ) represents the likelihood. After simple derivations, one can show that sFIM also has the following form (Pawitan, 2001; Yuen, 2010): sF IM= −E \u0014∂2 log P(y|x; θ) ∂θ2 \u0015 . (11) Therefore, when the loss function is defined as the negative log-likelihood, the sFIM in (11) is equivalent to Hessian defined in (9). Why is the FIM defined in (6) equivalent to (10). Back to deep learning, the FIM is often defined as (6). It is thus meaningful to derive the equivalence between these two forms. Suppose P(y|x; θ) here means the conditional probability distribution of output y given input data x. If P(y|x; θ) is assumed to take the following form: P(y|x; θ) = 1√ 2π exp \u0012 −1 2∥y − fθ(x)∥2 \u0013 , (12) then the MSE estimator minθ 1 2 ∥y − fθ(x)∥2 is equivalent to the maximum likelihood estimation of P(y|x; θ). Then, plugging (12) into (10), we have: sF IMmse = E[∥y − fθ(x)∥2∇θfθ(x)∇θfθ(x)T ]. (13) We now expand sF IMmse by the definition of expectation, and we have the following (Karakida et al., 2019b): sF IMmse = Z R Z R ∥y − fθ(x)∥2∇θfθ(x)∇θfθ(x)T p(x, y; θ)dydx (14) = Z R Z R ∥y − fθ(x)∥2∇θfθ(x)∇θfθ(x)T P(y|x; θ)q(x)dydx (15) = Z R \u0014Z R 1√ 2π ∥y − fθ(x)∥2 exp \u0012 −1 2∥y − fθ(x)∥2 \u0013 dy \u0015 ∇θfθ(x)∇θfθ(x)T q(x)dx (16) = Z R ∇θfθ(x)∇θfθ(x)T q(x)dx (17) = E[∇θfθ(x)∇θfθ(x)T ], (18) where (14) follows from the definition of expectation, q(x) is input distribution, and (17) holds because the integral of y in the brackets [] equals 1 due to the property of Gamma function Γ( ·). 190.9 1.0 1.1 1.2 1.3 Hessian PL exponent s 1.55 1.60 1.65 1.70 1.75 1.80 1.85Output covariance PL exponent s linear fitting Figure 9: We train a MLP for 50 epochs and fit PL exponent s for both the output covariance and the Hessian. For models trained with epochs {1, 10, 20, 30, 40, 50 }, we see their PL exponents s show a strong correlation. Therefore, from (18), we find that sF IMmse is just equal to F IM, defined in (6). Also, plugging (12) into E h ∂2logP (y|x;θ) ∂θ2 i and taking the loss function L(θ) as the mean-square loss, we will again find that E h ∂2logP (y|x;θ) ∂θ2 i is equal to H(θ). Therefore, jointly considering (11), we can see that FIM is equal to the Hessian H(θ). PL in the covariance matrix and PL in Hessian are tightly correlated. Next, we consider the relationship between the covariance matrix and the Hessian. Suppose the NN function fθ is a Lipchitz function (Zhang et al., 2022). Then, it can be seen that the covariance matrix (5) may be controlled and estimated by FIM defined in (6), which is equivalent to being controlled by Hessian. Although deriving an exact equivalent between these two can be hard, we numerically show that the PL in one matrix informs the PL in the other. To visualize their relationship in the presence of PL, we train a simple MLP on MNIST (Deng, 2012) with one hidden layer and 2000 neurons for 50 epochs. We leverage the spectral regularization from Nassar et al. (2020) to make the output covariance matrix exhibit a PL spectrum. Meanwhile, we calculate the top eigenvalues of the covariance and the Hessian (Yao et al., 2020), fit the PL exponent s for each matrix, and compare the PL exponents against each other. More specifically, we take trained NNs from epochs {1, 10, 20, 30, 40, 50 } and plot the Hessian PL exponent s versus the output covariance PL exponent s. From the results shown in Figure 9, we can see that their PL exponent s shows a strong correlation, which supports our claim that the PL phenomena in one matrix can inform the other. Connections to the NTK matrix. Interestingly, if we ignore the constant in (8) and switch the two matrices multiplied together, we obtain ∇θ ˜fθ(x)T ∇θ ˜fθ(x). This matrix is equal to Neural Tangent Kernel(NTK) (Jacot et al., 2018), which is a kernel used to approximate the deep NN when NN’s width is infinite. We thus conjecture that NTK should show PL when the NN is well trained (Gu et al., 2022). Indeed, Karakida et al. (2019b) and Karakida et al. (2019a) study the eigenvalues of NTK, showing a PL trend. Some other work on stochastic gradient (Xie et al., 2022b) claim that the so-called “stochastic gradient matrix” (which is similar to the NTK matrix) shows a PL spectrum as well, which matches our expectations. Also, Dyer and Gur-Ari (2020); Lewkowycz et al. (2020) show that the eigenvalues of NTK are similar to those in the Hessian, which again meets our expectation because the Hessian tends to be PL when NNs are well-trained (Xie et al., 2022a). In summary, this section investigates different “important matrices” and shows that they are tightly correlated to each other in terms of the PL trends: if one matrix shows a PL spectrum, there is a high chance that the other ones show something similar. 20A.2 Connections between PL in ESD and PL in decaying eigenvalues Next, we derive the connection between our PL Alpha Hill metric and the exponent of PL distribution on decaying eigenvalues. Take the covariance matrix (5) as an instance. According to Nassar et al. (2020), the HT phenomenon in the output covariance matrix is similar to the layer-wise covariance matrices. Thus, without the loss of generality, we can consider the case when there is only one layer in the NN. We assume the weight matrix L is in RN×Q. According to prior works, when L is well-trained, the ESD follows a PL distribution: p(λ) = 1 H λ−α, λ min < λ < λmax. (19) Here, H is a normalizing constant, and α is the PL exponent. Another way to characterize the PL phenomenon is to consider eigenvalues directly following a PL series. For example, Xie et al. (2022a) show that the decaying eigenvalues follows the following PL series: λk = λ1k−s, k= 1, 2, ··· , Q, (20) where λ1 is the same as λmax used in the main paper. Now, we will analytically and empirically show that these two ways of characterizing PL are strongly related. Furthermore, the two PL coefficients satisfy s = 1 α−1 . An analytical way to show that s = 1 α−1 . The derivation is actually quite simple. Consider the case that λk = λ1k−s (i.e., (20) holds), and suppose Λ is a random variable distributed according to the empirical distribution from these eigenvalues λk = λ1k−s. Now, from (20), we can see that the distribution function takes the following form: P(Λ > λ1k−s) = k Q. (21) By changing variables λ1k−s = λ, we get the cumulative distribution function of Λ: P(Λ > λ) ∼ λ−1 s . (22) After that, we take the derivative with respect to λ, and we get the ESD: p(λ) ∼ λ−( 1 s +1). (23) In other words, we have λ−( 1 s +1) = λ−α, which means s = 1 α−1 . An empirical way to show that s = 1 α−1 . We consider matrices of size Q × Q, where we choose Q in {16, 32, 64, 128, 256, 512, 768, 1024 }, and we assign the parameters such that the decaying eigenvalues obey the formula λ1k−s, for s in {0.2, 0.3, 0.4, ··· , 3.2 }. Then, we fit the ESD and get our estimate PL Alpha Hill. We plot the relationship between PL Alpha Hill and s in Figure10. From Figure 10, we find that the connection between PL Alpha Hill and s shows a good fit with the formula s = 1 α−1 . With increasing matrix size Q, the fitting becomes increasingly accurate. When s = 1 α−1 , s = 1 corresponds to α = 2. Some prior works (Bartlett et al., 2020; Nassar et al., 2020; Xie et al., 2022a) measure the HT phenomena from the perspective of decaying eigenvalues with PL exponent s, and they show either theoretically or empirically that s = 1 is the optimal exponent. Now that we have s = 1 α−1 in the linear case, and from the theory of NTK (Jacot et al., 2018), the infinite wide NN is approximated as a linear model, we tend to believe that α = 2 satisfies a similar property. Indeed, one of the main contributions of Martin and Mahoney (2021b) is to establish different HT families of ESDs, and α = 2 is believed to be the boundary between “moderately HT” and “very HT,” corresponding to the best models. Martin and Mahoney (2021b) further argue that the optimal exponent for PL Alpha is in the range [2,4]. Combining the perspective from Bartlett et al. (2020); Nassar et al. (2020); Xie et al. (2022a) and those from Martin and Mahoney (2021b), it is reasonable to believe that the optimal exponent for PL Alpha is around 2. When PL Alpha is much higher or lower than 2, the NN probably has some issue in training. Although we argued in the main paper that the absolute numerical value of PL Alpha is unimportant in implementing our TempBalance algorithm, it is, however, helpful to have an “optimal” PL Alpha value to test 211 2 3 4 5 6 7 PL_Alpha_Hill 1 2 3s PL_Alpha_Hill  vs s    M=16 1/(PL_Alpha_Hill-1) Fitted PL_Alpha_Hill vs s 2 3 4 5 6 7 PL_Alpha_Hill 1 2 3s PL_Alpha_Hill  vs s    M=32 1/(PL_Alpha_Hill-1) Fitted PL_Alpha_Hill vs s 2 3 4 5 6 PL_Alpha_Hill 1 2 3s PL_Alpha_Hill  vs s    M=64 1/(PL_Alpha_Hill-1) Fitted PL_Alpha_Hill vs s 2 3 4 5 6 PL_Alpha_Hill 1 2 3s PL_Alpha_Hill  vs s    M=128 1/(PL_Alpha_Hill-1) Fitted PL_Alpha_Hill vs s 2 3 4 5 6 PL_Alpha_Hill 1 2 3s PL_Alpha_Hill  vs s    M=256 1/(PL_Alpha_Hill-1) Fitted PL_Alpha_Hill vs s 2 3 4 5 6 PL_Alpha_Hill 1 2 3s PL_Alpha_Hill  vs s    M=512 1/(PL_Alpha_Hill-1) Fitted PL_Alpha_Hill vs s 2 3 4 5 6 PL_Alpha_Hill 1 2 3s PL_Alpha_Hill  vs s    M=768 1/(PL_Alpha_Hill-1) Fitted PL_Alpha_Hill vs s 2 3 4 5 6 PL_Alpha_Hill 1 2 3s PL_Alpha_Hill  vs s    M=1024 1/(PL_Alpha_Hill-1) Fitted PL_Alpha_Hill vs s Figure 10: We show the connection between PL Alpha Hill and the PL exponent of the decaying eigenvalues (denoted as s) satisfy s = 1 PL Alpha Hill−1 . Results are shown for different matrix size Q. In particular, we see that PL Alpha Hill = 2 (Martin and Mahoney, 2021b) is equivalent to s = 1 (Agrawal et al., 2022) in the linear case. if our algorithm actually works in controlling the ESDs. We will show visualization results in Appendix B that TempBalance leads to a better distribution of our estimated PL Alpha Hill. In summary, this section explores two distinct methods for determining PL fit. We demonstrate that, although these two methods yield numerically distinct PL exponents, they essentially capture the same underlying phenomenon. Moreover, it is noteworthy that the “optimal” values of the PL exponents reported in various papers are consistent with one another (Bartlett et al., 2020; Martin and Mahoney, 2021b; Nassar et al., 2020; Xie et al., 2022a). B Visualization results: how does TempBalance control ESDs We demonstrate that the proposed method, TempBalance, effectively controls the shape of ESDs, resulting in a more favorable distribution of PL Alpha Hill among the layers of NNs compared to the baseline method CAL. This observation elucidates the superior performance of TempBalance over CAL in our main experiment, as presented in Section 4.2. We evaluate the models reported in the main paper. For each individual NN, we compute and aggregate PL Alpha Hill values across all layers, excluding the first and last layers that have an extremely small number of eigenvalues and thus cause inaccurate PL Alpha Hill estimation. We aggregate the PL Alpha Hill values from five models trained using different random seeds for each method. Figure 11 shows the distribution of PL Alpha Hill of TempBalance and the baseline CAL. Comparing TempBalance with CAL, we see that TempBalance consistently yields a more concentrated distribution. Furthermore, TempBalance causes the median and mean of the distribution to approach 2 (shown in each subplot respectively as the middle vertical line and the red star). The value 2 represents the theoretically optimal PL Alpha Hill value, as we have justified in Appendix A. Next, in Figure 12, we group the models into different subgroups based on their architectures and/or datasets, aggregating the PL Alpha Hill values and comparing the distributions of the two methods TempBalance and CAL. Once again, we observe that TempBalance results in a more concentrated distri- bution, with a larger number of samples (layers) having PL Alpha Hill values closer to 2. We provide visualization to demonstrate how the learning rates are distributed over layers during the training. In Figure 13, we report the learning rate and PL Alpha Hill every epoch throughout the 200-epoch training duration. The key observation includes the following. 22(a) ResNet18, CIFAR100  (b) ResNet34, CIFAR100  (c) VGG16, CIFAR100  (d) VGG19, CIFAR100 (e) ResNet18, TIN  (f) ResNet34, TIN  (g) WRN16-8, TIN  (h) WRN28-6, TIN (i) ResNet18, CIFAR10  (j) VGG16, CIFAR10  (k) ResNet18, SVHN  (l) VGG16, SVHN Figure 11: Comparing the distribution of PL Alpha Hill of NNs trained by our method TempBalance (TB) and CAL. The mean of each distribution is indicated by a red star marker. Each distribution aggregates the PL Alpha Hill values from models trained using five different random seeds. Across all experiments, our method TempBalance consistently yields a more concentrated distribution, resulting in the mean and median approaching the theoretically optimal PL Alpha Hill value of 2, as supported in Appendix A. 1. How does the learning rate vary across layers? We observed a correlation between the layer-wise learning rate and the layer-wise PL Alpha Hill distribution: layers with larger PL Alpha Hill are allocated larger learning rates, whereas those with smaller PL Alpha Hill receive smaller learning rates. 2. How does the layer-wise learning rate evolve during training? The variations in layer- wise learning rates closely reflect shifts in the layer-wise PL Alpha Hill distribution. Initially, the PL Alpha Hill distributes uniformly across layers but eventually converge to a layer-wise pattern where earlier layers have smaller PL Alpha Hill and later layers have larger ones. We present visualizations of howPL Alpha Hill and learning rate evolve through training. In Figure 14, we show PL Alpha Hill and learning rate of two layers within the same ResNet18 during the training process. The two layers arelayer1. 0. conv2 (index=1) and layer4. 0. conv2 (index=15). From Figure 14b and 14d, we can see that with the baselineCAL scheduler (blue curves), the earlier layer (index=1) achieves a smallerPL Alpha Hill value compared to the larger PL Alpha Hill value of the later layer (index=15). In contrast, TempBalance (orange curves) narrows this gap, indicating our approach balances the undertraining/overtraining levels (as signified by PL Alpha Hill) of different layers. This balancing effect is further corroborated by Figures 11 and 12 , where our method consistently refines the layer-wise PL Alpha Hill distribution. Regarding the learning rate plots in Figure 14a and 14c, TempBalance allocates a lower learning rate for earlier layers and a higher one for later layers than the baseline does. This leads to a more balanced PL Alpha Hill distribution 23(a) Total  (b) ResNet  (c) VGG  (d) WRN (e) TIN  (f) CIFAR100  (g) CIFAR10  (h) SVHN Figure 12: Comparing our method TempBalance (TB) to CAL in terms of the distribution of PL Alpha Hill of aggregating NNs into different architectures and datasets. Each distribution aggregates the PL Alpha Hill of models trained with five random seeds. Across all subgroups, our method TempBalance consistently exhibits a more concentrated distribution, accompanied by a higher number of layers approaching a PL Alpha Hill value close to 2. This value of 2 corresponds to the theoretically optimal PL Alpha Hill value, as justified in Appendix A. 0 50 100 150 200 Training Epochs 0 5 10 15 Layer index 0.00 0.05 0.10 0.15Learning Rate (a) LR, ResNet18 0 5 10 15 Layer index 1.5 2.0 2.5Alpha Stage 1 Stage 2 Stage 3 Stage 4 (b) Alpha, ResNet18 0 10 20 30 Layer index 0.00 0.05 0.10 0.15Learning Rate (c) LR, ResNet34 0 10 20 30 Layer index 1.5 2.0 2.5Alpha Stage 1 Stage 2 Stage 3 Stage 4 (d) Alpha, ResNet34 Figure 13: (Visualization of layer-wise learning rate (LR) andPL Alpha Hill (Alpha) over training). (a-b) The layer-wise LR and PL Alpha Hill of ResNet18 over training. (c-d) The layer-wise LR and PL Alpha Hill of ResNet34 over training. between layers as mentioned above. Additionally, we noted instability in the learning rate curves during early training phases, while smoother transitions emerge in later phases. C Ablation studies We provide additional ablation studies on the choices of learning rate assignment function, assignment hyperparameters. Varying LR assignment function. For TempBalance, we selected the linear interpolation (Equation 2) 240 50 100 150 200 Epoch 0.00 0.05 0.10Learning Rate Layer index = 1 (a) LR, layer index=1 0 50 100 150 200 Epoch 1.50 1.75 2.00Alpha Layer index = 1 (b) Alpha, layer index=1 0 50 100 150 200 Epoch 0.00 0.05 0.10Learning Rate Layer index = 15 (c) LR, layer index=15 0 50 100 150 200 Epoch 1.50 1.75 2.00Alpha Layer index = 15 (d) Alpha, layer index=15 Figure 14: (Visualization of learning rate (LR) and PL Alpha Hill (Alpha) of two layers during training) (a-b) LR and PL Alpha Hill of one layer with index = 1 in ResNet18. (c-d) LR and PL Alpha Hill of one layer with index = 15 in ResNet18. The ResNet18 is trained on CIFAR100. for learning rate assignment function ft, based on its superior performance in our ablation study. We evaluated three alternative learning rate assignment functions: Square root (Sqrt), Log2, and Step: • Sqrt : ft(i) = ηt √ αi t 1 L PL j=1 √ αj t , • Log2: ft(i) = ηt log(αi t) 1 L PL j=1 log(αj t) , • Step: For layer i with k-th minimum PL Alpha Hill among all the layers, ft(i) = ηt(s1 + (k − 1)s2 − s1 L − 1 ) Here, ηt denotes the base global learning rate at epoch t, (s1, s2) represents the minimum and maximum learning rate scaling ratios relative to ηt, αi t is the PL Alpha Hill estimate of the layer i at epoch t, and L is the total number of model layers. All these notations are consistently used in the main paper. As depicted in Figure 15, TempBalance (TB), with the current assignment function, surpasses the other designs when tested on VGG and ResNet architectures on CIFAR100. All hyperparameters are consistent with the main paper. Each experiment was conducted with five random seeds. (a) ResNet18, CIFAR100  (b) ResNet34, CIFAR100  (c) VGG16, CIFAR100  (d) VGG19, CIFAR100 Figure 15: (Different designs for learning rate assignment function .) Results of using different learning rate assignment functions on different architectures and CIFAR-100. Our design in the main paper TempBalance (TB) outperforms others. Reporting mean/std over five random seeds. Varying LR assignment function hyperparameters. We provide additional results of a hyperparam- eter study on ( s1, s2), in which we consider five different settings for ( s1, s2): {(0.5, 1.5), (0.6, 1.4), (0.7, 1.3), (0.8, 1.2), (0.9, 1.1) }. We run tasks on CIFAR100 with four VGG and ResNet architectures, each with five random seeds. Our results in Figure 16 show that a larger learning rate scaling range (0 .5, 1.5) performs best. 25This hyperparameter setting is the default setting used in our paper. All hyperparameters are consistent with those described in the main paper. (a) ResNet18, CIFAR100  (b) ResNet34, CIFAR100  (c) VGG16, CIFAR100  (d) VGG19, CIFAR100 Figure 16: (Hyperparameter study on ( s1, s2)). Search for hyperparameters ( s1, s2) with different architectures on CIFAR100. The current hyperparameter choice (0 .5, 1.5) used in the paper performs best among all the cases. Reporting mean/std over five random seeds. D Hyperparameter settings for reproducing our results We report all hyperparameters, random seeds and all numerical values of experimental results shown in the main paper (in Section 4). First, we report the common hyperparameters shared by all the experiments: the default optimizer is SGD, trained with batch size 128, number of training epochs 200, weight decay 5e-4, and momentum 0.9. The default HT-SR metric used in TempBalance is PL Alpha Hill. For each experimental setting, we use five random seeds, which are always {43, 37, 13, 51, 71 }, and we report the mean and standard deviation of the test accuracy across these seeds. First, Table 1 reports the details of experiments shown in Figure 3. We carefully tune the initial learning rate η0 and λsr for the two baseline methods CAL and SNR. Then, Table 2 reports the detailed hyperparameter settings of the experiments shown in Figure 4. We again carefully tune the hyperparameters of various baseline optimizers and schedulers, as specified in their papers. Finally, Table 3, Table 4, Table 5 and Tabel 6 respectively report the details of the experiments shown in Figure 5, Figure 6, Figure 7 and Figure 8. E Comparison with more baselines In Figure 17, we provide additional results by comparingTempBalance with LAMB and Adam. We found that our method outperforms both baseline methods. Furthermore, we also found that the Adam-based methods do not provide better results than the SGD baseline with cosine annealing ( CAL) in our experiment setting, which was mentioned in Section 4.2. For Adam, we searched the initial learning rate over {0.00005, 0.0001, 0.001, 0.01, 0.1}, and we used ϵ = 10−8. For LAMB, we searched the initial learning rate over {0.005, 0.01, 0.02}, and we used ϵ = 10−6. Both methods used weight decay 5 .0 × 10−4, β1 = 0.9, β2 = 0.999, learning rate decay with cosine annealing. Each experiment was conducted with five random seeds. We also discuss the difference between TempBalance and these two types of learning rate scheduling. • Compared to layer-wise learning rate scheduling (e.g., LARS): TempBalance uses a more precise model quality metric, PL Alpha Hill from HT-SR Theory, to enhance the performance of deep models during training. This “shape-based” metric estimates the shape of the eigenspectrum of weight matrices. In contrast, LARS uses a “norm-based” metric, such as the layer-wise gradient norm. A recent study in HT-SR (Martin et al., 2021) has shown that the shape-based metrics surpasses norm-based ones in assessing model quality and performance. Figure 3 confirms that our method outperforms the layer-wise scheduler LARS in test accuracy. • Compared to parameter-wise learning rate scheduling (e.g., Adam): Similarly, our method employs the “shape-based” metric PL Alpha Hill to improve the generalization, an approach not incorporated in traditional parameter-wise methods. 26Table 1: Parameter settings of the experiment reported in Section 4.2 Figure 3. The hyperparameter in bold is the best hyperparameter selection reported in the main paper. The five random seeds for each setting are {43, 37, 13, 51, 71 }, and the means and standard deviations of the test accuracy among the five seeds are reported. Index Dataset Model Method Initial learning rateη0 λsr Test Acc (best hyperparam.) scaling ratio (s1, s2) 0 CIFAR100 ResNet18 CAL 0.05,0.1, 0.15 - 78.31 ±0.05 - 1 ResNet18 SNR 0.1 0.001, 0.005, 0.01, 0.015 78.65 ±0.29 - 2 ResNet18 TB 0.1 - 78.97 ±0.29 (0.5, 1.5) 3 ResNet18 TB + SNR 0.1 0.001 79.06 ±0.32 (0.6, 1.4) 4 ResNet34 CAL 0.05, 0.1, 0.15 - 78.98 ±0.14 - 5 ResNet34 SNR 0.1 0.001, 0.005, 0.01, 0.015 79.97±0.21 - 6 ResNet34 TB 0.1 - 79.89 ±0.15 (0.5, 1.5) 7 ResNet34 TB + SNR 0.1 0.005 80.09 ±0.35 (0.6, 1.4) 8 VGG16 CAL 0.025,0.05, 0.1 - 74.59 ±0.23 - 9 VGG16 SNR 0.05 0.001, 0.005, 0.01, 0.015 74.80 ±0.28 - 10 VGG16 TB 0.05 - 74.96 ±0.15 (0.5, 1.5) 11 VGG16 TB + SNR 0.05 0.005 75.52 ±0.46 (0.6, 1.4) 12 VGG19 CAL 0.025,0.05, 0.1 - 73.26 ±0.37 - 13 VGG19 SNR 0.05 0.001, 0.005, 0.01, 0.015 74.37 ±0.16 - 14 VGG19 TB 0.05 - 73.77 ±0.43 (0.5, 1.5) 15 VGG19 TB + SNR 0.05 0.01 74.74 ±0.10 (0.5, 1.5) 16 ResNet18 CAL 0.05,0.1, 0.15 - 66.25±0.17 - 17 ResNet18 SNR 0.1 0.001, 0.005,0.01, 0.015 66.20±0.22 - 18 ResNet18 TB 0.1 - 66.77±0.25 (0.6, 1.4) 19 ResNet18 TB + SNR 0.1 0.001 66.86±0.22 (0.6, 1.4) 20 ResNet34 CAL 0.05,0.1, 0.15 - 68.19±0.16 - 21 ResNet34 SNR 0.1 0.001, 0.005,0.01, 0.015 68.69±0.13 - 22 ResNet34 TB 0.1 - 69.12±0.16 (0.6, 1.4) 23 ResNet34 TB + SNR 0.1 0.001 69.27±0.21 (0.6, 1.4) 24 WRN16-8 CAL 0.05,0.1, 0.15 - 63.67±0.09 - 25 WRN16-8 SNR 0.1 0.00005,0.0001, 0.001 63.98±0.23 - 26 WRN16-8 TB 0.1 - 64.09±0.17 (0.6, 1.4) 27 WRN16-8 TB + SNR 0.1 0.0001 64.08±0.07 (0.6, 1.4) 28 WRN28-6 CAL 0.05, 0.1, 0.15 - 65.88±0.20 - 29 WRN28-6 SNR 0.1 0.00005,0.0001, 0.001 66.09±0.25 - 30 WRN28-6 TB 0.1 - 66.58±0.23 (0.6, 1.4) 31 TinyImageNet WRN28-6 TB + SNR 0.1 0.0001 66.79±0.25 (0.6, 1.4) 32 CIFAR10 ResNet18 CAL 0.05,0.1, 0.15 - 95.53 ±0.12 - 33 ResNet18 SNR 0.1 0.001, 0.005, 0.01, 0.015 95.57 ±0.06 - 34 ResNet18 TB 0.1 - 95.63 ±0.08 (0.5, 1.5) 35 ResNet18 TB + SNR 0.1 0.001 95.66 ±0.09 (0.6, 1.4) 36 VGG16 CAL 0.025, 0.05,0.1 - 93.98 ±0.12 - 37 VGG16 SNR 0.05 0.001, 0.005, 0.01, 0.015 94.04 ±0.07 - 38 VGG16 TB 0.05 - 94.14 ±0.06 (0.5, 1.5) 39 VGG16 TB + SNR 0.05 0.005 94.26 ±0.10 (0.6, 1.4) 40 ResNet18 CAL 0.05,0.1, 0.15 - 96.59±0.08 - 41 ResNet18 SNR 0.1 0.001,0.005, 0.015, 0.01 96.65±0.12 - 42 ResNet18 TB 0.1 - 96.63±0.06 (0.5, 1.5) 43 ResNet18 TB + SNR 0.1 0.01 96.67±0.09 (0.6, 1.4) 44 VGG16 CAL 0.025,0.05, 0.1 - 96.28±0.04 - 45 VGG16 SNR 0.05 0.001, 0.005,0.015, 0.01 96.32±0.07 - 46 VGG16 TB 0.05 - 96.33±0.06 (0.5, 1.5) 47 SVHN VGG16 TB + SNR 0.05 0.005 96.40±0.08 (0.6, 1.4) 27Table 2: Parameter settings of the experiment reported in Section 4.2 Figure 4. The hyperparameter in bold is the best hyperparameter selection reported in the main paper. The five random seeds for each setting are {43, 37, 13, 51, 71 }, and the means and standard deviations of the test accuracy among the five seeds are reported. Index Dataset Model Method Initiallearning rateη0 SGDR(T0, Tmul) Lookaheadk Lookaheadα Test Acc(best hyperparams.) scaling ratio (s1, s2) 0 ResNet18 CAL 0.05,0.1, 0.15 - - - 78.31 ±0.05 - 1 ResNet18 SGDR 0.05,0.1, 0.15 (100,1), (10, 2),(1, 2) - - 77.69 ±0.20 - 2 ResNet18 LARS 26,28, 30, 32, 34 - - - 78.44 ±0.12 - 3 ResNet18 Lookahead 0.05,0.1, 0.15 - 10, 5 0.8, 0.5 78.46 ±0.18 - 4 ResNet18 SGDP 0.01, 0.05,0.1, 0.15, 0.2 - - - 78.74 ±0.11 - 5 ResNet18 TB 0.05,0.1, 0.15 - - - 78.97 ±0.29 (0.5, 1.5) 6 ResNet18 TB + SGDP 0.05,0.1, 0.15 - - - 79.13 ±0.15 (0.5, 1.5) 7 ResNet34 CAL 0.05, 0.1, 0.15 - - - 78.98 ±0.14 - 8 ResNet34 SGDR 0.05, 0.1, 0.15 (100,1), (10, 2), (1, 2) - - 78.61 ±0.20 - 9 ResNet34 LARS 26, 28, 30,32, 34 - - - 78.94 ±0.19 - 10 ResNet34 Lookahead 0.05, 0.1,0.15 - 10, 5 0.8, 0.5 79.19 ±0.12 - 11 ResNet34 SGDP 0.01, 0.05,0.1, 0.15, 0.2 - - - 79.34 ±0.21 - 12 ResNet34 TB 0.05,0.1, 0.15 - - - 79.89 ±0.15 (0.5, 1.5) 13 CIFAR100 ResNet34TB + SGDP 0.05,0.1, 0.15 - - - 79.94 ±0.30 (0.5, 1.5) Table 3: Parameter settings of the experiment reported in Section 4.3 Figure 5. The five random seeds for each setting are {43, 37, 13, 51, 71 }, and the means and standard deviations of the test accuracy among the five seeds are reported. Index Dataset Model Method Initial learning rateη0 Test Acc scaling ratio (s1, s2) 0 CIFAR100 ResNet18 CAL 0.05, 0.1, 0.15 78.08 ± 0.19, 78.31± 0.05, 77.72± 0.44 - 1 ResNet18 TB 0.05, 0.1, 0.15 78.48 ± 0.27, 78.97± 0.29, 78.69± 0.11 (0.5, 1.5) 2 ResNet34 CAL 0.05, 0.1, 0.15 78.98 ± 0.14, 78.89± 0.24, 78.51± 0.34 - 3 ResNet34 TB 0.05, 0.1, 0.15 79.36 ± 0.18, 79.89± 0.15, 79.09± 0.64 (0.5, 1.5) 4 VGG16 CAL 0.025, 0.05, 0.1 73.96 ± 0.27, 74.59± 0.23, 74.46± 0.12 - 5 VGG16 TB 0.025, 0.05, 0.1 74.40 ± 0.31, 74.96± 0.15, 74.94± 0.16 (0.5, 1.5) 6 VGG19 CAL 0.025, 0.05, 0.1 72.57 ± 0.45, 73.26± 0.37, 72.98± 0.16 - 7 VGG19 TB 0.025, 0.05, 0.1 73.47 ± 0.16, 73.77± 0.43, 73.40± 0.38 (0.5, 1.5) Table 4: Parameter settings of the experiment reported in Section 4.3 Figure 6. The five random seeds for each setting are {43, 37, 13, 51, 71 }, and the means and standard deviations of the test accuracy among the five seeds are reported. Index Dataset Model Method Initial learning rateη0 Width Test Acc scaling ratio (s1, s2) 0 CIFAR100 ResNet18 CAL 0.1 256, 512, 768 75.05 ±0.26, 78.31±0.05, 79.44±0.26 - 1 ResNet18 TB 0.1 256, 512, 768 75.63 ±0.12, 78.97±0.29, 80.47±0.18 (0.5, 1.5) 2 ResNet34 CAL 0.1 256, 512, 768 76.79 ±0.34, 78.89±0.24, 79.94±0.31 - 3 ResNet34 TB 0.1 256, 512, 768 77.25 ±0.14, 79.89±0.15, 80.23±0.53 (0.5, 1.5) 4 VGG16 CAL 0.05 256, 512, 768 71.04 ±0.14, 74.59±0.23, 75.53±0.32 - 5 VGG16 TB 0.05 256, 512, 768 71.26 ±0.26, 74.96±0.15, 76.19±0.14 (0.5, 1.5) 6 VGG19 CAL 0.05 256, 512, 768 69.58 ±0.39, 73.26±0.37, 74.39±0.33 - 7 VGG19 TB 0.05 256, 512, 768 69.96 ±0.25, 73.77±0.43, 74.80±0.35 (0.5, 1.5) 28Table 5: Parameter settings of the experiment reported in Section 4.3 Figure 7. The five random seeds for each setting are {43, 37, 13, 51, 71 }, and the means and standard deviations of the test accuracy among the five seeds are reported. Index Dataset Model Method HT-SR Metric Initial learning rateη0 Test Acc scaling ratio (s1, s2) 0 CIFAR100 ResNet18 TB SpectralNorm 0.05, 0.1, 0.15 77.83±0.21, 78.30±0.32, 78.27±0.25 (0.5, 1.5) 1 ResNet18 TB AlphaWeighted 0.05, 0.1, 0.15 78.18±0.27, 78.67±0.17, 78.48±0.24 (0.5, 1.5) 1 ResNet18 TB PL AlphaHill 0.05, 0.1, 0.15 78.48±0.27, 78.97±0.29, 78.69±0.11 (0.5, 1.5) 2 ResNet34 TB SpectralNorm 0.05, 0.1, 0.15 78.25±0.16, 78.71±0.15, 78.92±0.28 (0.5, 1.5) 3 ResNet34 TB AlphaWeighted 0.05, 0.1, 0.15 78.36±0.39, 78.87±0.34, 78.83±0.23 (0.5, 1.5) 3 ResNet34 TB PL AlphaHill 0.05, 0.1, 0.15 79.36±0.18, 79.89±0.15, 79.09±0.64 (0.5, 1.5) 4 VGG16 TB SpectralNorm 0.025, 0.05, 0.1 73.58±0.19, 74.29±0.16, 74.17±0.28 (0.5, 1.5) 5 VGG16 TB AlphaWeighted 0.025, 0.05, 0.1 73.97±0.22, 74.19±0.11, 74.42±0.31 (0.5, 1.5) 5 VGG16 TB PL AlphaHill 0.025, 0.05, 0.1 74.40±0.31, 74.96±0.15, 74.94±0.16 (0.5, 1.5) 6 VGG19 TB SpectralNorm 0.025, 0.05, 0.1 72.34±0.26, 72.91±0.35, 73.04±0.39 (0.5, 1.5) 7 VGG19 TB AlphaWeighted 0.025, 0.05, 0.1 72.85±0.16, 73.41±0.17, 73.33±0.21 (0.5, 1.5) 7 VGG19 TB PL AlphaHill 0.025, 0.05, 0.1 73.47±0.16, 73.77±0.43, 73.40±0.38 (0.5, 1.5) Table 6: Parameter settings of the experiment reported in Section 4.3 Figure 8. The five random seeds for each setting are {43, 37, 13, 51, 71 }, and the means and standard deviations of the test accuracy among the five seeds, the means and standard deviations of the computation time of using TB among the 10 times are reported. Index Dataset Model Method PL fitting method Initial learning rateη0 Test Acc Computation Time (sec) scaling ratio (s1, s2) 0 CIFAR100 ResNet18 TB Goodness-of-fit 0.1 78.59 ±0.21 8.20 ±0.53 (0.5, 1.5) 1 ResNet18 TB Fix-finger 0.1 79.06 ±0.22 7.24 ±0.74 (0.5, 1.5) 1 ResNet18 TB Median 0.1 78.97 ±0.29 1.14 ±0.04 (0.5, 1.5) 2 ResNet34 TB Goodness-of-fit 0.1 79.13 ±0.21 16.45 ±0.48 (0.5, 1.5) 3 ResNet34 TB Fix-finger 0.1 79.64 ±0.22 15.13 ±1.05 (0.5, 1.5) 3 ResNet34 TB Median 0.1 79.89 ±0.15 2.27 ±0.06 (0.5, 1.5) 4 VGG16 TB Goodness-of-fit 0.05 74.46 ±0.24 8.54 ±0.10 (0.5, 1.5) 5 VGG16 TB Fix-finger 0.05 74.48 ±0.20 8.45 ±0.59 (0.5, 1.5) 5 VGG16 TB Median 0.05 74.96 ±0.15 1.37 ±0.05 (0.5, 1.5) 6 VGG19 TB Goodness-of-fit 0.05 73.36 ±0.16 11.48 ±0.15 (0.5, 1.5) 7 VGG19 TB Fix-finger 0.05 73.52 ±0.16 11.15 ±0.79 (0.5, 1.5) 7 VGG19 TB Median 0.05 73.77 ±0.43 1.85 ±0.05 (0.5, 1.5) AdamLAMBCALSGDRLARS Lookahead SGDP TB TB + SGDP 72.0 73.5 75.0 76.5 78.0 79.5Test Accuracy (a) ResNet18, CIFAR100 AdamLAMBCALSGDRLARS Lookahead SGDP TB TB + SGDP 74.5 76.0 77.5 79.0 80.5Test Accuracy  (b) ResNet34, CIFAR100 Figure 17: (Comparison with additional baselines). Comparing our method, TempBalance (TB), with other baselines such as parameter-wise learning rate schedulers Adam and LAMB, using ResNet18/34 trained on CIFAR100. Each cross represents the mean test accuracy of five random seeds. 290 1 2 3 4 5 6 7 8 9 10 Epochs 100 102 Gradient Norm (log10) Maximum of Layerwise Gradient Norm CAL TB (a) Maximum 0 1 2 3 4 5 6 7 8 9 10 Epochs 10 2 10 1 Gradient Norm (log10) Minimum of Layerwise Gradient Norm CAL TB (b) Minimum 0 1 2 3 4 5 6 7 8 9 10 Epochs 10 1 100 101 Gradient Norm (log10) Mean of Layerwise Gradient Norm CAL TB (c) Mean Figure 18: (Layerwise gradient norm during training). From left to right: maximum, minimum, and mean of the layerwise gradient norm at every 30 iterations for the first 10 epochs. ResNet18 on CIFAR-100. 0.025 0.050 0.075 0 50Density Initialization 0 200 0.00 0.02 Epoch:1  Iter: 30 0 10 0 2 Epoch:1  Iter: 60 0.0 2.5 0 2 Epoch:1  Iter: 90 0 2 0 2 Epoch:1  Iter: 120 0 10 0 2 Epoch:1  Iter: 150 0.5 1.0 1.5 0 2 Epoch:1  Iter: 180 0 2 Gradient Norm 0 2Density Epoch:1  Iter: 210 0 2 Gradient Norm 0 2 Epoch:1  Iter: 240 0 2 Gradient Norm 0 2 Epoch:1  Iter: 270 0.5 1.0 Gradient Norm 0 2 Epoch:1  Iter: 300 1 2 Gradient Norm 0 2 Epoch:1  Iter: 330 0 1 Gradient Norm 0 2 Epoch:1  Iter: 360 0 2 Gradient Norm 0 2 Epoch:1  Iter: 390 Figure 19: (Histogram of gradient norm distribution during first epoch). ResNet18 on CIFAR-100. 10 2 10 1 100 101 Initialization 10 2 10 1 10 1 100 101 Epoch: 1, Iter: 30 10 2 10 1 10 1 100 101 Epoch: 1, Iter: 150 10 2 10 1 100 102 Epoch: 1, Iter: 390 10 2 10 1 10 1 101 Epoch: 2 10 2 100 10 1 101 Epoch: 5 Figure 20: (Impact of large rank-1 updates on ESD). Large rank-1 updates result in the spikes of ESD, observed exclusively during the first epoch. From the second epoch onward, the ESD exhibits a heavy-tailed distribution. ResNet18 on CIFAR-100. 30CAL TB   (start epoch=1) TB   (start epoch=2) TB   (start epoch=5) TB   (start epoch=10) 78.5 79.0 Figure 21: (Varying the starting epoch of applying TempBalance (TB)). Postponing the usage of TempBalance to Epochs 2, 5, and 10 doesn’t affect the performance of TempBalance (originally starting from Epoch 1). F Does addressing other training issues lead to TempBalance’s im- provement? We discuss whether the improvement from the proposed method, TempBalance, is due to indirectly addressing another fundamental training issue that could distort the ESD, specifically the gradient magnitude excur- sions (Pascanu et al., 2013) (explosion/vanishing). This discussion further strengthens the connection between our method and the HT structure, as discussed in the Sections 1, A, and B. We first summarize the questions and the corresponding primary findings, with subsequent detailing of our experiment and supporting results. • Does gradient excursion exist? We discovered that gradient explosion does exist, but it is confined to the first epoch out of a total of 200 training epochs, leading us to believe it does not significantly impact the test accuracy. We observed no gradient vanishing. • Does the observed gradient explosion impact the estimation of PL Alpha Hill? We discovered that the large rank-1 updates resulting from the gradient explosion do indeed affect the ESD as well as the PL Alpha Hill estimation. However, this effect is again restricted to the first epoch. • Does TempBalance boil down to addressing gradient explosion? We found that postponing the use of TempBalance until the epoch when neither the gradient explosion nor the PL Alpha Hill estimation is affected does not compromise the test accuracy. To support the above answers, we conducted three experiments. We discuss the setup of these experiments first and then analyze the results. • (Figures 18, 19) We aim to detect gradient excursion by tracking the gradient norm across layers during training. We examine the model every 30 iterations over the first 10 epochs, calculating the L2 norm of each gradient update across layers using the training batches of size 128. This produces an empirical gradient norm distribution with a total sample size of update numbers × layer numbers. Figure 18 presents the maximum/minimum/mean of the distribution, while Figure 19 visualizes these distributions for several iterations of Epoch 1. • (Figure 20) We aim to assess the impact of gradient explosion onPL Alpha Hill estimation by monitoring the ESD. Figure 20 examines the change of the ESD of a single weight matrix over several iterations, tracked in the experiment depicted in Figures 18 and 19. • (Figure 21) We aim to see if TempBalance enhances generalization by implicitly addressing gradient explosion. Since the gradient explosion and its effect on PL Alpha Hill estimation only transpire in the first epoch, we postpone the starting epoch of TempBalance to Epochs 2, 5, and 10 and see if it affects the test accuracy. Our answers to the above questions are supported by the results obtained from the three experiments: • First question (Figure 18 and 19) : We observed that the notable exploding gradients only occur 31Table 7: (a) Object Detection (OD): mean Average Precision (mAP) on PASCAL VOC 2007 using model Yolov8n. (b) Language Modeling (LM): test perplexity (PPL) on Penn TreeBank (PTB) using the tensorized transformer. TempBalance (TB) consistently outperforms the CAL in different tasks. CAL + Adam TB + Adam CAL + AdamW TB + AdamW 59.59 60.03(+0.44) 59.68 59.96(+0.28) (a) OD, VOC2007, mAP ( ↑) CAL + Adam TB + Adam 49.94 47.30(-2.64) (b) LM, PTB, PPL ( ↓) in the initial 200 iterations of the first epoch. In Figure 18, we pinpoint a singular peak of maximum gradient norm within the first epoch. This aligns with the abnormal distribution with a large gradient norm in the subfigure of Figure 19 titled “Epoch 1, iteration 30.” • Second question (Figure 20): Note that large rank-one updates have been studied in random matrix theory, which manifests as a “bulk+spike” pattern. This has been analyzed in, e.g., Theorem 2.13 of Couillet and Liao (2022). Figure 20 shows this “bulk+spike” pattern, but only in the first epoch. The ESD exhibits a heavy-tail distribution in subsequent epochs, suggesting the influence of rank-one updates is limited. • Third question (Figure 21): Delaying the application of TempBalance until after the first epoch does not adversely affect the test accuracy. Figure 21 illustrates that applying TempBalance from Epochs 2, 5, and 10 results in test performance comparable to when TempBalance is applied from Epoch 1. Since the gradient explosion only occurs in the first epoch and its effect on PL Alpha Hill estimation diminishes after this, the effectiveness of TempBalance does not rely on addressing gradient explosion or biased PL Alpha Hill estimation from large rank-one updates. • Third question: We compare TempBalance with the baseline method LARS, which uses gradient norms to determine layer-wise learning rates in combating gradient vanishing/explosion issues. As illustrated in Figure 4, TempBalance outperforms LARS in terms of generalization performance. G Corroborating results on other tasks We provide corroborating results of applying TempBalance to two different tasks: object detection (OD) and language modeling (LM). In both tasks, TempBalance consistently improves generalization, outperforming the baseline scheduler cosine annealing ( CAL) when both are combined with Adam/AdamW optimizers. For OD, we studied the PASCAL VOC2007 (Everingham et al., 2010) dataset with YOLO series (Redmon et al., 2016) pre-trained model. We compared TempBalance with the baseline scheduler CAL with both applied to Adam/AdamW optimizer. For both scheduler methods, we trained for 200 epochs with batch size 64, and we set the same hyperparameter for the optimizers: β1 = 0.9, β2 = 0.999, ϵ = 10−8, weight decay = 5 .0 × 10−4. We searched the initial learning rate for all methods among {7.5 × 10−6, 1 × 10−5, 2.5 × 10−5}. For metrics we use the COCO (Lin et al., 2014) version mean Average Precision (mAP, higher is better), which is calculated for 10 IOUs varying in a range of 0.5 to 0.95 with steps of 0.05. We report the mean of mAP over five random seeds on the test set. We set the scaling factors ( s1, s2) of TempBalance to be (0.6, 1.4). Here are the experimental settings for LM. We studied the Penn Treebank (PTB) dataset (Marcus et al., 1993) using a three-layer “tensorized transformer core-1” (Ma et al., 2019). We compared TempBalance with the baseline scheduler CAL with both applied to Adam optimizer. For both scheduler methods, we trained the models for 40K iterations with a batch size of 120, and a dropout rate of 0.3. We searched the initial learning rate for baseline methods among {1.25 × 10−4, 2.5 × 10−4, 5 × 10−4, 1 × 10−3, 1.25 × 10−3, 2.5 × 10−3, 5 × 10−3} for baseline CAL. The hyperparameters for Adam are β1 = 0.9, β2 = 0.999, ϵ = 10−8. The mean of perplexity (PPL, lower is better) across five random seeds on the test set is reported. We observed improved performance of TempBalance in this task when extending our hyperparameter search to include the scaling factors (s1, s2) ∈ {(0.5, 1.5), (1.0, 2.0)}, the power-law fitting hyperparameter λmin index k ∈ {n 2 , n 1.25 }, and the TempBalance update interval over {10, 25, 50} iterations. 32Figure 22: (Applying the TempBalance (TB) to different sizes of ResNets) . TempBalance consistently outperforms the baseline CAL method in the larger model ResNet101. The dataset is CIFAR100. Reporting mean/std over five random seeds. 18 34 50 101 Depth of ResNet 1 2 3Time of using TB once (sec) 25 50 75 Time of one training epoch (sec) (a) Scaling depth 18 34 50 101 Depth of ResNet 4 6 8Time increment (%) (b) Scaling depth 512 768 1024 2048 Width of ResNet18 5 10Time of using TB once (sec) 50 100 150 Time of one training epoch (sec) (c) Scaling width 512 768 1024 2048 Width of ResNet18 6 7 8 9Time increment (%) (d) Scaling width Figure 23: (Computation overhead of TempBalance (TB) in scaling the model depth/width). (a)(c) Time duration (second) of one training epoch (blue) and using TempBalance once (red). (b)(d) Time increment of using TempBalance once per epoch. The dataset is CIFAR100, reporting mean/std over 10 epochs. The computational overhead of using TempBalance remains low (less than 9%) even when applied to exceptionally wide or deep models. We present additional results in Figure 22, showing the application of our method TempBalance to ResNet 101 on CIFAR-100, and we compare it with the baseline ( CAL). We searched the initial learning rate among {0.05, 0.1, 0.15} for both the baseline and our method. The results report the mean and standard deviation across five seeds. We found that TempBalance offers improvements for the larger ResNet101 model comparable to those observed for ResNet18/34, demonstrating its potential for larger models. H Analysis of computation overhead We conducted a study on the computational overhead of TempBalance, demonstrating that our method is both applicable and scalable for large models. To do so, we conducted a scaling experiment to demonstrate that the computational cost remains low for different sizes of models. We recorded the duration of a single training epoch and the time taken to apply our method once. From this, we calculated the percentage increase in time when using TempBalance once per epoch, using this as an indicator of computational overhead. The experiment setup is based on ResNet-series on CIFAR100. We studied models of depth in {18, 34, 50, 101 } and ResNet18 models of width in {512, 768, 1024, 2048 }. We report the mean and the standard deviation of the results over 10 runs. The test platform was one Quadro RTX 6000 GPU with Intel Xeon Gold 6248 CPU. The results are presented in Figure 23. Our findings reveal that the computational overhead remains low (less than 9%) even when applied to exceptionally wide or deep models (ResNet18 with width 2048 or ResNet101). The computation overhead is not large because: 1) we select the efficient PL fitting method to obtain PL Alpha Hill, which is demonstrated in Figure 8; and 2) the most computation-intensive part of our 33Figure 24: (Varying the TempBalance (TB) update interval). Reducing the update interval from 390 iters (used in the paper) brings mild improvement in test accuracy. Both use ResNet18 on CIFAR100. Reporting mean/std over five random seeds. method is SVD decomposition, which we have optimized using GPU implementation and batch processing. We conducted an experiment on reducing the update interval of the learning rate schedule to see if it affects the test accuracy of TempBalance. Figure 24 shows the experiments conducted with ResNet18 on CIFAR-100. We reduce the update interval from 390 iterations used in our paper (equivalent to one epoch) to 300, 200, 100, and 50. We observed that there indeed exists a trade-off between the computation time and test accuracy, but reducing the update interval only brings mild improvement. 34",
      "meta_data": {
        "arxiv_id": "2312.00359v1",
        "authors": [
          "Yefan Zhou",
          "Tianyu Pang",
          "Keqin Liu",
          "Charles H. Martin",
          "Michael W. Mahoney",
          "Yaoqing Yang"
        ],
        "published_date": "2023-12-01T05:38:17Z",
        "pdf_url": "https://arxiv.org/pdf/2312.00359v1.pdf",
        "github_url": "https://github.com/YefanZhou/TempBalance"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces TempBalance, a novel layer-wise learning rate schedule grounded in Heavy-Tailed Self-Regularization (HT-SR) Theory. It effectively balances 'temperature' (learning rates) across network layers by analyzing the heavy-tail structure of Empirical Spectral Density (ESD) using the PL Alpha Hill metric. Key findings include improved test accuracy over ordinary SGD, spectral norm regularization (SNR), and several state-of-the-art optimizers, demonstrating that a scale-free mapping of PL Alpha Hill to learning rates and fixing λmin as the ESD median enhances stability and performance. The work also confirms the complementary roles of TempBalance and SNR for optimal training.",
        "methodology": "TempBalance assigns layer-wise learning rates based on the PL Alpha Hill metric derived from the Empirical Spectral Density (ESD) of neural network weight matrices. It calculates eigenvalues of the correlation matrix (W^T W) for each layer and fits a power law distribution to the heavy-tail part of the ESD to extract PL Alpha Hill using the Hill estimator (with k=n/2). Layers with smaller PL Alpha are considered 'overtrained' and receive lower learning rates, while 'undertrained' layers (larger PL Alpha) get higher rates. A linear, scale-free mapping function is used to convert PL Alpha Hill values to layer-specific learning rates, scaled relative to a global base learning rate. The λmin parameter for PL fitting is fixed as the median of the ESD for stability and computational efficiency.",
        "experimental_setup": "The method was evaluated on CIFAR10, CIFAR100, SVHN, and TinyImageNet datasets using VGG (VGG16, VGG19), ResNet (ResNet18, ResNet34, ResNet101), and WideResNet (WRN16-8, WRN28-6) architectures, varying depths and widths. Baselines included ordinary SGD with Cosine Annealing Learning Rate (CAL), Spectral Norm Regularization (SNR), and state-of-the-art optimizers/schedulers like SGDR, LARS, Lookahead, SGDP, Adam, and LAMB. Hyperparameters, including initial learning rates and regularization coefficients, were carefully tuned. Validation focused on test accuracy (mean and standard deviation over five random seeds). Ablation studies were conducted on initial learning rates, model widths, different HT-SR layer-wise metrics, and PL fitting methods. Further evaluation included object detection (PASCAL VOC2007 with YOLOv8n) and language modeling (Penn TreeBank with tensorized transformer). Computational overhead was analyzed on a Quadro RTX 6000 GPU.",
        "limitations": "The current computation of ESDs and PL Alpha Hill is performed once per epoch, leading to a minimal increase in training time (e.g., 6% for ResNet18 on CIFAR100), but could be accelerated to enable more adaptive learning rate adjustments (e.g., every few gradient updates). The current scope of HT-SR metrics applied in TempBalance is limited to layer-wise learning rate schedules, and its extension to parameter-wise or global learning rate schedules or other hyperparameters is an open question. While optimal PL Alpha values exist, the method relies on the relative ranking of layer-wise PL Alpha for learning rate assignment rather than their absolute numerical values for direct interpretation/tuning.",
        "future_research_directions": "Future work includes extending HT-SR metrics to parameter-wise and global learning rate schedules, or other hyperparameters, to develop a more comprehensive suite of hyperparameter tuning tools. Another direction is to accelerate the computation of ESDs and PL Alpha Hill to enable a more adaptive learning rate scheduler that can adjust learning rates more frequently, potentially every few gradient updates, instead of once per epoch. Further research into layer-wise tuning approaches and the broader concept of load-temperature balancing in deep neural network training is also suggested.",
        "experimental_code": "def net_esd_estimator(net=None,EVALS_THRESH=0.00001,bins=100,fix_fingers=None,xmin_pos=2,conv_norm=0.5, filter_zeros=False):results = {'alpha':[],'spectral_norm': [],'D': [],'longname':[],'eigs':[],'norm':[],'alphahat': []}for name, m in net.named_modules():if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):if name == 'model.22.dfl.conv':continue;matrix = m.weight.data.clone();if isinstance(m, nn.Conv2d):matrix = torch.flatten(matrix, start_dim=2) * math.sqrt(conv_norm);matrix = matrix.transpose(1, 2).transpose(0, 1);eigs = torch.square(torch.linalg.svdvals(matrix).flatten());eigs, _ = torch.sort(eigs, descending=False);spectral_norm = eigs[-1].item();fnorm = torch.sum(eigs).item();if filter_zeros:nz_eigs = eigs[eigs > EVALS_THRESH];N = len(nz_eigs);if N == 0:nz_eigs = eigs;N = len(nz_eigs);else:nz_eigs = eigs;N = len(nz_eigs);log_nz_eigs  = torch.log(nz_eigs);if fix_fingers == 'xmin_mid':i = int(len(nz_eigs) / xmin_pos);xmin = nz_eigs[i];n = float(N - i);seq = torch.arange(n).cuda();final_alpha = 1 + n / (torch.sum(log_nz_eigs[i:]) - n * log_nz_eigs[i]);final_D = torch.max(torch.abs(1 - (nz_eigs[i:] / xmin) ** (-final_alpha + 1) - seq / n));else:alphas = torch.zeros(N-1);Ds     = torch.ones(N-1);if fix_fingers == 'xmin_peak':hist_nz_eigs = torch.log10(nz_eigs);min_e, max_e = hist_nz_eigs.min(), hist_nz_eigs.max();counts = torch.histc(hist_nz_eigs, bins, min=min_e, max=max_e);boundaries = torch.linspace(min_e, max_e, bins + 1);h = counts, boundaries;ih = torch.argmax(h[0]);xmin2 = 10 ** h[1][ih];xmin_min = torch.log10(0.95 * xmin2);xmin_max = 1.5 * xmin2;for i, xmin in enumerate(nz_eigs[:-1]):if fix_fingers == 'xmin_peak':if xmin < xmin_min:continue;if xmin > xmin_max:break;n = float(N - i);seq = torch.arange(n).cuda();alpha = 1 + n / (torch.sum(log_nz_eigs[i:]) - n * log_nz_eigs[i]);alphas[i] = alpha;if alpha > 1:Ds[i] = torch.max(torch.abs(1 - (nz_eigs[i:] / xmin) ** (-alpha + 1) - seq / n));min_D_index = torch.argmin(Ds);final_alpha = alphas[min_D_index];final_D = Ds[min_D_index];final_alpha = final_alpha.item();final_D = final_D.item();final_alphahat=final_alpha*math.log10(spectral_norm);results['spectral_norm'].append(spectral_norm);results['alphahat'].append(final_alphahat);results['norm'].append(fnorm);results['alpha'].append(final_alpha);results['D'].append(final_D);results['longname'].append(name);results['eigs'].append(eigs.detach().cpu().numpy());return results;def get_layer_temps(args, temp_balance, n_alphas, epoch_val):n = len(n_alphas);idx = [i for i in range(n)];temps = np.array([epoch_val] * n);if temp_balance == 'tbr':idx = np.argsort(n_alphas);temps = [epoch_val * (args.lr_min_ratio + args.lr_slope * i / n) for i in range(n)];return [value for _, value in sorted(list(zip(idx, temps)), key=itemgetter(0))];elif temp_balance == 'tb_linear_map':lr_range = [args.lr_min_ratio * epoch_val,  (args.lr_min_ratio + args.lr_slope) * epoch_val];score_range = [min(n_alphas),  max(n_alphas)];temps = np.interp(n_alphas, score_range, lr_range);return temps;elif temp_balance == 'tb_sqrt':temps = np.sqrt(n_alphas)/np.sum(np.sqrt(n_alphas)) * n * epoch_val;return temps;elif temp_balance == 'tb_log2':temps = np.log2(n_alphas)/np.sum(np.log2(n_alphas)) * n * epoch_val;return temps;else:raise NotImplementedError;class BaseTrainer: # excerpt from ultralytics/yolo/engine/trainer.py with relevant modifications for TempBalance integration\n    # ... (lines before _setup_train in BaseTrainer) ...\n    def _setup_train(self, world_size):\n        # ... (lines before ESD analysis in _setup_train) ...\n        ###########################ESD analysis###########################\n        ##################################################################\n        dir = self.save_dir / 'stats'\n        if not os.path.exists(dir):\n            os.makedirs(dir)\n\n        filtered_layers = []\n        metrics = net_esd_estimator(self.model, \n                    EVALS_THRESH = 0.00001,\n                    bins = 100,\n                    fix_fingers=self.args.fix_fingers,\n                    xmin_pos=self.args.xmin_pos, \n                    filter_zeros = self.args.filter_zeros=='True')\n        \n        # pd.DataFrame(metrics).to_csv(os.path.join(self.save_dir, 'stats',  f\"metrics.csv\")) \n        \n        # summary and submit to wandb\n        metric_summary = {}\n        for key in metrics:\n            if key != 'eigs' and key != 'longname':\n                metric_summary[key] = np.mean(metrics[key])\n\n        #######################  Filter out layers who has little amount of eigenvalues ##########################\n        layer_with_few_eigs = []\n        for i, name in enumerate(metrics['longname']):\n            if len(metrics['eigs'][i]) <= self.args.tb_eig_filter:\n                # print(f\"layer [{name}] has {len(metrics['eigs'][i])} eigenvalues, less than or equal to {self.args.tb_eig_filter}, remove it\")\n                layer_with_few_eigs.append(name)\n\n\n        layer_stats=pd.DataFrame({key:metrics[key] for key in metrics if key!='eigs'})\n        layer_stats_origin = layer_stats.copy()\n        \n        # pd.DataFrame(layer_with_few_eigs).to_csv(os.path.join(self.save_dir, 'stats',  f\"removed layers.csv\")) \n        # layer_stats_origin.to_csv(os.path.join(self.save_dir, 'stats',  f\"origin_layer_stats_epoch_start.csv\"))\n        # np.save(os.path.join(self.save_dir, 'stats', 'esd_epoch_{0}.npy'), metrics)\n\n\n        ###################End  ESD analysis############################\n        ##################################################################\n        ######################  TBR scheduling ##########################\n        ##################################################################\n        if self.args.temp_balance_lr != 'None':\n            # print(\"--------------Enable temp balance --------------\")\n            \n            if self.args.remove_first_layer == 'True':\n                # print(\"remove first layer of alpha<---------------------\")\n                layer_stats = layer_stats.drop(labels=0, axis=0)\n                # index must be reset otherwise may delete the wrong row \n                layer_stats.index = list(range(len(layer_stats[self.args.metric])))\n            if self.args.remove_last_layer == 'True':\n                # print(\"remove last layer of alpha<---------------------\")\n                layer_stats = layer_stats.drop(labels=len(layer_stats) - 1, axis=0)\n                # index must be reset otherwise may delete the wrong row \n                layer_stats.index = list(range(len(layer_stats[self.args.metric])))\n\n            ####remove with the few eig values\n            drop_layers = layer_stats['longname'].isin(layer_with_few_eigs)\n            layer_stats = layer_stats[~drop_layers]\n            \n            metric_scores = np.array(layer_stats[self.args.metric])\n            #args, temp_balance, n_alphas, epoch_val\n            scheduled_lr = get_layer_temps(self.args, temp_balance=self.args.temp_balance_lr, n_alphas=metric_scores, epoch_val=self.args.lr0)\n            layer_stats['scheduled_lr'] = scheduled_lr\n\n            # these params should be tuned\n            layer_name_to_tune = list(layer_stats['longname'])\n            all_params = []\n            params_to_tune_ids = []\n\n            # these params should be tuned\n            for name, module in self.model.named_modules():\n                # these are the conv layers\n                if name in layer_name_to_tune:\n                    params_to_tune_ids += list(map(id, module.parameters()))\n                    scheduled_lr_item = layer_stats[layer_stats['longname'] == name]['scheduled_lr'].item()\n                    all_params.append({'params': module.parameters(), 'lr': scheduled_lr_item})\n                # decide should we tune the batch norm accordingly,  is this layer batchnorm and does its corresponding conv in layer_name_to_tune\n                elif self.args.batchnorm == 'True' \\\n                        and isinstance(module, nn.BatchNorm2d) \\\n                            and name.replace('bn', 'conv') in layer_name_to_tune:\n                    params_to_tune_ids += list(map(id, module.parameters()))\n                    scheduled_lr_item = layer_stats[layer_stats['longname'] == name.replace('bn', 'conv')]['scheduled_lr'].item()\n                    all_params.append({'params': module.parameters(), 'lr': scheduled_lr_item})\n                # another way is to add a else here and append params with self.args.lr0\n\n            # those params are untuned\n            untuned_params = filter(lambda p: id(p) not in params_to_tune_ids, self.model.parameters())\n            all_params.append({'params': untuned_params, 'lr': self.args.lr0}) \n\n            # create optimizer\n            if self.args.optim_type == 'SGDP':\n                # print(f\"---->>>> Initialze the SGDP with lr {self.args.lr0}  {weight_decay}\")\n                optimizer = SGDP(all_params, \n                            lr=self.args.lr0,  \n                            momentum=0.9, \n                            weight_decay=weight_decay)\n            elif self.args.optim_type == 'SNR':\n                optimizer = SGDSNR(all_params, \n                                    momentum=0.9, \n                                    weight_decay=weight_decay, \n                                    spectrum_regularization=self.args.sg,\n                                    stage_epoch=self.args.stage_epoch,\n                                    epoch=1)\n            elif self.args.optim_type == 'SGD':\n                optimizer = optim.SGD(all_params, \n                                    lr=self.args.lr0,  \n                                    momentum=0.9, \n                                    weight_decay=weight_decay) \n            elif self.args.optim_type == 'Adam':\n                optimizer = optim.Adam(all_params, \n                                    lr=self.args.lr0,  \n                                    weight_decay=weight_decay) \n            elif self.args.optim_type == 'AdamW':\n                optimizer = optim.AdamW(all_params, \n                                    lr=self.args.lr0,  \n                                    weight_decay=weight_decay) \n            else:\n                raise NotImplementedError\n        else:\n            # print(\"-------------> Disable temp balance\")\n            if self.args.optim_type == 'SNR':\n                optimizer = SGDSNR(self.model.parameters(), \n                                    momentum=0.9, \n                                    lr=self.args.lr0,  \n                                    weight_decay=weight_decay, \n                                    spectrum_regularization=self.args.sg,\n                                    stage_epoch=self.args.stage_epoch,\n                                    epoch=1)\n            elif self.args.optim_type == 'SGD':\n                optimizer = optim.SGD(self.model.parameters(), \n                                    lr=self.args.lr0,  \n                                    momentum=0.9, \n                                    weight_decay=weight_decay) \n            elif self.args.optim_type == 'Adam':\n                optimizer = optim.Adam(self.model.parameters(), \n                                    lr=self.args.lr0,  \n                                    weight_decay=weight_decay) \n            elif self.args.optim_type == 'AdamW':\n                optimizer = optim.AdamW(self.model.parameters(), \n                                    lr=self.args.lr0,  \n                                    weight_decay=weight_decay) \n            else:\n                raise NotImplementedError\n\n            \n        # save scheduled learning rate \n        # layer_stats.to_csv(os.path.join(self.save_dir, 'stats', f\"layer_stats_with_lr_epoch_{0}.csv\"))\n\n        #########################################################################\n\n        # ... (lines for self.optimizer, self.lf, self.scheduler, self.stopper, self.train_loader, self.test_loader, self.validator, self.ema) ...\n\n\n    def _do_train(self, world_size=1):\n        # ... (lines before epoch loop) ...\n        for epoch in range(self.start_epoch, self.epochs):\n            self.epoch = epoch\n            # self.run_callbacks('on_train_epoch_start')\n\n            # consider use another (maybe bigger) minimum learning rate in tbr\n            if self.args.stage_epoch > 0 and epoch >= self.args.stage_epoch:\n                # print(\"------> Enter the second stage!!!!!!!!!!\")\n                self.args.lr0_min_ratio = self.args.lr0_min_ratio_stage2\n            else:\n                pass\n            \n            # this is current LR\n            current_lr = self.untuned_lr\n            # print(f\"##############Epoch {epoch+1}  current LR: {current_lr:.8f}################\")\n\n            ####################start training#####################\n            self.model.train()\n            if RANK != -1:\n                self.train_loader.sampler.set_epoch(epoch)\n            pbar = enumerate(self.train_loader)\n            # Update dataloader attributes (optional)\n            if epoch == (self.epochs - self.args.close_mosaic):\n                # LOGGER.info('Closing dataloader mosaic')\n                if hasattr(self.train_loader.dataset, 'mosaic'):\n                    self.train_loader.dataset.mosaic = False\n                if hasattr(self.train_loader.dataset, 'close_mosaic'):\n                    self.train_loader.dataset.close_mosaic(hyp=self.args)\n                self.train_loader.reset()\n\n            if RANK in (-1, 0):\n                # LOGGER.info(self.progress_string())\n                pbar = tqdm(enumerate(self.train_loader), total=nb, bar_format=TQDM_BAR_FORMAT)\n            self.tloss = None\n            self.optimizer.zero_grad()\n            for i, batch in pbar:\n                # self.run_callbacks('on_train_batch_start')\n                # Warmup\n                ni = i + nb * epoch\n                # ... (warmup code) ...\n\n                # Forward\n                with torch.cuda.amp.autocast(self.amp):\n                    batch = self.preprocess_batch(batch)\n                    preds = self.model(batch['img'])\n                    self.loss, self.loss_items = self.criterion(preds, batch)\n                    if RANK != -1:\n                        self.loss *= world_size\n                    self.tloss = (self.tloss * i + self.loss_items) / (i + 1) if self.tloss is not None \\\n                        else self.loss_items\n\n                # Backward\n                self.scaler.scale(self.loss).backward()\n\n                # Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\n                if ni - last_opt_step >= self.accumulate:\n                    self.optimizer_step()\n                    last_opt_step = ni\n\n                # Log            \n                # mem = f'{torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0:.3g}G'  # (GB)\n                # loss_len = self.tloss.shape[0] if len(self.tloss.size()) else 1\n                # losses = self.tloss if loss_len > 1 else torch.unsqueeze(self.tloss, 0)\n                if RANK in (-1, 0):\n                    # pbar.set_description(\n                    #     ('%11s' * 2 + '%11.4g' * (2 + loss_len)) %\n                    #     (f'{epoch + 1}/{self.epochs}', mem, *losses, batch['cls'].shape[0], batch['img'].shape[-1]))\n                    self.run_callbacks('on_batch_end')\n                    if self.args.plots and ni in self.plot_idx:\n                        self.plot_training_samples(batch, ni)\n\n                # self.run_callbacks('on_train_batch_end')\n\n            self.lr = {f'lr/pg{ir}': x['lr'] for ir, x in enumerate(self.optimizer.param_groups)}  # for loggers\n\n            self.scheduler.step()\n            # self.run_callbacks('on_train_epoch_end')\n            ########epoch end#############\n\n            if RANK in (-1, 0):\n\n                # Validation\n                self.ema.update_attr(self.model, include=['yaml', 'nc', 'args', 'names', 'stride', 'class_weights'])\n                final_epoch = (epoch + 1 == self.epochs) or self.stopper.possible_stop\n\n                if self.args.val or final_epoch:\n                    self.metrics, self.fitness = self.validate()\n                self.save_metrics(metrics={**self.label_loss_items(self.tloss), **self.metrics, **self.lr})\n                self.stop = self.stopper(epoch + 1, self.fitness)\n\n                # Save model\n                if self.args.save or (epoch + 1 == self.epochs):\n                    self.save_model()\n                    self.run_callbacks('on_model_save')\n                 # save in interval\n                # state = {\n                    \n                #         'net': self.model.state_dict(),\n                #         'precision': self.metrics['metrics/precision(B)'],\n                #         'recall': self.metrics['metrics/recall(B)'],\n                #         'mAP50': self.metrics['metrics/mAP50(B)'],\n                #         'mAP': self.metrics['metrics/mAP50-95(B)'],\n                #         'epoch':epoch\n                #     }\n                # #torch.save(state, os.path.join(self.save_dir, 'stats', f'epoch_{epoch}.ckpt'))\n                # # save best\n                # if self.best_fitness == self.fitness:\n                #     # print('| Saving Best model')\n                #     state = {\n                #         'net': self.model.state_dict(),\n                #         'optimizer': self.optimizer.state_dict(),\n                #         'precision': self.metrics['metrics/precision(B)'],\n                #         'recall': self.metrics['metrics/recall(B)'],\n                #         'mAP50': self.metrics['metrics/mAP50(B)'],\n                #         'mAP': self.metrics['metrics/mAP50-95(B)'],\n                #         'epoch':epoch\n                #     }\n                # #    torch.save(state, os.path.join(self.save_dir, 'stats', f'epoch_best.ckpt'))\n\n                \n            #######################ESD analysis###############################\n            ##################################################################\n            self.model.eval()\n\n            if self.epoch == 1 or self.epoch % self.args.ww_interval == 0:\n                # print(\"------------ Start ESD analysis -----------\")\n                # a=datetime.now() \n                model_input = self.model\n\n\n                metrics = net_esd_estimator(model_input, \n                            EVALS_THRESH = 0.00001,\n                            bins = 100,\n                            fix_fingers=self.args.fix_fingers,\n                            xmin_pos=self.args.xmin_pos,\n                            filter_zeros=self.args.filter_zeros=='True')\n                \n                metric_summary = {}\n                for key in metrics:\n                    if key != 'eigs' and key != 'longname':\n                        metric_summary[key] = np.mean(metrics[key])\n\n                \n                #######################  Filter out layers who has little amount of eigenvalues ##########################\n                layer_with_few_eigs = []\n                for i, name in enumerate(metrics['longname']):\n                    if len(metrics['eigs'][i]) <= self.args.tb_eig_filter:\n                        # print(f\"layer [{name}] has {len(metrics['eigs'][i])} eigenvalues, less than or equal to {self.args.tb_eig_filter}, remove it\")\n                        layer_with_few_eigs.append(name)\n\n\n                layer_stats=pd.DataFrame({key:metrics[key] for key in metrics if key!='eigs'})\n                \n                # save metrics to disk and ESD\n                layer_stats_origin = layer_stats.copy()\n                # layer_stats_origin.to_csv(os.path.join(self.save_dir, 'stats',  f\"origin_layer_stats_epoch_{epoch}.csv\"))\n                # np.save(os.path.join(self.save_dir, 'stats', f'esd_epoch_{epoch}.npy'), metrics)\n                # if self.best_fitness == self.fitness:\n                #     np.save(os.path.join(self.save_dir, 'stats',f'esd_best.npy'), metrics)\n\n                # b=datetime.now() \n                # print('seconds:', (b-a).seconds)\n                ###################End  ESD analysis#############\n            else:\n                metric_summary = {}\n\n            ##################################################################\n            # Reschedule the learning rate\n            self.untuned_lr = self.lr_schedule(self.args.lr0, epoch=self.epoch, total_epoch=self.epochs, warmup_epochs=self.args.warmup_epochs)\n            # print(f\"------------>Rescheduled decayed LR: {self.untuned_lr:.8f}<--------------------\")\n\n            if self.args.temp_balance_lr != 'None':\n                ######################  TBR scheduling ##########################\n                ##################################################################\n\n                # print(\"---------- Schedule by Temp Balance---------------\")\n                assert len(metric_summary) > 0, \"in TBR, every epoch should has an updated metric summary\"\n                if self.args.remove_first_layer == 'True':\n                    # print('remove first layer <--------------------')\n                    layer_stats = layer_stats.drop(labels=0, axis=0)\n                    # index must be reset otherwise next may delete the wrong row \n                    layer_stats.index = list(range(len(layer_stats[self.args.metric])))\n                if self.args.remove_last_layer == 'True':\n                    # print('remove last layer <--------------------')\n                    layer_stats = layer_stats.drop(labels=len(layer_stats) - 1, axis=0)\n                    # index must be reset otherwise may delete the wrong row \n                    layer_stats.index = list(range(len(layer_stats[self.args.metric])))\n                \n                ####remove with the few eig values\n                drop_layers = layer_stats['longname'].isin(layer_with_few_eigs)\n                layer_stats = layer_stats[~drop_layers]\n\n                metric_scores = np.array(layer_stats[self.args.metric])\n                scheduled_lr = get_layer_temps(self.args, self.args.temp_balance_lr, metric_scores, self.untuned_lr)\n                layer_stats['scheduled_lr'] = scheduled_lr\n                layer_name_to_tune = list(layer_stats['longname'])\n                all_params_lr = []\n                c = 0\n                \n                #####check the few eig values layers were removed\n                for name, module in self.model.named_modules():\n                    if name in layer_name_to_tune:\n                        assert name not in layer_with_few_eigs\n\n\n                for name, module in self.model.named_modules():\n                    if name in layer_name_to_tune:\n                        # params_to_tune_ids += list(map(id, module.parameters()))\n                        scheduled_lr_item = layer_stats[layer_stats['longname'] == name]['scheduled_lr'].item()\n                        all_params_lr.append(scheduled_lr_item)\n                        c = c + 1\n                    elif self.args.batchnorm == 'True' \\\n                        and isinstance(module, nn.BatchNorm2d) \\\n                            and name.replace('bn', 'conv') in layer_name_to_tune:\n                        # params_to_tune_ids += list(map(id, module.parameters()))\n                        scheduled_lr_item = layer_stats[layer_stats['longname'] == name.replace('bn', 'conv')]['scheduled_lr'].item()\n                        all_params_lr.append(scheduled_lr_item)\n                        c = c + 1\n\n                # layer_stats.to_csv(os.path.join(self.save_dir, 'stats', f\"layer_stats_with_lr_epoch_{self.epoch}.csv\"))\n                # if self.best_fitness == self.fitness:\n                #     layer_stats.to_csv(os.path.join(self.save_dir, 'stats', f\"layer_stats_with_lr_epoch_best.csv\"))\n                for index, param_group in enumerate(self.optimizer.param_groups):\n                    #param_group['epoch'] = param_group['epoch'] + 1\n                    if index <= c - 1:\n                        param_group['lr'] = all_params_lr[index]\n                    else:\n                        param_group['lr'] = self.untuned_lr\n            ##################################################################\n            ##################################################################\n            else:\n                # print(\"------------>  Schedule by default\")\n                for param_group in self.optimizer.param_groups:\n                    #param_group['epoch'] = param_group['epoch'] + 1\n                    param_group['lr'] = self.untuned_lr\n\n            # tnow = time.time()\n            # self.epoch_time = tnow - self.epoch_time_start\n            # self.epoch_time_start = tnow\n            # self.elapsed_time += self.epoch_time\n            # print('| Elapsed time : %d:%02d:%02d'  %(cf.get_hms(self.elapsed_time)))\n            # print('--------------------> <-----------------')\n            # self.run_callbacks('on_fit_epoch_end')\n            # torch.cuda.empty_cache()  # clears GPU vRAM at end of epoch, can help with out of memory errors\n\n            # Early Stopping\n            if RANK != -1:  # if DDP training\n                broadcast_list = [self.stop if RANK == 0 else None]\n                dist.broadcast_object_list(broadcast_list, 0)  # broadcast 'stop' to all ranks\n                if RANK != 0:\n                    self.stop = broadcast_list[0]\n            if self.stop:\n                break  # must break all DDP ranks\n            \n            # print(\"################ END Epoch#############\")\n        #########epoch circle end#############\n        # if RANK in (-1, 0):\n        #     # Do final val with best.pt\n        #     LOGGER.info(f'\\n{epoch - self.start_epoch + 1} epochs completed in '\n        #                 f'{(time.time() - self.train_time_start) / 3600:.3f} hours.')\n            \n        #     float_array = [epoch - self.start_epoch + 1, (time.time() - self.train_time_start)]\n        #     df = pd.DataFrame({\"Float Values\": float_array})\n        #     csv_file_path = \"end_time.csv\"\n        #     df.to_csv(os.path.join(self.save_dir, 'stats', csv_file_path))\n\n        #     self.final_eval()\n        #     if self.args.plots:\n        #         self.plot_metrics()\n        #     self.run_callbacks('on_train_end')\n        # torch.cuda.empty_cache()\n        # self.run_callbacks('teardown')",
        "experimental_info": "Method: TempBalance dynamically adjusts layer-wise learning rates based on the PL Alpha Hill metric derived from the Empirical Spectral Density (ESD) of neural network weight matrices. The method calculates eigenvalues of the correlation matrix (W^T W) for each `nn.Conv2d` and `nn.Linear` layer.\n\nExperimental Settings:\n- **PL Alpha Hill Metric**: The `alpha` value extracted from the power law fit is used as the metric to assign learning rates.\n- **Power Law Fitting**: The Hill estimator is used, with `\nmin` fixed as the median of the Empirical Spectral Density (ESD). This corresponds to `fix_fingers='xmin_mid'` and `xmin_pos=2`.\n- **Learning Rate Assignment Function**: A linear, scale-free mapping (`temp_balance_lr='tb_linear_map'`) is used to convert PL Alpha values to layer-specific learning rates.\n  - Layers with smaller PL Alpha (considered 'overtrained') receive lower learning rates.\n  - Layers with larger PL Alpha (considered 'undertrained') receive higher learning rates.\n- **Layer-wise Learning Rate Range**: Determined relative to a global base learning rate (`lr0`). The range is bounded by `lr_min_ratio` and `lr_slope`. Default values from `runtrain.py` are `lr_min_ratio=0.0` and `lr_slope=0.0` (which would imply all tuned layers get `0.0 * lr0` unless specifically overridden).\n- **Layers Excluded from Tuning**: \n  - The first layer (`remove_first_layer=True`).\n  - The last layer (`remove_last_layer=True`).\n  - Layers with a number of eigenvalues less than or equal to a threshold (`tb_eig_filter=64`).\n- **Batch Normalization Layers**: Learning rates for `nn.BatchNorm2d` layers are adjusted to correspond to their preceding `nn.Conv2d` or `nn.Linear` layers (`batchnorm=True`).\n- **ESD Analysis Frequency**: ESD analysis and subsequent learning rate rescheduling are performed every `ww_interval=1` epoch.\n- **Stage-wise Learning Rate Adjustment**: If `stage_epoch` is set and the current epoch surpasses it, `lr_min_ratio` can be updated to `lr_min_ratio_stage2` (default 1).\n- **Eigenvalue Threshold for Filtering**: `EVALS_THRESH=0.00001` to filter near-zero eigenvalues. `filter_zeros=False` by default, meaning small eigenvalues are generally not filtered before fitting, unless `tb_eig_filter` applies."
      }
    },
    {
      "title": "AutoLRS: Automatic Learning-Rate Schedule by Bayesian Optimization on the Fly",
      "abstract": "The learning rate (LR) schedule is one of the most important hyper-parameters\nneeding careful tuning in training DNNs. However, it is also one of the least\nautomated parts of machine learning systems and usually costs significant\nmanual effort and computing. Though there are pre-defined LR schedules and\noptimizers with adaptive LR, they introduce new hyperparameters that need to be\ntuned separately for different tasks/datasets. In this paper, we consider the\nquestion: Can we automatically tune the LR over the course of training without\nhuman involvement? We propose an efficient method, AutoLRS, which automatically\noptimizes the LR for each training stage by modeling training dynamics. AutoLRS\naims to find an LR applied to every $\\tau$ steps that minimizes the resulted\nvalidation loss. We solve this black-box optimization on the fly by Bayesian\noptimization (BO). However, collecting training instances for BO requires a\nsystem to evaluate each LR queried by BO's acquisition function for $\\tau$\nsteps, which is prohibitively expensive in practice. Instead, we apply each\ncandidate LR for only $\\tau'\\ll\\tau$ steps and train an exponential model to\npredict the validation loss after $\\tau$ steps. This mutual-training process\nbetween BO and the loss-prediction model allows us to limit the training steps\ninvested in the BO search. We demonstrate the advantages and the generality of\nAutoLRS through extensive experiments of training DNNs for tasks from diverse\ndomains using different optimizers. The LR schedules auto-generated by AutoLRS\nlead to a speedup of $1.22\\times$, $1.43\\times$, and $1.5\\times$ when training\nResNet-50, Transformer, and BERT, respectively, compared to the LR schedules in\ntheir original papers, and an average speedup of $1.31\\times$ over\nstate-of-the-art heavily-tuned LR schedules.",
      "full_text": "Published as a conference paper at ICLR 2021 AUTO LRS: A UTOMATIC LEARNING -RATE SCHEDULE BY BAYESIAN OPTIMIZATION ON THE FLY Yuchen Jin, Tianyi Zhou, Liangyu Zhao University of Washington {yuchenj, tianyizh, liangyu}@cs.washington.edu Yibo Zhu, Chuanxiong Guo ByteDance Inc. {zhuyibo, guochuanxiong}@bytedance.com Marco Canini KAUST marco@kaust.edu.sa Arvind Krishnamurthy University of Washington arvind@cs.washington.edu ABSTRACT The learning rate (LR) schedule is one of the most important hyper-parameters needing careful tuning in training DNNs. However, it is also one of the least auto- mated parts of machine learning systems and usually costs signiﬁcant manual effort and computing. Though there are pre-deﬁned LR schedules and optimizers with adaptive LR, they introduce new hyperparameters that need to be tuned separately for different tasks/datasets. In this paper, we consider the question: Can we auto- matically tune the LR over the course of training without human involvement? We propose an efﬁcient method, AutoLRS, which automatically optimizes the LR for each training stage by modeling training dynamics. AutoLRS aims to ﬁnd an LR applied to every τ steps that minimizes the resulted validation loss. We solve this black-box optimization on the ﬂy by Bayesian optimization (BO). However, col- lecting training instances for BO requires a system to evaluate each LR queried by BO’s acquisition function forτ steps, which is prohibitively expensive in practice. Instead, we apply each candidate LR for onlyτ′≪τ steps and train an exponential model to predict the validation loss after τ steps. This mutual-training process be- tween BO and the loss-prediction model allows us to limit the training steps invested in the BO search. We demonstrate the advantages and the generality of AutoLRS through extensive experiments of training DNNs for tasks from diverse domains using different optimizers. The LR schedules auto-generated by AutoLRS lead to a speedup of 1.22×, 1.43×, and 1.5×when training ResNet-50, Transformer, and BERT, respectively, compared to the LR schedules in their original papers, and an average speedup of 1.31×over state-of-the-art heavily-tuned LR schedules. 1 I NTRODUCTION In the regime of deep learning, the success of training largely depends on the choice of the learning rate (LR) schedule, since most optimizers will have difﬁculty traversing a non-smooth and non-convex loss landscape with multiple local minimums and possibly saddle points (Kawaguchi, 2016; Jin et al., 2017; Goodfellow et al., 2016; Li et al., 2018a). To achieve stable and fast convergence towards a solution with good generalization performance, one has to tune the LR schedules carefully for different tasks (Nar & Sastry, 2018; Jastrz˛ ebski et al., 2017). This tuning is usually non-trivial and requires many trial-and-error iterations that are computationally expensive. Moreover, the randomness of the widely-used mini-batch stochastic gradient descent (SGD) may introduce more uncertainty and difﬁculty in the tuning process. For the same reasons, it is also hard to directly formulate the search of the LR schedule as a well-posed optimization problem and address it through standard optimization. 1 arXiv:2105.10762v1  [cs.LG]  22 May 2021Published as a conference paper at ICLR 2021 The broadly-adopted strategy is to either pick one from a family of pre-deﬁned LR schedules or apply an optimizer that has a built-in mechanism changing the LR adaptively. However, we have a limited number of choices for pre-deﬁned LR schedules, most of which are simple functions such as exponent or cosine and thus cannot perfectly align with the non-smooth loss landscape. The latter set of adaptive optimizers, e.g., Adam (Kingma & Ba, 2015) and Adadelta (Zeiler, 2012), are extended from convex optimization and rely on strong assumptions to make the convergence properties hold. Moreover, the methods in both categories introduce new hyper-parameters that have to be tuned separately for different tasks or datasets, requiring signiﬁcant human involvement. In this paper, we study the question: can we automatically tune the LR over the course of training without human involvement? At the beginning of everyτ steps (i.e., a “stage” in our method), we seek to identify an LR that optimizes the validation loss (i.e., an empirical estimate of the generalization error) at the end of the stage. To do so, we employ Bayesian optimization (BO) that treats the validation loss as a black-box function of LR. BO simultaneously updates a posterior estimation of the black-box function and searches for the best LR with respect to the posterior. This approach is, however, computationally expensive since estimating the posterior needs many (input, output) instances of the function, and acquiring each instance costs τ steps of training. We, therefore, develop a simple yet efﬁcient approximation: for every LR that BO decides to evaluate, we train the model by using the LR for only τ′≪τ steps and use the validation loss over the τ′steps to train a time-series forecasting model that provides a prediction of the validation loss afterτ steps. As we will show later, an exponential model sufﬁces to produce accurate predictions when using a small τ′= τ/10. Then, AutoLRS can allow BO to explore ten different LRs in each stage and still bound the total running time to approximately twice the training cost associated with the generated schedule, i.e., the time spent to ﬁnd the stage-speciﬁc LRs is roughly equal to the time spent training the model with the identiﬁed LRs. AutoLRS does not depend on a pre-deﬁned LR schedule, dataset, or a speciﬁed task and is compatible with almost all optimizers. Hence, it can be generally deployed across a broad range of ML tasks without much human involvement or expensive tuning over choices of LR schedules and their hyperparameters. Moreover, since it directly minimizes the validation loss, it does not only accelerate the convergence but also improves the generalization compared to just minimizing the training loss. Furthermore, AutoLRS only needs to update two extremely light-weight models, i.e., the BO posterior and the exponential forecasting model, and it is efﬁcient in exploring the loss landscape. Hence, it does not result in notable extra costs in either memory or computation. Note that AutoLRS searches for better LRs based on the training dynamics, which can be seen as a form of self- supervision. The interaction between BO and the forecasting model is an example of mutual learning, where one produces training data for the other. In experiments, we apply AutoLRS to train three representative DNNs widely used in practice, i.e., ResNet-50 (He et al., 2016a) on ImageNet classiﬁcation (Russakovsky et al., 2015); Trans- former (Vaswani et al., 2017) and BERT (Devlin et al., 2019) for NLP tasks. Though they have been extensively studied and have hand-tuned LR schedules, the LR schedules computed byAutoLRS are faster than the original, hand-tuned, LR schedules by1.22×, 1.43×, and 1.5×for training ResNet-50, Transformer, and BERT, respectively, in terms of the training steps used to update the DNN (i.e., excluding the costs of the LR/hyperparameter search). It meanwhile achieves test-set performance better or on par with state-of-the-art results. We also carefully hand-tuned two state-of-the-art learning rate schedules, CLR (Smith, 2017) and SGDR (Loshchilov & Hutter, 2017), and conducted more than ten experiments with different CLR/SGDR hyperparameters on each model. AutoLRS still has an average speedup of 1.29×and 1.34×across the three models, in terms of training steps, compared to the best CLR and SGDR LR schedules, respectively. The AutoLRS implementation is available at https://github.com/YuchenJin/autolrs. 2 R ELATED WORK Learning rate scheduling:In contrast to traditional LR schedules with a monotone decreasing sequence of LRs and multi-step LR schedule, a recent class of LR schedules propose to apply multiple cycles of LR decay. Cyclical Learning Rate (CLR) changes LR from a maximal LR (ηmax) to a minimal LR ( ηmin) at a pre-deﬁned frequency and achieves faster convergence for some DNNs (Smith, 2017). The approach requires a “LR range test” to estimate the minimal and maximal LR. The LR range test trains the model with a linearly-increasing LR between a low LR 2Published as a conference paper at ICLR 2021 and a high LR, and ﬁnds the LR range ( [ηmin,ηmax]) over which the training loss decreases. The authors proposed three variants of CLR: triangular2 that halves the maximum LR bound after each cycle; exp_range that exponentially reduces the maximum LR bound after each cycle; and 1cycle containing only one triangular cycle (Smith, 2018). Similar to CLR, Stochastic Gradient Descent with Warm Restarts (SGDR) restarts the LR and then applies cosine annealing/decay at a pre-deﬁned frequency (Loshchilov & Hutter, 2017). Neither CLR or SGDR is automatic, because they are quite sensitive to their hyperparameters, which require careful hand-tuning. CLR and SGDR may even cause undesirable divergence in loss during training with suboptimal hyperparameters (see §5). Learning rate adaptation with hypergradient descent:Aiming for the same goal of automatically tuning the LR, the hypergradient based technique (Almeida et al., 1998; Franceschi et al., 2017; Baydin et al., 2018; Donini et al., 2020) optimizes the LR schedule by applying gradient descent of the objective function w.r.t. the LR during training. In addition to the initial value of the regular LR, it introduces an additional hypergradient LR whose initial value is another hyperparameter to be speciﬁed. We experimentally show that this technique is subject to overﬁtting, it is quite sensitive to its two hyperparameters, and it is unable to match the state-of-the-art test-set performance on the models we test (§A.5.1). We also compare its performance against AutoLRS (§A.5.2). DNN hyperparameter optimization:Automatic hyperparameter searching for DNNs has been broadly studied in recent years. When applied to learning rates, they can determine an optimized value for LR that is kept constant (or constrained to be a pre-deﬁned shape) through the entire training process, as opposed to determining an LR schedule. They can be primarily categorized into Bayesian optimization based approaches (Hutter et al., 2011; Snoek et al., 2012; Bergstra et al., 2013), bandit-based solutions (Li et al., 2017; 2018b), hybrid approaches that combine bandit-based and Bayesian optimization based approaches (Falkner et al., 2018; Zela et al., 2018), and population-based methods (Jaderberg et al., 2017; Parker-Holder et al., 2020). It might be possible to extend these techniques to determine a LR schedule with an optimized LR for each training stage, but it is not sample-efﬁcient and time-efﬁcient to do so since the LR schedule would correspond to hundreds or thousands of hyperparameters. Optimization methods with adaptive LR:These optimizers can adaptively adjust LR for each training step by maintaining an estimate of a better learning rate separately for each parameter in the DNN. Adagrad (Duchi et al., 2011) applies lower LRs to parameters with larger accumulated gradients and higher learning rates to the ones with smaller accumulated gradients. RMSprop (Tieleman & Hinton, 2012), AdaDelta (Zeiler, 2012), and Adam (Kingma & Ba, 2015) were later proposed to address the issue in Adagrad that the model stops learning due to the continual decay of LR. These optimizers with adaptive LR are orthogonal to our automatic LR scheduler, and they still require a global learning rate schedule, which can be obtained from our AutoLRS. In particular, their default hyperparameters do not always work well and need careful tuning, e.g., Adam’s default LR0.001 performs poorly in training BERT and Transformer, and a better-tuned LR schedule can signiﬁcantly reduce the training time (§5). Recent optimization methods (Schaul et al., 2013; Mahsereci & Hennig, 2015) proposed to remove the need for LR tuning in SGD altogether, but they are not widely used potentially due to their limited applicability and sub-optimal performance (Baydin et al., 2018). 3 P ROBLEM FORMULATION Training of DNNs can be written in a general form of minimizing a loss functionL(x; θ) over training samples x∈Dtrain, where θrepresents the model weights being optimized. The minimization is conducted by applying an optimizer that updates θiteratively. For example, at each step t, mini-batch SGD updates θusing the gradient computed on a mini-batch of samples Btrain ⊆Dtrain: θt+1 = θt − ηt |Btrain| ∑ x∈Btrain ∇θL(x; θt), (1) where ηt is the learning rate (LR) at step tand ∇θL(x; θt) denotes the gradient of the loss L(x; θ) w.r.t. θt at step t. Given Btrain and θt, θt+1 can be represented as a function of LR ηt, i.e., θt+1(ηt). Our ultimate goal is to search for an optimal schedule of LR, i.e., a sequence of LRs η1:T ≜ (η1,η2,··· ,ηT) applied to the total T training steps, such that the generalization error can be minimized. Ideally, we need to optimize the entire sequence of LRs. This, however, is intractable in practice given the large number of possible LR schedules and since evaluating each one of those possible LR schedules requires a full training of T steps. Hence, we break down the LR schedule optimization into a dynamic optimization of a constant LR for every τ steps, which we refer to 3Published as a conference paper at ICLR 2021 as a “training stage”. Since most tasks prefer a relatively small LR due to the non-smoothness of DNNs’ loss landscapes, whenτ is also small, the LR-resulted change on the validation loss might be too small and overwhelmed by the randomness of mini-batch SGD. Hence, in this case, we need to increase τ, so the effect of LR η on the validation loss can be accumulated for more steps to overcome noise. A large τ also reduces the frequency of applying LR search and saves computation. On the other hand, setting τ to be too large might lose some optimality of the induced LR schedule. Therefore, we need to trade-off the above two issues to ﬁnd an appropriate τ. In our ﬁnal algorithm, we propose a curriculum for τ, i.e., we start from a small τ, in line with the greater volatility during early stages, and gradually increase τ as training proceeds (as described in §4.4). Since we mainly focus on LR search within a stage, for simplicity, we will use τ instead of τt for the exposition below. We study a greedy approach and split the whole training process into multiple stages of τ steps each. We choose an LR at the beginning of each stage and apply τ steps of optimization using this LR, i.e., at step-t= 0,τ, 2τ,··· ,T −τ, we aim to ﬁnd the LR ηt:t+τ that minimizes the validation loss on Dval (i.e., an estimate of the generalization error) after step-(t+ τ). This can be formulated as: minη ∑ x∈Dval L(x; θt+τ(η)), t= 0,τ, 2τ,··· ,T −τ. (2) We try to sequentially solve ⌊T/τ⌋sub-problems of the above form. However, we cannot apply stan- dard optimization to solve each sub-problem in practice because: (i) it is a high-order optimization of ηsince we need to unrollθt+τ in Eq. (2) backward for τsteps using Eq.(1), which requires prohibitive memory and is unstable for DNNs;(ii) one step of optimizingηneeds to apply τsteps of optimization on θ, which is costly and weakens the advantage of searching LR for better efﬁciency. To avoid these issues, we treat the objective function in Eq. (2) for t: t+ τ as a black-box function ft(η) and study how to optimize it based on the observed training dynamics through Bayesian optimization (BO). 4 A UTOMATIC LEARNING RATE SCHEDULE SEARCH We ﬁrst elaborate on the details of our BO algorithm (§4.1) that identiﬁes the LR for each stage 1. However, collecting even one data point(η,f(η)) for BO requires us to train the model for τ steps, which is costly and impractical since the LR computed by the entire BO process is used for only τ steps. To reduce the cost of generating instances of (η,f(η)), in §4.2 and §A.3, we propose to train a light-weight time-series forecasting model to predict f(η) based on the validation loss observed during the ﬁrst τ′(τ′≪τ) steps of applying LR η. We ﬁnd that a simple exponential model sufﬁces to produce accurate predictions. Our LR search then reduces to a multi-training process between BO and the forecasting model, where one produces training instances for the other. The resulting algorithm can automatically ﬁnd an LR schedule without introducing signiﬁcant extra computation. 4.1 B AYESIAN OPTIMIZATION BO (Shahriari et al., 2016) is one of the state-of-the-art techniques for black-box optimization. It applies exploration and exploitation to the objective by sequentially and actively querying the function values of some input instances. Speciﬁcally, BO uses Gaussian process as a surrogate model (prior) to ﬁt the black-box objective function f(η). It sequentially updates a posterior of f(η) by using its likelihood on newly evaluated (η′ i,yi = f(η′ i) + ϵ) pairs2, where yi is a noisy observation of f(η′ i) and is the validation loss after τ steps. Then, it ﬁnds the next η′ i+1 to evaluate based on an acquisition function ui(η) deﬁned by the posterior mean µi(η) and standard deviation σi(η). ui(η) performs a trade-off between exploration (i.e., large σi(η)) and exploitation (i.e., small µi(η)). In AutoLRS, we use Lower Conﬁdence Bound (LCB) (Cox & John, 1992; Auer, 2002) as ui(η). Given η′ 1:i and their corresponding validation loss y1:i, we determine the next LR ηi+1 by minimizing LCB, i.e., η′ i+1 = arg minηui(η), ui(η) ≜ µi(η) −κσi(η), (3) where µi(η) and σi(η) are deﬁned in Eq. (7) in §A.1, κis a positive hyper-parameter to balance exploration and exploitation. In experiments, κ = 1000 works consistently well. BO repeats the above process until it achieves a precise posterior distribution of f(η). See §A.1 for more details. 1Since this section mainly focuses to solve a sub-problem of Eq.(2) within one stage t: t+τ, we temporarily remove the subscript tfrom ft(η) and other variables/functions that do not change within a stage for simplicity. 2Here i = 1,··· ,k for ksteps in BO: it indexes the exploration step of BO within a training stage and differs from subscript tindexing the training steps. We use superscript ′on η′to mark the LRs explored by BO. 4Published as a conference paper at ICLR 2021 Algorithm 1:AutoLRS Input : (1) Number of steps in each training stage, τ (2) Learning-rate search interval (ηmin,ηmax) (3) Number of LRs to evaluate by BO in each training stage, k (4) Number of training steps to evaluate each LR in BO, τ′ (5) Trade-off weight in the acquisition function of BO, κ 1 while not converge do 2 initialize a GP prior: µ0(η) = 0,σ2 0(η) = K(η,η) deﬁned in Eq. (4) in §A.1; 3 c←checkpoint of model parameters and optimizer states; 4 for i←1 to k do /* mutual-training loop between BO and loss forecasting model */ 5 choose the next LR to explore: η′ i = arg minηµi−1(η) −κσi−1(η); 6 y1:τ′ ←train the DNN with LR η′ i for τ′steps and record the corresponding validation loss series; 7 yn ←train an exponential forecasting model on y1:τ′and predict the validation loss after τ steps; 8 update the GP posterior by (η′ i,yi) and update new µi(η) and σi(η) using Eq. (7) in §A.1; 9 restore the checkpoint cof model parameters and optimizer states; 10 end 11 η∗←the LR with the minimal predicted validation loss µk(η) among the k explored LRs η′ 1:k above; 12 train the DNN using LR η∗for τ steps; /* training model using BO-searched best learning rate */ 13 end 4.2 T IME -SERIES FORECASTING MODEL OF LOSS Typically, BO would requiresτ training steps to measure the validation loss associated with every LR ηthat it considers during a stage. This is computationally expensive. We now introduce a simple yet effective approach that substantially reduces the number of training steps required to evaluate each LR candidate: for each LR ηthat is evaluated, we only apply it for τ′≪τ steps and use the validation loss observed in the τ′steps to train a short-term time-series forecasting model. We then use the resulting forecasting model to predict the validation loss after τ steps. In numerous experiments, we observed that when a DNN is trained with a reasonable LR, the validation loss typically decreases exponentially and converges to a small value. We show examples of practical loss time series and their exponential-model ﬁtting results in Figure 3. Moreover, recent deep learning theory (Allen-Zhu et al., 2019b) also proves the linear convergence of training DNNs. In addition, a simple model to ﬁt the observed loss time-series can ﬁlter the noise and avoid possible overﬁtting. Hence, we propose to train an exponential model in the form of L(t) = aexp(bt) + c with parameters a,c and b< 0 and for t= 1,...,τ , as the forecasting model for the time series of the validation loss in a training stage of τ steps with a given LR η. §A.2 describes how we estimate a, b, and cbased on the validation loss observed in the ﬁrst τ′steps, and §A.3 describes how we ﬁlter out noise and outliers. 4.3 M UTUAL TRAINING BETWEEN BO AND EXPONENTIAL PREDICTION We present the complete procedure of AutoLRS in Algorithm 1. It sequentially optimizes LR for every training stage during the training of a DNN model, solely based on the observed training dynamics, and it can be seen as a form of self-supervision. For each training stage, it searches for the LR that leads to the largest improvement in the validation loss via an efﬁcient black-box function optimization conducted by a mutual training loop between Bayesian optimization and a short-term forecasting model for each loss series. It then applies the best LR among the explored ones for τ steps and repeats the above process until convergence. In line 5, the algorithm solves a constrained optimization problem overη, in the range of[ηmin,ηmax]. In practice, we prefer a large learning-rate search interval (ηmin,ηmax), across orders of magnitude, but also need ﬁne-grained optimization over small LRs. Hence, we operate on ηin its log-scale space, i.e., we replace η by log η in Algorithm 1, except in lines 6 and 12 when we use the original LR (rather than log η) to train the DNN. At the end of each iteration in the mutual training loop (line 9), we restore the checkpoint cof model parameters and optimizer states to the one saved at the beginning of the training stage 3. By doing so, 3We save the checkpoint in the CPU memory, soAutoLRS does not have GPU memory overhead. 5Published as a conference paper at ICLR 2021 we guarantee that the kdifferent LRs all start from the same model and their losses can be compared. §A.4 illustrates how BO learns the underlying function in practice for early and late stages of training. Hyperparameters: AutoLRS substantially reduces the amount of hyperparameters that need to be hand-tuned in existing LR schedules or policies. However, as shown in Algorithm 1, we still have hyperparameters in AutoLRS. First, we need to set a search interval (ηmin,ηmax) for LR. However, this interval can be reasonably wide by using an LR range test (Loshchilov & Hutter, 2017) as we will show in §5. Secondly, our default settings of k, τ′, τ, and κwork well for a diverse set of DNN models from different domains and tasks, though it is possible to achieve further improvements by ﬁne-tuning them. 4.4 P RACTICAL IMPROVEMENTS We found the following modiﬁcations can further improve the performance of AutoLRS in practice. Gradually increaseτ over the course of training:Often, in DNN training, the loss and the model parameters experience rapid changes only during the ﬁrst few epochs before they enter a phase of stable improvement. Our approach can adapt to this phenomenon. For the early stages, when the loss is less predictable for the time-series forecasting model, we use a small τ (and τ′). As training proceeds and the model becomes stable, we gradually increase τ (and τ′) and adjust the LR more lazily. This curriculum of increasing τ places more exploration in earlier stages and more exploitation in later stages. In practice, we start with τ = 1000 and τ′ = 100, and double them after every stage until it reaches τmax. τmax is a hyperparameter that limits the maximum number of steps in a stage. We will discuss more of τmax in §5. This gradual increase of τ can provide stability to the LR schedule search. Similar strategies have been widely used in previous pre-deﬁned LR schedules, e.g., the multi-stage schedule with increasing epochs within each stage, and some recent cyclical LR schedules (Loshchilov & Hutter, 2017). Minimizing training loss in early stages:Computing the validation loss series for a candidate η′ requires considerable computation if we were to use the entire validation dataset at each step of mutual training. Recall, however, that the primary purpose of minimizing the validation loss instead of the training loss is to avoid overﬁtting on the training set when the training loss notoriously deviates from the generalization error. However, a variety of empirical evidence and recent theory (Allen-Zhu et al., 2019a) show that overﬁtting is unlikely while training over-parameterized DNNs due to the inductive bias of random initialization and SGD, especially during the early phase of training. Hence, in practice, for the ﬁrst several training stages, we can safely approximate the validation loss in our method by the corresponding training loss, which is a by-product of forward propagation and free to obtain. In later stages (i.e., once τ reaches τmax), since the model is stable and the loss changes smoothly, we can evaluate the validation loss on a small subset of the validation set without compromising robustness. In our experiments, this set is composed of merely 10 mini-batches, and we evaluate the validation loss on them every 50 training steps (as opposed to every step). Therefore, the evaluation of validation loss in our approach does not introduce notable extra computations4. 5 E XPERIMENTS We now evaluateAutoLRS by applying it to three widely-used and representative DNNs: ResNet-50, Transformer, and BERT. Here are some highlights: • The LR schedules computed by AutoLRS are 1.22×, 1.43×, and 1.5×faster, in terms of train- ing steps, than the original, hand-tuned LR schedules for ResNet-50, Transformer, and BERT, respectively. Meanwhile, it improves or matches the test-set performance. • For each model, we carefully hand-tuned CLR and SGDR using more than ten experiments with different CLR/SGDR hyperparameters. Across the three models, the LR schedules computed by AutoLRS achieve an average speedup of 1.29×and 1.34×, in terms of training steps, over the best tuned LR schedules under CLR and SGDR, respectively. While CLR and SGDR had to be run 4The per-candidate-LR total cost of evaluating validation loss during the BO search in a later stage is 10 ·∆ ·⌊τ′/50⌋, where ∆ is the time for computing the validation loss on one mini-batch. Since the cost of back propagation is roughly twice as that of forward propagation (Wen et al., 2018), the total cost of evaluating validation loss is approximately 1/15 of the training time spent on BO search. 6Published as a conference paper at ICLR 2021 for at least 10 trials to ﬁnd a good LR schedule, AutoLRS only costs slightly over 2×the training time associated with the computed LR schedule even after accounting for the BO search cost. • AutoLRS is robust to the change of hyperparameters and consistently ﬁnds better LR schedules than other baselines. In contrast, CLR and SGDR are sensitive to the choices of hyperparameters. • We perform ablation studies in §A.5.4 to demonstrate that both BO and the exponential forecasting model are essential for AutoLRS to ﬁnd good LR schedules. • Hypergradient descent is subject to overﬁtting, and it is unable to match the state-of-the-art test-set performance using all the guideline values of its two hyperparameters on VGG-16 (Simonyan & Zisserman, 2015) and ResNet-50 (§A.5.1). In contrast, AutoLRS can consistently improve or match the state-of-the-art test-set performance with different τmax values using fewer training steps than the hand-tuned LR schedules (§A.5.2). • Using Hyperband (Li et al., 2017) for LR schedule search incurs a high computational overhead. Moreover, it cannot ﬁnd an LR schedule that matches the state-of-the-art accuracy (§A.5.3). Baseline Setup:ML practitioners typically need to hand-tune the LR schedules carefully for a long time to achieve satisfying performance, so the LR schedule adopted in each model’s original paper is a presumably tough-to-beat baseline to compare with. For CLR and SGDR, we hand-tune their hyperparameters separately for each DNN. Hyperparameters in CLR include the high/low LR for the LR range test to sweep, the number of steps to perform the test, the number of steps in each triangular cycle, and the choice of variants (triangular2, exp_range, 1cycle) introduced in §2. Hyperparameters in SGDR include the number of steps/epochs in each cycle and the initial LR at the beginning of each cycle. We carefully tuned these hyperparameters separately for each DNN and chose the LR schedule producing the best validation-set performance among ≥10 trials of different hyperparameters. Hyperparameters in AutoLRS:In our default setting, we set k = 10 and τ′ = τ/10 so that the training steps spent on BO equals the training steps spent on updating the DNN model. We start from τ = 1000 and τ′ = 100 and double τ and τ′after each stage until τ reaches τmax. We use τmax = 8000 for ResNet-50 and Transformer, τmax = 32000 for BERT. We also triedτmax = 8000, 16000, and 32000 for each DNN and found that the resulting LR schedules are not very sensitive to τmax. (An analysis of the sensitivity to τmax is in §A.5.2.) The LR search interval (ηmin,ηmax) for ResNet-50, Transformer, and BERT are (10−3,1), (10−4,10−2), and (10−6,10−3), respectively. These are easily found by an LR range test (Loshchilov & Hutter, 2017). ResNet-50: ResNet (He et al., 2016a;b) is one of the most popular DNNs in computer vision tasks. We train ResNet-50 on ImageNet (Russakovsky et al., 2015) using SGD with momentum on 32 NVIDIA Tesla V100 GPUs with data parallelism and a mini-batch size of 1024. The LR schedule in the original paper adopts a warmup phase of 5 epochs at the beginning and performs a 3-step decay as in (Goyal et al., 2017). Figure 1a presents different LR schedules for training ResNet-50 on ImageNet. We report how their top-1 accuracy on the validation set 5 changes during training in Figure 1b. AutoLRS achieves a speedup of 1.19×and 1.22×over SGDR and the original LR schedule respectively but is slightly (i.e., 5.4%) slower than CLR. Note that the best CLR result is achieved after 10 trials of heavy hand-tuning to hyperparameters. (In fact, 7 out of 10 CLR trials failed to achieve the best possible test-set accuracy, and the second best and the third best trials are 5.4% and 7.9% slower than AutoLRS). AutoLRS achieves competitive speed even though it invests a signiﬁcantly lower search cost that is comparable to the overall model update time associated with the identiﬁed LR schedule. Transformer: Transformer (Vaswani et al., 2017) is a neural machine translation (NMT) model that is built upon a multi-head self-attention mechanism to capture the contextual dependencies and achieves promising translation performance. We train Transformer6 on a standard benchmark, i.e., WMT 2014 English-German dataset, using 8 NVIDIA Tesla V100 GPUs. Following (Vaswani et al., 2017), we use Adam (Kingma & Ba, 2015) with β1 = 0.9, β2 = 0.98, and ϵ = 10−9. The LR schedule in the original paper starts from a linear warmup of 4,000 steps from 0 to 7e−4, followed by 96,000 steps of decaying the LR proportionally to 1/ √ tfor step-t. In AutoLRS, we also use the same linear warmup. The current AutoLRS does not search LR for warmup steps since warmup 5All the top-1 accuracy on ImageNet/CIFAR-10/CIFAR-100 we report in the paper is evaluated on all the validation/test data excluding every sample used during training. We report top-1 validation accuracy on ImageNet and top-1 test accuracy on CIFAR-10/CIFAR-100 becasue Imagenet does not disclose the labels of its test set, and CIFAR-10/CIFAR-100 datasets only have the test sets (10,000 test images). 6We used the code at https://github.com/tensorﬂow/models/tree/fd3314e/ofﬁcial/transformer, which achieves 27.3 BLEU (uncased) using the original LR schedule. 7Published as a conference paper at ICLR 2021 0 20 40 60 80 Training Epoch 0.0 0.5 1.0Learning Rate AutoLRS Original CLR SGDR (a) LR on ResNet-50. 0 10 20 30 40 50 60 70 80 90 Training Epoch 0.0 0.2 0.4 0.6Top-1 Accuracy AutoLRS Original CLR SGDR (b) Val.Acc. on ResNet-50. 0 20000 40000 60000 80000 100000 Training Step 0.0 0.2 0.4 0.6 0.8 1.0Learning Rate 1e 3 AutoLRS Original CLR SGDR (c) LR for Transformer. 0 20000 40000 60000 80000 100000 Training Step 20 22 24 26 28BLEU AutoLRS Original CLR SGDR (d) BLEU of Transformer. Figure 1: Comparison of different LR schedules in training ResNet-50 on ImageNet (a, b), and the Transformer base model (c, d). When training ResNet-50, AutoLRS, CLR, SGDR, and the original LR achieve 75.9% top-1 accuracy at epoch 74, 70, 88, and 90, respectively. When training Transformer base, AutoLRS, SGDR, and original achieve 27.3 BLEU score (uncased) at step 69,000, 91,000, 98,000, respectively. CLR (the best we were able to ﬁnd) achieves 27.2 BLEU score at step 99,000. 0 50000 100000 150000 200000 250000 Training Step 0 2 4 6Learning Rate 1e 4 AutoLRS Original CLR SGDR (a) LR schedules (Phase 1 + 2). 0 50000 100000 150000 200000 Training Step 2 4 6 8Training Loss AutoLRS Original CLR SGDR (b) Training loss in Phase 1. 150000 175000 200000 225000 250000 Training Step 2 3 4Training Loss AutoLRS Original CLR SGDR (c) Training loss in Phase 2. Figure 2: Comparison of different LR schedules and training loss in pre-training BERTBASE. does not have an explicit optimization objective, such as minimizing the validation loss. Warmup usually takes very few steps, and its main purpose is to prevent deeper layers in a DNN from creating training instability (Gotmare et al., 2019). Figure 1c visualizes different LR schedules in training the Transformer model. Their BLEU scores on the test set during training are reported in Figure 1d. Overall, the LR schedule searched by AutoLRS yields a 1.32 −1.43×speedup over the hand-tuned LR schedules. AutoLRS consistently achieves a similar amount of speedup over three trials – they achieve 27.3 BLEU score (uncased) at step 69,000, 69,000, and 70,000, respectively. Interestingly, if we continue the LR search of AutoLRS, we can get 27.4 BLEU score (uncased) at step 99,000. BERT Pre-training:BERT (Devlin et al., 2019) is a recent model that achieved state-of-the-art results on 11 NLP tasks. It ﬁrst pre-trains a language representation model on a large text corpus by unsupervised learning and then ﬁne-tunes it for downstream NLP tasks. The BERTBASE model has 110M parameters, which makes the pre-training phase expensive, and hand-tuning the LR schedule might be impractical. We pre-train BERTBASE with mixed precision (Micikevicius et al., 2018) on the English Wikipedia and the BooksCorpus dataset 7 (Zhu et al., 2015a). Following the original paper, we use Adam with L2 weight decay of 0.01 and β1 = 0.9, β2 = 0.999. The pre-training is divided into two phases: Phase 1 includes 90% of the total training steps and uses a sequence length of 128, while Phase 2 uses a sequence length of 512 for the rest 10% of training steps. We apply this two-phase training in the experiments of all LR schedules. We pre-train BERTBASE on 32 NVIDIA Tesla V100 GPUs using a mini-batch size of 1024 sequences, which is 4×the batch size in the original paper. To adapt the original LR schedule to our batch size, we tried both the linear scaling rule (Goyal et al., 2017) and the square root scaling rule (Krizhevsky, 2014), and found that the square root scaling rule works better while the linear scaling rule made the loss diverge. As shown in Figure 2, Phase 1 contains 150,000/225,000 steps and Phase 2 contains 16,000/25,000 steps respectively for AutoLRS and all baselines, since AutoLRS requires much less total steps. In both AutoLRS and SGDR, we apply a linear warmup in the ﬁrst 2,500 steps to make the deeper layers of BERT stable. in Figures 2b and 2c, we report the training loss achieved by different schemes. We ﬁne-tune the pre-trained models on four downstream NLP tasks: Microsoft Research Paraphrase Corpus (MRPC) for identifying semantic textual similarity (Dolan & Brockett, 2005); Multi-Genre Natural Language Inference (MNLI) for entailment classiﬁcation (Williams et al., 2018); Corpus of Linguistic Acceptability (CoLA) for predicting whether an English sentence is linguistically ac- ceptable (Warstadt et al., 2019); and Stanford Question Answering Dataset (SQuAD) v1.1 (Rajpurkar et al., 2016). Table 1 reports the after-ﬁne-tuning performance on the four tasks. Since ﬁne-tuning performance is unstable on small datasets like MRPC, we ﬁne-tuned on each task several times and report the best Dev-set performance. It shows that the model pre-trained by AutoLRS outperforms 7We collected books from smashwords.com and built our own dataset with 980M words because the authors of BooksCorpus and no longer have the dataset available for public download (Zhu et al., 2015b). 8Published as a conference paper at ICLR 2021 Table 1: Fine-tuning BERTBASE that is pre-trained using different LR schedules on 4 downstream tasks. We report the accuracy on the Dev set of MRPC, MNLI, and CoLA, and F1 scores on the Dev set of SQuAD v1.1. LR schedule (Phase 1/Phase 2) MRPC MNLI CoLA SQuAD v1.1 Original (225,000/25,000) 86.5 82.2 47.8 87.0 CLR (225,000/25,000) 86.0 80.7 44.4 86.5 SGDR (225,000/25,000) 84.8 81.6 38.7 86.2 AutoLRS (150,000/16,000) 88.0 82.5 47.6 87.1 Table 2: Performance comparison with LR schedules searched by prior solutions on CIFAR-10 training with VGG-16 (batch size = 128). Note that the hand-tuned LR schedule can achieve 93.70% top-1 test accuracy in 350 epochs. The Runtime column shows how long each method takes on one NVIDIA Titan RTX GPU to ﬁnd the LR schedule shown in the previous column. The runtime of HD and MARTHE include trying the guideline values of their hyperparameters to get a decent LR schedule. Method Best top-1 accuracy achieved in 350 epochs Runtime (seconds) HD 91.31% 187,110 MARTHE 92.99% 67,578 Hyperband 93.24% 109,454 AutoLRS 94.13% 6,538 those using other LR schedules in most downstream tasks and meanwhile achieves a speedup of 1.5×. Note AutoLRS consistently achieves this speedup over 3 trials (details in §A.5.5). We also tried pre-training using other LR schedules for fewer steps but the ﬁne-tuning performances were worse. Notably, when we use CLR and SGDR for pre-training BERTBASE, the training loss diverged after 100,000 steps in several trials, even as we decreased the maximal LR and increased the number of steps per cycle. This illustrates how difﬁcult and computationally intensive it is to hand-tune the hyperparameters of existing LR schedules on complicated models and tasks. In contrast, AutoLRS signiﬁcantly simpliﬁes the process and saves human effort. Experimental comparison to prior methods:Hypergradient descent (HD) (Baydin et al., 2018) is a hypergradient based method to adjust the learning rate in an online fashion by deriving the derivative of the training loss with respect to the learning rate, and performing gradient descent on the learning rate during training. MARTHE (Donini et al., 2020) is a generalization of two hypergradient based methods, HD and RTHO (Franceschi et al., 2017). One distinction between MARTHE and HD is that MARTHE computes the gradient of the validation loss instead of training loss with respect to the learning rate. Hyperband is a multi-armed bandit approach for DNN hyperparameter optimization. We use HD, MARTHE, and Hyperband to tune the LR schedules for CIFAR-10 training with VGG-16, and compare their performance with AutoLRS in Table 2. AutoLRS achieves higher best top-1 test accuracy than the other methods as well as the hand-tuned LR schedule, with much less overhead. Detailed descriptions of these methods and the experimental results are in §A.5.1 and §A.5.3. 6 C ONCLUSION We propose an automatic learning-rate schedule method, AutoLRS, as a more efﬁcient and versatile alternative to hand-tuning that can be broadly applied to train different DNNs for tasks in diverse application domains. We break down the sequence optimization to learning rate search for minimizing validation loss in each training stage and then solve this sub-problem by Bayesian optimization (BO). To reduce the cost of BO exploration, we train a light-weight loss-forecasting model from the early- stage training dynamics of BO exploration. AutoLRS achieves a speedup of 1.22×, 1.43×, and 1.5×on training ResNet-50, Transformer, and BERT compared to their highly hand-tuned schedules. ACKNOWLEDGMENTS We would like to thank the anonymous ICLR reviewers for their valuable feedback. We would also like to thank Damien Fay for his suggestions on time series analysis. This work was partially supported by DARPA. For computer time, this research used the resources at ByteDance and the Supercomputing Laboratory at KAUST. 9Published as a conference paper at ICLR 2021 REFERENCES Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized neural networks, going beyond two layers. In Advances in neural information processing systems, pp. 6158–6169, 2019a. Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over- parameterization. In Proceedings of the 36th International Conference on Machine Learning , volume 97, pp. 242–252, 2019b. Luís B Almeida, Thibault Langlois, José D Amaral, and Alexander Plakhov. Parameter adaptation in stochastic optimization. On-Line Learning in Neural Networks, Publications of the Newton Institute, pp. 111–134, 1998. Peter Auer. Using conﬁdence bounds for exploitation-exploration trade-offs. Journal of Machine Learning Research, 3(Nov):397–422, 2002. Atılım Güne¸ s Baydin, Robert Cornish, David Martínez Rubio, Mark Schmidt, and Frank Wood. Online learning rate adaptation with hypergradient descent. In International Conference on Learning Representations, 2018. James Bergstra, Dan Yamins, and David D Cox. Hyperopt: A python library for optimizing the hyperparameters of machine learning algorithms. In Proceedings of the 12th Python in science conference, pp. 13–20. Citeseer, 2013. Dennis D Cox and Susan John. A statistical method for global optimization. In Proceedings of the 1992 IEEE International Conference on Systems, Man, and Cybernetics, pp. 1241–1246. IEEE, 1992. Zhongxiang Dai, Haibin Yu, Bryan Kian Hsiang Low, and Patrick Jaillet. Bayesian optimization meets Bayesian optimal stopping. In International Conference on Machine Learning, pp. 1496–1506, 2019. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. William B Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005. Tobias Domhan, Jost Tobias Springenberg, and Frank Hutter. Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves. InIJCAI, pp. 3460–3468, 2015. Michele Donini, Luca Franceschi, Orchid Majumder, Massimiliano Pontil, and Paolo Frasconi. MARTHE: Scheduling the learning rate via online hypergradients. In IJCAI-20, pp. 2119–2125, 2020. John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(61):2121–2159, 2011. Stefan Falkner, Aaron Klein, and Frank Hutter. BOHB: Robust and efﬁcient hyperparameter opti- mization at scale. arXiv preprint arXiv:1807.01774, 2018. Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. Forward and reverse gradient-based hyperparameter optimization. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 1165–1173. PMLR, 2017. Marc G Genton. Classes of kernels for machine learning: a statistics perspective. Journal of machine learning research, 2(Dec):299–312, 2001. Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT press, 2016. 10Published as a conference paper at ICLR 2021 Deepak Akhilesh Gotmare, Shirish Nitish Keskar, Caiming Xiong, and Richard Socher. A closer look at deep learning heuristics: Learning rate restarts, warmup and distillation. international conference on learning representations, 2019. Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training ImageNet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016a. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European conference on computer vision, pp. 630–645. Springer, 2016b. Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Sequential model-based optimization for general algorithm conﬁguration. In International conference on learning and intelligent optimization, pp. 507–523. Springer, 2011. Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, et al. Population based training of neural networks. arXiv preprint arXiv:1711.09846, 2017. Kevin Jamieson and Ameet Talwalkar. Non-stochastic best arm identiﬁcation and hyperparameter optimization. In Artiﬁcial Intelligence and Statistics, pp. 240–248, 2016. Stanisław Jastrz˛ ebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amos Storkey. Three factors inﬂuencing minima in SGD. arXiv preprint arXiv:1711.04623, 2017. Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle points efﬁciently. In Proceedings of the 34th International Conference on Machine Learning- Volume 70, pp. 1724–1732. JMLR. org, 2017. Kenji Kawaguchi. Deep learning without poor local minima. In Advances in neural information processing systems, pp. 586–594, 2016. Diederick P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015. A. Klein, Stefan Falkner, Jost Tobias Springenberg, and F. Hutter. Learning curve prediction with Bayesian neural networks. In International Conference on Learning Representations, 2017. A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Master’s thesis, Department of Computer Science, University of Toronto, 2009. Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks. arXiv preprint arXiv:1404.5997, 2014. Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. CIFAR-100 (Canadian Institute for Advanced Research). URL http://www.cs.toronto.edu/~kriz/cifar.html. Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of neural nets. In Advances in Neural Information Processing Systems, pp. 6389–6399, 2018a. Liam Li, KG Jamieson, Afshin Rostamizadeh, Ekaterina Gonina, MH Jonathan Ben-Tzur, B Recht, and A Talwalkar. A system for massively parallel hyperparameter tuning. In Conference on Machine Learning and Systems, 2020a, volume 1, 2018b. Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyperband: A novel bandit-based approach to hyperparameter optimization. The Journal of Machine Learning Research, 18(1):6765–6816, 2017. Ilya Loshchilov and Frank Hutter. SGDR: stochastic gradient descent with warm restarts. In International Conference on Learning Representations, 2017. 11Published as a conference paper at ICLR 2021 Maren Mahsereci and Philipp Hennig. Probabilistic line searches for stochastic optimization. In Advances in Neural Information Processing Systems, volume 28, 2015. Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training. In International Conference on Learning Representations, 2018. Kamil Nar and Shankar Sastry. Step size matters in deep learning. In Advances in Neural Information Processing Systems, pp. 3436–3444, 2018. Jack Parker-Holder, Vu Nguyen, and Stephen Roberts. Provably efﬁcient online hyperparameter optimization with population-based bandits. Advances in Neural Information Processing Systems, 2020. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2383–2392, Austin, Texas, November 2016. Association for Computational Linguistics. CE. Rasmussen and CKI. Williams. Gaussian Processes for Machine Learning. Adaptive Computa- tion and Machine Learning. MIT Press, Cambridge, MA, USA, January 2006. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115 (3):211–252, 2015. Tom Schaul, Sixin Zhang, and Yann LeCun. No more pesky learning rates. In International Conference on Machine Learning, pp. 343–351, 2013. Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P. Adams, and Nando De Freitas. Taking the human out of the loop: A review of Bayesian optimization. Proceedings of the IEEE, 104(1): 148–175, January 2016. ISSN 0018-9219. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations, 2015. Leslie N Smith. Cyclical learning rates for training neural networks. In 2017 IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 464–472. IEEE, 2017. Leslie N Smith. A disciplined approach to neural network hyper-parameters: Part 1–learning rate, batch size, momentum, and weight decay. arXiv preprint arXiv:1803.09820, 2018. Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine learning algorithms. In Advances in neural information processing systems, pp. 2951–2959, 2012. Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Freeze-thaw bayesian optimization. arXiv preprint arXiv:1406.3896, 2014. T. Tieleman and G. Hinton. Lecture 6.5—RmsProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pp. 5998–6008, 2017. Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments. Transactions of the Association for Computational Linguistics, 7:625–641, 2019. Yeming Wen, Paul Vicol, Jimmy Ba, Dustin Tran, and Roger Grosse. Flipout: Efﬁcient pseudo- independent weight perturbations on mini-batches. In International Conference on Learning Representations, 2018. 12Published as a conference paper at ICLR 2021 Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technolo- gies, Volume 1 (Long Papers), pp. 1112–1122, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. Yuhuai Wu, Mengye Ren, Renjie Liao, and Roger Grosse. Understanding short-horizon bias in stochastic meta-optimization. In International Conference on Learning Representations, 2018. Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012. Arber Zela, Aaron Klein, Stefan Falkner, and Frank Hutter. Towards automated deep learning: Efﬁcient joint neural architecture and hyperparameter search. In ICML 2018 AutoML Workshop, July 2018. Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. InProceedings of the 2015 IEEE International Conference on Computer Vision (ICCV), ICCV ’15, pp. 19–27, USA, 2015a. IEEE Computer Society. ISBN 9781467383912. Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. BookCorpus website. https://yknzhu.wixsite.com/mbweb, 2015b. 13Published as a conference paper at ICLR 2021 A A PPENDIX A.1 B AYESIAN OPTIMIZATION (MORE DETAILS ) BO (Shahriari et al., 2016) is one of the state-of-the-art techniques for black-box optimization. It applies exploration and exploitation to the black-box objective by sequentially and actively querying the function values of some input instances. Speciﬁcally, BO uses Gaussian process as a surrogate model to ﬁt the black-box objective function f(η). It updates a posterior distribution of f(η) by using its likelihood on newly evaluated (η,y = f(η) + ϵ) pairs, where yis a noisy observation of f(η) and is the validation loss after τ steps in our case. Then, it determines the next LR ηto evaluate as the one maximizing an acquisition function, which is computed from the updated posterior. The acquisition function performs a trade-off between exploration and exploitation in evaluating the candidates of LR. BO repeats the above process until achieving a precise posterior predictive distribution of f(η). Surrogate model (prior): We apply a commonly used surrogate model — Gaussian process (GP) (Rasmussen & Williams, 2006) as the prior of the black-box objective function in Eq. (2). A GP prior is speciﬁed by its mean function µ(·) and its covariance function (i.e., kernel function) K(·,·). We adopt a common choice µ(·) = 0 and set K(·,·) to be the Matern kernel (Genton, 2001) with smoothness factor ν = 2.5 and length scale l= 1, which is deﬁned as K(ηi,ηj) = 1 Γ(ν)2ν−1 (√ 2ν∥ηi −ηj∥2 l )ν Kν (√ 2ν∥ηi −ηj∥2 l ) , (4) where Kν(·) is a modiﬁed Bessel function and Γ(·) is the gamma function, and K(ηi,ηj) performs a convolution of the unit ball. Comparing to the radial basis function (RBF) kernel which always generates inﬁnitely differentiable functions that might be overly smooth, GP with Matern kernel can control the smoothness of generated functions to be ⌈ν⌉−1 times differentiable (Rasmussen & Williams, 2006). This helps to capture the less-smooth local changes. In our case, ν = 2.5 leads to twice-differentiable functions. Posterior prediction:In the following, we will use simpliﬁed notations η′ 1:k and f(η′ 1:k) for vectors composed of {η′ i}k i=1 and {f(η′ i)}k i=1, respectively. The GP prior indicates a Gaussian distribution over function values, i.e., f(η′ 1:k)|η′ 1:k ∼N (0,K) where Ki,j = K(η′ i,η′ j),∀i,j ∈[k]. After τ training steps using LR η′ i, we evaluate the validation loss denoted by yi as a noisy observation of f(η′ i). i.e., yi = f(η′ i) + ϵwhere Gaussian white noise ϵ∼N(0,σ2). Given the noisy observations y1:k, we can update the GP posterior of the black-box function f(·) as f(η′ 1:k)|η′ 1:k,y1:k ∼N(y1:k,K + σ2I). (5) Given a new LR η, we can now use the above GP posterior to predict the distribution of f(η) by the following reasoning based on Bayes’ theorem, i.e., P(f(η)|η′ 1:k,y1:k) = ∫ P(f(η)|f(η′ 1:k))P(f(η′ 1:k)|η′ 1:k,y1:k)df(η′ 1:k), (6) which yields the posterior predictive distribution of f(η) as f(η)|η′ 1:k,y1:k ∼ N(µn(η),σ2 n(η)), µn(η) ≜ k(K + σ2I)−1y1:k, σ2 n(η) ≜ K(η,η) −kT(K + σ2I)−1k. (7) where ki = K(η,η′ i). The above result about single LR ηcan be trivially extended to multiple LRs. Acquisition function: Given the posterior predictive distribution of f(η) in Eq (7), BO ﬁnds the next η′ i+1 to evaluate based on an acquisition function ui(η) deﬁned by the posterior mean µi(η) and standard deviation σi(η). A promising acquisition function should balance the trade-off between exploration (i.e., large σi(η)) and exploitation (i.e., small µi(η)). In AutoLRS, we use Lower Conﬁdence Bound (LCB) (Cox & John, 1992; Auer, 2002) as our acquisition function. In particular, given η′ 1:k and their corresponding validation lossy1:k, we determine the next LRη′ i+1 by minimizing LCB, i.e., η′ i+1 = arg min η ui(η), ui(η) ≜ µi(η) −κσi(η), (8) where µi(η) and σi(η) were deﬁned in Eq. (7), κis a positive hyper-parameter to balance the trade-off between exploration and exploitation. In experiments, we setκ= 1000 and it works consistently well. 14Published as a conference paper at ICLR 2021 0 20 40 60 80 100 Training Step 8.2 8.4 8.6Loss Training loss Exponential fit (a) Trainingloss during 100 train- ing steps and ﬁtting it by an ex- ponential time-series forecasting model. 0 200 400 600 800 Training Step 2.0 2.1 2.2Loss Validation loss Exponential fit (b) Validation loss during 800 training steps and ﬁtting it by an exponential time-series forecast- ing model. 0 50 100 150 200 Training Step 12 14Loss Training loss Exponential fit (c) A corner casewhen exponen- tial model cannot fully capture the non-monotone change of the loss during the ﬁrst 50 steps. Figure 3: Fitting the time-series of loss by exponential model when training ResNet-50 on ImageNet. A.2 E XPONENTIAL MODEL (MORE DETAILS ) We take two steps to estimate a,b, and cin ﬁtting the exponential model L(t) = aexp(bt) +c, based on the validation loss observed in the ﬁrst τ′steps, which is represented by yt,t = 1,··· ,τ′. First, we reduce the ﬁtting problem to an optimization problem. Deﬁne function g(b) as the least squared error between predictions and observations w.r.t. aand c. We can write the original ﬁtting problem in the following two-stage form. min b<0 g(b), g(b) ≜ min a,c τ′ ∑ t=1 (aexp(bt) + c−yt)2 (9) It is a 1-dimensional optimization problem. Moreover, with bﬁxed, the minimization problem w.r.t. a, cis a linear regression problem that has a closed-form solution. Hence, we apply a simple gradient descent method that starts from an initial b, computes the linear least squares w.r.t. a,c under b, search for the next bby the gradient descent method, and repeats these two steps. Thereby, in practice we can achieve a fast decrease on the regression error. In addition, to enforce the negative constraint for b, we re-parameterize it to be b←−exp(b′). The problem now reduces to min b′ min a,c τ′ ∑ t=1 (aexp(−exp(b′)t) + c−yt)2 (10) Although there might exist other possible strategies to optimize Eq. (9), we ﬁnd the above method is stable and fast in reducing the regression error and thus keeps the ﬁtting process highly efﬁcient. We empirically test whether the exponential model obtained by our method can ideally ﬁt the loss time-series in different cases. Figure 3a and Figure 3b are two typical examples ﬁtting the time series of training loss and validation loss by the proposed model. They show that the model can precisely predict the main trends of the time-varying loss, though ruling out some less informative noises. In Figure 3c, we also show a rare corner case when the model fails to ﬁt the increasing loss in early steps. However, the loss-increasing stage usually does not last long and thus the inaccuracy is not so harmful to the prediction of later-stage loss, which is our major goal since τ is usually larger than the length of loss-increasing stage. To overcome such corner cases and outliers in the observed validation loss series, we present a pre-processing strategy to make stable exponential ﬁtting in §A.3. Every time we predict the validation loss after τ steps, we ﬁrst pre-process the loss observed in the τ′steps, and then ﬁt the pre-processed loss series with the exponential model. In our empirical study, we also tried other more sophisticated time-series forecasting models including Holt-Winters, autoregressive integrated moving average (ARIMA) and singular spectrum analysis (SSA). We show two examples to compare their performance with our simple exponential prediction model in Figure 4. Some prior works also ﬁt and predict learning curves (Swersky et al., 2014; Domhan et al., 2015; Klein et al., 2017; Dai et al., 2019) for early termination of evaluations of poorly-performing hyperparameters when doing DNN hyperparameter optimization, but they need non-negligible time for training their models and performing inference. They are much more computationally intensive than our lightweight exponential prediction model, and this makes them less practical to be used in automatic LR schedule tuning. 15Published as a conference paper at ICLR 2021 0 500 1000 1500 2000 Training Step 3.5 4.0 4.5Loss Training loss Groud Truth Exponential fit ARIMA Holt-Winters SSA (a) Predict the training loss after 2000 steps. 0 1000 2000 3000 4000 Training Step 1.8 2.0 2.2Loss Training loss Groud Truth Exponential fit ARIMA Holt-Winters SSA (b) Predict the training loss after 4000 steps. Figure 4: Examples of forecasting the loss series by various time-series forecasting models when training ResNet-50 on ImageNet. Our simple exponential prediction model yields the least mean squared error (MSE) among all the models. 0 50 100 150 200 Training Step 11 12 13 14 15 16Loss n = 1 Loss curve Spline fit 0 50 100 150 200 Training Step 11 12 13 14 15 16Loss n = 5 Loss curve Spline fit 0 50 100 150 200 Training Step 11 12 13 14 15 16Loss n = 10 Loss curve Spline fit Figure 5: The loss sequence in Figure 3c and its quadratic spline smoothing result after 1, 5, and 10 iterations of our spline smoothing. A.3 P RE-PROCESS LOSS SERIES BY ITERATIVE SPLINE SMOOTHING We show a corner case in Figure 3c where the loss decreases rapidly at ﬁrst, then increases for a while, but ﬁnally decreases stably. It might be a result of a large LR or happens when escaping from a possibly poor local minimum or a saddle point (Goodfellow et al., 2016). Our exponential model cannot fully capture the loss change in early steps of this case. But we also consistently observe in our experiments that the early instability of loss only lasts for at most hundreds of steps after we switch to a new LR. Nevertheless, we ﬁnd that adding a pre-processing step to eliminate the noises, anomalies, and corner cases in the observed validation loss series makes the exponential ﬁtting easier and more stable. Hence, we propose to apply an iterative spline smoothing to the validation loss observed in τ′steps before training the forecasting model. In particular, when evaluating a LR ηfor a training stage, we ﬁrstly run τ′training steps and ﬁt the observed sequence of validation loss by a quadratic spline. We do such spline smoothing for multiple iterations. At the end of each iteration, we remove the loss values that are among the farthest 3% from the spline smoothing results if they are collected in the ﬁrst τ′/2 steps (when the corner cases like the one in Figure 3c might happen). So the next iteration’s spline smoothing only aims to ﬁt the rest loss values. After certain number of iterations, we use the ﬁnal spline smoothing values to train the exponential forecasting model. Empirically, we ﬁnd that 10 iterations of the above spline smoothing are necessary before training the exponential forecasting model. Figure 5 shows the loss sequence from Figure 3c before smoothing and after 1, 5 and 10 iterations of smoothing. As shown in the plots, the iterative spline smoothing can effectively remove the unnecessary noise and unstable changes during the early phase. A.4 P OSTERIOR LEARNED BY BO Figure 6 shows how the BO posterior gradually learns an increasingly more accurate estimation of the underlying black-box objective. We also visualize the “learning progress” of BO in an earlier stage and a later stage during training. It shows that in both early and late stages, by exploring more LRs, BO can achieve a more accurate posterior estimation of the objective function, and k = 10 sufﬁces to obtain a satisfying estimate. Moreover, the posteriors in the later stages have much smaller variance/uncertainty than in the earlier stages. 16Published as a conference paper at ICLR 2021 Early Stage 10 3  10 2  10 1  100 0 5 10 15 f( ) i=2 10 3  10 2  10 1  100 0 5 10 15 i=5 10 3  10 2  10 1  100 0 5 10 15 i=10 Late Stage 10 3  10 2  10 1  100 0.0 2.5 5.0 7.5 f( ) i=2 ( ) Observations 10 3  10 2  10 1  100 Learning rate  0.0 2.5 5.0 7.5 i=5 10 3  10 2  10 1  100 0.0 2.5 5.0 7.5 i=10 Figure 6: BO’s posterior of the black-box objective function after exploringiLRs (red dots ) determined by Eq. (3) at an early stage and a late stage during the training of ResNet-50 on ImageNet. The dashed lines show the mean function µi(η) (indicating the predicted validation loss of applying LR ηfor τ steps) and the shaded areas show the standard deviation σi(η) (indicating the prediction uncertainty) in the form of µi(η) ±σi(η). A.5 E XPERIMENTS (MORE DETAILS ) A.5.1 ANALYSIS OF ONLINE LEARNING RATE ADAPTATION WITH HYPERGRADIENT -BASED METHODS Hypergradient descent (HD) (Baydin et al., 2018) is a method to adjust the learning rate in an online fashion by performing gradient descent on the learning rate at the same time as the underlying DNN is optimized. For simplicity, we rewrite Eq. (1), which performs mini-batch SGD updates on model weights θat each step t, as: θt+1 = θt −ηt∇L(θt), (11) where ηt is the learning rate (LR) at step tand ∇L(θt) denotes the gradient of the loss function L w.r.t. the model weights θt at step t. By making the assumption that the optimal value of LR does not change much between two consecutive iterations, HD derives the partial derivative of the loss function Lwith respect to the learning rate η: ∂L(θt) ∂η = ∇L(θt)∂(θt−1−η∇L(θt−1)) ∂η = ∇L(θt)(−∇L(θt−1)) (12) An update rule for the learning rate is constructed as: ηt+1 = ηt −β∂L(θt) η = ηt + β∇L(θt)∇L(θt−1), (13) which introduces a hyperparameter βas the hypergradient learning rate. Updating the learning rate is a single vector multiplication between the gradient of the model weights at the previous step and the one at the current step. By updating both the learning rate ηt and the model weights θt using Eq.(13) and Eq.(11) in each step, the HD algorithm performs gradient descent on both learning rate and the model weights during training. HD can be applied to optimizers including SGD, SGD with Nesterov momentum, and Adam. The original paper empirically shows that these optimizers equipped with HD are much less sensitive to the choice of the initial regular learning rate and the convergence rate is improved on a set of tasks. However, the paper only compares HD with constant LR baselines on small models and small datasets. To study how HD compares to hand-tuned LR schedules on larger models and datasets, we train VGG-16 (Simonyan & Zisserman, 2015) and ResNet-50 neural networks on the CIFAR-10 image recognition dataset (Krizhevsky & Hinton, 2009) with a mini-batch size of 128 using a PyTorch implementation.8 A hand-tuned LR schedule consists of a total of 350 epochs, starting with 0.1 and multiplying the learning rate by 0.1 at epoch 150 and 250. This hand-tuned LR schedule can achieve around 93.70% and 95.56% top-1 accuracy on the test set for VGG-16 and ResNet-50, respectively when we train the models on one NVIDIA Titan RTX GPU. We apply SGD with HD (SGD-HD)9 to train the two models, sweep all the guideline values of the two hyperparameters (regular LR and 8https://github.com/kuangliu/pytorch-cifar 9https://github.com/gbaydin/hypergradient-descent 17Published as a conference paper at ICLR 2021 hypergradient LR) in SGD-HD, and report the best top-1 accuracy that SGD-HD can achieve for VGG-16 and ResNet-50 within 500 epochs in Table 4 and Table 5. We have three observations: (1) Hypergradient descent is very sensitive to the selection of the regular LR and the hypergradient LR. The top-1 accuracy ranges from 10.00% to 91.80% for VGG-16 and ranges from 10.00% to 92.52% for ResNet-50, with all suggested values of the two hyperparameters. (2) It cannot match the top-1 accuracy achieved with hand-tuned LR schedules: the best top-1 accuracy it can achieve among all the different hyperparameter settings are 1.90% and 3.04% behind the accuracy achieved with hand-tuned LR schedules for VGG-16 and ResNet-50, even though we ran each of them 150 epochs more than the hand-tuned LR schedule. (3) It is prone to overﬁtting. For example, when using regular LR = 10−3 and hypergradient LR = 10−5 to train VGG-16, the top-1 accuracy is only 90.74% while the training accuracy is already 99.98%. MARTHE (Donini et al., 2020) adaptively interpolates between two hypergradient based methods, HD and RTHO (Franceschi et al., 2017), and it computes the gradient of the loss function on the validation set instead of training set w.r.t. the learning rate. Besides the two hyperparameters in HD, MARTHE introduces another hyperparameter µthat controls how quickly past history is forgotten. We sample µbetween 0.9 and 0.999, sample the hypergradient LR in [10−3,10−6] log-uniformly, and set the initial LR to 0.1, as how the MARTHE paper set its hyperparameters for training VGG-11 on CIFAR-10. We apply SGD with MARTHE 10 to train VGG-16 on CIFAR-10. The best top-1 accuracy MARTHE can achieve among all the hyperparameter settings in 350 epochs is 92.99%, which is 0.71% lower than the accuracy achieved with the hand-tuned LR schedule. A.5.2 S ENSITIVITY TEST OF τmax IN AutoLRS AND MEASURE OF VARIABILITY Recall from §4.4 that AutoLRS starts with τ = 1000 and τ′= 100, and doubles them after every stage until it reaches τmax. We test the sensitivity of AutoLRS to this hyperparameter, τmax, by comparing the generated LR schedules with different τmax values for the VGG-16 neural network on CIFAR-10 as in §A.5.1. The LR search interval (ηmin,ηmax) we use is (10−3,10−1). We report the training epochs to reach the target 93.70% top-1 accuracy using the LR schedules generated among 5 trials for different τmax values in Table 6. AutoLRS with different τmax values can consistently achieve the target top-1 accuracy achieved with the hand-tuned LR schedule (i.e.,93.70%) in fewer training steps. We also see that the best AutoLRS-generated LR schedule can achieve 94.13% top-1 accuracy within 350 training epochs (excluding the costs of the LR search). In the last column of Table 6, we report the mean and standard deviation of the top-1 accuracy achieved by AutoLRS over 5 trials for each τmax. To further measure the variability of AutoLRS, we train VGG-16 on CIFAR-100 (Krizhevsky et al.) with a mini-batch size of 128. A carefully hand- tuned LR schedule consists of a total of 200 epochs, starting with 0.1 and dividing the learning rate by 5 at epoch 60, 120, and 160.11 This hand-tuned LR schedule can achieve 72.93% top-1 accuracy. We train VGG-16 on CIFAR-100 for 200 epochs withAutoLRS for 10 trials using different random seeds, and report the top-1 accuracy they achieve in Table 9. The LR search interval (ηmin,ηmax) we use is (10−3,10−1), and τmax is set to 8000. The top-1 accuracy achieved by AutoLRS-generated LR schedules over 10 trials are distributed with a mean of 73.05% and a standard deviation of 0.14%. The best AutoLRS-generated LR schedule can achieve 73.30% top-1 accuracy, which is 0.37% higher than the accuracy achieved using the hand-tuned LR schedule. A.5.3 L EARNING RATE SCHEDULE SEARCH WITH HYPERBAND Hyperband is a multi-armed bandit approach for DNN hyperparameter optimization. It dynamically allocates resources to randomly sampled conﬁgurations and uses successive halving (Jamieson & Talwalkar, 2016) to early stop poorly-performing conﬁgurations. We attempt to use Hyperband to optimize the LR schedule on CIFAR-10 training with VGG-16 by searching for an exponential decay LR schedule, which can be parameterized with an initial learning rate and a decay factor. The learning rate is decayed by the decay factor every epoch. Exponential decay is a commonly used LR schedule and is also used in other DNN hyperparameter optimization methods (Falkner et al., 2018). We use the search space of (10−3,10−1) for the initial LR, and the search space of (0.9,1) for the decay rate. The decay rate is uniformly random sampled, and the initial LR is uniformly random sampled in its log-scale space. We use the default setting of Hyperband that sets the maximum epochs that can be 10https://github.com/awslabs/adatune 11https://github.com/weiaicunzai/pytorch-cifar100 18Published as a conference paper at ICLR 2021 allocated to a single conﬁguration to 350 and discards two-thirds of the conﬁgurations in each round of successive halving. This results in evaluating 384 conﬁgurations with different numbers of epochs with a total of 12600 epochs, which has a computational overhead of 36×compared to a single run of training with the hand-tuned LR schedule. The best conﬁguration found by Hyperband achieves 93.24% top-1 accuracy, which is 0.46% lower than the accuracy achieved with the hand-tuned LR schedule. A.5.4 A BLATION STUDY To illustrate the effects of the exponential model and BO of AutoLRS, we perform ablation studies using the VGG-16 neural network on CIFAR-10 as in §A.5.1. Exponential model: What if we remove the exponential forecasting model and simply use the validation loss at τ′step to update the BO posterior? Will the LR schedules generated by AutoLRS be signiﬁcantly worse? We apply AutoLRS without the exponential forecasting to ﬁnd the LR schedules for VGG-16 on CIFAR-10. With τmax being chosen from the set of {4000,8000,16000}, the best top-1 test accuracy that AutoLRS can achieve within 350 training epochs are 91.73%, 92.59%, 92.24%, respectively. Therefore, it is unable to match the target accuracy in reasonable training steps without the exponential forecasting model. The reason for this is that the objective of BO has become to minimize the validation loss at the τ′step, which will lead to short-horizon issues (Wu et al., 2018). As a consequence, it tends to select a conservative LR, which is often a small LR around ηmin in the late stages. In contract, with the exponential forecasting model, the goal of the BO is to ﬁnd the LR that minimizes the predicted validation loss in τ steps. This allows the LR selected in the current stage to be higher than that in the past stages, and the loss to even increase in a short period of time, as long as the predicted loss in τ steps is low. This phenomenon can be seen in Figure 1 and Figure 2. Bayesian Optimization:What if we replace BO in AutoLRS with random search or grid search? Will the LR schedules generated by AutoLRS get worse? We replace the BO part in AutoLRS with random search and grid search while keeping the exponential forecasting part of it, and apply it to ﬁnd the LR schedules for VGG-16 on CIFAR-10. The LR search interval is (10−3,10−1), the same as in §A.5.2. Table 7 and Table 8 show the results of random search and grid search with differentτmax values, respectively. We observe that both random search and grid search have at least one trial that fails to match the hand-tuned LR schedule to achieve 93.70% top-1 test accuracy within 350 epochs (denoted by N/A in the tables). The top-1 accuracy achieved on average across trials in 350 epochs by random search and grid search is 0.09% and 0.24% behind AutoLRS with BO, respectively. We also replace BO with grid search and apply it to ﬁnd the LR schedules for VGG-16 on CIFAR-100. The top-1 accuracy achieved over 10 trials are distributed with a mean of 72.63% and a standard deviation of 0.56%. Compared to the AutoLRS-generated LR schedules in Table 9, the mean of the grid search accuracy is out of two standard deviations from the BO accuracy 73.05%±0.14%. A.5.5 AutoLRS FINE -TUNING RESULTS OF BERT BASE ACROSS 3 TRIALS We pre-trained BERTBASE with AutoLRS for 3 trials, and report their ﬁne-tuning results in Table 3. Table 3: Fine-tuning results of BERTBASE models pre-trained with AutoLRS for 3 trials. Accuracy scores on the Dev set are reported for MRPC, MNLI, and CoLA. F1 scores on the Dev set are reported for SQuAD v1.1. MRPC MNLI CoLA SQuAD v1.1 Trial 1 88.0 82.5 47.6 87.1 Trial 2 88.0 82.7 46.5 87.0 Trial 3 87.8 82.3 47.0 86.6 19Published as a conference paper at ICLR 2021 Table 4: The accuracy information of tuning the regular LR and the hypergradient LR of SGD-HD for CIFAR-10 training with VGG-16 (batch size = 128). We train the model for 500 epochs using SGD-HD with each suggested value for the regular LR and the hypergradient LR, and report the best top-1 test accuracy it can achieve, its corresponding training accuracy, and the epoch number. Note that a hand-tuned LR schedule can achieve93.70% top-1 test accuracy in 350 epochs. regular LR hypergradient LR Top-1 Test Accuracy Training Accuracy Epoch 10−6 10−6 86.13% 99.77% 438 10−6 10−5 88.79% 99.98% 480 10−6 10−4 86.31% 98.10% 494 10−6 10−3 90.70% 99.95% 499 10−6 10−2 10.30% 9.90% 40 10−6 10−1 10.00% 10.00% 1 10−5 10−6 86.14% 99.73% 394 10−5 10−5 88.49% 99.95% 448 10−5 10−4 87.67% 98.78% 483 10−5 10−3 88.70% 99.49% 469 10−5 10−2 10.22% 9.92% 170 10−5 10−1 10.00% 10.00% 1 10−4 10−6 86.09% 99.84% 481 10−4 10−5 88.82% 99.94% 304 10−4 10−4 86.63% 95.37% 479 10−4 10−3 10.22% 10.13% 1 10−4 10−2 10.02% 10.00% 1 10−4 10−1 10.00% 10.00% 1 10−3 10−6 86.13% 99.73% 406 10−3 10−5 88.78% 99.94% 346 10−3 10−4 90.74% 99.98% 484 10−3 10−3 44.12% 43.02% 500 10−3 10−2 88.48% 99.55% 467 10−3 10−1 10.00% 10.00% 1 10−2 10−6 91.69% 99.97% 389 10−2 10−5 88.53% 99.89% 397 10−2 10−4 89.11% 99.92% 484 10−2 10−3 10.07% 9.90% 265 10−2 10−2 10.00% 10.02% 1 10−2 10−1 10.00% 9.99% 1 10−1 10−6 91.80% 99.93% 476 10−1 10−5 91.48% 99.85% 317 10−1 10−4 88.81% 99.57% 499 10−1 10−3 90.42% 99.80% 393 10−1 10−2 11.24% 10.45% 1 10−1 10−1 10.00% 10.02% 1 20Published as a conference paper at ICLR 2021 Table 5: The accuracy information of tuning the regular LR and the hypergradient LR of SGD-HD for CIFAR-10 training with ResNet-50 (batch size = 128). We train the model for 500 epochs using SGD-HD with each suggested value for the regular LR and the hypergradient LR, and report the best top-1 test accuracy it can achieve, its corresponding training accuracy, and the epoch number. Note that a hand-tuned LR schedule can achieve 95.56% top-1 test accuracy in 350 epochs. regular LR hypergradient LR Top-1 Test Accuracy Training Accuracy Epoch 10−6 10−6 83.67% 99.71% 410 10−6 10−5 88.75% 99.44% 490 10−6 10−4 83.77% 99.68% 494 10−6 10−3 71.03% 72.14% 491 10−6 10−2 10.11% 10.03% 261 10−6 10−1 10.0% 10.0% 1 10−5 10−6 83.99% 99.64% 420 10−5 10−5 89.15% 99.97% 460 10−5 10−4 10.12% 9.95% 206 10−5 10−3 19.73% 18.53% 13 10−5 10−2 10.03% 9.98% 137 10−5 10−1 10.0% 10.0% 1 10−4 10−6 84.98% 99.85% 488 10−4 10−5 89.27% 99.94% 482 10−4 10−4 84.36% 97.78% 424 10−4 10−3 88.72% 99.84% 484 10−4 10−2 10.00% 10.00% 1 10−4 10−1 10.00% 10.00% 1 10−3 10−6 83.22% 99.81% 487 10−3 10−5 88.56% 99.98% 492 10−3 10−4 86.00% 97.32% 440 10−3 10−3 10.10% 9.76% 367 10−3 10−2 42.80% 40.11% 497 10−3 10−1 10.00% 10.00% 1 10−2 10−6 92.40% 99.99% 459 10−2 10−5 88.51% 99.98% 440 10−2 10−4 90.72% 99.91% 452 10−2 10−3 10.19% 9.64% 315 10−2 10−2 10.05% 9.99% 8 10−2 10−1 10.00% 10.00% 1 10−1 10−6 92.18% 99.97% 487 10−1 10−5 92.52% 99.97% 494 10−1 10−4 87.74% 99.86% 492 10−1 10−3 84.32% 97.23% 477 10−1 10−2 10.00% 10.11% 1 10−1 10−1 10.00% 10.00% 1 21Published as a conference paper at ICLR 2021 Table 6: Performance of AutoLRS with different τmax values for CIFAR-10 training with VGG-16 (batch size = 128). Note that a hand-tuned LR schedule can achieve 93.70% top-1 test accuracy in 350 epochs. We report the top-1 accuracy achieved within 350 epochs for each trial, and the mean and standard deviation of the top-1 accuracy achieved by AutoLRS over 5 trials for each τmax. τmax Trial Number Epoch to 93.70% Top-1 Accuracy Top-1 Accuracy Achieved Mean±std 4000 Trial 1 108 94.13% Trial 2 181 93.96% Trial 3 223 94.07% 94.01% ±0.13% Trial 4 315 93.82% Trial 5 287 94.09% 8000 Trial 1 115 94.03% Trial 2 265 93.92% Trial 3 203 93.94% 93.96% ±0.07% Trial 4 194 94.02% Trial 5 305 93.87% 16000 Trial 1 229 93.77% Trial 2 250 93.95% Trial 3 267 93.73% 93.80% ±0.10% Trial 4 313 93.71% Trial 5 330 93.82% Table 7: Experimental results after replacing BO in AutoLRS with random search for CIFAR-10 training with VGG-16 (batch size = 128). We also report the top-1 accuracy achieved within 350 epochs for each trial. τmax Trial Number Epoch to 93.70% Top-1 Accuracy Top-1 Accuracy Achieved 4000 Trial 1 199 93.80% Trial 2 209 93.97% Trial 3 298 93.84% 8000 Trial 1 344 93.71% Trial 2 225 93.98% Trial 3 175 93.91% 16000 Trial 1 N/A 93.64% Trial 2 316 93.96% Trial 3 310 93.86% Table 8: Experimental results after replacing BO in AutoLRS with grid search for CIFAR-10 training with VGG-16 (batch size = 128). We also report the top-1 accuracy achieved within 350 epochs for each trial. τmax Trial Number Epoch to 93.70% Top-1 Accuracy Top-1 Accuracy Achieved 4000 Trial 1 304 93.88% Trial 2 233 93.88% Trial 3 180 93.91% 8000 Trial 1 239 93.72% Trial 2 296 93.95% Trial 3 N/A 93.32% 16000 Trial 1 N/A 93.02% Trial 2 153 93.78% Trial 3 288 93.70% Table 9: Top-1 test accuracy achieved by AutoLRS-generated LR schedules for CIFAR-100 training with VGG-16 over 10 trials. 73.12% 73.20% 72.90% 72.93% 73.03% 73.16% 73.30% 72.85% 73.00% 72.97% 22",
      "meta_data": {
        "arxiv_id": "2105.10762v1",
        "authors": [
          "Yuchen Jin",
          "Tianyi Zhou",
          "Liangyu Zhao",
          "Yibo Zhu",
          "Chuanxiong Guo",
          "Marco Canini",
          "Arvind Krishnamurthy"
        ],
        "published_date": "2021-05-22T16:41:10Z",
        "pdf_url": "https://arxiv.org/pdf/2105.10762v1.pdf",
        "github_url": "https://github.com/YuchenJin/autolrs"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces AutoLRS, an automatic and efficient method for tuning learning rate (LR) schedules in deep neural network (DNN) training, eliminating significant manual effort and computational cost. AutoLRS achieves this by optimizing the LR for each training stage using on-the-fly Bayesian Optimization (BO). To make BO computationally feasible, it employs a lightweight exponential time-series forecasting model to predict validation loss after a full stage based on short initial training steps. AutoLRS demonstrated speedups of 1.22x for ResNet-50, 1.43x for Transformer, and 1.5x for BERT compared to their original hand-tuned schedules, and an average speedup of 1.31x over heavily-tuned state-of-the-art schedules like CLR and SGDR, while maintaining or improving test-set performance.",
        "methodology": "AutoLRS divides DNN training into stages of 'τ' steps, seeking to find a constant LR 'η' for each stage that minimizes validation loss. This is treated as a black-box optimization problem solved by Bayesian Optimization (BO). BO uses a Gaussian Process (GP) with a Matern kernel as a surrogate model to infer the objective function's posterior, and then selects the next LR to evaluate by minimizing the Lower Confidence Bound (LCB) acquisition function (µ(η) - κσ(η), with κ=1000). To reduce BO's computational cost (which would typically require 'τ' steps per evaluation), AutoLRS only runs each candidate LR for 'τ'' << 'τ' steps (e.g., τ/10). The validation loss observed during these 'τ'' steps is used to train a simple exponential time-series forecasting model (L(t) = a*exp(b*t) + c) to predict the validation loss after 'τ' steps. An iterative spline smoothing pre-processes the loss series for stable fitting. BO and the forecasting model operate in a mutual training loop. Practical improvements include gradually increasing 'τ' and 'τ'' over training (starting at 1000/100, doubling until τ_max) and minimizing training loss in early stages (or a small validation subset in later stages) to reduce validation cost. Model states are checkpointed and restored for each LR evaluation within a stage to ensure fair comparison.",
        "experimental_setup": "AutoLRS was evaluated on three representative DNNs: ResNet-50 for ImageNet classification (SGD with momentum, 32 NVIDIA Tesla V100 GPUs, mini-batch size 1024), Transformer for WMT 2014 English-German Neural Machine Translation (Adam, 8 NVIDIA Tesla V100 GPUs), and BERTBASE for pre-training (Adam with L2 weight decay, mixed precision, 32 NVIDIA Tesla V100 GPUs, mini-batch size 1024) and subsequent fine-tuning on MRPC, MNLI, CoLA, and SQuAD v1.1 NLP tasks. Ablation studies and comparisons with prior methods (Hypergradient Descent, MARTHE, Hyperband) were conducted using VGG-16 and ResNet-50 on CIFAR-10/CIFAR-100 (SGD, NVIDIA Titan RTX GPU, mini-batch size 128). Baselines included original paper's hand-tuned LR schedules, and heavily hand-tuned Cyclical Learning Rate (CLR) and Stochastic Gradient Descent with Warm Restarts (SGDR). Performance was measured by training steps to reach target accuracy/BLEU score, and final validation/test accuracy or F1 scores. AutoLRS hyperparameters were typically k=10, τ'=τ/10, κ=1000, initial τ=1000, τ'=100 doubling to τ_max (8000 for ResNet-50/Transformer, 32000 for BERT), with LR search intervals set using an LR range test.",
        "limitations": "AutoLRS still requires setting several hyperparameters, including the number of LRs to evaluate by BO (k), the short evaluation steps (τ'), the maximum stage length (τ_max), the trade-off weight in the acquisition function (κ), and the initial LR search interval (η_min, η_max). While defaults often work and τ_max sensitivity is tested, these still represent tuning points. The method does not automatically tune the initial 'warmup' phase of training, which is still handled separately. The efficacy of the exponential forecasting model relies on the assumption that validation loss generally decreases exponentially and converges, which may not hold universally, especially during transient non-monotonic loss behaviors, although pre-processing helps mitigate this. The computational overhead, while significantly reduced compared to full BO, is still approximately twice the training cost of the final schedule.",
        "future_research_directions": "Not mentioned",
        "experimental_code": "File Path: autolrs_callback.py\nContent:\nimport torch\nimport os\nimport logging\nimport socket\nimport string\nimport random\nimport time\n\nclass AutoLRS():\n    def __init__(self, model, optimizer, val_fn, listening_host='localhost', listening_port=12315, warmup_steps=0, warmup_lr=0, summary_steps=1):\n        self._net = model\n        self._optimizer = optimizer\n        self._val_fn = val_fn \n        self._lr = 0.000001\n        self._warmup_steps = warmup_steps\n        self._warmup_lr = warmup_lr \n        self._global_step = 0\n        self._socket = socket.socket()\n        self._started = False\n        self._summary_steps = summary_steps\n        self._checkpoint_path = './checkpoint/autolrs_ckpt_' + ''.join(random.choices(string.ascii_uppercase + string.digits, k = 7))  + '.pth'\n        self._listening_host = listening_host\n        self._listening_port = listening_port\n        self._best_acc = 0\n\n        if not os.path.exists('checkpoint'):\n            os.makedirs('checkpoint')\n\n        self.connect_server()\n    \n    def connect_server(self):\n        self._socket.connect((self._listening_host, self._listening_port))\n\n    def _verbose_operation(self, _op):\n        if self._global_step % self._summary_steps == 0:\n            logging.info(\"[AutoLRS at {}] {}\".format(self._global_step, _op))\n\n    def save_variables(self):\n        \"\"\"Save model parameters and optimizer states.\"\"\"\n        _start_time = time.time()\n        torch.save({\n            'model_state_dict': self._net.state_dict(),\n            'optimizer_state_dict': self._optimizer.state_dict()\n            }, self._checkpoint_path)\n        logging.info(\"[AutoLRS] backup variables, elapsed: {}s\".format(time.time() - _start_time))\n\n    def restore_variables(self):\n        _start_time = time.time()\n        checkpoint = torch.load(self._checkpoint_path)\n        self._net.load_state_dict(checkpoint['model_state_dict'])\n        self._optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        logging.info(\"[AutoLRS] restoring variables, elapsed: {}s\".format(time.time() - _start_time))\n\n    def on_train_batch_end(self, loss):\n        if self._global_step < self._warmup_steps:\n            # linear warmup\n            self._lr = (self._warmup_lr / self._warmup_steps) * (self._global_step + 1)\n            for param_group in self._optimizer.param_groups:\n            \tparam_group['lr'] = self._lr\n            self._global_step += 1\n\n        elif not self._started:\n            self.save_variables()\n            print(\"backup trainable variables to CPU\") \n            self._started = True\n            self._socket.send(\",\".join(('startBO', str(loss))).encode(\"utf-8\"))\n            self._verbose_operation(\"Start Bayesian Optimization(BO)\")\n            data = self._socket.recv(1024).decode(\"utf-8\")\n            self._verbose_operation(\"Received data: \" + data)\n            self._lr = (float(data.split(\",\")[-1]))\n            for param_group in self._optimizer.param_groups:\n            \tparam_group['lr'] = self._lr\n        else:\n            self._socket.send(','.join(('loss', str(loss))).encode('utf-8'))\n            data = self._socket.recv(1024).decode(\"utf-8\")\n            self._verbose_operation(\"Received data: \" + data)\n            if data.startswith(\"restore\"):\n                self.restore_variables()\n                self._verbose_operation(\"restore trainable variables\")\n            elif data.startswith(\"ckpt\"):\n                self.save_variables()\n                self._verbose_operation(\"backup trainable variables\")\n            elif data.startswith('evaluate'):\n                val_loss = self._val_fn()\n                self._socket.send(\",\".join((\"val_loss\", str(val_loss))).encode(\"utf-8\"))\n                data = self._socket.recv(1024).decode(\"utf-8\")\n            elif data.startswith('save'):\n                pass\n            else:\n                self._lr = (float(data.split(\",\")[-1]))\n                for param_group in self._optimizer.param_groups:\n                    param_group['lr'] = self._lr\n                self._global_step += 1\n\nFile Path: autolrs_server.py\nContent:\nimport argparse\nimport socket                \nimport random\nimport numpy as np \nimport threading\nimport time\nimport math\nimport os\nfrom skopt import Optimizer\nfrom skopt.space import Real\nfrom scipy.interpolate import UnivariateSpline\nfrom scipy import optimize \nimport sys\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\n\ndef f(b, x, y):\n    A = np.vstack([np.exp(-np.exp(b) * x), np.ones(len(x))]).T\n    res = np.linalg.lstsq(A, y, rcond=None)[1]\n    return res\n\ndef spline_iter(xs, ys, is_training, spline_deg=2, filter_ratio=0.03, num_of_iter=10, bound=0.5):\n    \"\"\" Use iterative spline to eliminate noise and outliers in the loss series.\n        is_training specifies whether the loss series in use is training loss or validation loss.\n    \"\"\"\n    bound = xs[int((len(xs) - 1) * bound)]\n    if is_training:\n        num_of_iter = 10\n    else:\n        num_of_iter = 1\n\n    for _ in range(num_of_iter):\n        spline_ys = UnivariateSpline(xs, ys, k=spline_deg)(xs)\n        dys = np.abs(ys - spline_ys)\n\n        if is_training:\n            outliers = set(sorted(range(len(dys)), key=lambda i: dys[i])[int(round(-len(dys) * filter_ratio)):])\n        else:\n            outliers = set(sorted(range(len(dys)), key=lambda i: dys[i])[-1:])\n        outliers = [i for i in outliers if i < bound]\n\n        xs2 = np.zeros(len(xs) - len(outliers))\n        ys2 = np.zeros(len(xs) - len(outliers))\n        i1 = 0\n        for i2 in range(len(xs)):n            if i2 not in outliers:\n                xs2[i1], ys2[i1] = xs[i2], ys[i2]\n                i1 += 1\n        xs, ys = xs2, ys2\n    return xs, ys\n\ndef exp_forecast(loss_series, end_step, is_training, spline_order=2):\n    \"\"\" Do exponential forecasting on a loss series.\"\"\"\n    xs = np.arange(end_step - len(loss_series), end_step)\n    xs2, ys2 = spline_iter(xs, loss_series, is_training)\n    ys = UnivariateSpline(xs2, ys2, k=spline_order)(xs)\n    logging.debug('ys after spline iter: {}'.format(ys))\n    b = optimize.fmin(f, 0, args=(xs, ys), xtol=1e-5, ftol=1e-5, disp=False)[0]\n    b = -np.exp(b)\n    A = np.vstack([np.exp(b * xs), np.ones(len(xs))]).T\n    a, c = np.linalg.lstsq(A, ys, rcond=None)[0]\n    return a, b, c\n\nclass RingBuffer:\n    \"\"\" A class for storing and manipulating loss series and do exponential forecasting. \"\"\"\n\n    def __init__(self, size):\n        self.data = [None for i in range(size)]\n\n    def reset(self):\n        self.data = [None for i in self.data]\n\n    def append(self, x):\n        self.data.pop(0)\n        self.data.append(x)\n\n    def get(self):\n        return self.data\n\n    def average(self):\n        return sum(self.data)/len(self.data)\n\n    def exponential_forcast(self, pred_index, is_training):\n        loss_series = self.data[:]\n        end_epoch = len(loss_series)\n        x = np.arange(end_epoch - len(loss_series), end_epoch)\n        y = np.array(loss_series)\n        a3, b3, c3 = exp_forecast(y, len(y), is_training)\n        forcast_y = a3 * np.exp(b3 * pred_index) + c3\n        logging.debug(\"Exponential fit: {}, {}, {}\".format(a3, b3, c3))\n        return forcast_y\n\nclass Controller(object):\n    def __init__(self, host, port, min_lr, max_lr):\n        # Constants\n        EXPLOITATION_STEP = 1000\n        LR_STEPS = 100\n        RING_BUFFER_LEN = 100\n        LR_TO_EXPLORE = 10\n        TAU_MAX = 8000\n\n        self.min_lr = float(min_lr)\n        self.max_lr = float(max_lr)\n        self.host = host\n        self.port = port\n        self.threads = []\n        self.num_threads = 1\n        self.event = threading.Event()\n        self.sock = socket.socket()\n        self.sock.bind((self.host, self.port))\n        self.global_step = 0\n        self.last_total_loss = 0.0\n        self.average_loss = 0.0\n        self.loss_vector = []\n        self.lr = 0\n        self.lr_steps = LR_STEPS \n        self.tau_max = TAU_MAX\n        self.val_freq = int(self.lr_steps/16)\n        self.lr_counter = 0\n        self.lr_to_explore = LR_TO_EXPLORE\n        self.BO_stage = True\n        self.val_stage = False\n        self.message = ''\n        self.loss_after_exploitation = None\n        self.ring_buffer_len = RING_BUFFER_LEN\n        if self.val_stage:\n            self.ring_loss_buffer = RingBuffer(self.ring_buffer_len // self.val_freq)\n        else:\n            self.ring_loss_buffer = RingBuffer(self.ring_buffer_len)\n\n        self.exploitation_step = EXPLOITATION_STEP\n        self.exploitation_flag = False \n        self.exploitation_counter = 0\n        \n        self.opt = None\n        self.x_func_dict = dict()\n        self.x_iters = []\n        self.func_val_iters = []\n\n        self.num_ranks = 0\n        self.finished_minions = 0\n        self.lock1 = threading.Lock()\n        self.lock2 = threading.Lock()\n\n    def listen(self):\n        self.sock.listen(20)\n        while True:\n            client, address = self.sock.accept()\n            logging.info('[Server]: Got connection from {}'.format(address))\n            self.threads.append(threading.Thread(target = self.run, args = (client, address, self.event)))\n            if len(self.threads) == self.num_threads:\n                self.num_ranks = len(self.threads)\n                self.num_minions = self.num_ranks - 1\n                logging.info('[Server]: num_ranks: {}'.format(self.num_ranks))\n                for thread in self.threads:\n                    thread.start()\n                logging.info('[Server]: threads started')\n                for thread in self.threads:\n                    thread.join()\n                    self.threads = []\n                logging.info('[Server]: join finished')\n                sys.exit()\n\n    def run(self, c, address, event):\n        size = 1024\n        while True:\n            data = c.recv(size).decode()\n            if not data:\n                sys.exit()\n            logging.debug(data.split(','))\n            total_loss = float(data.split(',')[-1])\n            self.lock2.acquire()\n            self.loss_vector.append(total_loss)\n\n            # compute average loss across ranks\n            if len(self.loss_vector) == self.num_ranks:\n                self.average_loss = sum(self.loss_vector) / len(self.loss_vector) \n                logging.info('[Server]: average loss = {}, step = {}'.format(self.average_loss, self.global_step))\n                if self.val_stage:\n                    if 'val' in data:\n                        self.ring_loss_buffer.append(self.average_loss)\n                    else:\n                        self.global_step += 1\n                else:\n                    self.ring_loss_buffer.append(self.average_loss)\n                    self.global_step += 1\n                self.loss_vector = []\n            self.lock2.release()\n\n            if 'val' in data:\n                c.send(str(self.lr).encode('utf-8')) \n                continue\n\n            if 'minion' in data:\n                # blocking\n                event_is_set = event.wait()\n                c.send(self.message.encode('utf-8')) \n                logging.debug('[Server] message: %s', self.message)\n                self.lock1.acquire()\n                self.finished_minions += 1\n                self.lock1.release()\n                if self.finished_minions == self.num_minions:\n                    event.clear()\n                    self.finished_minions = 0\n                continue\n\n            else:\n                if data.startswith('startBO'):\n                    self.last_total_loss = self.average_loss\n                    self.init_loss = self.average_loss\n                    self.loss_after_exploitation = self.average_loss\n\n                # exploitation stage -- actual training stage using the best-found LR\n                if self.exploitation_flag:\n                    logging.debug('[Server exploitation]: average loss ' + str(self.average_loss) + ' step=' + str(self.global_step))\n                    if self.exploitation_counter == self.exploitation_step:\n                        self.BO_stage = True\n                        self.exploitation_flag = False\n                        self.exploitation_counter = 0\n                        logging.info('[Server]: exploitation stage done')\n                        logging.info('[Server]: reconfigure...')\n                        if self.lr_steps < self.tau_max / 10:\n                            self.lr_steps = self.lr_steps * 2\n                            self.val_freq = int(self.lr_steps/16)\n                            self.ring_buffer_len = self.lr_steps \n                            self.exploitation_step = self.exploitation_step * 2\n                            self.ring_loss_buffer = RingBuffer(self.ring_buffer_len)\n                        else:\n                            self.val_stage = True\n                        if self.val_stage:\n                            self.ring_loss_buffer = RingBuffer(self.ring_buffer_len // self.val_freq)\n                        self.loss_after_exploitation = self.average_loss\n\n                        self.message = 'save'\n                        c.send(self.message.encode('utf-8'))\n                        event.set()\n                        continue\n\n                    else:\n                        self.exploitation_counter += 1\n                        self.message = str(self.lr)\n                        c.send(str(self.lr).encode('utf-8')) \n                        event.set()\n                        continue\n\n                # BO stage -- LR search stage\n                if self.BO_stage:\n                    self.opt = Optimizer([Real(self.min_lr, self.max_lr, 'log-uniform')], \"GP\", n_initial_points=1, acq_func='LCB', acq_func_kwargs={'kappa':1e6})\n                    self.BO_stage = False\n                    self.lr = self.opt.ask()[0]\n\n                    # prevent BO in scikit-optimize from searching for the same LR explored before\n                    while True:\n                        if self.lr in self.x_func_dict:\n                            self.opt.tell([self.lr], self.x_func_dict[self.lr])\n                            self.lr = self.opt.ask()[0]\n                        else:\n                            break\n\n                    self.message = str(','.join(('ckpt', str(self.lr))))\n                    c.send(','.join(('ckpt', str(self.lr))).encode('utf-8'))\n                    event.set()\n                    logging.debug('[Server]: checkpoint command sent')\n                    continue\n\n                # ask BO to suggest the next LR \n                if self.lr_counter == self.lr_steps:\n                    logging.debug('ring_buffer: {}'.format(self.ring_loss_buffer.get()))\n                    if any([math.isnan(x) for x in self.ring_loss_buffer.get()]):\n                        predicted_loss = \"nan\"\n                    elif self.val_stage:\n                        predicted_loss = self.ring_loss_buffer.exponential_forcast(pred_index=int(self.exploitation_step/self.val_freq), is_training=False)\n                        current_loss = sum(self.ring_loss_buffer.get()[-1:])/1.0\n                    else:\n                        predicted_loss = self.ring_loss_buffer.exponential_forcast(pred_index=self.exploitation_step, is_training=True)\n                        current_loss = sum(self.ring_loss_buffer.get()[-10:])/10.0\n\n                    logging.info('[Server]: predicted loss: {} due to LR {}'.format(predicted_loss, self.lr))\n\n                    # Huge loss jump can make the exponential prediction inaccurate, so set a threshold here. \n                    #if self.loss_after_exploitation is not None and max(self.ring_loss_buffer.get()) > 10 * self.loss_after_exploitation:\n                    #    predicted_loss = current_loss \n                    #    logging.info('New predicted_loss: ' + str(predicted_loss))\n\n                    if self.loss_after_exploitation is not None and max(self.ring_loss_buffer.get()) >= 1.0 * self.init_loss and self.val_stage:\n                        predicted_loss = current_loss \n                        logging.info('[Server]: New predicted_loss: ' + str(predicted_loss))\n\n                    if self.val_stage:\n                        self.ring_loss_buffer = RingBuffer(int(math.floor(self.ring_loss_buffer_len)/self.val_freq))\n                    else:\n                        self.ring_loss_buffer = RingBuffer(self.ring_buffer_len)\n\n                    # feed a (LR, predicted loss in tau steps) instance to BO.\n                    if str(predicted_loss) == 'nan':\n                        self.opt.tell([float(self.lr)], 1e6)\n                    else:\n                        self.opt.tell([float(self.lr)], predicted_loss)\n                    self.x_iters.append(float(self.lr))\n                    self.func_val_iters.append(predicted_loss)\n                    self.x_func_dict[self.lr] = predicted_loss\n                    self.lr_counter = 1\n\n                    if len(self.func_val_iters) == self.lr_to_explore:\n                        min_index = self.func_val_iters.index(min(self.func_val_iters))\n\n                        # log the best lr found for the next stage.\n                        logging.info('[Server]: best LR: {}, min loss: {}'.format(self.x_iters[min_index], self.func_val_iters[min_index]))\n\n                        self.lr = self.x_iters[min_index]\n                        self.message = str(','.join(('restore', str(self.lr))))\n                        c.send(','.join(('restore', str(self.lr))).encode('utf-8'))\n                        event.set()\n                        logging.debug('[Server]: restore command sent')\n                        self.exploitation_flag = True\n\n                        self.func_val_iters = []\n                        self.x_iters = []\n                        self.x_func_dict = dict()\n                    else:\n                        # Ask BO for the next LR to explore\n                        self.lr = self.opt.ask()[0]\n                        while True:\n                            if self.lr in self.x_func_dict:\n                                self.opt.tell([self.lr], self.x_func_dict[self.lr])\n                                self.lr = self.opt.ask()[0]\n                            else:\n                                break\n                        self.message = str(','.join(('restore', str(self.lr))))\n                        c.send(','.join(('restore', str(self.lr))).encode('utf-8'))\n                        event.set()\n                        logging.debug('[Server]: restore command sent')\n                else:\n                    self.lr_counter += 1\n                    if self.val_stage and self.lr_counter % self.val_freq == 0:\n                        self.message = \"evaluate\"\n                        c.send(self.message.encode('utf-8')) \n                    else:\n                        self.message = str(self.lr)\n                        c.send(str(self.lr).encode('utf-8')) \n                    event.set()\n\ndef main():\n    parser = argparse.ArgumentParser(description='AutoLRS server.')\n    parser.add_argument('--min_lr', \n                        help='minimum LR',\n                        required=True)\n    parser.add_argument('--max_lr', \n                        help='maximum LR',\n                        required=True)\n    parser.add_argument('--host', \n                        help='host',\n                        default='localhost',\n                        type=str)\n    parser.add_argument('--port', \n                        help='port',\n                        required=True,\n                        type=int)\n    args = parser.parse_args()\n    Controller(args.host, args.port, args.min_lr, args.max_lr).listen()\n\nif __name__ == '__main__': \n    main() \n",
        "experimental_info": "### AutoLRS Constants and Configuration:\n- **Initial Stage Length (τ)**: `EXPLOITATION_STEP = 1000` steps.\n- **Initial Evaluation Length (τ')**: `LR_STEPS = 100` steps for each candidate LR.\n- **Maximum Stage Length (τ_max)**: `TAU_MAX = 8000` steps.\n- **Loss Ring Buffer Length**: `RING_BUFFER_LEN = 100`.\n- **Number of LR Candidates to Explore**: `LR_TO_EXPLORE = 10` within each BO stage.\n- **Bayesian Optimization (BO) Setup**:\n    - **Optimizer**: `skopt.Optimizer`.\n    - **Search Space**: `Real(min_lr, max_lr, 'log-uniform')` (log-uniform distribution for learning rate).\n    - **Surrogate Model**: Gaussian Process (\"GP\").\n    - **Number of Initial Points**: `n_initial_points=1`.\n    - **Acquisition Function**: Lower Confidence Bound (`'LCB'`).\n    - **Kappa Parameter for LCB**: `kappa=1e6`.\n- **Loss Forecasting Model**:\n    - **Type**: Exponential time-series forecasting `L(t) = a * np.exp(b * t) + c`.\n    - **Spline Smoothing (Pre-processing loss series)**:\n        - `UnivariateSpline` with `spline_deg=2`.\n        - For training loss (`is_training=True`): `num_of_iter=10`, `filter_ratio=0.03`.\n        - For validation loss (`is_training=False`): `num_of_iter=1`, `filter_ratio` implicitly used but primarily `outliers = set(sorted(range(len(dys)), key=lambda i: dys[i])[-1:])` to remove only the largest outlier.\n- **Dynamic Stage Length Adjustment**:\n    - Stage length (`lr_steps`, `exploitation_step`) is doubled if `lr_steps < tau_max / 10`.\n    - Once `lr_steps` reaches `tau_max / 10`, `val_stage` is set to `True`.\n- **Loss Collection Strategy**:\n    - In early stages (before `val_stage`), training loss is used for forecasting (implied by `is_training=True` in `exponential_forcast` call).\n    - In later stages (`val_stage=True`), validation loss is used for forecasting (implied by `is_training=False` in `exponential_forcast` call).\n- **Validation Frequency**: When `val_stage` is `True`, validation loss is collected every `val_freq = int(lr_steps/16)` steps.\n- **Model State Management**: Model and optimizer states are checkpointed (`save_variables`) and restored (`restore_variables`) between candidate LR evaluations and BO stages to ensure fair comparison.\n\n### CIFAR-10 Example Specifics:\n- **Validation Subset Size**: Validation loss is calculated on a small subset of the validation set, consisting of `VAL_LEN = 10` mini-batches.\n- **Initial Optimizer Settings (before AutoLRS takes over)**: `optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)`.\n- **Warmup Steps**: `warmup_steps=0` (default, not explicitly set in the example).\n- **Total Training Epochs**: The training loop runs for `350 * 2 = 700` epochs."
      }
    },
    {
      "title": "Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks",
      "abstract": "This study investigates how weight decay affects the update behavior of\nindividual neurons in deep neural networks through a combination of applied\nanalysis and experimentation. Weight decay can cause the expected magnitude and\nangular updates of a neuron's weight vector to converge to a steady state we\ncall rotational equilibrium. These states can be highly homogeneous,\neffectively balancing the average rotation -- a proxy for the effective\nlearning rate -- across different layers and neurons. Our work analyzes these\ndynamics across optimizers like Adam, Lion, and SGD with momentum, offering a\nnew simple perspective on training that elucidates the efficacy of widely used\nbut poorly understood methods in deep learning. We demonstrate how balanced\nrotation plays a key role in the effectiveness of normalization like Weight\nStandardization, as well as that of AdamW over Adam with L2-regularization.\nFinally, we show that explicitly controlling the rotation provides the benefits\nof weight decay while substantially reducing the need for learning rate warmup.",
      "full_text": "Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks Atli Kosson 1 * Bettina Messmer 1 * Martin Jaggi 1 Abstract This study investigates how weight decay affects the update behavior of individual neurons in deep neural networks through a combination of applied analysis and experimentation. Weight decay can cause the expected magnitude and angular updates of a neuron’s weight vector to converge to a steady state we call rotational equilibrium. These states can be highly homogeneous, effectively balanc- ing the average rotation—a proxy for the effective learning rate—across different layers and neurons. Our work analyzes these dynamics across optimiz- ers like Adam, Lion, and SGD with momentum, offering a new simple perspective on training that elucidates the efficacy of widely used but poorly understood methods in deep learning. We demon- strate how balanced rotation plays a key role in the effectiveness of normalization like Weight Stan- dardization, as well as that of AdamW over Adam with ℓ2-regularization. Finally, we show that ex- plicitly controlling the rotation provides the bene- fits of weight decay while substantially reducing the need for learning rate warmup. 1. Introduction The use of weight decay or ℓ2-regularization has be- come ubiquitous in deep learning optimization. Although originally proposed as an explicit regularization method, Van Laarhoven (2017) showed that this interpretation does not hold for modern networks with normalization layers. This is because normalization can make a weight vector scale-invariant, meaning that the network output is unaf- fected by the magnitude of the vector (see §2). For scale- invariant vectors, weight decay instead acts as a scaling factor for some notation of an “effective” learning rate with varying definitions (Van Laarhoven, 2017; Zhang et al., *Primary contributor 1EPFL, Switzerland. Correspondence to: <atli.kosson@epfl.ch>. Code available at: https://github.com/epfml/REQ Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). t E[∥ωt∥] transient phaseequilibrium balanced rotation fast rotation slow rotation t E[∠(ωt,ωt+1)] Figure 1: Conceptual figure of the norm (left) and angular updates (right) of the weight vector ωt for different neurons (each line color) over time t with a constant learning rate. Weight decay modulates and stabilizes both metrics. 2019; Li & Arora, 2020; Wan et al., 2021). We explore this idea further, aiming to describe the effects of weight de- cay on the optimization dynamics of modern neural network (NN) training through applied analysis and experimentation. We specifically examine the update dynamicsof individual neurons. A neuron computes a scalar feature by comparing a learnable weight vector ωt ∈ RC with an incoming acti- vation vector xt ∈ RC, followed by a non-linear activation function φ and an optional learnable bias bt ∈ R: φ(⟨ωt, xt⟩+bt) = φ(∥ωt∥∥xt∥cos(∠(ωt, xt))+ bt) (1) Here ⟨·⟩ denotes a dot product which can be rewritten with a cosine similarity, showing that the direction of ωt deter- mines which “patterns” in xt the neuron responds to. We describe the neuronal update dynamics through theexpected weight normE[∥ωt∥], root-mean-square (RMS) update size ηg for the bias, and expected angular update sizeηr: ηg := p E[(bt+1 − bt)2] (2) ηr := E[∠(ωt, ωt+1)] = E h arccos \u0010 ⟨ωt,ωt+1⟩ ∥ωt∥∥ωt+1∥ \u0011i (3) where the expectation accounts for noise from randomly sampled minibatches (e.g., data shuffling). We assume weight decay is only applied to ω, not b or other param- eters, which is common and good practice (Jia et al., 2018). Figure 1 shows how the weight norm and angular updates (ηr) of different neurons could behave over time in a typical case. This behavior is caused by Spherical Motion Dynam- ics (Wan et al., 2021) which arise from the interaction of 1 arXiv:2305.17212v4  [cs.LG]  3 Jun 2024Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks weight decay and stochastic gradient updates (described further in §3). Over time, the weight norm reaches a sta- ble equilibrium magnitudein expectation, at which point the opposing effects of gradient updates (which increase the norm) and weight decay (which reduces it) cancel each other out. Interestingly, this simply stems from the geometry of the stochastic optimizer updates (especially with normal- ization layers), not the convergence of the underlying loss function, and can therefore be analyzed for a random walk. The angular updates shown in fig. 1R have an inverse depen- dency on the weight magnitude. During an initial transient phase the rotation is somewhat arbitrary, depending on the initial weight magnitude and gradient norm (for some op- timizers). The convergence of the weight norm results in a steady-state we call rotational equilibrium, character- ized by the average angular update having a specific, stable magnitude. For some setups the rotational equilibrium is identical for different layers and neurons (even if the weight norms differ), resulting in balanced rotationwhich we em- pirically observe aids optimization. We discuss our intuition for why this helps in appx. M. This study significantly expands upon prior investigations into the interactions between weight decay and normaliza- tion, demystifying the effects of weight decay and other common tricks in deep learning. While we touch upon cer- tain previous works throughout, please refer to appx. A for an extended discussion. Earlier research has primarily fo- cused on the general mechanisms and properties of weight decay and normalization, especially for plain SGD. We fo- cus on two main new directions, rotational equilibrium in other optimizers like AdamW (Loshchilov & Hutter, 2019), and the importance of balanced rotation in the optimization of neural networks. Compared to prior work, our analy- sis also targets more fine-grained update dynamics at the neuron level, applies to networks without scale-invariance from perfectly placed normalization layers, investigates ad- ditional aspects of the dynamics (e.g. transient phase, bias behavior), and relates standard optimizers to the LARS-style optimizers (You et al., 2017). Our key contributions are: • Deriving the steady-state neuronal update dynamics of AdamW, Adam with ℓ2-regularization, Lion and SGD with momentum for a random walk. We experimentally validate that the results hold for NN training in practice. • Showing how the interaction of weight decay and learn- ing rate shapes the rotation in the initial transient phase and results in two distinct “effective step sizes” in the steady state, ηg for biases and ηr for weights. • Demonstrating how explicitly controlling the angular updates via Rotational Optimizer Variants provides an alternative way of achieving the benefits of weight decay and normalization, while also simplifying the update dy- namics and reducing the need for learning rate warmup. • Revealing how balanced rotation contributes to the per- formance benefit of AdamW vs Adam+ℓ2, and certain normalization layers like Weight Standardization. 2. Preliminaries Scale-Invariance: Li & Arora (2020); Wan et al. (2021) describe how properly placed normalization can make the weight vector ω of a neuron/layer scale invariant w.r.t. a loss L and the resulting properties of the gradient ∇ωL(ω, ·): Scale Invariance: L(rω, ·) = L(ω, ·), ∀r >0 (4) Gradient orthogonality: ∇ωL(ω, ·) ⊥ ω (5) Inverse proportionality: ∥∇ωL(ω, ·)∥ ∝ ∥ω∥−1 (6) See appx. B for an overview. Note that different normal- ization operations can result in scale-invariance at a differ- ent granularity, for example individual neurons for Batch Normalization (Ioffe & Szegedy, 2015) and Weight Stan- dardization (Qiao et al., 2019; Huang et al., 2017b) but only whole layers for Layer Normalization (Ba et al., 2016). The Effective Learning Rate: In related literature there are multiple definitions of “effective” learning rates (Van Laarhoven, 2017; Chiley et al., 2019; Wan et al., 2021) which aim to describe how fast the neural network is being updated based on some metric. We use the average angular change ηr for this purpose, but will refer to it as an effective update size. Note that the direction of ω in eq. (1) controls which patterns in the inputs x the neuron detects, and ηr thus directly captures how fast this important aspect in the underlying functional representation of the neuron changes. This applies to all neurons, but particularly when the weight vector is scale-invariant and the direction thus fully deter- mines its effect. Simple alternatives like ∥ωt+1 − ωt∥ are scale dependent and do not directly measure changes in the encoded function. For other parameters we use the RMS change ηg as a measure of the update size. This is generally not a “functional” update measure (which would vary based on the architecture), but still informative and easy to analyze. Finally we note that an update size only measures the size of individual updates. Momentum affects the correlation of the updates over time, which also influences long-term “learning speed”, see appx. L. 3. Analysis In this section we analyze the rotational equilibrium of a weight vector ω to obtain simple expressions for the equi- librium magnitude d∥ω∥and the expected angular update in equilibrium, bηr. We focus on a simplified setting where up- dates are dominated by noise, resulting in a type of random walk. The equilibrium dynamics derived in this analytically tractable setting are predictive of the behavior we empiri- cally observe in neural networks (see experiments), but this 2Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks d∥ω∥ d∥ω∥ ∆gω ∆λω d∥ω∥ d∥ω∥ E[∥u⊥∥] E[∥d−u∥∥] Figure 2: Two views of equilibrium where the weight norm d∥ω∥is preserved because the gradient and weight decay com- ponents balance out on average. Left: Standard optimizer update eq. (7). Right: The total update contributions over the course of training, u and d, derived from the gradient and weight decay of a given timestep, respectfully. is not meant to be a formal theoretical analysis that fully captures all the intricacies of neural network optimization. Specifically, we assume the loss is in the form of empirical risk, i.e. L(ω, X) = 1 |X| P x∈X L(ω, x) where X is our training dataset, ω are our weights and x is a data point. The true noiseless gradient is then gX = ∇ωL(ω, X) and the gradient for a minibatch B is gB = ∇ωL(ω, B). We can define the noise in the gradient as gN = gB − gX with EB[gN ] = 0, because EB[gB] = gX for a randomly sampled B. Our simplifying assumption of the noise dominating can be stated gB = gX + gN ≈ gN , resulting in a random walk for the neural network parameters. Analogous assumptions have been successfully used elsewhere, e.g. with stochastic differential equations to derive the batch size scaling be- havior of optimizers (Li et al., 2021; Malladi et al., 2022). In our experiments we find that the final predictions hold very well for a variety of networks despite being derived for this simplified setting. Appx. G further describes the analytical setting and explores how the differences between a random walk and real neural network optimization affect the predictions. 3.1. Geometric Model for Equilibrium In this section we present a simple geometric derivation of the equilibrium norm d∥ω∥for different optimizers inspired in part by the analysis in Online Normalization (Chiley et al., 2019). We divide a parameter update ωt → ωt+1 into: ωt+1 − ωt = ∆gω + ∆λω (7) where ∆gω comes from the gradients and ∆λω from the weight decay. Equilibrium is an abstract state where the effects of ∆gω and ∆λω on the expected weight magnitude balance out on average. These components typically have different monotonic dependencies on the weight magnitude, with weight decay being proportional while the gradient component is either constant or inversely proportional, de- pending on the setting. As a result, the effects of these components can balance out in expectation at the equilib- rium magnitude d∥ω∥. As shown in fig. 2L, the geometry of this is not necessarily simple. Due to the averaging effects of momentum over time, ∆gω is not necessarily orthogo- nal to the weights even in cases where individual gradients are (e.g. for scale-invariant weights). Similarly, the weight decay (or ℓ2-regularization) component ∆λω may not be perfectly anti-parallel to the weights with momentum. To simplify the effects of momentum, we instead consider a different view of equilibrium shown in fig. 2R. Here we consider the total weight change throughout training aris- ing from the weight decay term and gradient from a given time step, instead of the update that is applied in that iter- ation. The Total Update Contribution (TUC)of the gra- dient at time step t, denoted u, is the sum of the contri- butions of ∇ωL(ωt, ·) to subsequent updates ωt → ωt+1, ωt+1 → ωt+2, and so on. Analogously, the TUC of the weight decay, denoted d, is the total change due to the weight decay or ℓ2-regularization of the weights ωt at itera- tion t. Note that without momentum u = ∆gω, d = ∆λω and that if ∆gω and ∆λω balance out on average, then so must u and d. In many cases u is orthogonal to the weights on average due to scale-invariance or randomness. Otherwise, we can split it into an orthogonal u⊥ and a radial (outwards) u∥ component. The u∥ term then has a similar effect as the weight decay term d which is anti-parallel to the weights in the cases we consider. If we can obtain an expression for the orthogonal ∥u⊥∥ and radial ∥d − u∥∥ total update contributions, we can apply the Pythagorean theorem to the dashed triangle in fig. 2R: ( d∥ω∥ −E[∥d − u∥∥])2 + E[∥u⊥∥]2 = d∥ω∥2 (8) We can then solve for d∥ω∥, making sure to account for the dependency of u and d on the weight norm. Once we have an expression for d∥ω∥, we can compute a prediction for the corresponding RMS update size bηg = p E[∥∆gω∥2]. This gives a prediction for the ex- pected relative update size bηg/d∥ω∥ := bηr, which closely approximates the angular update ηr in equilibrium. We do this for AdamW in the next subsection and for SGDM, Lion (Chen et al., 2023) and Adam with ℓ2-regularization in appx. C, D and E. The resulting predictions for the update dynamics of each optimizer are summarized in table 1. 3.2. AdamW Equilibrium We write AdamW (Loshchilov & Hutter, 2019) updates as: mt = β1mt−1 + (1 − β1)gt (9) vt = β2vt−1 + (1 − β2)g2 t (10) pt = pt−1 − η · \u0010 mt/(1−βt 1)√ vt/(1−βt 2)+ε + λpt−1 \u0011 (11) 3Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks Table 1: Analytical predictions for the steady state neuronal update dynamics of different optimizers. The bηg values apply to any parameter p ∈ RC with gradient g. The bηr and d∥ω∥values apply to scale-invariant weights ω ∈ RC in equilibrium. Expressions with ˜g := ∥p∥g use the inverse proportionality from eq. (6) and would therefore differ without scale-invariance. SGDM (45) AdamW (11) Adam+ ℓ2 (78) Lion (57) RMS update size bηg η q E[∥g∥2] 1−α2 η q C 1−β1 1+β1 η q C 1−β1 1+β1 η √ C Expected rotation bηr q 2ηλ 1+α q 2ηλ 1−β1 1+β1 3 r 2η2λ ⟨1, √ E[˜g2]⟩ q 1−β1 1+β1 C √πηλ \u0010 (1−β1)2 + β2 1 1−β2 1+β2 \u00111 2 Equilibrium norm d∥ω∥ 4 q ηE[∥˜g∥2] 2λ·(1−α) q ηC 2λ 3 q η 2λ ·⟨1, p E[˜g2]⟩ q ηC πλ \u0010 (1−β1)2 + β2 1 1−β2 1+β2 \u0011−1 2 where pt ∈ RC is a parameter vector at iteration t, gt = ∇pL(pt, . . .) is the gradient, m is the first moment and v is the second moment. The learning rate η ≥ 0, weight decay λ ≥ 0 (zero expect for weight vectors), mo- ment coefficients β1, β2 ∈ (0, 1) and ε ≥ 0 are hyperpa- rameters. For simplicity we assume that ε and the bias correction can be ignored, i.e. that ε, βt 1 and βt 2 are all ≈ 0. Equilibrium Magnitude: When applying AdamW to a weight vector ω ∈ RC, the total update contributions are: u = −η P∞ k=t βk−t 1 (1 − β1) gt√vk , d = −ηλω (12) We note that due to symmetry, each coordinate of u has a zero-mean distribution in the random walk setup. Since u is independent from ω, this makes them orthogonal in ex- pectation i.e. E[⟨u, ω⟩] = 0. It is also reasonable to assume that the variance of each coordinate remains constant when the gradient distribution is not changing over time, resulting in ∀t, k: E[∥gt/√vk∥2] = C (the vector dimension) and therefore E[∥u∥2] = η2C. Defining ω = ∥ω∥, u = ∥u∥, u∥ = ⟨ω, u⟩/∥ω∥, u2 ⊥ = u2 − u2 ∥ and d = ∥d∥ we can write a recurrence relation based on eq. (8): E[ω2 i+1] = E[(ωi − d + u∥)2 + u2 ⊥] (13) = E[ω2 i − 2dωi + 2u∥ωi − 2du∥ + d2 + u2 ∥ + (u2 − u2 ∥)] (14) = E[ω2 i ](1 − 2ηλ + η2λ2) + η2C (15) where we used independence, E[u∥] = 0, and E[u] = η2C. The solution is: E[ω2 i ] = E[ω2 0]ai + η2C 2ηλ−η2λ2 (1 − ai) (16) a = 1 − 2ηλ + η2λ2 (17) The recurrence relation is written in terms of the u and d in- stead of ∆gω and ∆λω. This is thus only an approximation of how the real system converges to equilibrium over time, but still informative. It may be a good approximation if∥ω∥ changes slowly compared to how fast u is applied and v is updated, i.e. when β1, β2 are low compared to a. The limit i → ∞gives us the equilibrium norm listed in table 1: d∥ω∥ = q ηC 2λ−ηλ2 ≈ q ηC 2λ (for λη ≪ 2) (18) Expected Update Size: We can estimate the RMS update size ηg of a parameter p ∈ RC with ∆gp = ηmt√vt as follows: E[∥∆gp∥2] = E[∥ η√vt (1 − β1) Pt−1 k=0 βt−k 1 gt−k∥2] (19) = η2(1 − β1)2 Pt−1 k=0 β2t−2k 1 E[∥gt−k√vt ∥2] (20) ≈ η2 1−β1 1+β1 C (21) where we have approximated the geometric sum with its limit t → ∞, used the fact that for the random walk ∀j ̸= k : E[⟨gj, gk⟩] = 0 as well as our previous assump- tion ∀t, k: E[∥gt/√vk∥2] = C. This gives us the predic- tion bηg ≈ E[ p ∥∆gp∥2] listed in table 1. Approximating the equilibrium angular update size with the expected rel- ative update size p E[∥∆gω∥2]/ d∥ω∥ gives the bηr value. This approximation is good for small relative updates and a relatively small radial component in ∆gω. 3.3. AdamW vs Adam with ℓ2-Regularization Loshchilov & Hutter (2019) proposed the use of decoupled weight decay instead of ℓ2-regularization in Adam (Kingma & Ba, 2015). In their experiments they find that Adam with decoupled weight decay (i.e. AdamW, see eq. (11)) outperforms the ℓ2-regularized form (i.e. Adam+ℓ2, eq. (78)) across a wide range of settings. Since then AdamW has been widely adopted, but as far as we know the reason for its effectiveness over Adam+ℓ2 is not well understood. Our analysis of Adam+ℓ2 in appx. E reveals that both the equilibrium norm and angular update size depend on the gradient magnitude (through ˜g) unlike AdamW, see table 1. When the gradient norm varies between neurons or layers, this results in imbalanced rotation. We believe the bal- anced vs imbalanced equilibrium rotation is a key difference between AdamW and Adam+ℓ2, which may explain why decoupled weight decay is more effective for Adam-like methods. In our experiments (§5.3) we explore this further along with the general impact of imbalanced rotation. 4Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks Algorithm 1 Rotational Wrapper for constrained dynamics. Require: Inner optimizer F, decay factor 0 ≤ β <1, ε ≥ 0 for numerical stability, iteration count T, rotational set Ω 1: for p in Ω : # For each neuronal weight vector 2: νp ← 0 # Initialize the update RMS tracker 3: np ← ∥p∥ # Save the initial magnitude 4: p ← np · p−¯p ∥p−¯p∥ # Remove the mean component of p 5: for t ∈ {1, ..., T} : 6: Perform backpropagation, obtain gradients for all params 7: for all p : # For each parameter 8: # Get update components (§ 3.1): 9: ∆gp, ∆λp ← F.get_update(p, ∇pL(p, . . .)) 10: if p ∈ Ω : # If p is a neuronal weight vector 11: ∆gp ← ∆gp/η # Undo η used in F 12: # Remove projections, s.t. ∆gp ⊥ p, ∆gp ⊥ 1: 13: ∆gp ← ∆gp − ⟨∆gp,p⟩ ∥p∥2 p − ⟨∆gp,1⟩ ∥1∥2 1 14: # Update RMS tracker: 15: νp ← β · νp + (1 − β) · ∥∆gp∥2 16: # Rotate p by bηr from table 1 on average: 17: p ← p + bηr · np · ∆gp√ νp/(1−βt)+ε 18: # Normalize p to the initial magnitude: 19: p ← np · p ∥p∥ 20: else: # p is not a neuronal weight vector 21: p ← p + ∆gp + ∆λp # Standard update 3.4. Rotational Dynamics of Scale-Sensitive Parameters Prior work has primarily focused on the dynamics of scale- invariant weights. Note that any weight vector can be made scale-invariant by simply applying normalization to it, e.g. Weight Standardization (Qiao et al., 2019). For a random walk the gradient component is always orthogonal in expec- tation, but for real tasks scale-sensitive weights can have an average radial gradient component E[u∥] ̸= 0 (fig. 2R). In appx. H we explore how this affects the rotational dy- namics of these weights (for SGDM). We find that a radial gradient component acts like an additional weight decay λu = −E[u∥]/(η∥ω∥) giving a new “effective” weight de- cay of λe = λ + λu, resulting in update dynamics similar to scale-invariant weights with the adjusted value. 4. Rotational Variants of Optimizers (RVs) Our analysis shows how weight decay causes standard op- timizers to transition towards equilibrium over time. In the steady state, weight decay balances the rotation across weight vectors and scales ηr relative to ηg. The same effect can be achieved without weight decay by directly control- ling the average angular update as shown in algo. 1. We keep the weight magnitude constant and optionally introduce a learnable gain to compensate, which can matter for scale- sensitive weights and avoids numerical issues. These Rota- tional Variants (RVs)of existing optimizers serve as valu- able research tools that allow us to verify our understanding of weight decay and perform ablation studies. Since RVs 0 2 4 6 8Weight Norm [×10] i1k ResNet-50 (SGDM) OWT GPT2+WS (AdamW) 0 25 50 75 100 Step [×103] 0 2 4 6Avg Rotation [×10 3] 0 25 50 75 100 Step [×103] Figure 3: Measured weight norms and average rotation for different layers (solid colors) in two real neural net- work training tasks, ResNet-50 on ImageNet-1k (SGDM) and Weight Standardized GPT2-124M on OpenWebText (AdamW). The predicted equilibrium rotation (dashed black) from table 1 holds very well for all scale-invariant layers. The final fully-connected layer in RN-50 (pink) is not scale-invariant with a radial gradient component that de- creases the effective weight decay, slowing the rotation (see §3.4). The learning rate is constant for easier comparison. mimic the steady-state behavior of standard optimizers we expect similar performance. However, the update dynamics also differ in certain ways: • The RVs can perfectly balance the average rotation with- out relying on scale-invariance from e.g. normalization. • The RVs are always in equilibrium so there is no transient- phase. This means that the specified learning rate sched- ule directly controls ηr unlike in standard optimizers. This simplifies the optimization dynamics and makes them more robust to architectural choices such as normalization. Note that the RVs closely resemble relative optimizers like LARS (You et al., 2017) and LAMB (You et al., 2020), revealing how they relate to and differ from standard opti- mizers. The RVs are further described in appx. I. 5. Experiments & Discussion In this section we experimentally validate our analysis of the neuronal update dynamics and explore their impact on training. See appx. K for experimental setup and details. 5.1. Measuring & Constraining the Update Dynamics In §3 we derived how weight decay affects the neuronal update dynamics of a network undergoing a carefully con- structed analytically tractable random walk. Here we show that these dynamics occur in practical problems and that they suffice for obtaining the benefits of weight decay. 5Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks Table 2: Test set performance (mean±std) over three seeds for the baseline optimizer, AdamW, and its rotational variant (RV). We use the baseline hyperparameters directly for the no weight decay (λ = 0) and zero-shot results, but do minor tuning for the few-shot RV results where needed. The performance parity of the RV suggests that the benefits of weight decay, which are clear from the baseline degradation without it, can be achieved by directly controlling angular updates. AdamW RV-AdamW Dataset Model Batch Size Metric ( ↑ ↓) Baseline λ = 0 (zero-shot) (few-shot) CIFAR-10 ResNet-20 128 Top-1 Acc. [%] ( ↑) 92.2±0.2 91.1±0.3 92.4±0.3 N/A CIFAR-10 ResNet-20 2048 Top-1 Acc. [%] ( ↑) 91.5±0.3 90.4±0.3 91.4±0.3 92.2±0.3 Imagenet-1k DeiT tiny 1024 Top-1 Acc. [%] ( ↑) 72.1 69.3 71.3 72.5 IWSLT2014 de-en Transformer-S 4096 Bleu ( ↑) 34.6±0.1 34.6±0.1 20.0±0.1 34.7±0.1 Wikitext GPT2-124M 165 ×512 Perplexity ( ↓) 19.6±0.1 unstable 19.0±0.2 N/A OpenWebText GPT2-124M 480 ×1024 Loss ( ↓) 3.18±0.02 3.21±0.01 3.18±0.01 N/A OpenWebText (25k) GPT2-124M 480 ×1024 Loss ( ↓) 2.94±0.01 unstable 2.93±0.01 N/A Measurements: Figure 3 shows the weight norm and aver- age angular updates over time for several layers in RN-50 and a GPT2 variant. For scale-invariant layers, the average rotation converges to the predicted equilibrium rotation bηr over time as anticipated. This value depends solely on the hyperparameters with no dependency on other unknown or time-varying quantities such as the gradient magnitude. For layers that are not scale-invariant, the equilibrium ro- tation is affected by the average radial gradient component as expected (§3.4), but this deviation is often not very sig- nificant as seen for the final FC layer in fig. 3. Note that transformers are not scale-invariant without additional tricks like Weight Standardization (WS), but we find the equilib- rium rotation is still often close to the scale-invariant value (see appx. J, fig. 15 for GPT2 without WS). For fig. 3 the length of the transient phase is small compared to the length of typical training. ResNet-50 was originally trained for 90 epochs (∼450k steps) and GPT2 for ∼600k steps. However, this need not be the case, and will also depend on the hyperparameters as predicted by eq. (16). Constraining the Rotational Dynamics: Table 2 shows the impact of replacing weight decay in AdamW via algo. 1 (RV-AdamW) for various network architectures and tasks. To quantify the effect of weight decay on the baselines, we repeat them with the same hyperparameters aside from disabling weight decay, i.e. with λ = 0 . Training with- out weight decay is similar to using a different learning rate schedule (Li & Arora, 2020). In particular, for scale- invariant weights, disabling weight decay is similar to mul- tiplying the learning rate schedule for ηr with an exponen- tially decaying function. An example of this can be seen in fig. 14 in appx. J, which also shows the accompanying increase in the weight norms. The effects of this will vary with training length, the weight decay value used, the ini- tial weight norms etc. Without weight decay GPT training eventually diverges (at least in bfloat16), which was also observed before by Andriushchenko et al. (2023). Table 2 shows that in most cases the RV perform similarly to the original baseline using exactly the same hyperparam- eters (zero-shot). This is expected if the baseline training is dominated by the steady-state equilibrium phase that the RV is designed to model. This is not always the case, for example in the IWSLT2014 the weight decay of the baseline is too low to have a significant impact over the course of training. The resulting training never reaches the equilib- rium the RV models, where the average rotation would be very low (remember that the weight decay value affects the convergence rate towards equilibrium, see eq. (16)). In such cases we also perform a “few-shot” experiment where we do light tuning of the weight decay parameter. We can roughly predict the modified value based on measurements of the observed rotation during the baseline training using table 1. Appx. J shows experiments for other base optimizers. In all cases we are able to recover the baseline performance with the RV . This suggests that controlling the average angular update size is sufficient to obtain the benefits of weight decay. Note that our intention here is not to outperform the baselines, although we believe the insights from the RVs could help with this in the future. 5.2. Transient Effects & The Interaction of η and λ In our analysis we describe two different update sizes, the angular update size ηr (for neuronal weight vectors) and the RMS update size ηg (for biases, gains etc). The steady state value of ηr depends on both the learning rate η and weight decay λ hyperparameters, while ηg is not directly affected by weight decay (which is not applied to biases etc). The initial ηr also only depends on η, but λ modulates it over time creating an induced “effective” learning rate schedule ηr(t) that can deviate from the specified learning rate schedule η(t). Here we explore these effects experimentally. 6Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks 10 4  10 2  100 Learning Rate 88 89 90 91 92 93Validation Accuracy RV-AdamW AdamW 0 20 40 60 Step [×103] 100 102 Avg Weight Norm 0 20 40 60 Step [×103] 10 4 10 3 10 2 Avg Rotation Figure 4: Weight decay influences transient behavior and how fast weights are updated relative to biases. Left: Validation accuracy for ResNet-20 on CIFAR-10 for learning rate, weight decay pairs with a constant product (ηλ = 5·10−4) resulting in a specific bηr but different bηg and d∥ω∥(table 1). Middle/Right: The weight norm ∥ω∥ and angular update size ηr over time for three (η, λ) pairs corresponding to the colored circles on the left with equilibrium predictions in dashed red. 100 101 102 103 Step 3 4 6 10Train Loss AdamW No warmup 10 3  10 2 Learning Rate 3.1 3.2 3.3 3.4 3.5 3.6Validation Loss AdamW No warmup RV-AdamW No warmup 10 1  100 101 Learning Rate 30 40 50 60Validation Accuracy SGDM No warmup RV-SGDM No warmup Figure 5: The RVs display a reduced need for learning rate warmup compared to standard optimizers, offering insights into the utility of warmups. Left: GPT2-124M OWT loss curve with/without learning rate warmup. Middle: GPT2-124M OWT final validation loss for different learning rates with AdamW/RV-AdamW and with/without warmup.Right: ResNet-50 i1k validation accuracy (short, large batch training) for different learning rates with SGDM/RV-SGDM, with/without warmup. Learning Rate vs Weight Decay: Figure 4L shows the final network performance obtained for different (η, λ) pairs with a constant ηλ product and therefore the same equi- librium rotation value. With the RV , this is equivalent to changing the size of the bias updates via ηg, while keeping the rotation ηr constant. The performance varies quite a bit, the leftmost values at ηg ≈0 roughly match the results with frozen gains/biases (91.1%), while the rightmost val- ues have rapidly changing biases degrading performance. Although we don’t show it here, the ηr value also matters significantly so η and λ jointly determine two distinct effec- tive step sizes, ηr and ηg, both of which affect performance. Weight decay can therefore be seen as a scaling factor for the effective (equilibrium) update size of the weights (ηr) relative to other parameters such as biases (ηg). As a result we need to keep the λ hyperparameter in the RVs even if we don’t actually decay the weights. The baseline optimizer (black in fig. 4L) shows a similar trend but is also affected by changes in the transient phase which we will look at next. Transient Effects: Although the weight decay and learning rate have an identical effect on the equilibrium rotation bηr for AdamW, they affect the norm differently (see table 1). In fig. 4M we plot the weight norm and equilibrium pre- diction for three different (η, λ) pairs corresponding to the colored circles in fig. 4L. These experiments use a cosine decay learning rate schedule and a five epoch warmup. Note how configurations with a higher learning rate (and lower weight decay) converge towards a higher equilibrium norm after the initial transient phase. Towards the later stages of training the weights fall back out of equilibrium as the weights can not decay fast enough to keep up with the shift- ing equilibrium norm (which decreases with the learning rate schedule). We expect longer training would keep the weights in equilibrium for proportionally longer. Figure 4R shows the measured angular updates correspond- ing to the previous (η, λ) pairs. We note how the rotational behavior out of equilibrium is inverted compared to that of the norm. Weight norms below the equilibrium magni- tude result in rotation that is faster than in equilibrium, and vice versa. This means that depending on the initialization magnitude of the weights compared to the equilibrium mag- nitude, we either observe overly fast or slow rotation during the initial transient phase. The purple example shows that even though we use a learning rate warmup, the induced “effective” update schedule ofηr can still include overly fast rotation. The RVs eliminate these transient phases causing differences at both the start and end of training which can be significant, but could be replicated by scheduling η or λ appropriately if desired. 7Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks -  -5 -4 -3 -2 -1 0 1 2 3 4 2-coefficient =10 3 2x -9 -7 -5 -3 -1 1 Learning Rate =10 1 2y Adam+ 2 r g -  -2 -1 0 1 2 3 4 5 6 7 8 9 Weight Decay =10 2 2x AdamW r g 92.0 92.5 93.0 93.5 94.0 94.5 Validation Accuracy 0 50 100 150 200 Epoch 10 4 10 3 10 2 10 1 Avg Rotation (Per Layer)   Adam+ 2 AdamW Figure 6: Balanced vs imbalanced rotation appears to be a key difference between AdamW and Adam+ℓ2. Left: A sweep of the learning rate and ℓ2-regularization / weight decay of Adam+ℓ2 and AdamW on CIFAR-10 ResNet-18. Adam+ℓ2 is unable to match the performance of AdamW. The orange dashed lines represent contour lines for bηr based on table 1 along which bηg varies, demonstrating how both update sizes matter. This dashed line is analogous to the sweep depicted in fig. 4. Right: The observed average rotation of each layer during training with the best setting for each optimizer. Here the rotation is affected by a cosine learning rate schedule (no warmup). Need for Learning Rate Warmup: We conjecture that the irregular update dynamics observed in the initial transient phase can hinder optimization. In fig. 5L we show train- ing curves for GPT2-124M OWT with and without a 5% learning rate warmup. The run without warmup is stable and makes faster initial progress but eventually falls behind, creating a loss gap that is not closed throughout training. Figure 5M compares the final validation loss with and with- out warmup for different learning rates for both AdamW and its rotational variant. The AdamW runs show a significant gap between the two, but the difference is minimal with the RV . We even observe the no-warmup runs being stable at slightly higher learning rates with the RV , but believe this is noise due to non-deterministic divergence. In fig. 5R we do a similar experiment with 10 epoch ResNet- 50 i1k training with a large batch size of 8k. We again ob- serve that the baseline benefits significantly from warmups, but the RV only marginally (which could be due to the dy- namics of other parameters like biases which the RV does not stabilize). We observed the same trend when extending the best runs to full 90 epochs (see appx. J). This suggests learning rate warmup may aid training in part by stabilizing the transient phase, and that explicitly controlling the angular updates could offer similar benefits. 5.3. The Importance of Balanced Rotation The previous sections have mostly glossed over the fact that equilibrium forms independently for various neurons and layers. In certain settings this leads to different (imbalanced) equilibrium rotation between network components. Here we explore the performance impact of this experimentally. Adam vs AdamW: Our analysis revealed the the equilib- rium rotation of Adam+ℓ2 has a dependency on the gradient magnitude unlike AdamW. When the gradient norm differs between neurons/layers this should lead to imbalanced rota- tion (even for scale-invariant layers). Figure 6L reproduces the performance gap between AdamW and Adam+ ℓ2 ob- served by Loshchilov & Hutter (2019), around 0.5% on the validation set. Note how the structure of these heatmaps corresponds to changes in the two equilibrium update sizes bηr and bηg, demonstrating how both matter in practice as sug- gested in §5.2. The original decoupled version of AdamW described by Loshchilov & Hutter (2019) differs from the one used in PyTorch and described here, replacingηλ in the weight decay term of eq. (11) with just λ, but scaling both η and λ over time according to the learning rate schedule. In this case bηr depends only on λ and bηg on η, explaining the separability of the hyperparameter space they observe. Figure 6R shows the rotation of different layers over train- ing for the best configuration of each optimizer. In AdamW all layers behave similarly, even the scale-sensitive fi- nal FC layer (dashed) does not deviate significantly, un- like Adam+ℓ2 where the rotation differs drastically. The ∼30× range in observed rotation corresponds to a factor of ∼1000× in the learning rate (table 1). In appx. J we show that enforcing balanced rotation in Adam+ℓ2 roughly closes the gap. We also observe imbalanced rotation impeding training in other settings (see later), so we hypothesize that balanced equilibrium rotation is the main benefit of AdamW over Adam+ℓ2. 8Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks 10 2  10 1  100 Learning Rate 68 70 72 74Validation Accuracy SGDM SGDM+WS RV-SGDM 0 50 100 150 200 Epoch 0.00 0.01 0.02 0.03Avg Rotation (Per Neuron) SGDM + LN Mean±std 0 50 100 150 200 Epoch SGDM + LN + WS Mean±std Figure 7: Balanced neuronal rotation obtained through e.g. RVs or Weight Standardization seems to aid training.Left: Final accuracy for a Layer-Normalized (LN) ResNet-18 on CIFAR-100 for different peak learning rates (cos+warmup schedule). Baseline compared with additional Weight Standardization (WS) and RV training. Right: The baseline has imbalanced rotation across neurons, WS balances them. Example neurons (each color) and mean±std (black/gray) shown for one layer. The Benefit of Weight Standardization: Layer Normal- ization can make a whole layer scale-invariant but not in- dividual neurons (unlike e.g. Batch Normalization). This means that the gradient is orthogonal to the weights of the whole layer (see eq. (5)) but individual neurons may have a non-zero radial component. In fact, the normalized output magnitude of one neuron can be increased by decreasing the output magnitude of the other neurons in the layer and vice versa, potentially encouraging radial gradient components on the neuron level. This changes the “effective” weight decay as discussed in §3.4, which can lead to imbalanced rotation between neurons. Weight Standardization (WS) makes each neuron scale invariant eliminating this effect. Note that WS is fully independent of the network inputs and can thus be seen as a reparameterization or an optimization trick. Other types of normalization like Batch Normalization can have additional effects like affecting signal propagation (Brock et al., 2021a) or causing a dropout-like regularization effect (Kosson et al., 2024). Figure 7L shows the final accuracy achieved when training a Layer-Normalized ResNet-18 on CIFAR-100 for different learning rates. Training using either Weight Standardiza- tion or an RV significantly outperforms standard training. fig. 7R shows imbalanced neuronal rotation in the baseline which the WS eliminates as expected (the RV does as well). Weight Standardization thus appears to facilitate optimiza- tion by balancing the equilibrium dynamics across neurons, especially when applied on top of Layer Normalization or Group Normalization (Wu & He, 2018). Imbalanced Rotation: We can modify the RVs to directly test the impact of imbalanced rotation without potential con- founding effects. In appx. J (fig. 18) we show that an RV with artificially imbalanced neuronal rotation, where some neurons are intentionally rotated slower or faster, suffers from a performance degradation compared to a standard balanced version. This gap persists despite tuning the hy- perparameters for each configuration. We observe that even small imbalances in the rotational speed ( ηr) can signifi- cantly degrade performance. 6. Conclusion In this work we have described how weight decay modulates the update dynamics of individual neurons and explored how their variance across components (balanced vs imbalanced) and time (transient vs equilibrium phases) impacts train- ing. Despite their simplicity, neural update dynamics offer a surprisingly insightful perspective of neural network opti- mization and demystify the effectiveness of widely adopted techniques. We believe this area presents numerous oppor- tunities for further theoretical and practical research. Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. Acknowledgements We thank Maksym Andriushchenko for insightful conversa- tions related to this study. We also appreciate Alex Hagele, Dongyang Fan and Nikita Doikov for their feedback on the manuscript which enhanced its quality and clarity. References Andriushchenko, M., D’Angelo, F., Varre, A., and Flammar- ion, N. Why do we need weight decay in modern deep learning? arXiv preprint arXiv:2310.04415, 2023. URL https://arxiv.org/abs/2310.04415. 6, 13 Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. URL https: //arxiv.org/abs/1607.06450. 2 Brock, A., De, S., and Smith, S. L. Characterizing signal propagation to close the performance gap in unnormal- ized resnets. In 9th International Conference on Learning Representations, ICLR, 2021a. arXiv:2101.08692. 9, 29 9Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks Brock, A., De, S., Smith, S. L., and Simonyan, K. High-performance large-scale image recognition with- out normalization. In International Conference on Machine Learning , pp. 1059–1071. PMLR, 2021b. arXiv:2102.06171. 14, 29 Cettolo, M., Niehues, J., Stüker, S., Bentivogli, L., and Federico, M. Report on the 11th IWSLT evaluation cam- paign. In Proceedings of the 11th International Workshop on Spoken Language Translation: Evaluation Campaign, pp. 2–17, Lake Tahoe, California, December 4-5 2014. URL https://aclanthology.org/2014.iw slt-evaluation.1. 33 Chen, X., Liang, C., Huang, D., Real, E., Wang, K., Pham, H., Dong, X., Luong, T., Hsieh, C.-J., Lu, Y ., and Le, Q. V . Symbolic discovery of optimization algorithms. InThirty- seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net /forum?id=ne6zeqLFCZ. arXiv:2302.06675. 3, 17 Chiley, V ., Sharapov, I., Kosson, A., Koster, U., Reece, R., Samaniego de la Fuente, S., Subbiah, V ., and James, M. Online normalization for training neural networks. Advances in Neural Information Processing Systems, 32, 2019. arXiv:1905.05894. 2, 3, 13, 17, 36 Fu, J., Wang, B., Zhang, H., Zhang, Z., Chen, W., and Zheng, N. When and why momentum accelerates sgd: An empirical study. arXiv preprint arXiv:2306.09000, 2023. URL https://arxiv.org/abs/2306.09000 . 36 Goodfellow, I., Bengio, Y ., and Courville, A.Deep Learning. MIT Press, 2016. http://www.deeplearningbo ok.org. 13 He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016. arXiv:1512.03385. 33 Heo, B., Chun, S., Oh, S. J., Han, D., Yun, S., Kim, G., Uh, Y ., and Ha, J.-W. Adamp: Slowing down the slowdown for momentum optimizers on scale-invariant weights. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum ?id=Iz3zU3M316D. arXiv:2006.08217. 13 Hoffer, E., Banner, R., Golan, I., and Soudry, D. Norm mat- ters: efficient and accurate normalization schemes in deep networks. Advances in Neural Information Processing Systems, 31, 2018. arXiv:1803.01814. 13 Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. URL https://arxiv.org/abs/2203.15556 . 34 Huang, L., Liu, X., Lang, B., and Li, B. Projection based weight normalization for deep neural networks. arXiv preprint arXiv:1710.02338, 2017a. URL https://ar xiv.org/abs/1710.02338. 14 Huang, L., Liu, X., Liu, Y ., Lang, B., and Tao, D. Centered weight normalization in accelerating training of deep neu- ral networks. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pp. 2803–2811, 2017b. URL https://openaccess.thecvf.co m/content_iccv_2017/html/Huang_Cente red_Weight_Normalization_ICCV_2017_p aper.html. 2, 15, 29 Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pp. 448– 456. pmlr, 2015. arXiv:1502.03167. 2, 14 Jia, X., Song, S., He, W., Wang, Y ., Rong, H., Zhou, F., Xie, L., Guo, Z., Yang, Y ., Yu, L., et al. Highly scal- able deep learning training system with mixed-precision: Training imagenet in four minutes. arXiv preprint arXiv:1807.11205, 2018. arXiv:1807.11205. 1 Karpathy, A. nanogpt. https://github.com/karpa thy/nanoGPT/, 2023. 33, 34 Karras, T., Aittala, M., Lehtinen, J., Hellsten, J., Aila, T., and Laine, S. Analyzing and improving the train- ing dynamics of diffusion models. arXiv preprint arXiv:2312.02696, 2023. 14 Kingma, D. and Ba, J. Adam: A method for stochastic optimization. In International Conference on Learn- ing Representations (ICLR), San Diega, CA, USA, 2015. arXiv:1412.6980. 4, 14, 19 Kodryan, M., Lobacheva, E., Nakhodnov, M., and Vetrov, D. P. Training scale-invariant neural networks on the sphere can happen in three regimes. Advances in Neural Information Processing Systems, 35:14058–14070, 2022. arXiv:2209.03695. 13 Kosson, A., Fan, D., and Jaggi, M. Ghost noise for regulariz- ing deep neural networks. Proceedings of the AAAI Con- ference on Artificial Intelligence, 38(12):13274–13282, Mar. 2024. doi: 10.1609/aaai.v38i12.29228. URL https://ojs.aaai.org/index.php/AAAI/ article/view/29228. arXiv:2305.17205. 9 Krizhevsky, A. Learning multiple layers of features from tiny images. self-published, 2009. URL https://ww w.cs.toronto.edu/~kriz/learning-featu res-2009-TR.pdf. 33 10Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks Li, Z. and Arora, S. An exponential learning rate sched- ule for deep learning. In International Conference on Learning Representations , 2020. URL https: //openreview.net/forum?id=rJg8TeSFDH . arXiv:1910.07454. 1, 2, 6, 13 Li, Z., Lyu, K., and Arora, S. Reconciling modern deep learning with traditional optimization analyses: The intrinsic learning rate. Advances in Neural In- formation Processing Systems, 33:14544–14555, 2020. arXiv:2010.02916. 13 Li, Z., Malladi, S., and Arora, S. On the validity of modeling SGD with stochastic differential equations (SDEs). In Beygelzimer, A., Dauphin, Y ., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net /forum?id=goEdyJ_nVQI. arXiv:2102.12470. 3 Li, Z., Bhojanapalli, S., Zaheer, M., Reddi, S., and Kumar, S. Robust training of neural networks using scale in- variant architectures. In International Conference on Machine Learning , pp. 12656–12684. PMLR, 2022a. arXiv:2202.00980. 14 Li, Z., Wang, T., and Yu, D. Fast mixing of stochastic gradient descent with normalization and weight decay. Advances in Neural Information Processing Systems, 35: 9233–9248, 2022b. URL https://proceedings. neurips.cc/paper_files/paper/2022/ha sh/3c215225324f9988858602dc92219615-A bstract-Conference.html. 13 Liu, Y ., Bernstein, J., Meister, M., and Yue, Y . Learning by turning: Neural architecture aware optimisation. In International Conference on Machine Learning, pp. 6748– 6758. PMLR, 2021. arXiv:2102.07227. 14 Loshchilov, I. and Hutter, F. Decoupled weight decay reg- ularization. In International Conference on Learning Representations, 2019. URL https://openreview .net/forum?id=Bkg6RiCqY7. arXiv:1711.05101. 2, 3, 4, 8, 13, 29, 34 Malladi, S., Lyu, K., Panigrahi, A., and Arora, S. On the SDEs and scaling rules for adaptive gradient algorithms. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Sys- tems, 2022. URL https://openreview.net/f orum?id=F2mhzjHkQP. arXiv:2205.10287. 3 McCandlish, S., Kaplan, J., Amodei, D., and Team, O. D. An empirical model of large-batch training. arXiv preprint arXiv:1812.06162, 2018. 23 Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models. In International Conference on Learning Representations , 2017. URL https: //openreview.net/forum?id=Byj72udxe . arXiv:1609.07843. 33 Neyshabur, B., Salakhutdinov, R. R., and Srebro, N. Path- sgd: Path-normalized optimization in deep neural net- works. Advances in neural information processing sys- tems, 28, 2015. arXiv:1506.02617. 27, 37 Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and Auli, M. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of NAACL- HLT 2019: Demonstrations , 2019. arXiv:1904.01038. 33, 34 Pagliardini, M. llm-baseline. https://github.com /epfml/llm-baselines, 2023. 33, 34 Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. arXiv:1912.01703. 22 Qiao, S., Wang, H., Liu, C., Shen, W., and Yuille, A. L. Weight standardization. CoRR, abs/1903.10520, 2019. URL http://arxiv.org/abs/1903.10520 . 2, 5, 15, 29 Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners. self-published, 2019. URL https://d4mu cfpksywv.cloudfront.net/better-langu age-models/language_models_are_unsupe rvised_multitask_learners.pdf. 33 Roburin, S., de Mont-Marin, Y ., Bursuc, A., Marlet, R., Perez, P., and Aubry, M. A spherical analysis of adam with batch normalization. arXiv preprint arXiv:2006.13382, 2020. URL https://arxiv.or g/abs/2006.13382. 14 Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV) , 115(3):211–252, 2015. doi: 10.1007/s11263-015-0816-y. arXiv:1409.0575. 33 Salimans, T. and Kingma, D. P. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. Advances in neural information process- ing systems, 29, 2016. arXiv:1602.07868. 13, 14 Shallue, C. J., Lee, J., Antognini, J., Sohl-Dickstein, J., Frostig, R., and Dahl, G. E. Measuring the effects of data parallelism on neural network training. Jour- nal of Machine Learning Research, 20(112):1–49, 2019. arXiv:1811.03600. 23 11Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks Simonyan, K. and Zisserman, A. Very deep convolu- tional networks for large-scale image recognition. In 3rd International Conference on Learning Representa- tions, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings , 2015. URL http: //arxiv.org/abs/1409.1556. 27 Touvron, H., Cord, M., Douze, M., Massa, F., Sablay- rolles, A., and Jegou, H. Training data-efficient im- age transformers & distillation through attention. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning , volume 139 of Proceedings of Machine Learning Re- search, pp. 10347–10357. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/t ouvron21a.html. arXiv:2012.12877. 33 Van Laarhoven, T. L2 regularization versus batch and weight normalization. arXiv preprint arXiv:1706.05350, 2017. URL https://arxiv.org/abs/1706.05350 . 1, 2, 13 Wan, R., Zhu, Z., Zhang, X., and Sun, J. Spherical mo- tion dynamics: Learning dynamics of normalized neural network using sgd and weight decay. In Ranzato, M., Beygelzimer, A., Dauphin, Y ., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Process- ing Systems, volume 34, pp. 6380–6391. Curran Asso- ciates, Inc., 2021. URL https://proceeding s.neurips.cc/paper/2021/file/326a8c0 55c0d04f5b06544665d8bb3ea-Paper.pdf . arXiv:2006.08419. 1, 2, 13, 16, 17 Wightman, R. Pytorch image models. https://gith ub.com/rwightman/pytorch-image-models , 2019. 33, 34 Wu, Y . and He, K. Group normalization. InProceedings of the European conference on computer vision (ECCV), pp. 3–19, 2018. arXiv:1803.08494. 9, 34 Xie, Z., zhiqiang xu, Zhang, J., Sato, I., and Sugiyama, M. On the overlooked pitfalls of weight decay and how to mitigate them: A gradient-norm perspective. In Thirty- seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net /forum?id=vnGcubtzR1. arXiv:2011.11152. 13 You, Y ., Gitman, I., and Ginsburg, B. Large batch training of convolutional networks. arXiv preprint arXiv:1708.03888, 2017. URL https://arxiv.or g/abs/1708.03888. 2, 5, 14 You, Y ., Li, J., Reddi, S., Hseu, J., Kumar, S., Bhojanapalli, S., Song, X., Demmel, J., Keutzer, K., and Hsieh, C.-J. Large batch optimization for deep learning: Training bert in 76 minutes. In International Conference on Learning Representations, 2020. URL https://openreview .net/forum?id=Syx4wnEtvH. arXiv:1904.00962. 5, 14 Zhang, G., Wang, C., Xu, B., and Grosse, R. Three mech- anisms of weight decay regularization. In International Conference on Learning Representations , 2019. URL https://openreview.net/forum?id=B1lz -3Rct7. arXiv:1810.12281. 1, 13 Zhou, Y ., Sun, Y ., and Zhong, Z. Fixnorm: Dissecting weight decay for training deep neural networks. arXiv preprint arXiv:2103.15345, 2021. URL https://ar xiv.org/abs/2103.15345. 13 12Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks A. Expanded Related Work In this section we discuss more related works divided into five main categories. A.1. Scale-Invariance and Effective Learning Rates Several works have investigated how the scale-invariance results in a certain “effective learning rate” based on the relative change that varies based on the norm of the weights, often in the form of η/∥ω∥2. The works in this section do not describe how ∥ω∥ can converge to an equilibrium value that results in a fixed relative or rotational learning rate. In Weight Normalization, Salimans & Kingma (2016) point out how normalization can make parameters scale-invariant and that the gradient magnitude varies based on the weight magnitude. They describe how the gradient can “self-stabilize its norm”, with larger gradients becoming smaller over time due to growth in the weight magnitude, but do not consider the effects of weight decay on this process. Zhang et al. (2019) and Hoffer et al. (2018) empirically find that the regularization effects of weight decay are primarily caused by increases in the effective learning rate due to decreased weight norms. Li & Arora (2020) show that weight decay can be replaced by an exponentially increasing learning rate when optimizing scale-invariant weights with SGDM. A.2. Equilibrium The works in this section also consider the fact that the weight norm converges to a specific value and they explore the resulting effects on the relative update size. Van Laarhoven (2017) points out the scale-invariance property of normalization and how it interacts with ℓ2-regularization. They derive the η/∥ω∥2 as the effective learning rate and show there exists a fixed point where the weight norms are stable. Their work does not consider convergence of the weight magnitude as a separate process from the overall convergence of the loss and weights. In Online Normalization, Chiley et al. (2019) show a simple derivation of the equilibrium condition in SGD and how it results in a relative update size that is identical across layers. The Spherical Motion Dynamics (SMD) (Wan et al., 2021) expands on prior work by deriving the convergence of the weight norm and extending the analysis to include momentum. They also show plots of the weight norm over the course of training, providing empirical evidence for early convergence of the weight norm and note how it can fall out of equilibrium with sudden learning rate changes or when the learning rate becomes too small. They also consider the angular updates, empirically and analytically showing that they converge to an equilibrium value. Li et al. (2020) analyze the convergence of SGD to equilibrium by modelling it as a stochastic differential equation, arriving at similar conclusion as the SMD paper (without momentum). This is expanded upon by Li et al. (2022b). A.3. Understanding and Improving Weight Decay In traditional literature (e.g. Goodfellow et al. (2016)), ℓ2-regularization is introduced as a simple and commonly used regularization method achieved by adding 1 2 λ∥ω∥2 to the objective function. This strategy is commonly referred to as weight decay, but this is confusing since it is technically different from directly decaying the weights as pointed out by Loshchilov & Hutter (2019). As previously mentioned, Van Laarhoven (2017) showed that this interpretation does not hold for modern networks with normalization layers. Normalization can make a weight vector scale-invariant, meaning that the network output is unaffected by the magnitude of the vector (see §2). In this work, we focus on building upon the insight that weight decay serves as a scaling factor for an “effective” learning rate, as suggested by previous research (Van Laarhoven, 2017; Zhang et al., 2019; Li & Arora, 2020; Wan et al., 2021). However, other lines of work have explored different facets of understanding and improving weight decay. Xie et al. (2023) have aimed at mitigating pitfalls of weight decay and improving optimization performance. They propose a weight decay schedule that ensures better convergence towards the end of training. Lastly, Andriushchenko et al. (2023) investigated weight decay’s role in regularization. They specifically, examined how weight decay influences the implicit regularization of SGD noise and the bias-variance trade-off. A.4. Projected Optimization Some existing works remove weight decay and rely on projections of either the updates or weights instead. AdamP (Heo et al., 2021) orthogonalizes the update of the Adam and SGDM optimizers by removing the radial component of ∆gω. The main reason for this is to avoid a rapid increase in the weight norm during the initial phases of training. Zhou et al. (2021) propose keeping the weight magnitude constant, projecting it onto a sphere after every step and removing the weight decay. Kodryan et al. (2022) analyze training using projections onto the unit sphere after every optimizer update. Both of these works consider the union of all scale-invariant weights. Fixing the norm of this total parameter vector has a similar effect as 13Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks weight decay and will balance the effective learning rates for each scale-invariant group over time (but does not eliminate the transient phase). This differs from Huang et al. (2017a) which projects the weights of each neuron individually, which does generally not result in balanced rotation when used with SGD as in their case. Performing a similar operation with Adam or Lion could result in balanced rotation when the weights of different neurons have identical RMS values. Roburin et al. (2020) analyzes the spherical projection of the Adam optimization trajectory during standard training. A.5. Relative Optimization LARS (You et al., 2017) and LAMB (You et al., 2020) are variants of SGDM and AdamW that scale the update of a weight to be proportional to its norm (sometimes a clamped version of the weight norm). They apply this to linear and convolutional layer weights, keeping the original update for weights and biases. LARS and LAMB were proposed for large batch size training and found to work well there. Although they are not inspired by the Spherical Motion Dynamics, their form is quite similar to the Rotational Optimizer Variants (algo. 1) with a few important differences. The default form of the RVs is applied filter-wise, centers the weights and allows the update magnitude to vary between steps while keeping the average relative update constant. The RV also doesn’t apply weight decay while LARS and LAMB consider it a part of the update and take into account when scaling the relative update. Finally, the RVs adjust the learning rate based on the rotational equilibrium value. This makes it more compatible with the underlying optimizer variants in terms of hyperparameters. One notable difference is the square root dependency on the relative updates in equilibrium, while LARS and LAMB are directly proportional. This means that any learning rate schedule for these optimizers is more similar to applying a squared version of this schedule to standard optimizers in equilibrium or the RVs. This does not fully eliminate the differences however, because changing the schedule also affects gains and biases where the update magnitude is directly proportional to the learning rate for all the optimizers and variants discussed here. Nero (Liu et al., 2021) is another optimizer that applies relative updates that are directly proportional to the learning rate and weight magnitude. Like LARS and LAMB, Nero is not inspired by the neuronal update dynamics of standard optimizers with weight decay, and to the best of our knowledge their relationship has not been pointed out before. Like the RVs, Nero is applied filter-wise and centers the weights. Overall, Nero is similar to the SGDM RV without momentum and the hyperparameter mapping, but also applies Adam like updates to the gains and biases, using a separate learning rate. By making the relative updates directly proportional to the learning rate, it has the same learning rate scheduling differences as LARS and LAMB mentioned above. Nero lacks momentum which is something that we observed can hurt large batch size training (exploratory experiments not shown). Instead of controlling the average relative update size, Brock et al. (2021b) and Li et al. (2022a) clip the maximum relative update size instead. The Adaptive Gradient Clipping from Brock et al. (2021b) is applied on a per filter basis and is constant throughout training, i.e. does not vary with the learning rate or weight decay. The clipping introduced in Li et al. (2022a) scales with the learning rate and weight decay in a manner inspired by the equilibrium norm for SGD. They seem to apply this globally (i.e., not per neuron or layer). In parallel with this work, Karras et al. (2023) studied diffusion model training and proposed combining neuron-wise Weight Normalization (Salimans & Kingma, 2016), projections and the Adam algorithm (Kingma & Ba, 2015) for optimization. They also forgo weight decay, ending up with a training approach that is very similar to our RV-AdamW, showing this significantly improves the optimization of their networks. The main differences are that they do not use a separate parameter such as the weight decay coefficient to set the relative update size of non-weight parameters like gains and biases (which they remove entirely), do not center the weights, and in their case the rotation is proportional to the learning rate. B. Normalization and Scale-Invariance This section provides an overview of normalization and scale-invariance. Setup: We use Batch Normalization (Ioffe & Szegedy, 2015) as an example of a normalization operation. Let x = Zω for x ∈ RB×1, ω ∈ RC×1 and Z ∈ RB×C correspond to a single output feature of a linear layer (i.e. a neuron). We can write the batch normalization of this feature as: ˆx = N(x) = x − µ√ σ2 + ε , µ = 1 B BX i=1 xi, σ 2 = 1 B BX i=1 (xi − µ)2 (22) where x = [x1, . . . , xB]⊤ ∈ RB is a vector and ε ≥ 0 is a small hyperparameter added for numerical stability. Backpropa- 14Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks gation accurately treats µ and σ as functions of x. When ε is sufficiently small to be ignored, the output of the normalization is not affected by a positive scaling of the input: N(rx) = (rx − rµ)/ √ r2σ2 + ε ≈ (x − µ)/ √ σ2 + ε = N(x), r > 0 (23) If the training loss L does not depend on x in other ways than through N(x), this makes x scale-invariant with respect to the loss, i.e. L(rω) = L(ω) for r >0. Note that although we sometimes write L(ω) for brevity the loss generally depends on other weights and inputs as well, ω is generally only a portion of the parameters used in the network, and could for example be a particular row in the weight matrix of a fully connected layer. Some normalization operations like Centered Weight Normalization (Huang et al., 2017b) a.k.a. Weight Standardization (Qiao et al., 2019) are performed directly on the weights instead of activations. This also makes the weight scale-invariant and in case of the aforementioned methods also makes ∇ωL(ω) ⊥ 1. Properties: Scale-invariance results in the properties stated in eq. (5) and eq. (6), repeated below: Gradient orthogonality: ∇ωL(ω) ⊥ ω (24) Inverse proportionality: ∥∇ωL(ω)∥ ∝ ∥ω∥−1 (25) Intuition: The first property is a result of the loss surface being invariant along the direction ofω. Hence the directional derivative of L(ω) in the direction of ω is zero: ⟨∇ωL(ω), ω ∥ω∥⟩ = lim h→0 L(ω + hω/∥ω∥) − L(ω) h (26) = lim h→0 L(ω) − L(ω) h (27) = lim h→0 0 h = 0 (28) The second property is a result of the backpropagation through N, which scales the gradient by the factor used on the forward pass 1/ √ σ2 + ε ≈ σ−1 as if it were a constant, and the fact that σ ∝ ∥ω∥. Backpropagation: The properties can also be shown using expressions for the backpropagation through the normalization layers. For completeness we include the learnable affine transformation that typically follows normalization operations: y = γ ˆx + β (29) For the backpropagation we have: ∇γL(p) = ⟨ˆx, ∇yL(p)⟩ (30) ∇βL(p) = ⟨1B, ∇yL(p)⟩ (31) ∇xL(p) = γ√ σ2+ε · \" ∇yL(p) − 1 B ⟨1B, ∇yL(p)⟩1B − 1 B ⟨ˆx, ∇yL(p)⟩ˆx # (32) Assuming that ε is small gives: ∇xL(p) = γ σ \" ∇yL(p) − 1 B ⟨1B, ∇yL(p)⟩1B − 1 B ⟨ˆx, ∇yL(p)⟩ˆx # (33) In this case we have: ⟨∇xL(p), 1B⟩ = γ σ \" ⟨1B, ∇yL(p)⟩ −1 B ⟨1B, ∇yL(p)⟩⟨1B, 1B⟩| {z } =B − 1 B ⟨ˆx, ∇yL(p)⟩⟨ ˆx, 1B⟩| {z } =0 # = 0 (34) 15Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks and similarly: ⟨∇xL(p), ˆx⟩ = γ σ \" ⟨ˆx, ∇yL(p)⟩ −1 B ⟨1B, ∇yL(p)⟩⟨1B, ˆx⟩| {z } =0 − 1 B ⟨ˆx, ∇yL(p)⟩⟨ ˆx, ˆx⟩| {z } =B # = 0 (35) which gives: ⟨∇xL(p), x⟩ = ⟨∇xL(p), σˆx + µ1B⟩ = 0 (36) This allows us to obtain the properties of the weight gradient: ∇pL(p) = Z⊤∇xL(p) (37) First we note that: ∥∇pL(p)∥ ∝ ∥∇xL(p)∥ ∝σ−1 ∝ ∥p∥−1 (38) where the second proportionality follows from eq. (33) and the final one from eq. (22). This gives the inverse proportionality listed in eq. (25). We can also derive the gradient orthogonality in eq. (24) as follows: ⟨∇pL(p), p⟩ = ⟨Z⊤∇xL(p), p⟩ (39) = p⊤Z⊤∇xL(p) (40) = x⊤∇xL(p) (41) = ⟨∇xL(p), x⟩ (42) = 0 (43) These properties can also be shown directly from the scale-invariance using calculus theorems as done in Wan et al. (2021). C. SGDM Equilibrium The standard version of SGD with momentum (SGDM) can be written as: mt = αmt−1 + gt + λpt−1 (44) pt = pt−1 − η · mt (45) where pt ∈ RC is a parameter vector at time t, gt = ∇pL(pt) is the gradient and m is the first moment (i.e. momentum or velocity). The learning rate (η ≥ 0), weight decay (λ ≥ 0), and momentum coefficient (0 < α <1) are hyperparameters. We compute the total update contribution due to gt, i.e. u in eq. (8) as: u = −η P∞ k=t αk−tgt = −η 1−α gt (46) Analogously, the total update contribution of the ℓ2-regularization of wt−1, i.e. d in eq. (8), is: d = −η P∞ k=t αk−tλpt−1 = −ηλ 1−α pt−1 (47) Combining eq. (46) and eq. (47), this allows us to solve eq. (8) for a scale-invariant weight vector ω. Here we assume scale-invariance since it slightly changes the resulting expression due to the dependency of ∥u∥ on ∥ω∥. It also simplifies the math a bit, with u ⊥ ω, not just in expectation. We get: d∥ω∥2 = E \u0002 ( d∥ω∥ − ∥d∥)2 + ∥u∥2\u0003 (48) = E \u0014\u0010d∥ω∥ −ηλ 1−α d∥ω∥ \u00112 + \u0010 η 1−α ∥˜g∥ d∥ω∥ \u00112\u0015 (49) 16Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks Where we define ˜gt = gt∥ωt∥ using ∥gt∥ ∝ ∥ωt∥−1 due to the inverse proportionality of the gradient magnitude, see eq. (6) or eq. (25). We can interpret ˜gt as the gradient for weights of unit norm ∥ωt∥ = 1. Solving for d∥ω∥ and assuming that ηλ ≪ 2 · (1 − α) gives: d∥ω∥ = 4 s ηE[∥˜g∥2] 2λ · (1 − α) − ηλ2 ≈ 4 s ηE[∥˜g∥2] 2λ · (1 − α) (50) To obtain the absolute size of an update, we further assume that E[∥gt∥2] can be approximated as a constant E[∥g∥2] when computing the size of mt, and that successive gradients are roughly orthogonal giving mt−1 ⊥ gt in expectation. For the random walk setting, the first is reasonable when the norm is stable e.g. around equilibrium and the second always holds. The average square size of an update is then: E[∥∆gp∥2] = η2E h ∥αmt−1 + gt∥2 i (51) = η2E h ∥αmt−1∥2 i + E h ∥gt∥2 i (52) = η2 Pt k=0 E[ \u0000 αt−k∥gk∥ \u00012 ] (53) ≈ η2 E[∥g∥2] 1−α2 (54) where (52) comes from the orthogonality, (53) by recursively writing out m in terms of g, and (54) from assuming that t is high enough to approximate the sum of the geometric series as an infinite sum. Simplifying, we get the ηg = p E[∥∆gp∥2] and ηr = p E[∥∆gω∥2]/ d∥ω∥ in table 1. We note that the derived rates are consistent with the ones derived in the Spherical Motion Dynamics (Wan et al., 2021) and Online Normalization (Chiley et al., 2019) (when α = 0). D. Lion Equilibrium The standard version of Lion (Chen et al., 2023) can be written as: vt = sign(β1mt−1 + (1 − β1)gt) (55) mt = β2mt−1 + (1 − β2)gt (56) pt = pt−1 − η · (vt + λpt−1) (57) where pt ∈ RC is a parameter vector at time t, gt = ∇pL(pt) is the gradient, m is the first moment and v is the update velocity. The learning rate ( η ≥ 0), weight decay ( λ ≥ 0), and moment coefficients ( 0 < β1 < 1, 0 < β2 < 1) are hyperparameters. In our analysis we look at the arguments of the sign function which we define as: nt := β1mt−1 + (1 − β1)gt, vt = sign(nt) (58) To obtain an estimate of the magnitude ∥nt∥, we assume that the gradient magnitude E[∥gt∥2] can be approximated as a constant E[∥g∥2], and that successive gradients are roughly orthogonal giving mt−1 ⊥ gt in expectation. For the random walk setting, the first is reasonable when the norm is stable e.g. around equilibrium and the second always holds. This gives: E[∥nt∥2] = E \u0002 ∥β1mt−1 + (1 − β1)gt∥2\u0003 (59) = β2 1E \u0002 ∥β2mt−1 + (1 − β2)gt−1∥2\u0003 + (1 − β1)2E \u0002 ∥gt∥2\u0003 (60) = β2 1(1 − β2)2 k=t−1X k=0 βt−1−k 2 E[∥gk∥2] + (1− β1)2E \u0002 ∥gt∥2\u0003 (61) ≈ β2 1(1 − β2)2 ∞X k=0 βk 2 E[∥g∥2] + (1− β1)2E \u0002 ∥g∥2\u0003 (62) 17Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks = \u0012 (1 − β1)2 + β2 1 1 − β2 1 + β2 \u0013 E[∥g∥2] (63) where we have used the gradient orthogonality and constant magnitude, and approximated the geometric sum as extending to infinity. To compute the total update contribution of the gradient, ∥u∥ in eq. (8), we first need to model how the sign non-linearity affects the magnitude and direction of the update. We note that for a parameter of dimension C, we have: ∥vt∥ = √ C (64) so the sign function has an average scaling effect: ∥vt∥ ∥nt∥ = vuut C\u0010 (1 − β1)2 + β2 1 1−β2 1+β2 \u0011 E[∥g∥2] (65) The sign function will also rotate nt resulting in two components, one parallel to nt and the other orthogonal. We will assume that the orthogonal one cancels out on average without significantly affecting equilibrium and focus on the parallel component. This component depends on the average angle between nt and sign(nt) which is determined by the distribution and correlation between the elements. In the random walk setting, we can assume the components of nt = [n1, . . . , nC] are normally distributed with mean zero. However, the expression for the average angle is still complicated unless the components are independent and identically distributed (i.i.d.) so we make this assumption for this step with nk ∼ N(0, σ2) i.i.d. for all k. Then we can use the known expected absolute value for a centered normal distribution to get: E[⟨nt, sign(nt)⟩] = C · E[|nk|] = C · r 2σ2 π = r 2 π · ∥nt∥ · ∥sign(nt)∥ (66) Note that the angle is still bounded regardless of the distribution but will result in a different factor in the range that ∥n∥1/( √ C∥n∥2) can take, i.e. [C−1 2 , 1] instead of p 2/π. Based on the preceding analysis we will model the sign function for the computation of ∥u∥ as: sign(nt) ≈ r 2 π ∥vt∥ ∥nt∥nt = vuut 2C E[∥g∥2] · π · \u0010 (1 − β1)2 + β2 1 1−β2 1+β2 \u0011nt (67) which gives: E[∥u∥2] = 2ηC E[∥g∥2] · π · \u0010 (1 − β1)2 + β2 1 1−β2 1+β2 \u0011E   \r\r\r\r\r(1 − β1)g + β1(1 − β2) ∞X k=0 βk 2 g \r\r\r\r\r 2  (68) = 2ηC π · \u0010 (1 − β1)2 + β2 1 1−β2 1+β2 \u0011 (69) Combined with the total update contribution for the weight decay,d = −ηλωt−1, this allows us to write eq. (8) forω ∈ RC: d∥ω∥2 = E \u0002 ( d∥ω∥ − ∥d∥)2 + ∥u∥2\u0003 (70) = ( d∥ω∥ −ηλ d∥ω∥)2 + E[∥u∥2] (71) = (1 − 2ηλ + η2λ2) d∥ω∥2 + 2ηC π \u0012 (1 − β1)2 + β2 1 1 − β2 1 + β2 \u0013−1 (72) Solving for d∥ω∥ and assuming ηλ ≪ 2 gives: d∥ω∥ = q 2ηC π·(2λ−ηλ2) \u0010 (1 − β1)2 + β2 1 1−β2 1+β2 \u0011−1 2 ≈ q ηC πλ \u0010 (1 − β1)2 + β2 1 1−β2 1+β2 \u0011−1 2 (73) 18Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks ϕ d∥ω∥ d∥ω∥ E[∥u∥] E[∥d∥] Figure 8: Weight norm equilibrium. The total update contribution of the gradient isu and the TUC of the weight decay is d. Combined with ∥vt∥ = √ C for ω, p ∈ RC we get the expected equilibrium rotation and RMS update size: bηg = q E[∥∆gp∥2] = η √ C (74) bηr = q E[∥∆gω∥2]/ d∥ω∥ = p πηλ · \u0012 (1 − β1)2 + β2 1 1 − β2 1 + β2 \u00131 2 (75) E. Adam+ℓ2 Equilibrium In this section we apply a modified form of the geometric model from §3.1 to Adam (Kingma & Ba, 2015) with ℓ2- regularization (Adam+ℓ2 for short) to gain insight into how the rotational equilibrium differs from that of Adam with decoupled weight decay (AdamW, see §3.2). E.1. Adam+ℓ2 Formulation We will write the Adam+ℓ2 update as follows: mt = β1mt−1 + (1 − β1)(gt + λpt−1) (76) vt = β2vt−1 + (1 − β2)(gt + λpt−1)2 (77) pt = pt−1 − η · \u0010 mt/(1−βt 1)√ vt/(1−βt 2)+ε \u0011 (78) Similar to AdamW,pt ∈ RC is a parameter vector at time t, gt = ∇pL(pt) is the gradient, and all operations (e.g. division, squaring) are performed elementwise. In Adam+ℓ2, both the first and second moment of the gradient (m and v) include contributions from the ℓ2-regularization term. This differs from AdamW (see eq. (11)) where the ℓ2-regularization (or technically weight decay) does not affect m and v. The learning rate (η ≥ 0), ℓ2-regularization coefficient (λ ≥ 0), moment coefficients (0 < β1 < 1, 0 < β2 < 1) and ε ≥ 0 are hyperparameters similar to AdamW. Like before, we use ω to specifically denote the weight vector of a neuron or a layer that can form rotational equilibrium (as opposed to p that we use as a general symbol for any parameter vector, and could denote e.g. a bias). E.2. Simplifications The rotational dynamics of Adam+ℓ2 are more complicated than those of AdamW. The main difference is that the strength of the “weight decay” is affected by the gradient norm. As we will see, this makes the equilibrium norm and angular update depend on the gradient magnitude. Furthermore, the weight decay can be scaled differently for each coordinate of the weight vector as the gradient distribution may vary between them. This complicates the analysis, forcing us to treat each coordinate separately. Our analysis is based on the random walk setup introduced in §3 and described in appx. G. We further make several assumptions and simplifying approximations that allow us to obtain simpler expressions for the cases of interest: 1. We assume the rotational equilibrium exists as a steady state where hyperparameters are fixed (not varying over time), the expected weight norm p E[∥ωt∥2] = d∥ω∥is constant, and the second moment of the gradient E[g2 t ] is constant over time. For simplicity we will drop the t subscript. 19Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks 2. We focus on the case where the weights are scale-invariant, defining ˜g = ∥ω∥g as the gradient corresponding to a unit weight norm based on the inverse proportionality from eq. (6). 3. We will assume that ε and the bias correction can be ignored, i.e. that ε, βt 1 and βt 2 are all effectively zero. 4. We will assume the second moment tracker vt is dominated by the gradient component, i.e. that g2 ≫ λ2ω2, and that it perfectly tracks the expected value, i.e. that E[v] = E[g2]. This is a non-trivial approximation based on the geometry of equilibrium when the angular updates are small. For a small ϕ in fig. 8 we can can approximate: E[∥u∥] = d∥ω∥ ·tan(ϕ) ≈ d∥ω∥ ·ϕ (79) E[∥d∥] = d∥ω∥ ·(1 − cos(ϕ)) ≈ d∥ω∥ ·ϕ2 2 (80) As a result E[∥u∥] ≫ E[∥d∥]. For Adam+ ℓ2 we have u = −ηgt/√v and d = −ηλωt−1/√v. As long as v is relatively homogeneous across coordinates, we therefore have E[∥g∥] ≫ E[λ∥ω∥]. We assume this holds roughly coordinate wise as well, givingg2 ≫ λ2ω2. We note that this fourth assumption is not strictly necessary but significantly simplifies the resulting expressions, giving us an interpretable closed form solution instead a solution expressed as the root of a third-degree polynomial. E.3. Equilibrium Norm The total update contribution of the gradient gt, i.e. u in eq. (8), is given by: u = −η P∞ k=t βk−t 1 (1 − β1) gt√v = −η gt√v (81) Similarly, the total update contribution due to the weight decay of ωt−1 is: d = −η ∞X k=t βk−t 1 (1 − β1)λωt−1√v = −η λωt−1√v (82) Due to the coordinate-wise differences in the weight decay, we analyze a single element ωk at coordinate k in ω with corresponding elements dk, uk, gk, vk in d, u, g, v, respectively. Although the geometric model is not well defined coordinate-wise, we can still use the concept of orthogonality as defined for random variables. This gives us: E[ω2 k] = E[(ωk − dk)2 + u2 k] (83) = (1 − ηλ√vk )2E[ω2 k] + η2 E[g2 k] vk (84) = (1 − 2ηλ√vk + η2λ2 vk )E[ω2 k] + η2(1 − λ2E[ω2 k] vk ) (85) = (1 − 2ηλ√vk )E[ω2 k] + η2 (86) where we have used the fact that vk = E[g2 k] + λ2E[ω2 k]. Since we are targeting the scale-invariant case (Assumption 2) we can write: E[g2 k] = E[˜g2 k]E[∥ω∥2]−1 (87) where ˜gk corresponds to an element of the unit norm weight gradient ˜g. Accordingly we can write: vk = E[˜g2 k]E[∥ω∥2]−1 + λ2E[ω2 k] ≈ E[˜g2 k]E[∥ω∥2]−1 (88) where we used Assumption 4. Plugging this form of v into eq. (86), squaring and simplifying gives: E[ω2 k] = η 2λ s E[˜g2 k] E[∥ω∥2] (89) 20Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks We can now write an expression for the equilibrium norm: d∥ω∥ 2 = E[∥ω∥2] = CX k=1 E[ω2 k] = η 2λ CX k=1 s E[˜g2 k] E[∥ω∥2] (90) which gives: d∥ω∥ = 3 r η 2λ ·⟨1, p E[˜g2]⟩ (91) where ⟨·, ·⟩ denotes an inner product. Note that when the elements of g have the same second moment, e.g. when they are identically distributed, we can write ⟨1, p E[˜g2]⟩ = √ C p E[∥˜g∥2]. Also note how this behavior differs from that of AdamW, here the equilibrium norm depends on the gradient magnitude. Finally we note that without scale-invariance we would get a square root instead of a cube root. E.4. Equilibrium Angular Update To obtain the absolute size of an update for ω in equilibrium, we use the fact that in the random walk successive gradients are orthogonal in expectation. Similar to AdamW, we can then write the average square size of an update as: E[∥∆gω∥2] = η2(1 − β1)2 Pt k=0 β2t−2k 1 E h\r\rgk v \r\r2i (92) ≈ η2 1−β1 1+β1 C (93) where we approximated the geometric sum with its limit and used v ≈ E[g2] based on Assumption 4. Note that the use of Assumption 4 gives the same result as for AdamW. We can then approximate the expected angular update in equilibrium as: bηr = p E[∥∆gω∥2] d∥ω∥ = 3 s 2η2λ ⟨1, p E[˜g2]⟩ s 1 − β1 1 + β1 C (94) Note that the average angular update depends on the gradient magnitude unlike for other optimizers. Also note the different dependency on η and λ, here the angular update depends on the product η2λ, not ηλ like for other optimizers. This pattern is visible in the hyperparameter heatmap seen in fig. 6, the performance varies faster along the direction of increasing bηr than where it is constant. Finally there is an odd dependency on C that is not present in the other optimizers. Without scale-invariance, the first cube root would be replaced by a square root and the gradient dependency on C would cancel the C in the second root. F. Random Walk Experiments F.1. Measurements of a Random Walk We can directly perform a random walk to validate the expressions given in table 1. Figure 9 shows measurements for a random walk in a simple system described below for each of the metrics given in the table. We show four neurons (colored solid lines), the average over a layer (black) and the predicted equilibrium value (red dashed line) from table 1. As we can see the analytically derived expressions accurately describe the neuronal dynamics of the random walk for each optimizer. We use reasonable hyperparameters values in each case that we have found to work well for either ResNet-18 or ResNet-20 on CIFAR-10 (see details in the next section). F.2. Simple System for Random Walks Definition: We define the simple system as: f(X) = γout ⊙ N \u0000 W(γin ⊙ X) \u0001 (95) 21Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks 2 5 10 20 50 RMS Update [×10 3] SGDM AdamW Adam+ 2  Lion 2 5 10 20 50 Angular Update [×10 3] 0 5 10 15 Step [×103] 0.2 0.5 1 2 5 Weight Norm 0 5 10 15 Step [×103] 0 5 10 15 Step [×103] 0 5 10 15 Step [×103] Figure 9: Measurements of the neuronal update dynamics in a random walk, comparing them with the our analytical equilibrium predictions. Colors (pink, orange, green, blue) correspond to individual neurons with varying gradient norms from a single layer. Black lines represent averages over the whole layer, and red dashed lines show equilibrium predictions for the layer average from table 1. The predictions accurately describe the steady state value in each case. where X ∈ RC×B, W ∈ RK×C, γin ∈ RC×1, γout ∈ RK×1 and N is a batch normalization function (see eq. (22)) applied to each feature independently. The only learnable parameters are the weights W, the gammas γin and γout are kept constant. We initialize the weights using the default initialization for a linear layer in PyTorch (Paszke et al., 2019) i.e. each element is sampled independently and uniformly from the interval [− 1√ C , 1√ C ]. The gammas are initialized with elements independent and identically distributed (i.i.d.) following a standard normal distribution. The inputs are also sampled i.i.d. from a standard normal distribution at each iteration. The gradients of f(X), which are used to compute other gradients via the chain-rule or backpropagation, are sampled i.i.d. from a normal distribution with standard deviation 1 KB where the B simulates the typical averaging of the loss over a batch and the K gives a scale more similar to the derivatives of softmax-cross-entropy (the difference of two vectors with an L1-norm of 1 each). We can also scale the output gradients (that get backpropagated to compute the parameter gradients) further with a loss scale to obtain different gradient norms (especially important for Adam). Rationale: We use this system to study a random walk in a neural network as described in §3, which serves as a simplified model of a real optimization problem. The gammas give different variances for each input and output channel, causing the second gradient moment in Adam/AdamW to vary between elements of W like they may in real neural network training due to the rest of the network. The normalization ensures that the system is scale-invariant for each row of W. The randomly sampled inputs and output gradients ensure that everything is orthogonal in expectation. Compared to a real neural network training, the dynamics of this system are simplified with no loss converging over time and steady input / gradient distributions. Other complicated effects such as dead ReLUs do also not happen in this system. This makes this simple system a good setting to study the equilibrium dynamics in a controlled manner. 22Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks Details of Figure 9: Here we use B = 32, C= K = 128. We use the CIFAR-10 ResNet-20 hyperparameters from table 4 for SGDM and Lion (batch size 128), and the optimal configuration from the learning rate, weight decay sweep on CIFAR-10 ResNet-18 for AdamW and Adam+ℓ2 (fig. 6L, see details for that experiment in appx. K). The learning rate is constant and the experiments run for 15k steps, with the plots downsampled by a factor of 100x using RMS averaging. G. Differences between Real Networks and a Random Walk The random walk model we use assumes the gradient is dominated by the noise. As discussed by McCandlish et al. (2018), the mini-batch gradient noise is inversely proportional to the batch size. At the so called critical batch size, the noise term has a magnitude roughly equal to the true gradient. For sufficiently small batch sizes relative to the critical batch size, which is often large in practice (McCandlish et al., 2018; Shallue et al., 2019), the noise term thus dominates the mini-batch gradient. In this case, the optimization trajectory will locally look like a random walk, but its properties can change over time. Our analysis focuses on the simplified setting of a fully random walk, disregarding the influence of the true gradient on its dynamic behavior. This simplification allows us to make various assumptions that would not strictly hold for real neural network optimizations. Consequently, for real network optimization, we make several approximations that can affect the accuracy of the predictions in table 1. Here we evaluate how well the main simplifications from the random walk analysis of the AdamW optimizer in §3 apply to real neural network training tasks. Specifically, we cover a standard and unnormalized CNN trained on CIFAR-10 (fig. 10 and fig. 11) and a Transformer model trained on Wikitext (fig. 12). Below we discuss the key quantities we approximate and how this affects the predicted equilibrium norm and rotation. Orthogonality Between the Weights and the Gradient ∠(ωt−1, gt): We use the orthogonality between the weights and the gradient as a measure of E[⟨u, ω⟩] = 0. If we measure a small positive or negative bias, the gradient contributes to the effective weight decay λe. When the gradient gt is aligned with the weight ωt−1, it increases the weight decay. Conversely, when the gradient gt is negatively aligned with the weight ωt−1 the weight decay decreases. As a consequence, we tend to overestimate (or underestimate) the measured weight norm ∥ω∥ and underestimate (or overestimate) the expected angular update ηr. By defining the error as a scaling factor of λ (represented as λerr = λe λ ), we observe the following impact on our prediction ηr ≈ s 2η(λ · λerr)1 − β1 1 + β1 = bηr · p λerr (96) ∥ω∥ ≈ s ηC 2(λ · λerr) = d∥ω∥ · 1√λerr (97) Orthogonality Between the Momentum and the Gradient∠(gt, mt−1): We use the orthogonality between the momentum and the gradient as an approximation of ∀j ̸= k : E[⟨gj, gk⟩] = 0. It gives us information about the orthogonality of gt and the previous update directions. This means we have additional negative (or positive) terms in eq. (20) and thus tend to overestimate (or underestimate) the approximated RMS update size ηg in eq. (21). Scaled Gradient Norm ∥ gt√vt+ε ∥: The scaled norm directly measures the assumption ∀t, k: E[∥gt/√vk∥] = √ C. If E[∥gt/√vk∥] is larger than √ C, we underestimate the weight norm. If it is smaller, we overestimate the weight norm. For an error Cerr in the estimate, defined as E[∥gt/√vk∥] = C · Cerr, we have: ∥ω∥ ≈ r η(C · Cerr) 2λ = d∥ω∥ · p Cerr (98) Radial Gradient Component λu/λ: The radial gradient component approximates the impact of E[u∥] relative to λ. It measures how much the scaled gradient component influences the effective weight decay λe = λ + λu. A large λu affects our predictions similarly to what is described in eq. (96) and eq. (97). 23Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks 0 10 20 30 40 50 60 70 80 2.5 0.0 2.5 ( t 1, gt) 1e 6 0 10 20 30 40 50 60 70 80 0.1 0.0 (gt, mt 1) 1 (B) 2 (B) baseline prediction 0 20 40 60 80 100 200 300 gt vt + 0 20 40 60 80 0.1 0.0 0.1 u/ 0 20 40 60 80 50 100 0 20 40 60 80 0.005 0.010 ( t, t + 1) Step [×103] Figure 10: Measurement of how closely the random walk model approximates the training dynamics of two convolutional filters in a ResNet-20 trained on CIFAR-10 using AdamW, with a constant learning rate ofη = 0.01 and a weight decay of λ = 0.01. This standard ResNet employs Batch Normalization after each convolutional layer, ensuring scale-invariant convolutional weights. As expected, E[⟨u, ω⟩] and λu = 0 are close to zero. Similarly, ∠(gt, mt−1) is nearly zero, indicating that the approximation ∀j ̸= k : E[⟨gj, gk⟩] = 0 holds well in practice. Additionally, the approximation ∀t, k: E[∥gt/√vk∥] = √ C appears to hold in this case. Finally, we observe that the predictions closely match the measurements after the initial transient phase in this setup. 24Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks 0 20 40 60 80 100 120 140 160 0.02 0.00 ( t 1, gt) 0 20 40 60 80 100 120 140 160 0.3 0.2 0.1 0.0 (gt, mt 1) 1 2 baseline prediction 0 25 50 75 100 125 150 100 150 200 250 gt vt + 0 25 50 75 100 125 150 2 1 0 1 2 u/ 0 25 50 75 100 125 150 20 40 60 0 25 50 75 100 125 150 0.002 0.004 0.006 ( t, t + 1) Step [×103] Figure 11: Measurement of how closely the random walk model approximates the training dynamics of two convolutional filters in an unnormalized ResNet-20 trained on CIFAR-10 using AdamW, with a constant learning rate ofη = 0.002 and a weight decay of λ = 0.01. We observe similar overall behavior as in fig. 10, but the gradients are not necessarily fully orthogonal to the weights as we would expect. There is a slight alignment between the gradients and weights, causing the effective weight decay to change. Nevertheless, we observe that the predictions closely match the measurements after the initial transient phase in this setup. 25Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks 0 2 4 6 8 10 12 14 0.005 0.000 ( t 1, gt) 0 2 4 6 8 10 12 14 0.2 0.1 0.0 (gt, mt 1) 1 (B) 2 (B) baseline prediction 0 2 5 7 10 12 15 1200 1400 gt vt + 0 2 5 7 10 12 15 1 0 u/ 0 2 5 7 10 12 15 10 20 30 0 2 5 7 10 12 15 0.0050 0.0075 0.0100 ( t, t + 1) Step [×103] Figure 12: Measurement of how closely the random walk model approximates the training dynamics for a GPT2-124M model trained on Wikitext using AdamW, with a constant learning rate ofη = 0.0005 and a weight decay of λ = 0.5. In this model, the weights are not fully scale-invariant. We observe a small but consistent negative alignment between the gradients and weights, resulting in a reduced weight decay effect. Additionally, there is a slight overestimation of the scaled gradient and a consistent negative alignment between the gradients and momentum. Despite these deviations, the random walk approximations yield reasonable predictions, as evidenced by the measurements in the last row. 26Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks H. Rotational Dynamics of Scale-Sensitive Parameters Most neural network architectures have some scale-sensitive parameters. This commonly includes gains and biases as well as a final fully connected (FC) layer that is typically not followed by normalization. In networks without normalization, with infrequent normalization, or poorly placed normalization, most weight vectors can be scale-sensitive. The original, un-normalized, VGG (Simonyan & Zisserman, 2015) architecture is a good example of this. VGG consists of a series of convolutional layers, with ReLUs and occasional pooling layers between them, and series of fully connected layers towards the end. In this section we use it to investigate the rotational dynamics of scale-sensitive weights. First we would like to note that the weight and gradient magnitude of scale-sensitive weights can also be largely arbitrary, similar to scale-invariant weights. Although they can’t be scaled directly without affecting the loss, we can often scale two of them without affecting the network output. Consider two successive layers with a ReLU between them: f(X, W1, W2, b1, b2) = ReLU(XW1 + b1)W2 + b2 (99) where W1, W2 ∈ RC×C are weight matrices, b1, b2 ∈ R1×C are vectors, X ∈ RB×C are inputs and we broadcast the operations. Note that the ReLU is positively homogeneous, so for a positive scalar r >0 we have: f(X, rW1, r−1W2, rb1, b2) = ReLU(XW1r + b1r)W2r−1 + b2 = f(X, W1, W2, b1, b2) (100) Assuming the weights are scaled in-place (i.e. we don’t modify the computation graph, only the weight values), this type of rescaling operation scales the relative update of W1 by r−2 and W2 by r2 when optimizing using SGD. This can significantly affect the learning dynamics as studied in e.g. Path-SGD (Neyshabur et al., 2015). For a scale-sensitive weight ω, the gradient orthogonality eq. (5) and inverse scaling eq. (6) do not necessarily hold. The inverse scaling holds in terms of rescaling operations like the ones mentioned above if they are applicable. Generally, the gradient has some radial component in the direction of the weight. The expected magnitude of this component depends on the average angle between the gradient and the weight as well as the expected gradient magnitude itself. If we separate the gradient into radial and perpendicular components and view the radial component as a modification of the weights decay, we have a very similar setup to the one we analyzed for scale-invariant weights. If a stable equilibrium exists, this could give rise to rotational dynamics which may vary from weight to weight based on the “effective weight decay” for each one. We explore this with VGG-13 training on CIFAR-10 using SGDM. We compare two versions, a standard unnormalized network and a variant where weight standardization is applied to every convolutional and fully connected layer. For each setup, we measure the angular updates, the weight norms, and the relative radial gradient magnitude: λu = E[⟨ω, ∇ωL⟩/∥ω∥2] = E[cos (∠(ω, ∇ωL)) · ∥∇ωL∥/∥ω∥] (101) Note that we have written this in the case of no momentum by using−η∇ωL instead of u, but for the standard implementa- tion of SGDM the momentum magnifies both this version ofλu and the standard “weight decay” (ℓ2-regularization) term the same way so they are comparable. The λu term can therefore be viewed as modifying the weight decay, the effective weight decay parameter is λe = λ + λu and accounts for the entire radial portion of a weight update. We replace the standard λ with λe when showing predicted values for the unnormalized network. The results can be seen in fig. 13 for two weights. For the first one on the left,λu is relatively small compared toλ = 5·10−4 and the weight behaves similarly in both setups, with “standard” update dynamics in the unnormalized setup. The equilibrium predictions match well early in training after the initial transient phase, but the weight falls out of equilibrium towards the end when it can’t decay fast enough to keep up with the equilibrium weight magnitude. For the second weight shown on the right, λu is large causing a significant difference between the scale-invariant and scale-sensitive setups. The modified equilibrium predictions using λe capture the behavior well in the middle phase of training, after the initial transition before the weight falls out of equilibrium towards the end. We note that in the unnormalized setup λu changes over the course of training, starting out around 0 corresponding to an orthogonal gradient and growing larger in the later phases. This is likely due to the cross-entropy loss used, which is minimized with large (infinite) output magnitudes once the network has learned to accurately classify (overfit) the training data. This encourages the network to increase the output magnitude to fully overfit the data, resulting in a decreased effective weight decay and slower rotation. Our results for VGG13 suggest that scale-sensitive weights can also have rotational dynamics in real neural networks. The dynamics are less regular than in the normalized setup, with weights rotating at different speeds depending on the size of the 27Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks 0 2 4 ( t, t + 1) ×10 3  features.layer_05.conv.weight features.layer_09.conv.weight 0.0 0.5 1.0 0 25 50 75 100 125 150 175 200 Epoch 4 2 0 u ×10 4 0 25 50 75 100 125 150 175 200 Epoch Figure 13: Measured (solid) and predicted equilibrium values (dashed) when training unnormalized (blue) and weight standardized (orange) variants of VGG-13 on CIFAR-10. The blue predictions account for the modified “effective” weight decay caused by the radial component of the gradient. radial gradient component. The weight magnitude can also not vary freely like for scale-invariant weights, where we can trade off the weight decay and learning rate without affecting the dynamics much (once equilibrium is achieved). Using large amounts of weight decay in unnormalized networks can bring the weight norms out of balance, resulting in issues like vanishing gradients or activations. In unnormalized networks the magnitude of one weight matrix also affects the gradient magnitude of all others layers, further complicating the effect of weight decay. Our rotational optimizer variants constrain the dynamics to match the equilibrium dynamics of weight standardized networks throughout training, eliminating some of these effects. I. Rotational Optimizer Wrapper In this section, we provide further details on the algorithmic design choices used in our rotation optimizer wrapper, as shown in algo. 1. Note that the method can act as a wrapper around any given existing optimizer F with a known bηr. In cases where the true value is unknown or undesirable, we can also specify some different value of our choice. Rotational and Non-Rotational Updates: We use Ω to specify weights we apply rotational updates to, so a parameter p is treated differently based on whether p ∈ Ω or not. By default we consider each neuronal weight vector to be a separate vector in Ω, but we could also apply the RVs at a coarser scale like whole layers (which is done in e.g. LARS). The rotational wrapper leaves the update of non-rotational parameters unchanged. Rotational parameters are rotated by bηr on average and their magnitude is kept constant. Some weights in Ω may be scale-sensitive meaning their magnitude can matter for efficient training. Since the RVs constrain the weights we optionally introduce a learnable gain to allow the network to learn the right magnitude for these weights. This gain can be absorbed into the weights for inference. Keeping the Weight Magnitude Constant: Alternatively, we could vary the weight magnitude according to our derived value for the equilibrium norm. However, with a learning rate schedule this value can become arbitrarily small causing numerical issues. For scale-invariant weights the magnitude doesn’t matter so we simply keep it constant. This has the added benefit of removing the inverse scaling effect of the weight norm on the gradient magnitude eq. (6), potentially making it a more informative metric over the course of training with a learning rate schedule. 28Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks Controlling the Rotation Instead of the Relative Update: The rotation of a scale-invariant weight ω is generally caused by both ∆gω and ∆λω as can be seen in fig. 2L and 2R. In equilibrium, the sum of these components is roughly orthogonal to the weight vector. We want to avoid having to apply the weight decay and our constrained magnitude is generally not equal to the equilibrium magnitude. We therefore project ∆gp to be orthogonal to p and control the average size of this projected version of ∆gp instead of the original p. This lets us explicitly control the angular update, regardless of any radial component in ∆gp that the weight decay would eliminate on average. If we apply rotational updates to scale-sensitive weights, performing line 15 after line 13 prevents any radial component in the gradient from affecting the rotational speed. Centering the Weights: Different normalization setups can result in slightly different SMD properties. Layer Normalization typically makes an entire weight matrix scale-invariant whereas Batch Normalization makes individual filters (i.e., rows or columns) independent. The default form of the rotational wrapper corresponds to the rotational equilibrium dynamics obtained with Weight Standardization (Qiao et al., 2019) also known as Centered Weight Normalization (Huang et al., 2017b), where each filter is scale and shift invariant. We remove the mean ¯p = 1 C PC i=1 pi of p = [p1, . . . , pC] since it is irrelevant in this setup. This removal was also found to be beneficial in NF-Nets (Brock et al., 2021a;b). Note how we remove the mean component of the update before updating the RMS tracker. This ensures that the average rotation is not decreased when there is a significant mean component in the update. Hyperparameters: The algorithm requires an ε value for numerical stability but otherwise only adds one hyperparameter, a decay factor β similar to those in Adam. It determines the rate at which we update our estimate of the average update magnitude (Line 15). This in turn controls how much we let the rotation vary between steps. We could potentially derive an analytical value for β based on the convergence speed towards equilibrium. For example β should perhaps be roughly equal to √a from eq. (16) for AdamW, when trying to match the dynamics exactly. However, this rate may not be optimal and generally depends on the learning rate (which may vary according to a learning rate schedule). We use a default of β = 0.9 which should keep the expected angular update close to the equilibrium value over time, while still allowing some variation from step to step. There is likely batch size dependence in the optimal value of β, with larger batches potentially benefiting from smaller values since balancing the average rotation within in each step could be sufficient in these cases. An Adam-like bias correction is applied to the average update magnitude when it is used (Line 17). Resource Requirements: We need to keep track of two scalarsνp and np for each rotational parameter. Since p is generally a vector, such as a row in a weight matrix, the memory requirement is negligible compared to quantities like momentum that store a scalar for every element of p. The computational requirements in terms of floating-point operations are also relatively small, linear in the number of network (scalar) parameters like standard optimizers. However, the rotational variants are not applied fully elementwise, making efficient (parallel) implementations slightly harder. J. Additional Experiments Dynamics for a Random Walk: In appx. F we measure the update dynamics in a simplified system undergoing a random walk matching our analytical setting. We observe that our analytical predictions from table 1 accurately describe the dynamics of this system for each of the optimizers we analyzed. Dynamics Without Weight Decay: In §5.1, we discussed that training without weight decay is similar to multiplying the learning rate schedule for ηr with an exponentially decaying function. This can be beneficial in some scenarios, e.g. Loshchilov & Hutter (2019) found that for a constant learning rate schedule a weight decay of zero can be optimal, unlike when a cosine decay schedule is used. The exponential decay in the angular updates without weight decay evident in fig. 14, where we present measurements of a ResNet-20 trained on CIFAR-10 with a learning rate of η = 0.05 and no weight decay (λ = 0), as well as the same setup with a weight decay of λ = 0.01 for both AdamW and RV-Adam. All experiments use a cosine learning rate schedule. Note the equilibrium norm for RV-Adam is constant throughout training by design. Dynamics of Scale-Sensitive GPT Model: In §5.1 and fig. 3 we show the dynamics of a modified GPT2 model where Weight Standardization (WS, Qiao et al. (2019); Huang et al. (2017b)) is applied to all linear layers. Although we don’t show the results here, we found the Weight Standardized model to generally perform equally well or slightly better than the baseline. WS makes individual neurons scale-invariant which in turn helps balance and regulate the dynamics as discussed in §5.3. Without scale-invariance, radial components in the gradient can modify the effective weight decay (see §3.4) leading to different equilibrium values for the magnitude and rotation. We can predict modified values for SGDM (appx. H) based on measurements of the radial component, but have not attempted to do so for AdamW. In fig. 15 we compare the angular updates of the Weight Standardized GPT2 model to a standard one without scale-invariance. 29Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks 0 50 100 150 200 Epoch 0 200 400 600 0 50 100 150 200 Epoch 0.000 0.002 0.004 0.006 0.008 0.010 ( t, t + 1) = 0.01 RV = 0 Prediction Figure 14: Measurements of the weight norm ∥ω∥ and angular updates ηr for CIFAR10 ResNet20 training for AdamW and RV-AdamW compared to Adam without weight decay (λ = 0). The black line represents our equilibrium and angular update size predictions from table 1. Note how the angular updates without weight decay quickly approach zero, which would even happen with a constant learning rate schedule. 0 25 50 75 100 Step [×103] 10 3 10 2 Avg Rotation OWT GPT2 (AdamW) 0 25 50 75 100 Step [×103] OWT GPT2+WS (AdamW) 2.0 2.5 3.0 3.5 Avg Rotation [×10 3] 0 5 10Count Final Distribution (over layers) Standard With WS r Figure 15: Left: The measured rotation of every linear layer of GPT2-124M, with and without Weight Standardization, for 100k steps on OpenWebText using a constant learning rate. The predicted equilibrium rotation is shown in dashed black (not adjusted for the scale-sensitivity). Right: The distribution of the average rotation of the layers at step 100k for each setting. We show the predicted values based on the formula for scale-invariant layers. As can be seen the unmodified GPT2 model has layers that deviate from this value although it is a reasonable approximation in most cases. With Weight Standardization all layers end up very close to the predicted equilibrium value. In fact we include the extra ηλ2 term from eq. (18) in the prediction here since it is significant compared to the width of the final distribution. Two layers (in the first attention block) temporarily deviate slightly for reasons we are not fully sure of, but they coincide with significant changes in the gradient norms. Effects such as dead neurons or a negative average alignment of successive gradients could also cause deviations from the random walk behavior our analysis assumes (see appx. G). Constraining the Rotational Dynamics of Other Optimizers: In this section, we examine the performance of the Rotational Variants of SGDM and Lion. Table 3 shows that the RVs for both SGDM and Lion can match the baseline similar to what we observed for AdamW in table 2. In most cases no further hyperparameter tuning of the baseline values is needed (zero-shot) but otherwise light tuning (few-shot) suffices. This shows that the RVs can replicate the benefits of weight decay while simplifying the training dynamics by removing the transient phase and fully balancing rotation. Table 3: Test set performance for baseline optimizers SGDM and Lion and their RVs. Zero-shot results for RVs use the baseline hyparparameters, the few-shot is lightly tuned. Dataset Model Optimizer Batch Size Metric ( ↑ ↓) Baseline RV Zero-Shot RV Few-Shot CIFAR-10 ResNet-20 SGD 128 Top-1 Acc. ( ↑) 92.7±0.1 92.4 ±0.2 N/A CIFAR-10 ResNet-20 SGD 2048 Top-1 Acc. ( ↑) 92.0±0.2 92.0 ±0.3 N/A CIFAR-10 ResNet-20 Lion 128 Top-1 Acc. ( ↑) 92.1±0.2 91.9 ±0.2 N/A CIFAR-10 ResNet-20 Lion 2048 Top-1 Acc. ( ↑) 91.8±0.3 91.5 ±0.3 91.8 ±0.2 Imagenet-1k ResNet-50 SGD 256 Top-1 Acc. ( ↑) 77.4 77.3 N/A 30Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks 10 4  10 3  10 2  10 1 Learning Rate 18 19 20 21 22 Validation Perplexity( ) adamw rotational Figure 16: Validation perplexity for GPT2-124M model on Wikitext for different learning rate, weight decay pairs with a constant product (ηλ = 2.5·10−3) resulting in a specific bηr (table 1). 0 20 40 60 80 100 Epoch 0 20 40 60 80 Validation Accuracy RV-SGDM No warmup SGDM No warmup prediction 0 20 40 60 80 100 Epoch 0.00 0.01 0.02 ( t, t + 1) Figure 17: Left: Validation accuracy curves for SGDM and RV-SGDM with and without learning rate warmup for the best configuration of each based on fig. 5R. The final accuracy is 76.6% for SGDM with warmup significantly higher than 72.3% without warmup, compared to 77.1% for RV-SGDM with warmup which is only slightly higher than 76.9% without warmup. Right: Measured average angular updates averaged across all convolutional layers of the ResNet50. For both RV-SGDM runs, a learning rate of η = 6.4 was applied. For standard SGDM a learning rate of η = 3.2 compared to a significantly lower η = 0.2 was utilized for the SGDM without warmup. We believe the reduced learning rate for SGDM without warmup is necessary due to the instability encountered during the initial transient phase at higher learning rates. Learning Rate vs Weight Decay for a Transformer model:Here we repeat the Learning Rate vs Weight Decay Experiment from §5.2, fig. 4, for a GPT2-124M model trained on Wikitext. We vary the learning rate η while keeping the product of the weight decay λ and learning rate, i.e. λη constant. We include a learnable gain for each neuron when using the RV , as they are not scale-invariant and their magnitude may matter for learning. The results are shown in fig. 16. Varying the learning rate while keeping the ηλ product constant only affects the updates of biases and gains in the RV as the rotational rate is constant. For the baseline the relative size of the bias/gain updates compared to the angular updates is also affected but additional transient effects are also introduced. We observe that the RV is less sensitive, displaying better results across a wide range of learning rates. We believe this is primarily due to variations in the effective (rotational) step size schedule for the baseline, which can suffer from overly fast initial (transient) rotation at higher learning rates and an slow rotation for lower learning rates. Need for Learning Rate Warmup: In fig. 5R, we demonstrate that SGDM significantly benefits from a warmup period, while the RV-SGDM exhibits only marginal performance improvements. For the learning rate that performed best in this experiment, we extended the run to span the full 100-epoch duration. Figure 17 shows the validation accuracy curve over the course of training for each of these runs. Notably, while SGDM without warmup exhibits significant performance gains over the course of the 100 epochs, it fails to achieve the performance of SGDM with warmup. In contrast, training the ResNet50 with RV without a warmup period closely matches the performance with warmup. This finding reinforces our belief that a learning rate warmup may aid in stabilizing the transient phase of training, an effect that could potentially be achieved by directly controlling the expected angular update. Balancing Adam+ℓ2: Our analysis (§3.3) shows that in Adam+ℓ2 the equilibrium rotation depends on the gradient norm unlike AdamW and the other optimizers listed in table 1. In our experiments (§5.3) we found that this indeed holds for ResNet-18 on CIFAR-10, where we also reproduced a sweep from the original AdamW paper that shows a performance gap between the two. Although we observe imbalanced rotation causing performance degradation in multiple settings, the experiment does not directly show that the imbalanced rotation causes the difference in this case. 31Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks r/f  r  rf Rotation speed 0 (1-p)/2 1-p p % of neurons 100% 75% 50% Unaltered portion p 93.0 93.5 94.0 94.5Validation Accuracy 1x 3x 5x 7x 9x Scaling factor f 93.0 93.5 94.0 94.5 100% 75% 50% Model Width 93.0 93.5 94.0 94.5 Figure 18: The more imbalanced the rotation, the lower the final performance for ResNet-18 training on CIFAR-10. This happens even with extensive hyperparameter tuning of each setting, which does not compensate for the imbalance.Left: Two artificially imbalanced angular update size distributions (black/orange). A portion 1 − p of the neurons is rotated f times slower and/or faster than rest using a modified RV .Right-1: Varying the portionp for a fixed factor f =10. Right-2: Varying the factor f ∈ [1, 10] for a fixed portion p = 50%. Right-3: Reducing the network width unsurprisingly also degrades performance. Imbalanced rotation with p = 50% and f = 3 gives similar results as a network of half the width (93.7%), suggesting we could be better off without neurons with even seemingly small deviations in the angular update size. The updates of two optimizer can only differ in two ways, in the magnitude of the update and/or the direction of the update. We can construct a special RV that uses Adam+ℓ2 as the inner optimizer (F in algo. 1) but uses the rotational update size bηr computed for AdamW. If this RV performs similar to the standard RV-AdamW then the difference in the update direction should not be a significant factor. Note however that the impact on the update direction depends on the weight norm (which is kept fixed at initialization in the RV but varies in standard training) as well as the strength of the λ hyperparameter (which is much higher for the optimal AdamW configuration). We expect this to result in a larger change in the direction of ∆gp than would be observed along an Adam+ℓ2 optimization trajectory. However, we do not validate this directly and expect the effect to vary over time and between network components (layers / neurons). Running the two RVs at the optimal hyperparameter configuration from AdamW over 5 seeds gives 94.61 ±0.10% test accuracy for RV-AdamW and 94.53±0.11% for the special Adam+ℓ2 RV . This is slightly lower than the 94.71±0.16% for the baseline AdamW but around 0.5% higher than the 94.08±0.16% for Adam+ℓ2. The RV-AdamW performance is likely slightly lower than AdamW due to rotational scheduling effects as is often the case for a zero-shot hyperparameter transfer. The fact that the two RVs perform very similarly and noticeably better than the extensively tuned Adam+ℓ2 supports our conjecture that irregular rotation contributes to the lower performance of Adam+ℓ2. Imbalanced Rotation: In §5.3, we explore the importance of balanced rotation in neural network training. We find that training configurations that result in more balanced rotation perform better, both in the case of AdamW compared to Adam+ℓ2 and when comparing additional Weight Standardization to a baseline with only Layer Normalization. However, it is possible that other differences between these setups also effect the performance. To directly test the impact of imbalanced rotation, while eliminating as many confounding factors as possible, we construct a modified variant of RV-AdamW. In this modified RV , we additionally scale the angular updates of some fraction1 − p of the neurons in each layer by a factor of f. We consider two distributions shown in fig. 18L, either rotating all affected neurons slower or rotating half of them faster and the other half of them slower. For each configuration (p and f) we tune the weight decay, which shifts the rotational distribution through ηr while leaving the update size ηg for biases and gains unaffected. The results can be seen in fig. 18R for a ResNet-18 trained on CIFAR-10. We see that the performance degrades when increasing either the portion of affected neurons or the scaling factor for either artificially imbalanced rotational distribution. The final panel shows how the previous imbalanced rotation compares with simply reducing the width of the network (completely removing neurons). Interestingly, the second and third panels show that a scaling factor of around 3× applied to 50% of neurons results in performance comparable to a network with half the width. This indicates that imbalanced rotation directly and significantly harms performance. Why this happens is not entirely clear to us as we also discuss in appx. M. Neurons that rotate at a different speed may simply not learn effectively, which would work similarly to decreasing the number of neurons in the network. Intuitively, slow moving neurons could also contribute to overfitting (which is more prominent with lower global learning rates). We also speculate that fast rotating neurons may hinder the learning of other neurons, perhaps by limiting the maximum stable (global) learning rate or by changing the internal representations of the network too quickly. 32Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks Dataset Model Zero Mean Invariance CIFAR-10 ResNet-18 94.5±0.4 (yn) 94.5 ±0.4 (yn) 94.3±0.2 (nc) 94.4 ±0.2 (yt) Wikitext GPT2-124M 18.5±0.42 (yc) 18.5±0.42 (yc) 18.4±0.4 (nc) 18.4 ±0.15 (yt) 2  0 log10(1 - ) 94 95 Validation Accuracy CIFAR-10 -sensitivity 10  5  0 log10(1 - ) 18.0 18.5 19.0 Validation Perplexity Wikitext -sensitivity Figure 19: Experimental results for hyperparameter sensitivity. We report perplexity (↓) on Wikitext validation dataset and top-1 Acc. (↑) on a random validation split on CIFAR-10. In the default set up weight centering (y) and per neuron (n) scale-invariance is enabled. Hyperparameter Sensitivity: The RVs introduce one significant hyperparameter, the decay rate β. Further, we can decide whether to enable (y) or disable (n) centering of the weights (zero-mean) and whether to control the angular updates on the layer (l) or neuron (channel, c) level. In this section we study the sensitivity of these choices in two different setups. We train a ResNet-18 on a random train split of CIFAR-10 with the RV of SGDM and a GPT2-124M model on Wikitext with the RV of AdamW. The results are shown in fig. 19. They indicate that the performance of the RVs remains relatively stable when the hyperparameters are varied. Note that we tuned the effective update size for each configuration by varying the weight decay for the ResNet-18 experiments, but run the sensitivity study only for one learning rate, weight decay setting for the GPT2-124M model due to resource constraints. Here we find little difference between the neuron and layer levels, but believe this difference may be larger in other settings that are more prone to imbalanced rotation on the neuron level. K. Experimental Details We perform our experiments on several popular datasets, i.e., CIFAR-10/100 (Krizhevsky, 2009) and Imagenet-1k (Rus- sakovsky et al., 2015) for image classification, IWSLT2014 (Cettolo et al., 2014) for German-English translation, and Wikitext (Merity et al., 2017) and OpenWebText (Radford et al., 2019) for language modelling. Our code utilizes the TIMM library (Wightman, 2019) for vision tasks, FairSeq (Ott et al., 2019) for translation, and NanoGPT (Karpathy, 2023) and LLM-Baselines (Pagliardini, 2023) for language modelling. We train commonly used architectures, namely ResNet-20, ResNet-18, ResNet-50 (He et al., 2016), DeiT tiny (Touvron et al., 2021), a small transformer, and a GPT2-124M network (Radford et al., 2019) from scratch. General Setup: For all experiments trained with SGDM we use a momentum of 0.9, for experiments trained with AdamW we used β1 = 0 .9 and β2 = 0 .999 and for experiments trained with our RVs we used β = 0 .99, unless otherwise stated. For Lion we used β1 = 0.9 and β2 = 0.999 exclusively. Most of the experiments are run on a single NVIDIA A100-SXM4-40GB GPU. Note that we used default architectures for the baseline experiments, but used a learnable gain for DeiT trained on Imagenet- 1k, Transformer-S trained on IWSLT2014 de-en and the GPT2-124M architecture on Wikitext and OpenWebText. As mentioned in §4 we do this since transformers are not scale-invariant so constraining the weight norms may be harmful. Here we list additional details referenced in the details-column in table 4 and 5: D-1 We pre-process the data by normalizing it with mean (0.49140.48220.4465) and std (0.20230.19940.2010). For training we used simple data augmentation from He et al. (2016). D-2 We use the standard data augmentation from He et al. (2016) for Imagenet. D-3 Analogously to Touvron et al. (2021) we apply strong data augmentation. We use color jitter brightness up to 0.3, auto-augmentation, random erase with probability 0.25, drop path with probability 0.1, mixup with probability 0.8 and cutmix with probability 1.0. Additionally, we use label smoothing of 0.1. Note that in this experiment, we implemented a cos2 schedule for the RV few-shot training. We observed that the 33Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks baseline training exhibited a smaller rotation towards the end. Consequently, we adjusted the effective update size for the RV training to more closely align with the baseline run’s effective learning rate schedule. D-4 We use standard FairSeq library (Ott et al., 2019) with dropout probability 0.3. Note that additionally to using weight standardization with learnable gain, we set the weight decay to 0 for scale- sensitive weights. This is done by default for the vision tasks in TIMM library (Wightman, 2019), but not by FairSeq library (Ott et al., 2019) we used for this task. We observed no difference in performance for the baseline model, yet this adjustment allowed to tune the effective update size for the scale-invariant weights, without affecting the learning rate of the scale-sensitive weights significantly. D-5 For this experiment we use the llm-baseline library (Pagliardini, 2023). For the GPT2-124M architecture, we use vocabulary size of 50304, sequence length of 512, embedding size of 768. The model features 12 repeated blocks, each comprising a self-attention block followed by a two-layer MLP block with hidden dimension 3072. This results in a total of 124 million parameters. We use a drop probability of 0.2 for dropout. D-6 For this experiment we use the nanoGPT library (Karpathy, 2023). The details are the same as for D-1, except the sequence length is 1024 and no dropout is used. The model is trained for 5000 iterations which corresponds to roughly 20 tokens per parameter, inspired by Chinchilla (Hoffmann et al., 2022). The learning rate schedule goes down to 0, which we found to be easier to combine with learning rate and weight decay sweeps without affecting the performance significantly in exploratory experiments. Constraining the Rotational Dynamics: The experimental details for the experiments reported in tables 2 and 3 can be found in table 4. Measuring the Update Dynamics & Dynamics of Scale-Sensitive GPT Model: For the experiments in fig. 3, we generally used the same settings for ResNet-50 on ImageNet-1k trained with SGDM and the GPT2-124M model trained with AdamW on OpenWebText. However, we use a constant learning rate without warmup, train for a 100k steps and use the default learning rate from NanoGPT, 6e−4 for the GPT2 model. We use Weight Standardization (with learnable gains) for GPT2 in fig. 3 but not fig. 15. Learning Rate vs Weight Decay: For the ResNet-20 experiment on CIFAR-10 and language model task on Wikitext with a GPT2-124M model we use the few shot (zero shot) setting reported in table 4 as default. We then sweep over the learning rate keeping ηλ = 5·10−4, ηλ = 2.5·10−3 respectively, constant. Transient Effects: In the experiment described in the preceding paragraph, we monitored∥ω∥, ∠(ωt, ωt+1) during training for one of the runs with the chosen learning rates: 1·10−4, 8.3·10−3, and 3·10−1. Need for Learning Rate Warmup: For the ResNet-50 experiment on ImageNet-1k we follow the same base setup as we report in table 4. We train for a total of 10 epochs using a cosine decay schedule (applied stepwise) and no warmup. We use 32 local accumulations on top of batches of size 256 to emulate a batch size of 8’192 and sweep the learning λ in the range 2i · 0.1, with i ∈ [0, . . . ,9]. The GPT2 experiments follow the setup in table 4 with the learning rate swept over {6 · 10−4 · 2i} for i ∈ [−1, . . . ,5]. Adam vs AdamW: For the sweep we train a ResNet-18 on a 90/10 train/val split from the original train set. We use a step-wise cosine schedule and train for 200 epochs without warmup. In this sweep we try to reproduce the results in figure 2 from (Loshchilov & Hutter, 2019), albeit with a slightly different network and training for 200 epochs instead of 100. The best configuration for Adam was η = 7.813 · 10−4, λ = 1.250 · 10−4 resulting in a validation set accuracy of 93.919%. The best configuration for AdamW was η = 1.25 · 10−2, λ = 8.0 · 10−2 with a validation accuracy of 94.319%. On the test set we run each configuration over 5 different seeds, obtaining test accuracies of 94.08 ± 0.16% for Adam+ℓ2 and 94.74 ± 0.14% for AdamW. Benefit of Weight Standardization: We train a ResNet-18 with layer normalization and weight standardization on top of layer normalization on CIFAR-100 using the same augmentation, learning rate schedule and base hyperparameters as for the ResNet-20 on CIFAR-10 experiments, unless otherwise noted below. We train on a random subset containing 90% of the train set and use the remaining 10% for validation which we report. The inputs are normalized for mean (0.5071, 0.4867, 0.4408) and std (0.2675, 0.2565, 0.2761). We use a weight decay of 5 · 10−4 and a batch size of 256. The layer normalization is implemented with a Group Normalization (Wu & He, 2018) using a single group. 34Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks Table 4: Experimental set up (include training set and test set definition). Dataset Model Optimizer Batch Size zero shot few shot detailslr warmupepochs (e)/ train precisionschedule iteration (it) duration CIFAR-10 ResNet-20 SGD 128 wd=1e−4 N/A (D-1) cosine lr=1e−6 200 (e) 35min float32lr=0.5 5 epochs CIFAR-10 ResNet-20 SGD 2048 wd=1e−4 wd=2e−4 (D-1) cosine lr=1e−6 200 (e) 35min float32lr=4.8 lr=4.8 5 epochs CIFAR-10 ResNet-20 AdamW 128 wd=1e−2 β= 0.9 (D-1) cosine lr=1e−6 200 (e) 35min float32lr=5e−2 5 epochs CIFAR-10 ResNet-20 AdamW 2048 wd=1e−2 wd=8e−2 (D-1) cosine lr=1e−6 200 (e) 35min float32lr=1.6e−1 lr=1.6e−1 5 epochs CIFAR-10 ResNet-20 Lion 128 wd=1.0 N/A (D-1) cosine lr=1e−6 200 (e) 35min float32lr=5e−4 5 epochs CIFAR-10 ResNet-20 Lion 2048 wd=1.0 wd=2.0 (D-1) cosine lr=1e−6 200 (e) 35min float32lr=1.6e−2 lr=1.6e−2 5 epochs Imagenet-1k ResNet-50 SGD 256 wd=1e−4 N/A (D-2) cosine lr=1e−6 90 (e) 30h float16lr=1e−1 5 epochs Imagenet-1k DeiT tiny AdamW 1024 wd=5e−2 wd=2e−1 (D-3) cosine lr=1e−6 300 (e) 70h float16lr=5e−4 lr=5e−4 5 epochs IWSLT2014 de-en Transformer-S AdamW 4096 wd=1e−4 wd=4e−1 (D-4) cosine 4000 (it) 22021 (it) 50min float16lr=5e−4 lr=5e−4 β2 = 0.98 β2 = 0.98 Wikitext GPT2-124M AdamW 55 ×3 wd=0.5 N/A (D-5) cosine 2e−2(%) 15000 (it) 3h bfloat16lr=5e−3 div_f=1e2 β2 = 0.95 final_div_f=1e4 OpenWebText GPT2-124M AdamW 12×40 wd=0.1 N/A (D-6) cosine 250 (it) 5000 (it) 4h bfloat16lr=4.8e−3 min_lr=0 β2 = 0.95 Table 5: Experimental details for hyperparameter senstivity study. Dataset Model Optimizer training validation hyper- detailslr warmupepochs (e)/ train precisiondataset dataset parameters schedule iterations (it) duration CIFAR-10 ResNet-18 SGD 90% Train 10% Trainwd=N/A (D-1) cosine lr=1e−6 200 (e) 35min float32lr=1.0 5 epochs Wikitext GPT2-124M AdamW Train Validation wd=0.5 (D-5) cosine 2e−2(%) 15000 (it) 3h bfloat16lr=4e−3 div_f=1e2 β2 = 0.95 final_div_f=1e4 Imbalanced Rotation: We trained a ResNet-18 on a 90/10 train/val split from the original train set with learning rate η = 0.1 , batch size 256 and varying weight decay to tune the effective updated size. For each rotation speed f, portion p and network width scaling we report the average performance over the best configuration of this sweep. All other settings are equivalent to the settings reported in table 4. Hyperparameter Sensitivity: The experimental details for the experiments reported in fig. 19 can be found in table 5. Note, that we don’t specify the weight decay used for the ResNet-18 experiment, since we tuned the weight decay for each configuration with 2i · λ, i∈ [−64, 32]. We use a batch size of 256 and a batch size of 55 with 3 accumulation steps for the GPT2-124M model. 35Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks Table 6: An extension of table 1 showing values for diffusion rates based on the expected Total Update Contribution (TUC) of the gradients from a single timestep, corresponding to the update sizes. The TUC attempts to capture the rate of longer-term changes rather than the instantaneous update sizes, accounting for the increased alignment from momentum. Measure Definition SGDM eq. (45) AdamW eq. (11) Equilibrium norm d∥ω∥ p E[∥ω∥2] 4 q ηE[∥˜g∥2] 2λ·(1−α) q ηC 2λ RMS update size bηg p E[∥∆gp∥2] η q E[∥g∥2] 1−α2 η q C 1−β1 1+β1 RMS diffusion rate bτg p E[∥u∥2] η 1−α p E[∥g∥2] η √ C Expected rotation bηr p E[∥∆gp∥2]/ d∥ω∥ q 2ηλ 1+α q 2ηλ 1−β1 1+β1 Rotational diffusion rate bτr p E[∥u∥2]/ d∥ω∥ q 2ηλ 1−α √2ηλ L. Update Size vs Learning Rate An update size, such as the average angular update ηr or RMS update ηg, specifically refers to a measure of the size of individual updates. This does not necessarily correlate directly to the net change over a longer period of multiple steps, e.g. an epoch of training. The longer term change is determined by the shape of the optimization trajectory which depends on the size and direction of the individual steps. For a fixed update size, a more consistent direction in the updates will cause a larger net change over a time period. For this reason we prefer to use the term “effective update size” rather than “effective learning rate” which is sometimes used to refer to measures of long term changes. We note that the update sizes are easier to measure and control, although the long term changes may be more informative for hyperparameter tuning and adjustment. The difference is particularly important when using momentum which has a somewhat unintuitive effect in the random walk setting, e.g. for SGDM and AdamW. As seen in table 1 higher momentum coefficients decrease the average rotation in each step but we know they will also cause additional correlation between successive steps. This can be viewed as smoothing the optimization trajectory, it will have smaller random fluctuations in each step but the averaged “velocity” is not necessarily smaller. Comparing the average rotation per step between optimizers with different amounts of momentum will therefore not necessarily correlate with measures of how fast parameters are being updated over longer time intervals. In the random walk setting the long term change is likely to be proportional to the total update contribution ∥u∥ rather than the size of a single update step ∥∆gp∥ (see definitions in §3.1). Analogously, the total rotational contribution from a single gradient sample would be roughly ∥u∥/d∥ω∥rather then ∥∆gp∥/d∥ω∥used in §3.2. These total update contributions may then add up orthogonally assuming successive gradients behave like in the random walk (i.e. that they are independent and zero mean in expectation). We refer to this longer term speed as diffusion rates based on a loose analogy with random walks like Brownian Motion. Table 6 shows the modified rates for SGDM and AdamW. The diffusion rates are more consistent with the way learning rate is typically scaled with the momentum for SGDM, i.e. keeping η/(1 − α) constant rather than η/(1 + α), see e.g. Chiley et al. (2019) and Fu et al. (2023). 36Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks M. Why Balanced Rotation Might Work We empirically observe that balanced rotation seems to perform better than the imbalanced rotation resulting from various methods. The reasons for this are not fully clear to us, but we believe that balanced rotation is a heuristic that ensures that all neurons and layers are updated at a reasonable speed. For example, we intuitively expect that a layer which is updated very slowly, perhaps barely changing through the training process, will unlikely contribute optimally to the final model resulting in worse performance and wasted compute. Conversely, a rapidly changing layer may cause instability, limiting the maximum stable learning rate and preventing other layers from learning effectively. This suggests that sufficiently (and adversarially) imbalanced rates are not optimal. At the same time it seems unlikely that exactly balanced rates are optimal in all settings, exhaustively tuning the rotation speed of each component would likely result in some specific optimal, imbalanced, rotation for a given problem. Modern neural networks have a complex structure which can give rise to various effects that scale the gradient of one component relative to another in arbitrary ways. Neyshabur et al. (2015) gives an example of this for ReLU networks (without branches), where one layer can be scaled up by a positive factor and another down. This preserves the encoded function but will change the optimization trajectory and can significantly degrade performance. Scale-invariance is another example, where scaling the weights of a layer will not affect the network outputs but changes the angular updates and therefore the optimization trajectory for standard optimizers. Finally we can change the network architecture by e.g. inserting a fixed scaling factor into the computational graph that scales a given parameter (whose value is adjusted to compensate) which in turns scales the gradients changing the optimization trajectory. These examples show that the relative gradient magnitude of one component compared to another may simply not be very meaningful quantity in neural networks. Perhaps higher order information like curvature could allow us to determine what the relative update size should be. Without this information, normalizing the update magnitude through methods like Adam, Sign-SGD or Rotational Optimizer Variants may act as heuristics that are better than using the arbitrary gradient magnitude. Proving this might be infeasible, these heuristics may only help in specific cases dependent on the network architecture, initialization, dataset and so on, or it may at least be possible to construct adversarial scenarios where they hinder training. We do not pursue this further here and simply treat balanced rotation as an empirically useful heuristic that seems to play a role in the effectiveness of many influential methods in deep learning. 37",
      "meta_data": {
        "arxiv_id": "2305.17212v4",
        "authors": [
          "Atli Kosson",
          "Bettina Messmer",
          "Martin Jaggi"
        ],
        "published_date": "2023-05-26T19:14:01Z",
        "pdf_url": "https://arxiv.org/pdf/2305.17212v4.pdf",
        "github_url": "https://github.com/epfml/REQ"
      },
      "llm_extracted_info": {
        "main_contributions": "This study analyzes how weight decay influences the update dynamics of individual neurons in deep neural networks, introducing the concept of 'rotational equilibrium' where expected weight magnitude and angular updates stabilize. Key contributions include deriving steady-state neuronal update dynamics for AdamW, Adam with ℓ2-regularization, Lion, and SGD with momentum, and experimentally validating these. The research demonstrates how weight decay and learning rate jointly determine two distinct effective step sizes (ηg for biases, ηr for weights). It shows that explicitly controlling angular updates via Rotational Optimizer Variants (RVs) offers an alternative to weight decay and normalization, simplifying optimization dynamics and reducing the need for learning rate warmup. Finally, the work reveals that 'balanced rotation' is crucial for the performance benefits of AdamW over Adam+ℓ2 and the effectiveness of Weight Standardization.",
        "methodology": "The methodology combines applied analysis and experimentation, focusing on fine-grained update dynamics at the neuron level. It describes neuronal updates using expected weight norm (E[∥ωt∥]), RMS update size (ηg) for biases, and expected angular update size (ηr). Analysis is conducted in a simplified setting where updates are dominated by noise, resembling a random walk, allowing for geometric derivation of equilibrium norms and angular updates for various optimizers. The paper introduces Rotational Optimizer Variants (RVs) as a control mechanism, which keep weight magnitudes constant and directly manage average angular updates by projecting gradients to be orthogonal to the weight vector. The analysis also extends to scale-sensitive parameters by modeling radial gradient components as an effective weight decay.",
        "experimental_setup": "Experiments validate the analytical findings using various neural network architectures (ResNet-20/18/50, DeiT tiny, Transformer-S, GPT2-124M) across several popular datasets: CIFAR-10/100, ImageNet-1k (image classification), IWSLT2014 (German-English translation), Wikitext and OpenWebText (language modeling). Optimizers include SGD with momentum (SGDM), AdamW, Adam with ℓ2-regularization, Lion, and their respective Rotational Variants. Validation methods involve test set performance (mean±std over three seeds), validation accuracy/perplexity/loss curves, and direct measurements of weight norms and average rotation over training steps. A simple random walk system is also used for controlled validation. Experiments are typically run on a single NVIDIA A100-SXM4-40GB GPU.",
        "limitations": "The analytical models primarily rely on a simplified 'random walk' assumption where gradient noise dominates, which is not a formal theoretical analysis capturing all real-world neural network optimization intricacies. Approximations are used for the recurrence relations and specific optimizer dynamics (e.g., Adam+ℓ2 equilibrium assumes `g2 >> λ2ω2` and perfect second moment tracking). The Lion optimizer's analysis assumes independent and identically distributed normal components for the sign function's argument. The paper acknowledges that the reasons for balanced rotation's empirical effectiveness are not fully clear, and that RVs might not be universally optimal. The random walk model disregards the influence of the true gradient, and approximations in this context can affect prediction accuracy.",
        "future_research_directions": "The authors suggest numerous opportunities for further theoretical and practical research. This includes exploring if Rotational Optimizer Variants (RVs) can not only replicate but outperform baseline optimizers, and investigating how insights from RVs could lead to better transient phase management to reduce the need for learning rate warmups. Future work could also involve determining the optimal decay factor (β) for RVs based on convergence speed, and exploring whether higher-order information like curvature could guide the determination of relative update sizes more effectively. Further research is needed to fully understand why balanced rotation works and to explore optimal (potentially imbalanced) rotation strategies for specific problems.",
        "experimental_code": "import torch\ndef tensor_norm(tensor, per_neuron=True):\n    if per_neuron:\n        norm = torch.linalg.vector_norm(tensor.reshape(tensor.shape[0], -1), dim=1)\n        return norm.reshape(-1, *([1]*(tensor.dim()-1)))\n    else:\n        return torch.linalg.vector_norm(tensor)\ndef dot_product(a, b, per_neuron=True):\n    if per_neuron:\n        return (a.flatten(1)*b.flatten(1)).sum(dim=1).reshape(a.shape[0], *([1]*(a.dim()-1)))\n    else:\n        return torch.sum(a*b)\ndef zero_mean(tensor, per_neuron=True):\n    if per_neuron:\n        flat_tensor = tensor.reshape(tensor.shape[0], -1)\n        return (flat_tensor - flat_tensor.mean(dim=1, keepdim=True)).view_as(tensor)\n    else:\n        return tensor - tensor.mean()\n@torch.no_grad()\ndef center_rotational_weights(param_groups, verbose=True):\n    if verbose:\n        print(\"Centering Rotational Weight Vectors\")\n    for group in param_groups:\n        for p in group['params']:\n            if group.get('rotational') is not None:\n                rotational = group['rotational']\n            else:\n                rotational = group['weight_decay'] != 0 and p.dim() > 1\n            if rotational:\n                init_norm = tensor_norm(p, group['per_neuron'])\n                p_zero = zero_mean(p, group['per_neuron'])\n                p_zero = init_norm * p_zero / tensor_norm(p_zero, group['per_neuron'])\n                p.copy_(p_zero)\n@torch.no_grad()\ndef perform_rotational_update(p, d_p, state, group, avg_rotation):\n    if 'rotational_step' not in state:\n        state['rotational_step'] = 0\n    state['rotational_step'] += 1\n    if 'norm' not in state:\n        if group['zero_mean']:\n            p_zero = zero_mean(p, group['per_neuron'])\n            state['norm'] = tensor_norm(p_zero, group['per_neuron'])\n        else:\n            state['norm'] = tensor_norm(p, group['per_neuron'])\n    if group['zero_mean']:\n        d_p = zero_mean(d_p, group['per_neuron'])\n    d_p = d_p - p * (dot_product(d_p, p, group['per_neuron']) / state['norm']**2)\n    undf = group['update_norm_decay_factor']\n    d_p_norm2 = (tensor_norm(d_p, group['per_neuron'])**2).detach()\n    state['update_norm2'] = (1 - undf) * d_p_norm2 + undf * state.get('update_norm2',0)\n    avg_update_norm = torch.sqrt(state['update_norm2'] / (1 - undf**state['rotational_step']))\n    roteps = group['rotational_eps']\n    p_new = p - avg_rotation * (d_p / (roteps + avg_update_norm)) * state['norm']\n    p_new = p_new * state['norm'] / tensor_norm(p_new, group['per_neuron'])\n    p.copy_(p_new)\n\nimport math\nfrom torch.optim.optimizer import Optimizer\nclass RotationalAdamW(Optimizer):\n    def __init__(\n            self,\n            params,\n            lr=1e-3,\n            betas=(0.9, 0.999),\n            eps=1e-8,\n            weight_decay=1e-2,\n            update_norm_decay_factor=0.9,\n            per_neuron=True,\n            zero_mean=True,\n            rotational_eps=1e-8,\n            rotational=None,\n    ):\n        if not 0.0 <= lr:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n        defaults = dict(\n            lr=lr,\n            betas=betas,\n            eps=eps,\n            weight_decay=weight_decay,\n            update_norm_decay_factor=update_norm_decay_factor,\n            per_neuron=per_neuron,\n            zero_mean=zero_mean,\n            rotational_eps=rotational_eps,\n            rotational=rotational,\n        )\n        super().__init__(params, defaults)\n        if zero_mean:\n            center_rotational_weights(self.param_groups)\n    def __setstate__(self, state):\n        super().__setstate__(state)\n    @torch.no_grad()\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n        for group in self.param_groups:\n            weight_decay = group['weight_decay']\n            beta1, beta2 = group['betas']\n            eps = group['eps']\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad\n                if grad.is_sparse:\n                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n                state = self.state[p]\n                if len(state) == 0:\n                    state['step'] = 0\n                    state['exp_avg'] = torch.zeros_like(p)\n                    state['exp_avg_sq'] = torch.zeros_like(p)\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n                state['step'] += 1\n                bias_correction1 = 1 - beta1 ** state['step']\n                bias_correction2 = 1 - beta2 ** state['step']\n                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n                denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)\n                if (rotational := group['rotational']) is None:\n                    rotational = group['weight_decay'] != 0 and p.dim() > 1\n                if rotational:\n                    assert group['weight_decay'] != 0\n                    avg_rotation = (2*group['lr']*weight_decay*(1-beta1)/(1+beta1))**0.5\n                    d_p = torch.div(exp_avg, denom) / bias_correction1\n                    perform_rotational_update(p, d_p, state, group, avg_rotation)\n                else:\n                    p.data.mul_(1 - group['lr'] * group['weight_decay'])\n                    step_size = group['lr'] / bias_correction1\n                    p.addcdiv_(exp_avg, denom, value=-step_size)\n        return loss\n\nimport math\nfrom torch.optim.optimizer import Optimizer\nclass RotationalLion(Optimizer):\n    def __init__(\n        self,\n        params,\n        lr=1e-4,\n        betas=(0.9, 0.99),\n        weight_decay=0.0,\n        update_norm_decay_factor=0.9,\n        per_neuron=True,\n        zero_mean=True,\n        rotational_eps=1e-8,\n        rotational=None,\n    ):\n        if not 0.0 <= lr:\n            raise ValueError('Invalid learning rate: {}'.format(lr))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError('Invalid beta parameter at index 0: {}'.format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError('Invalid beta parameter at index 1: {}'.format(betas[1]))\n        defaults = dict(\n            lr=lr,\n            betas=betas,\n            weight_decay=weight_decay,\n            update_norm_decay_factor=update_norm_decay_factor,\n            per_neuron=per_neuron,\n            zero_mean=zero_mean,\n            rotational_eps=rotational_eps,\n            rotational=rotational,\n        )\n        super().__init__(params, defaults)\n        if zero_mean:\n            center_rotational_weights(self.param_groups)\n    def __setstate__(self, state):\n        super().__setstate__(state)\n    @torch.no_grad()\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n        for group in self.param_groups:\n            lr = group['lr']\n            weight_decay = group['weight_decay']\n            beta1, beta2 = group['betas']\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                state = self.state[p]\n                if len(state) == 0:\n                    state['exp_avg'] = torch.zeros_like(p)\n                d_p = torch.sign(state['exp_avg'].mul(beta1).add_(p.grad, alpha=1 - beta1))\n                state['exp_avg'].lerp_(p.grad, 1 - beta2)\n                if (rotational := group['rotational']) is None:\n                    rotational = group['weight_decay'] != 0 and p.dim() > 1\n                if rotational:\n                    assert group['weight_decay'] != 0\n                    avg_rotation = (math.pi/2.0)**0.5 * (2*lr*weight_decay)**0.5 \\\n                        * ((1-beta1)**2 + beta1**2*(1-beta2)/(1+beta2))**0.5\n                    perform_rotational_update(p, d_p, state, group, avg_rotation)\n                else:\n                    p.mul_(1 - lr * weight_decay)\n                    p.add_(d_p, alpha=-lr)\n        return loss\n\nimport torch\nfrom torch.optim.optimizer import Optimizer, required\nclass RotationalSGD(Optimizer):\n    def __init__(\n        self,\n        params,\n        lr=required,\n        momentum=0,\n        weight_decay=required,\n        nesterov=False,\n        update_norm_decay_factor=0.9,\n        per_neuron=True,\n        zero_mean=True,\n        rotational_eps=1e-8,\n        rotational=None,\n    ):\n        if lr is not required and lr < 0.0:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if momentum < 0.0:\n            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n        if weight_decay < 0.0:\n            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n        if nesterov and (momentum <= 0):\n            raise ValueError(\"Nesterov momentum requires a momentum\")\n        defaults = dict(\n            lr=lr,\n            momentum=momentum,\n            weight_decay=weight_decay,\n            nesterov=nesterov,\n            update_norm_decay_factor=update_norm_decay_factor,\n            per_neuron=per_neuron,\n            zero_mean=zero_mean,\n            rotational_eps=rotational_eps,\n            rotational=rotational,\n        )\n        super().__init__(params, defaults)\n        if zero_mean:\n            center_rotational_weights(self.param_groups)\n    @torch.no_grad()\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n        for group in self.param_groups:\n            weight_decay = group['weight_decay']\n            momentum = group['momentum']\n            nesterov = group['nesterov']\n            for p in group['params']:\n                if (rotational := group['rotational']) is None:\n                    rotational = group['weight_decay'] != 0 and p.dim() > 1\n                state = self.state[p]\n                if p.grad is None:\n                    continue\n                d_p = p.grad\n                if weight_decay != 0 and not rotational:\n                    d_p = d_p.add(p, alpha=weight_decay)\n                if momentum != 0:\n                    param_state = self.state[p]\n                    if 'momentum_buffer' not in param_state:\n                        buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n                    else:\n                        buf = param_state['momentum_buffer']\n                        buf.mul_(momentum).add_(d_p)\n                    if nesterov:\n                        d_p = d_p.add(buf, alpha=momentum)\n                    else:\n                        d_p = buf\n                if rotational:\n                    assert group['weight_decay'] != 0\n                    avg_rotation = (2*group['lr']*group['weight_decay']/(1+group['momentum']))**0.5\n                    perform_rotational_update(p, d_p, state, group, avg_rotation)\n                else:\n                    p.add_(d_p, alpha=-group['lr'])\n        return loss\n\nimport math\nimport torch\nfrom torch.optim import Optimizer\nfrom torch.utils._foreach_utils import _group_tensors_by_device_and_dtype\nclass RotationalWrapper(Optimizer):\n    def __init__(\n        self,\n        params,\n        inner_type='adamw',\n        etar_func=None,\n        update_norm_decay_factor=0.9,\n        per_neuron=True,\n        zero_mean=True,\n        rotational_eps=1e-8,\n        **inner_hyperparameters,\n    ):\n        if isinstance(etar_func, str):\n            if etar_func == 'adamw':\n                self.etar_func = adamw_etar_func\n            elif etar_func == 'sgdm':\n                self.etar_func = sgdm_etar_func\n            elif etar_func == 'lion':\n                self.etar_func = lion_etar_func\n            else:\n                raise ValueError(f\"Unknown {etar_func=}\")\n        else:\n            self.etar_func = etar_func\n        if inner_type == 'adamw':\n            self.init_state_get_lists = adamw_init_state_get_lists\n            self.get_inner_update = adamw_get_update\n            if self.etar_func is None:\n                self.etar_func = adamw_etar_func\n            inner_hyperparameters = {\n                'lr': 1e-3,\n                'weight_decay': 1e-2,\n                'betas': (0.9, 0.999),\n                'eps': 1e-8,\n                **inner_hyperparameters,\n            }\n        if inner_type == 'adam':\n            self.init_state_get_lists = adam_init_state_get_lists\n            self.get_inner_update = adam_get_update\n            if self.etar_func is None:\n                raise ValueError(\"No etar_func provided for Adam\")\n            inner_hyperparameters = {\n                'lr': 1e-3,\n                'weight_decay': 0.0,\n                'betas': (0.9, 0.999),\n                'eps': 1e-8,\n                **inner_hyperparameters,\n            }\n        if inner_type == 'sgdm':\n            self.init_state_get_lists = sgdm_init_state_get_lists\n            self.get_inner_update = sgdm_get_update\n            if self.etar_func is None:\n                self.etar_func = sgdm_etar_func\n            inner_hyperparameters = {\n                'momentum': 0.0,\n                'weight_decay': 0.0,\n                **inner_hyperparameters,\n            }\n        if inner_type == 'lion':\n            self.init_state_get_lists = lion_init_state_get_lists\n            self.get_inner_update = lion_get_update\n            if self.etar_func is None:\n                self.etar_func = lion_etar_func\n            inner_hyperparameters = {\n                'lr': 1e-4,\n                'weight_decay': 1.0,\n                'betas': (0.9, 0.99),\n                'eps': 1e-8,\n                **inner_hyperparameters,\n            }\n        defaults = dict(\n            per_neuron=per_neuron,\n            update_norm_decay_factor=update_norm_decay_factor,\n            zero_mean=zero_mean,\n            rotational_eps=rotational_eps,\n            **inner_hyperparameters,\n        )\n        super().__init__(params, defaults)\n        if zero_mean:\n            center_rotational_weights(self.param_groups)\n    @torch.no_grad()\n    def step(self):\n        for group in self.param_groups:\n            state_lists = self.init_state_get_lists(self.state, group)\n            unscaled_delta_grads, unscaled_delta_lambdas = self.get_inner_update(state_lists, group)\n            for p in group['params']:\n                if group.get('rotational') is not None:\n                    rotational = group['rotational']\n                    if rotational:\n                        assert group['weight_decay'] != 0\n                else:\n                    rotational = group['weight_decay'] != 0 and p.dim() > 1\n                if rotational:\n                    state = self.state[p]\n                    if 'norm' not in state:\n                        if group['zero_mean']:\n                            p_zero = zero_mean(p, group['per_neuron'])\n                            state['norm'] = tensor_norm(p_zero, group['per_neuron'])\n                        else:\n                            state['norm'] = tensor_norm(p, group['per_neuron'])\n                    update = unscaled_delta_grads[p]\n                    projection = p * (dot_product(update, p, group['per_neuron']) / state['norm']**2)\n                    update = update - projection\n                    if group['zero_mean']:\n                        update = zero_mean(update, group['per_neuron'])\n                    undf = group['update_norm_decay_factor']\n                    square_update_norm = tensor_norm(update, group['per_neuron'])**2\n                    state['update_norm_sq'] = (1-undf) * square_update_norm + undf * state.get('update_norm_sq', 0)\n                    avg_update_norm = torch.sqrt(state['update_norm_sq'] / (1 - undf**state['step']))\n                    eta_r = self.etar_func(group)\n                    p_new = p - eta_r * state['norm'] * (update / (avg_update_norm + group['rotational_eps']))\n                    p_new = p_new * state['norm'] / tensor_norm(p_new, group['per_neuron'])\n                    p.copy_(p_new)\n                else:\n                    p.add_(-group['lr']*(unscaled_delta_grads[p]+unscaled_delta_lambdas[p]))\n\ndef adamw_init_state_get_lists(optimizer_state, group):\n    state_lists = dict(\n        params_with_grad=(params_with_grad := []),\n        grads=(grads := []),\n        exp_avgs=(exp_avgs := []),\n        exp_avg_sqs=(exp_avg_sqs := []),\n        state_steps=(state_steps := []),\n    )\n    for p in group[\"params\"]:\n        if p.grad is None:\n            continue\n        params_with_grad.append(p)\n        if p.grad.is_sparse:\n            raise RuntimeError(\"AdamW does not support sparse gradients\")\n        grads.append(p.grad)\n        state = optimizer_state[p]\n        if len(state) == 0:\n            state[\"step\"] = torch.tensor(0.0)\n            state[\"exp_avg\"] = torch.zeros_like(\n                p, memory_format=torch.preserve_format\n            )\n            state[\"exp_avg_sq\"] = torch.zeros_like(\n                p, memory_format=torch.preserve_format\n            )\n        exp_avgs.append(state[\"exp_avg\"])\n        exp_avg_sqs.append(state[\"exp_avg_sq\"])\n        state_steps.append(state[\"step\"])\n    return state_lists\ndef adamw_get_update(state_lists, group):\n    params = state_lists['params_with_grad']\n    grads = state_lists['grads']\n    exp_avgs = state_lists['exp_avgs']\n    exp_avg_sqs = state_lists['exp_avg_sqs']\n    state_steps = state_lists['state_steps']\n    beta1, beta2 = group['betas']\n    weight_decay = group['weight_decay']\n    eps = group['eps']\n    unscaled_delta_grads = dict()\n    unscaled_delta_lambdas = dict()\n    if len(params) == 0:\n        return dict(), dict()\n    grouped_tensors = _group_tensors_by_device_and_dtype([\n        params, grads, exp_avgs, exp_avg_sqs, state_steps])\n    for ((\n        device_params,\n        device_grads,\n        device_exp_avgs,\n        device_exp_avg_sqs,\n        device_state_steps,\n    ), _) in grouped_tensors.values():\n        torch._foreach_add_(device_state_steps, 1)\n        torch._foreach_lerp_(device_exp_avgs, device_grads, 1 - beta1)\n        torch._foreach_mul_(device_exp_avg_sqs, beta2)\n        torch._foreach_addcmul_(device_exp_avg_sqs, device_grads, device_grads, 1 - beta2)\n        del device_grads\n        bias_correction1 = [1 - beta1 ** _get_value(step) for step in device_state_steps]\n        bias_correction2 = [1 - beta2 ** _get_value(step) for step in device_state_steps]\n        bias_correction2_sqrt = [_dispatch_sqrt(bc) for bc in bias_correction2]\n        exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)\n        torch._foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n        torch._foreach_add_(exp_avg_sq_sqrt, eps)\n        scaled_updates = torch._foreach_div(device_exp_avgs, exp_avg_sq_sqrt)\n        torch._foreach_div_(scaled_updates, bias_correction1)\n        unscaled_delta_wd_list = torch._foreach_mul(device_params, weight_decay)\n        for param, udg, udl in zip(device_params, scaled_updates, unscaled_delta_wd_list):\n            unscaled_delta_grads[param] = udg\n            unscaled_delta_lambdas[param] = udl\n    return unscaled_delta_grads, unscaled_delta_lambdas\ndef adamw_etar_func(group):\n    lr = group['lr']\n    wd = group['weight_decay']\n    beta1 = group['betas'][0]\n    return (2*lr*wd*(1-beta1)/(1+beta1))**0.5\n\ndef adam_init_state_get_lists(optimizer_state, group):\n    state_lists = dict(\n        params_with_grad=(params_with_grad := []),\n        grads=(grads := []),\n        grad_exp_avgs=(grad_exp_avgs := []),\n        l2_exp_avgs=(l2_exp_avgs := []),\n        total_exp_avg_sqs=(total_exp_avg_sqs := []),\n        state_steps=(state_steps := []),\n    )\n    for p in group[\"params\"]:\n        if p.grad is None:\n            continue\n        params_with_grad.append(p)\n        if p.grad.is_sparse:\n            raise RuntimeError(\"Adam does not support sparse gradients\")\n        grads.append(p.grad)\n        state = optimizer_state[p]\n        if len(state) == 0:\n            state[\"step\"] = torch.tensor(0.0)\n            state[\"grad_exp_avg\"] = torch.zeros_like(\n                p, memory_format=torch.preserve_format\n            )\n            state[\"l2_exp_avg\"] = torch.zeros_like(\n                p, memory_format=torch.preserve_format\n            )\n            state[\"total_exp_avg_sq\"] = torch.zeros_like(\n                p, memory_format=torch.preserve_format\n            )\n        grad_exp_avgs.append(state[\"grad_exp_avg\"])\n        l2_exp_avgs.append(state[\"l2_exp_avg\"])\n        total_exp_avg_sqs.append(state[\"total_exp_avg_sq\"])\n        state_steps.append(state[\"step\"])\n    return state_lists\ndef adam_get_update(state_lists, group):\n    params = state_lists['params_with_grad']\n    grads = state_lists['grads']\n    grad_exp_avgs = state_lists['grad_exp_avgs']\n    l2_exp_avgs = state_lists['l2_exp_avgs']\n    total_exp_avg_sqs = state_lists['total_exp_avg_sqs']\n    state_steps = state_lists['state_steps']\n    beta1, beta2 = group['betas']\n    weight_decay = group['weight_decay']\n    eps = group['eps']\n    unscaled_delta_grads = dict()\n    unscaled_delta_lambdas = dict()\n    if len(params) == 0:\n        return dict(), dict()\n    grouped_tensors = _group_tensors_by_device_and_dtype([\n        params, grads, grad_exp_avgs, l2_exp_avgs, total_exp_avg_sqs, state_steps\n    ])\n    for ((\n        device_params,\n        device_grads,\n        device_grad_exp_avgs,\n        device_l2_exp_avgs,\n        device_total_exp_avg_sqs,\n        device_state_steps,\n    ), _) in grouped_tensors.values():\n        torch._foreach_add_(device_state_steps, 1)\n        torch._foreach_lerp_(device_grad_exp_avgs, device_grads, 1 - beta1)\n        l2_term = torch._foreach_mul(device_params, weight_decay)\n        torch._foreach_lerp_(device_l2_exp_avgs, l2_term, 1 - beta1)\n        del l2_term\n        device_grads = torch._foreach_add(device_grads, device_params, alpha=weight_decay)\n        torch._foreach_mul_(device_total_exp_avg_sqs, beta2)\n        torch._foreach_addcmul_(device_total_exp_avg_sqs, device_grads, device_grads, 1 - beta2)\n        del device_grads\n        bias_correction1 = [1 - beta1 ** _get_value(step) for step in device_state_steps]\n        bias_correction2 = [1 - beta2 ** _get_value(step) for step in device_state_steps]\n        bias_correction2_sqrt = [_dispatch_sqrt(bc) for bc in bias_correction2]\n        total_exp_avg_sq_sqrt = torch._foreach_sqrt(device_total_exp_avg_sqs)\n        torch._foreach_div_(total_exp_avg_sq_sqrt, bias_correction2_sqrt)\n        torch._foreach_add_(total_exp_avg_sq_sqrt, eps)\n        unscaled_grad_updates = torch._foreach_div(device_grad_exp_avgs, total_exp_avg_sq_sqrt)\n        torch._foreach_div_(unscaled_grad_updates, bias_correction1)\n        unscaled_l2_updates = torch._foreach_div(device_l2_exp_avgs, total_exp_avg_sq_sqrt)\n        torch._foreach_div_(unscaled_l2_updates, bias_correction1)\n        for param, udg, udl in zip(device_params, unscaled_grad_updates, unscaled_l2_updates):\n            unscaled_delta_grads[param] = udg\n            unscaled_delta_lambdas[param] = udl\n    return unscaled_delta_grads, unscaled_delta_lambdas\n\ndef sgdm_init_state_get_lists(optimizer_state, group):\n    state_lists = dict(\n        params_with_grad=(params_with_grad := []),\n        grads=(grads := []),\n        grad_exp_avgs=(grad_exp_avgs := []),\n        l2_exp_avgs=(l2_exp_avgs := []),\n        state_steps=(state_steps := []),\n    )\n    for p in group[\"params\"]:\n        if p.grad is None:\n            continue\n        params_with_grad.append(p)\n        if p.grad.is_sparse:\n            raise RuntimeError(\"SGDM does not support sparse gradients\")\n        grads.append(p.grad)\n        state = optimizer_state[p]\n        if len(state) == 0:\n            state[\"step\"] = torch.tensor(0.0)\n            state[\"grad_exp_avg\"] = torch.zeros_like(\n                p, memory_format=torch.preserve_format\n            )\n            state[\"l2_exp_avg\"] = torch.zeros_like(\n                p, memory_format=torch.preserve_format\n            )\n        grad_exp_avgs.append(state[\"grad_exp_avg\"])\n        l2_exp_avgs.append(state[\"l2_exp_avg\"])\n        state_steps.append(state[\"step\"])\n    return state_lists\ndef sgdm_get_update(state_lists, group):\n    params = state_lists['params_with_grad']\n    grads = state_lists['grads']\n    grad_exp_avgs = state_lists['grad_exp_avgs']\n    l2_exp_avgs = state_lists['l2_exp_avgs']\n    state_steps = state_lists['state_steps']\n    momentum = group['momentum']\n    weight_decay = group['weight_decay']\n    assert not group.get('nesterov'), \"This implementation doesn't support it\"\n    unscaled_delta_grads = dict()\n    unscaled_delta_lambdas = dict()\n    if len(params) == 0:\n        return dict(), dict()\n    grouped_tensors = _group_tensors_by_device_and_dtype([\n        params, grads, grad_exp_avgs, l2_exp_avgs, state_steps\n    ])\n    for ((\n        device_params,\n        device_grads,\n        device_grad_exp_avgs,\n        device_l2_exp_avgs,\n        device_state_steps,\n    ), _) in grouped_tensors.values():\n        torch._foreach_add_(device_state_steps, 1)\n        torch._foreach_mul_(device_grad_exp_avgs, momentum)\n        torch._foreach_add_(device_grad_exp_avgs, device_grads)\n        torch._foreach_mul_(device_l2_exp_avgs, momentum)\n        torch._foreach_add_(device_l2_exp_avgs, device_params, alpha=weight_decay)\n        for param, udg, udl in zip(device_params, device_grad_exp_avgs, device_l2_exp_avgs):\n            unscaled_delta_grads[param] = udg\n            unscaled_delta_lambdas[param] = udl\n    return unscaled_delta_grads, unscaled_delta_lambdas\ndef sgdm_etar_func(group):\n    lr = group['lr']\n    wd = group['weight_decay']\n    alpha = group['momentum']\n    return (2*lr*wd/(1+alpha))**0.5\n\ndef lion_init_state_get_lists(optimizer_state, group):\n    state_lists = dict(\n        params_with_grad=(params_with_grad := []),\n        grads=(grads := []),\n        exp_avgs=(exp_avgs := []),\n        state_steps=(state_steps := []),\n    )\n    for p in group[\"params\"]:\n        if p.grad is None:\n            continue\n        params_with_grad.append(p)\n        if p.grad.is_sparse:\n            raise RuntimeError(\"AdamW does not support sparse gradients\")\n        grads.append(p.grad)\n        state = optimizer_state[p]\n        if len(state) == 0:\n            state[\"step\"] = torch.tensor(0.0)\n            state[\"exp_avg\"] = torch.zeros_like(\n                p, memory_format=torch.preserve_format\n            )\n        exp_avgs.append(state[\"exp_avg\"])\n        state_steps.append(state[\"step\"])\n    return state_lists\ndef lion_get_update(state_lists, group):\n    params = state_lists['params_with_grad']\n    grads = state_lists['grads']\n    exp_avgs = state_lists['exp_avgs']\n    state_steps = state_lists['state_steps']\n    beta1, beta2 = group['betas']\n    weight_decay = group['weight_decay']\n    eps = group['eps']\n    unscaled_delta_grads = dict()\n    unscaled_delta_lambdas = dict()\n    if len(params) == 0:\n        return dict(), dict()\n    grouped_tensors = _group_tensors_by_device_and_dtype([\n        params, grads, exp_avgs, state_steps])\n    for ((\n        device_params,\n        device_grads,\n        device_exp_avgs,\n        device_state_steps,\n    ), _) in grouped_tensors.values():\n        torch._foreach_add_(device_state_steps, 1)\n        if len(device_params) == 0:\n            return\n        device_grads = [torch.view_as_real(x) if torch.is_complex(x) else x for x in device_grads]\n        device_exp_avgs = [torch.view_as_real(x) if torch.is_complex(x) else x for x in device_exp_avgs]\n        device_params = [torch.view_as_real(x) if torch.is_complex(x) else x for x in device_params]\n        updates = torch._foreach_mul(device_exp_avgs, beta1)\n        torch._foreach_add_(updates, device_grads, alpha=1 - beta1)\n        unscaled_updates = [u.sign() for u in updates]\n        torch._foreach_mul_(device_exp_avgs, beta2)\n        torch._foreach_add_(device_exp_avgs, device_grads, alpha=1 - beta2)\n        unscaled_delta_wd_list = torch._foreach_mul(device_params, weight_decay)\n        for param, udg, udl in zip(device_params, unscaled_updates, unscaled_delta_wd_list):\n            unscaled_delta_grads[param] = udg\n            unscaled_delta_lambdas[param] = udl\n    return unscaled_delta_grads, unscaled_delta_lambdas\ndef lion_etar_func(group):\n    lr = group['lr']\n    wd = group['weight_decay']\n    beta1 = group['betas'][0]\n    beta2 = group['betas'][1]\n    return (math.pi/2.0)**0.5 * (2*lr*wd)**0.5 * ((1-beta1)**2 + beta1**2*(1-beta2)/(1+beta2))**0.5\n\ndef _get_value(x):\n    if not torch.jit.is_scripting() and torch._utils.is_compiling():\n        return x\n    else:\n        return x.item()\ndef _stack_if_compiling(x):\n    if not torch.jit.is_scripting() and torch._utils.is_compiling()():\n        return torch.stack(x)\n    else:\n        return x\ndef _dispatch_sqrt(x: float):\n    if not torch.jit.is_scripting() and isinstance(x, torch.Tensor):\n        return x.sqrt()\n    else:\n        return math.sqrt(x)\n\nimport torch.nn as nn\nclass SLinear(torch.nn.Linear):\n    def __init__(self, in_features, out_features, bias=True, device=None, dtype=None):\n        super().__init__(in_features, out_features, bias=bias, device=device, dtype=dtype)\n        self.gain = nn.Parameter(torch.ones(out_features, device=device, dtype=dtype))\n    def forward(self, x):\n        weight = self.weight\n        weight = self.gain.view(-1, 1) * weight\n        return torch.nn.functional.linear(x, weight, self.bias)",
        "experimental_info": "Model: Transformer (fairseq transformer_iwslt_de_en architecture)\nOptimizer: rvwrapper (Rotational Optimizer Variant) with inner_type=adamw and etar_func=adamw\nLearning Rate: 0.0005\nWeight Decays: [0.0001, 0.4]\nAdam Betas: (0.9, 0.98)\nClip Norm: 0.0\nLR Scheduler: cosine with warmup_updates=4000\nDropout: 0.3\nCriterion: label_smoothed_cross_entropy with label_smoothing=0.1\nMax Tokens: 4096\nEvaluation: BLEU score with beam=5, max_len_a=1.2, max_len_b=10, detokenized by moses, BPE removed, print samples\nBest Checkpoint Metric: bleu, maximized\nLinear Layer Configuration: slinear (Scale-invariant linear layer with learnable gain)\nMax Update: 22021\nAMP (Automatic Mixed Precision): Enabled\nWeight Decay Zero Bias: Not explicitly enabled/disabled, but `--weight-decay-zero-bias` is present in other non-RV adamw configs, not this one.\nWandB Project: constraining-smd\nSeeds: [0, 1, 2]\nEnvironment Variables: base_dir, lr, wd, iter, seed, output_dir (~/data/runs/fairseq), iwslt14 (data path), cf_linear (slinear)\nData: IWSLT2014 tokenized German-English"
      }
    },
    {
      "title": "Which Layer is Learning Faster? A Systematic Exploration of Layer-wise Convergence Rate for Deep Neural Networks"
    },
    {
      "title": "DP-HyPO: An Adaptive Private Framework for Hyperparameter Optimization",
      "abstract": "Hyperparameter optimization, also known as hyperparameter tuning, is a widely\nrecognized technique for improving model performance. Regrettably, when\ntraining private ML models, many practitioners often overlook the privacy risks\nassociated with hyperparameter optimization, which could potentially expose\nsensitive information about the underlying dataset. Currently, the sole\nexisting approach to allow privacy-preserving hyperparameter optimization is to\nuniformly and randomly select hyperparameters for a number of runs,\nsubsequently reporting the best-performing hyperparameter. In contrast, in\nnon-private settings, practitioners commonly utilize ``adaptive''\nhyperparameter optimization methods such as Gaussian process-based\noptimization, which select the next candidate based on information gathered\nfrom previous outputs. This substantial contrast between private and\nnon-private hyperparameter optimization underscores a critical concern. In our\npaper, we introduce DP-HyPO, a pioneering framework for ``adaptive'' private\nhyperparameter optimization, aiming to bridge the gap between private and\nnon-private hyperparameter optimization. To accomplish this, we provide a\ncomprehensive differential privacy analysis of our framework. Furthermore, we\nempirically demonstrate the effectiveness of DP-HyPO on a diverse set of\nreal-world datasets.",
      "full_text": "DP-HyPO: An Adaptive Private Hyperparameter Optimization Framework Hua Wang∗ Sheng Gao† Huanyu Zhang‡ Weijie J. Su§ Milan Shen¶ November 28, 2023 Abstract Hyperparameter optimization, also known as hyperparameter tuning, is a widely recognized technique for improving model performance. Regrettably, when training private ML models, many practitioners often overlook the privacy risks associated with hyperparameter optimization, which could potentially expose sensitive information about the underlying dataset. Currently, the sole existing approach to allow privacy-preserving hyperparameter optimization is to uniformly and randomly select hyperparameters for a number of runs, subsequently reporting the best- performing hyperparameter. In contrast, in non-private settings, practitioners commonly utilize “adaptive” hyperparameter optimization methods such as Gaussian process-based optimization, which select the next candidate based on information gathered from previous outputs. This substantial contrast between private and non-private hyperparameter optimization underscores a critical concern. In our paper, we introduce DP-HyPO, a pioneering framework for “adaptive” private hyperparameter optimization, aiming to bridge the gap between private and non-private hyperparameter optimization. To accomplish this, we provide a comprehensive differential privacy analysis of our framework. Furthermore, we empirically demonstrate the effectiveness of DP-HyPO on a diverse set of real-world datasets. 1 Introduction In recent decades, modern deep learning has demonstrated remarkable advancements in various applications. Nonetheless, numerous training tasks involve the utilization of sensitive information pertaining to individuals, giving rise to substantial concerns regarding privacy [31, 7]. To address these concerns, the concept of differential privacy (DP) was introduced by [13, 14]. DP provides a mathematically rigorous framework for quantifying privacy leakage, and it has gained widespread acceptance as the most reliable approach for formally evaluating the privacy guarantees of machine learning algorithms. ∗Department of Statistics and Data Science, University of Pennsylvania, Philadelphia, PA 19104, USA. Email: wanghua@wharton.upenn.edu. †Department of Statistics and Data Science, University of Pennsylvania, Philadelphia, PA 19104, USA. Email: shenggao@wharton.upenn.edu. ‡Meta Platforms, Inc., New York, NY 10003, USA. Email:huanyuzhang@meta.com. §Department of Statistics and Data Science, University of Pennsylvania, Philadelphia, PA 19104, USA. Email: suw@wharton.upenn.edu. ¶Meta Platforms, Inc., Menlo Park, CA 94025, USA. Email:milanshen@gmail.com. 1 arXiv:2306.05734v2  [cs.LG]  27 Nov 2023When training deep learning models, the most popular method to ensure privacy is noisy (stochastic) gradient descent (DP-SGD) [4, 37]. DP-SGD typically resembles non-private gradient- based methods; however, it incorporates gradient clipping and noise injection. More specifically, each individual gradient is clipped to ensure a boundedℓ2 norm. Gaussian noise is then added to the average gradient which is utilized to update the model parameters. These adjustments guarantee a bounded sensitivity of each update, thereby enforcing DP through the introduction of additional noise. In both non-private and private settings, hyperparameter optimization (HPO) plays a crucial role in achieving optimal model performance. Commonly used methods for HPO include grid search (GS), random search (RS), and Bayesian optimization (BO). GS and RS approaches are typically non-adaptive, as they select the best hyperparameter from a predetermined or randomly selected set. While these methods are straightforward to implement, they can be computationally expensive and inefficient when dealing with large search spaces. As the dimensionality of hyperparameters increases, the number of potential trials may grow exponentially. To address this challenge, adaptive HPO methods such as Bayesian optimization have been introduced [36, 15, 42]. BO leverages a probabilistic model that maps hyperparameters to objective metrics, striking a balance between exploration and exploitation. BO quickly emerged as the default method for complex HPO tasks, offering improved efficiency and effectiveness compared to non-adaptive methods. While HPO is a well-studied problem, the integration of a DP constraint into HPO has received little attention. Previous works on DP machine learning often neglect to account for the privacy cost associated with HPO [1, 41, 44, 44]. These works either assume that the best parameters are known in advance or rely on a supplementary public dataset that closely resembles the private dataset distribution, which is not feasible in most real-world scenarios. Only recently have researchers turned to the important concept of honest HPO [30], where the privacy cost during HPO cannot be overlooked. Private HPO poses greater challenges compared to the non-private case for two primary reasons. First, learning with DP-SGD introduces additional hyperparameters (e.g., clipping norm, the noise scale, and stopping time), which hugely adds complexity to the search for optimal hyperparameters. Second, DP-SGD is more sensitive to the selection of hyperparameter combinations, with its performance largely influenced by this choice [30, 11, 33]. To tackle this challenging question, previous studies such as [26, 34] propose running the base algorithm with different hyperparameters a random number of times. They demonstrate that this approach significantly benefits privacy accounting, contrary to the traditional scaling of privacy guarantees with the square root of the number of runs (based on the composition properties from [21]). While these papers make valuable contributions, their approaches only allow for uniformly random subsampling from a finite and pre-fixed set of candidate hyperparameters at each run. As a result, any advanced technique from HPO literature that requires adaptivity is either prohibited or necessitates a considerable privacy cost (polynomially dependent on the number of runs), creating a substantial gap between non-private and private HPO methods. Given these considerations, a natural question arises:Can private hyperparameter optimization be adaptive, without a huge privacy cost?In this paper, we provide an affirmative answer to this question. 1.1 Our Contributions • We introduce the pioneering adaptive private hyperparameter optimization frame- work, DP-HyPO, which enables practitioners to adapt to previous runs and focus on 2potentially superior hyperparameters. DP-HyPO permits the flexible use of non-DP adaptive hyperparameter optimization methods, such as Gaussian process, for enhanced efficiency, while avoiding the substantial privacy costs due to composition. In contrast to the non-adaptive methods presented in [34, 26], our adaptive framework, DP-HyPO, effectively bridges the gap between private and non-private hyperparameter optimization. Importantly, our framework not only encompasses the aforementioned non-adaptive methods as special cases, but also seamlessly integrates virtually all conceivable adaptive methods into the framework. • We provide sharp DP guarantees for the adaptive private hyperparameter optimiza- tion. Specifically, when the training procedure is executed multiple times, with each iteration being DP on its own, outputting the best repetition is DP ensured by the composition property. However, applying composition results in excessively loose privacy guarantees. Prior work in [26, 34] presents bounds that are either independent of the number of repetitions or depend logarithmically on it. Nevertheless, these results require that the hyperparameter selection for each iteration follows a uniform sampling distribution. In contrast, DP-HyPO allows arbitrary adaptive sampling distributions based on previous runs. Utilizing the Rényi DP framework, we offer a strict generalization of those uniform results by providing an accurate characterization of the Rényi divergence between the adaptive sampling distributions of neighboring datasets, without any stability assumptions. • Empirically, we observe that the Gaussian process-based DP-HyPO algorithm outperforms its uniform counterpart across several practical scenarios.Gener- ally, practitioners can integrate any non-private adaptive HPO methods into the DP-HyPO framework, opening up a vast range of adaptive private HPO algorithm possibilities. Further- more, DP-HyPO grants practitioners the flexibility to determine the privacy budget allocation for adaptivity, empowering them to balance between the adaptivity and privacy loss when confronting various hyperparameter optimization challenges. 1.2 Our Contributions • We introduce the pioneering adaptive private hyperparameter optimization frame- work, DP-HyPO, which enables practitioners to adapt to previous runs and focus on potentially superior hyperparameters. DP-HyPO permits the flexible use of non-DP adaptive hyperparameter optimization methods, such as Gaussian process, for enhanced efficiency, while avoiding the substantial privacy costs due to composition. In contrast to the non-adaptive methods presented in [34, 26], our adaptive framework, DP-HyPO, effectively bridges the gap between private and non-private hyperparameter optimization. Importantly, our framework not only encompasses the aforementioned non-adaptive methods as special cases, but also seamlessly integrates virtually all conceivable adaptive methods into the framework. • We provide sharp DP guarantees for the adaptive private hyperparameter optimiza- tion. Specifically, when the training procedure is executed multiple times, with each iteration being DP on its own, outputting the best repetition is DP ensured by the composition property. However, applying composition results in excessively loose privacy guarantees. Prior work in [26, 34] presents bounds that are either independent of the number of repetitions or depend logarithmically on it. Nevertheless, these results require that the hyperparameter selection for each iteration follows a uniform sampling distribution. In contrast, DP-HyPO allows arbitrary 3adaptive sampling distributions based on previous runs. Utilizing the Rényi DP framework, we offer a strict generalization of those uniform results by providing an accurate characterization of the Rényi divergence between the adaptive sampling distributions of neighboring datasets, without any stability assumptions. • Empirically, we observe that the Gaussian process-based DP-HyPO algorithm out- performs its uniform counterpartacross several practical scenarios. Generally, practitioners can integrate any non-private adaptive HPO methods into the DP-HyPO framework, opening up a vast range of adaptive private HPO algorithm possibilities. Furthermore, DP-HyPO grants practitioners the flexibility to determine the privacy budget allocation for adaptivity, empowering them to balance between the adaptivity and privacy loss when confronting various hyperparameter optimization challenges. 2 Preliminaries 2.1 Differential Privacy and Hyperparameter Optimization Differential Privacy is a mathematically rigorous framework for quantifying privacy leakage. A DP algorithm promises that an adversary with perfect information about the entire private dataset in use – except for a single individual – would find it hard to distinguish between its presence or absence based on the output of the algorithm [13]. Formally, forε >0, and0 ≤ δ <1, we consider a (randomized) algorithmM : Zn → Ythat takes as input a dataset. Definition 2.1(Differential privacy). A randomized algorithmM is (ε, δ)-DP if for any neighboring dataset D, D′ ∈ Zn differing by an arbitrary sample, and for any eventE, we have P[M(D) ∈ E] ⩽ eε · P \u0002 M \u0000 D′\u0001 ∈ E \u0003 + δ. Here, ε and δ are privacy parameters that characterize the privacy guarantee of algorithmM. One of the fundamental properties of DP is composition. When multiple DP algorithms are sequentially composed, the resulting algorithm remains private. The total privacy cost of the composition scales approximately with the square root of the number of compositions [21]. We now formalize the problem of hyperparameter optimization with DP guarantees, which builds upon the finite-candidate framework presented in [26, 34]. Specifically, we consider a set of base DP algorithms Mλ : Zn → Y, whereλ ∈ Λ represents a set of hyperparameters of interest,Zn is the domain of datasets, andY denotes the range of the algorithms. This setΛ may be any infinite set, e.g., the cross product of the learning rateη and clipping normR in DP-SGD. We require that the set Λ is a measure space with an associated measureµ. Common choices forµ include the counting measure or Lebesgue measure. We make a mild assumption thatµ(Λ) < ∞. Based on the previous research [34], we make two simplifying assumptions. First, we assume that there is a total ordering on the rangeY, which allows us to compare two selected models based on their “performance measure”, denoted byq. Second, we assume that, for hyperparameter optimization purposes, we output the trained model, the hyperparameter, and the performance measure. Specifically, for any input datasetD and hyperparameterλ, the return value ofMλ is (x, q) ∼ Mλ(D), wherex represents the combination of the model parameters and the hyperparameter λ, andq is the (noisy) performance measure of the model. 42.2 Related Work In this section, we focus on related work concerning private HPO, while deferring the discussion on non-private HPO to Appendix F. Historically, research in DP machine learning has neglected the privacy cost associated with HPO [1, 41, 44]. It is only recently that researchers have begun to consider the honest HPO setting [30], in which the cost is taken into account. A direct approach to addressing this issue involves composition-based analysis. If each training run of a hyperparameter satisfies DP, the entire HPO procedure also complies with DP through composition across all attempted hyperparameter values. However, the challenge with this method is that the privacy guarantee derived from accounting can be excessively loose, scaling polynomially with the number of runs. Chaudhuri et al. [8] were the first to enhance the DP bounds for HPO by introducing additional stability assumptions on the learning algorithms. [26] made significant progress in enhancing DP bounds for HPO without relying on any stability properties of the learning algorithms. They proposed a simple procedure where a hyperparameter was randomly selected from a uniform distribution for each training run. This selection process was repeated a random number of times according to a geometric distribution, and the best model obtained from these runs was outputted. They showed that this procedure satisfied(3ε, 0)-DP as long as each training run of a hyperparameter was (ε, 0)-DP. Building upon this, [34] extended the procedure to accommodate negative binomial or Poisson distributions for the repeated uniform selection. They also offered more precise Rényi DP guarantees for this extended procedure. Furthermore, [9] explored a generalization of the procedure for top-k selection, considering (ε, δ)-DP guarantees. In a related context, [30] explored a setting that appeared superficially similar to ours, as their title mentioned “adaptivity.” However, their primary focus was on improving adaptive optimizers such as DP-Adam, which aimed to reduce the necessity of hyperparameter tuning, rather than the adaptive HPO discussed in this paper. Notably, in terms of privacy accounting, their approach only involved composing the privacy cost of each run without proposing any new method. Another relevant area of research is DP selection, which encompasses well-known methods such as the exponential mechanism [27] and the sparse vector technique [14], along with subsequent studies (e.g., [6] and [17]). However, this line of research always assumes the existence of a low- sensitivity score function for each candidate, which is an unrealistic assumption for hyperparameter optimization. 3 DP-HyPO: General Framework for Private Hyperparameter Op- timization The obvious approach to the problem of differentially private hyperparameter optimization would be to run each base algorithm and simply return the best one. However, running such an algorithm on large hyperparameter space is not feasible due to the privacy cost growing linearly in the worst case. While [26, 34] have successfully reduced the privacy cost for hyperparameter optimization from linear to constant, there are still two major drawbacks. First, none of the previous methods considers the case when the potential number of hyperparameter candidates is infinite, which is common in most hyperparameter optimization scenarios. In fact, we typically start with a range of hyperparameters that we are interested in, rather than a discrete set of candidates. Furthermore, prior methods are 5limited to the uniform sampling scheme over the hyperparameter domainΛ. In practice, this setting is unrealistic since we want to “adapt” the selection based on previous results. For instance, one could use Gaussian process to adaptively choose the next hyperparameter for evaluation, based on all the previous outputs. However, no adaptive hyperparameter optimization method has been proposed or analyzed under the DP constraint. In this paper, we bridge this gap by introducing the first DP adaptive hyperparameter optimization framework. 3.1 DP-HyPO Framework To achieve adaptive hyperparameter optimization with differential privacy, we propose the DP-HyPO framework. Our approach keeps an adaptive sampling distributionπ at each iteration that reflects accumulated information. Let Q(D, π) be the procedure that randomly draws a hyperparameterλ from the distribution1 π ∈ D(Λ) , and then returns the output fromMλ(D). We allow the sampling distribution to depend on both the dataset and previous outputs, and we denote asπ(j) the sampling distribution at thej-th iteration on datasetD. Similarly, the sampling distribution at thej-th iteration on the neighborhood dataset D′ is denoted asπ′(j). We now present the DP-HyPO framework, denoted asA(D, π(0), T , C, c), in Framework 1. The algorithm takes a prior distributionπ(0) ∈ D(Λ) as input, which reflects arbitrary prior knowledge about the hyperparameter space. Another input is the distributionT of the total repetitions of training runs. Importantly, we require it to be a random variable rather than a fixed number to preserve privacy. The last two inputs areC and c, which are upper and lower bounds of the density of any posterior sampling distributions. A finiteC and a positivec are required to bound the privacy cost of the entire framework. Framework 1DP-HyPO A(D, π(0), T , C, c) Initialize π(0), a prior distribution overΛ. Initialize the result setA = {} Draw T ∼ T for j = 0 to T − 1 do (x, q) ∼ Q(D, π(j)) A = A ∪ {(x, q)} Update π(j+1) based onA according to any adaptive algorithm such that for allλ ∈ Λ, c ≤ π(j+1)(λ) π(0)(λ) ≤ C Output (x, q) from A with the highestq Note that we intentionally leave the update rule forπ(j+1) unspecified in Framework 1 to reflect the fact that any adaptive update rule that leverages information from previous runs can be used. However, for a non-private adaptive HPO update rule, the requirement of bounded adaptive density c ≤ π(j+1)(λ) π(0)(λ) ≤ C may be easily violated. In Section 3.2, We provide a simple projection technique 1Here, D(Λ) represents the space of probability densities onΛ. 6to privatize any non-private update rules. In Section 4, we provide an instantiation of DP-HyPO using Gaussian process. We now state our main privacy results for this framework in terms of Rényi Differential Privacy (RDP) [29]. RDP is a privacy measure that is more general than the commonly used(ε, δ)-DP and provides tighter privacy bounds for composition. We defer its exact definition to Definition A.2 in the appendix. We note that different distributions of the number of selections (iterations),T , result in very different privacy guarantees. Here, we showcase the key idea for deriving the privacy guarantee of DP-HyPO framework by considering a special case whenT follows a truncated negative binomial distribution2 NegBin(θ, γ) (the same assumption as in [34]). In fact, as we show in the proof of Theorem 1 in Appendix A, the privacy bounds only depend onT directly through its probability generating function, and therefore one can adapt the proof to obtain the corresponding privacy guarantees for other probability families, for example, the Possion distribution considered in [34]. From here and on, unless otherwise specified, we will stick withT = NegBin(θ, γ) for simplicity. We also assume for simplicity that the prior distributionπ(0) is a uniform distribution overΛ. We provide more detailed discussion of handling informed prior other than uniform distribution in Appendix D. Theorem 1. Suppose thatT follows truncated negative Binomial distributionT ∼ NegBin(θ, γ). Let θ ∈ (−1, ∞), γ ∈ (0, 1), and 0 < c≤ C. Suppose for allMλ : Zn → Yover λ ∈ Λ, the base algorithms satisfy(α, ε)-RDP and(ˆα, ˆε)-RDP for someε, ˆε ≥ 0, α∈ (1, ∞), and ˆα ∈ [1, ∞). Then the DP-HyPO algorithmA(D, π(0), NegBin(θ, γ), C, c) satisfies (α, ε′)-RDP where ε′ = ε + (1 +θ) · \u0012 1 − 1 ˆα \u0013 ˆε + \u0012 α α − 1 + 1 +θ \u0013 log C c + (1 + θ) · log(1/γ) ˆα + log E[T] α − 1 . To prove Theorem 1, one of our main technical contributions is Lemma A.4, which quantifies the Rényi divergence of the sampling distribution at each iteration between the neighboring datasets. We then leverage this crucial result and the probability generating function ofT to bound the Rényi divergence in the output ofA. We defer the detailed proof to Appendix A. Next, we present the case with pure DP guarantees. Recall the fact that(ε, 0)-DP is equivalent to (∞, ε)-RDP [29]. When bothα and ˆα tend towards infinity, we easily obtain the following theorem in terms of(ε, 0)-DP. Theorem 2. Suppose thatT follows truncated negative Binomial distributionT ∼ NegBin(θ, γ). Let θ ∈ (−1, ∞) and γ ∈ (0, 1). If all the base algorithmsMλ satisfies (ε, 0)-DP, then the DP-HyPO algorithm A(D, π(0), NegBin(θ, γ), C, c) satisfies \u0000 (2 + θ) \u0000 ε + log C c \u0001 , 0 \u0001 -DP. Theorem 1 and Theorem 2 provide practitioners the freedom to trade off between allocating more DP budget to enhance the base algorithm or to improve adaptivity. In particular, a higher value ofC c signifies greater adaptivity, while a largerε improves the performance of base algorithms. 3.1.1 Uniform Optimization Method as a Special Case We present the uniform hyperparameter optimization method [34, 25] in Algorithm 2, which is a special case of our general DP-HyPO Framework withC = c = 1. Essentially, this algorithm never updates the sampling distributionπ. 2Truncated negative binomial distribution is a direct generalization of the geometric distribution. See Appendix B for its definition. 7Algorithm 2Uniform Hyperparameter OptimizationU(D, θ, γ,Λ) Let π = Unif({1, ...,|Λ|}), andA = {} Draw T ∼ NegBin(θ, γ) for j = 0 to T − 1 do (x, q) ∼ Q(D, π) A = A ∪ {(x, q)} Output (x, q) from A with the highestq Our results in Theorem 1 and Theorem 2 generalize the main technical results of [34, 26]. Specifically, whenC = c = 1 and Λ is a finite discrete set, our Theorem 1 precisely recovers Theorem 2 in [34]. Furthermore, when we setθ = 1, the truncated negative binomial distribution reduces to the geometric distribution, and our Theorem 2 recovers Theorem 3.2 in [26] . 3.2 Practical Recipe to Privatize HPO Algorithms In the DP-HyPO framework, we begin with a prior and adaptively update it based on the accumulated information. However, for privacy purposes, we require the densityπ(j) to be bounded by some constants c and C, which is due to the potential privacy leakage when updatingπ(j) based on the history. It is crucial to note that this distributionπ(j) can be significantly different from the distribution π′(j) if we were given a different input datasetD′. Therefore, we require the probability mass/density function to satisfy c µ(Λ) ≤ π(j)(λ) ≤ C µ(Λ) for allλ ∈ Λ to control the privacy loss due to adaptivity. This requirement is not automatically satisfied and typically necessitates modifications to current non-private HPO methods. To address this challenge, we propose a general recipe to modify any non-private method. The idea is quite straightforward: throughout the algorithm, we maintain a non-private version of the distribution densityπ(j). When sampling from the spaceΛ, we perform a projection from π(j) to the space consisting of bounded densities. Specifically, we define the space of essentially bounded density functions bySC,c = {f ∈ ΛR+ : ess supf ≤ C µ(Λ), ess inff ≥ c µ(Λ), R α∈Λ f(α)dα = 1}. For such a space to be non-empty, we require thatc ≤ 1 ≤ C, where µ is the measure onΛ. This condition is well-defined as we assumeµ(Λ) < ∞. To privatizeπ(j) at thej-th iteration, we project it into the spaceSC,c, by solving the following convex functional programming problem: min f ∥f − π(j)∥2, s.t. f ∈ SC,c. (3.1) Note that this is a convex program sinceSC,c is convex and closed. We denote the output from this optimization problem byPSC,c(π(j)). Theoretically, problem(3.1) allows the hyperparameter space Λ to be general measurable space with arbitrary topological structure. However, empirically, practitioners need to discretizeΛ to some extent to make the convex optimization computationally feasible. Compared to the previous work, our formulation provides the most general characterization of the problem and allows pratitioners toadaptively and iteratively choose a proper discretization as needed. Framework 1 tolerates a much finer level of discretization than the previous method, as the performance of latter degrades fast when the number of candidates increases. We also provide 8examples using CVX to solve this problem in Section 4.2. In Appendix C, we discuss about its practical implementation, and the connection to information projection. 4 Application: DP-HyPO with Gaussian Process In this section, we provide an instantiation of DP-HyPO using Gaussian process (GP) [40]. GPs are popular non-parametric Bayesian models frequently employed for hyperparameter optimization. At the meta-level, GPs are trained to generate surrogate models by establishing a probability distribution over the performance measureq. While traditional GP implementations are not private, we leverage the approach introduced in Section 3.2 to design a private version that adheres to the bounded density contraint. We provide the algorithmic description in Section 4.1 and the empircal evaluation in Section 4.2. 4.1 Algorithm Description The following Algorithm (AGP) is a private version of Gaussian process for hyperparameter tuning. In Algorithm 3, we utilize GP to construct a surrogate model that generates probability distributions Algorithm 3DP-HyPO with Gaussian processAGP(D, θ, γ, τ, β,Λ, C, c) Initialize π(0) = Unif(Λ), andA = {} Draw T ∼ NegBin(θ, γ) for t = 0 to T − 1 do Truncate the density of currentπ(t) to be bounded into the range of[c, C] by projecting to SC,c. ˜π(t) = PSC,c(π(t)). Sample (x, q) ∼ Q(D, ˜π(j)), and updateA = A ∪ {(x, q)} Update mean estimation and variance estimation of the Gaussian processµλ, σ2 λ, and get the score assλ = µλ + τσλ. Update true (untruncated) posteriorπ(t+1) with softmax, byπ(t+1)(λ) = exp(β·sλ)R λ′∈Λ exp(β·s′ λ). Output (x, q) from A with the highestq for the performance measureq. By estimating the mean and variance, we assign a “score” to each hyperparameter λ, known as the estimated upper confidence bound (UCB). The weight factorτ controls the balance between exploration and exploitation, where larger weights prioritize exploration by assigning higher scores to hyperparameters with greater uncertainty. To transform these scores into a sampling distribution, we apply the softmax function across all hyperparameters, incorporating the parameterβ as the inverse temperature. A higher value ofβ signifies increased confidence in the learned scores for each hyperparameter. 4.2 Empirical Evaluations We now evaluate the performance of our GP-based DP-HyPO (referred to as “GP”) in various settings. Since DP-HyPO is the first adaptive private hyperparameter optimization method of its kind, we compare it to the special case of Uniform DP-HyPO (Algorithm 2), referred to as 9“Uniform”, as proposed in [26, 34]. In this demonstration, we consider two pragmatic privacy configurations: the white-box setting and the black-box setting, contingent on whether adaptive HPO algorithms incur extra privacy cost. In the white-box scenario (Section 4.2.1 and 4.2.2), we conduct experiments involving training deep learning models on both the MNIST dataset and CIFAR-10 dataset. Conversely, when considering the black-box setting (Section 4.2.3), our attention shifts to a real-world Federated Learning (FL) task from the industry. These scenarios provide meaningful insights into the effectiveness and applicability of our GP-based DP-HyPO approach. 4.2.1 MNIST Simulation We begin with the white-box scenario, in which the data curator aims to provide overall protection to the published model. In this context, to accommodate adaptive HPO algorithms, it becomes necessary to reduce the budget allocated to the base algorithm. In this section, we consider the MNIST dataset, where we employ DP-SGD to train a standard CNN. The base algorithms in this case are different DP-SGD models with varying hyperparameters, and we evaluate each base algorithm based on its accuracy. Our objective is to identify the best hyperparameters that produce the most optimal model within a given total privacy budget. Specifically, we consider two variable hyperparameters: the learning rateη and clipping normR, while keeping the other parameters fixed. We ensure that both the GP algorithm and the Uniform algorithm operate under the same total privacy budget, guaranteeing a fair comparison. Due to constraints on computational resources, we conduct a semi-real simulation using the MNIST dataset. For both base algorithms (with different noise multipliers), we cache the mean accuracy of5 independently trained models for each discretized hyperparameter and treat that as a proxy for the “actual accuracy” of the hyperparameter. Each time we sample the accuracy of a hyperparameter, we add a Gaussian noise with a standard deviation of0.1 to the cached mean. We evaluate the performance of the output model based on the “actual accuracy” corresponding to the selected hyperparameter. Further details on the simulation and parameter configuration can be found in Appendix E.1. In the left panel of Figure 1, we demonstrated the comparison of performance of the Uniform and GP methods with total privacy budgetε = 153 and δ = 1e − 5. The accuracy reported is the actual accuracy of the output hyperparameter. From the figure, we see that whenT is very small(T <8), GP method is slightly worse than Uniform method as GP spendslog(C/c) budget less than Uniform method for each base algorithm (the cost of adaptivity). However, we see that after a short period of exploration, GP consistently outperform Uniform, mostly due to the power of being adaptive. The superiority of GP is further demonstrated in Table 1, aggregating over geometric distribution. 4.2.2 CIFAR-10 Simulation When examining the results from MNIST, a legitimate critique arises: our DP-Hypo exhibits only marginal superiority over its uniform counterpart, which questions the assertion that adaptivity holds significant value. Our conjecture is that the hyperparameter landscape of MNIST is relatively uncomplicated, which limits the potential benefits of adaptive algorithms. 3The ε values are seemingly very large. Nonetheless, the reported privacy budget encompasses the overall cost of the entire HPO, which is typically overlooked in the existing literature. Given that HPO roughly incurs three times the privacy cost of the base algorithm, anε as high as15 could be reported as only5 in many other works. 10Figure 1: Left: The accuracy of the output hyperparameter in MNIST semi-real simulation, with ε = 15, δ = 0.00001. Middle: The accuracy of the output hyperparameter in CIFAR-10, with ε = 12, δ = 0.00001. Right: The loss of the output hyperparameter in FL. Error bars stands for95% confidence. Curves for GP are calculated by averaging400 independent runs, and curves for Uniform are calculated by averaging10000 independent runs. For a clearer demonstration, we compare the performance for each fixed value ofT, and recognize that the actual performance is a weighted average across different values ofT. To test the hypothesis, we conduct experiments on the CIFAR-10 dataset, with a setup closely mirroring the previous experiment: we employ the same CNN model for training, and optimize the same set of hyperparameters, which are the learning rateη and clipping normR. The primary difference lies in how we generate the hyperparameter landscape. Given that a single run on CIFAR-10 is considerably more time-consuming than on MNIST, conducting multiple runs for every hyperparameter combination is unfeasible. To address this challenge, we leverage BoTorch [3], an open-sourced library for HPO, to generate the landscape. Since we operate in the white-box setting, where the base algorithms have distinct privacy budgets for the uniform and adaptive scenarios, we execute 50 runs and generate the landscape for each case, including the mean (µλ) and standard error (σλ) of accuracy for each hyperparameter combinationλ. When the algorithm (GP or Uniform) visits a specificλ, our oracle returns a noisy scoreq(λ) drawn from a normal distribution of N(µλ, σλ). A more detailed description of our landscapes and parameter configuration can be found in Appendix E.2. In the middle of Figure 1, we showcase a performance comparison between the Uniform and GP methods with a total privacy budget ofε = 12 and δ = 1e − 5. Clearly, GP consistently outperforms the Uniform method, with the largest performance gap occurring when the number of runs is around 10. 4.2.3 Federated Learning In this section, we move to the black-box setting, where the privacy budget allocated to the base algorithm remains fixed, while we allow extra privacy budget for HPO. That being said, the adaptivity can be achieved without compromising the utility of the base algorithm. We explore another real-world scenario: a Federated Learning (FL) task conducted on a propri- etary dataset4 from industry. Our aim is to determine the optimal learning rates for the central server (using AdaGrad) and the individual users (using SGD). To simulate this scenario, we once again rely on the landscape generated by BoTorch [3], as shown in Figure 3 in Appendix E.3. 4We have to respect confidentiality constraints that limit our ability to provide extensive details about this dataset. 11Under the assumption that base algorithms are black-box models with fixed privacy costs, we proceed with HPO while varying the degree of adaptivity. The experiment results are visualized in the right panel of Figure 1, and Table 2 presents the aggregated performance data. We consistently observe that GP outperforms Uniform in the black-box setting. Furthermore, our findings suggest that allocating a larger privacy budget to the GP method facilitates the acquisition of adaptive information, resulting in improved performance in HPO. This highlights the flexibility of GP in utilizing privacy resources effectively. Geometric(γ) 0.001 0.002 0.003 0.005 0.01 0.02 0.025 0.03 GP 0.946 0.948 0.948 0.947 0.943 0.937 0.934 0.932 Uniform 0.943 0.945 0.945 0.944 0.940 0.935 0.932 0.929 Table 1:Accuracy of MNIST using Geometric Distribution with various different values ofγ for Uniform and GP methods. Each number is the mean of200 runs. Geometric(γ) 0.001 0.002 0.003 0.005 0.01 0.02 0.025 0.03 GP (C = 1.25) 0.00853 0.0088 0.00906 0.00958 0.0108 0.0129 0.0138 0.0146 GP (C = 1.33) 0.00821 0.00847 0.00872 0.00921 0.0104 0.0123 0.0132 0.0140 GP (C = 1.5) 0.00822 0.00848 0.00872 0.00920 0.0103 0.0123 0.0131 0.0130 Uniform 0.0104 0.0106 0.0109 0.0113 0.0123 0.0141 0.0149 0.0156 Table 2:Loss of FL using Geometric Distribution with various different values ofγ for Uniform and GP methods with different choice ofC and c = 1/C. Each number is the mean of200 runs. 5 Conclusion In conclusion, this paper presents a novel framework, DP-HyPO. As the first adaptive HPO framework with sharp DP guarantees, DP-HyPO effectively bridges the gap between private and non-private HPO. Our work encompasses the random search method by [26, 34] as a special case, while also granting practitioners the ability to adaptively learn better sampling distributions based on previous runs. Importantly, DP-HyPO enables the conversion of any non-private adaptive HPO algorithm into a private one. Our framework proves to be a powerful tool for professionals seeking optimal model performance and robust DP guarantees. The DP-HyPO framework presents two interesting future directions. One prospect involves an alternative HPO specification which is practically more favorable. Considering the extensive literature on HPO, there is a significant potential to improve the empirical performance by leveraging more advanced HPO methods. Secondly, there is an interest in establishing a theoretical utility guarantee for DP-HyPO. By leveraging similar proof methodologies to those in Theorem 3.3 in [26], it is feasible to provide basic utility guarantees for the general DP-HyPO, or for some specific configurations within DP-HyPO. 126 Acknowledgements The authors would like to thank Max Balandat for his thoughtful comments and insights that helped us improve the paper. References [1] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. InProceedings of the 2016 ACM SIGSAC conference on computer and communications security, pages 308–318, 2016. [2] Martin S Andersen, Joachim Dahl, Lieven Vandenberghe, et al. Cvxopt: A python package for convex optimization.Available at cvxopt. org, 54, 2013. [3] Maximilian Balandat, Brian Karrer, Daniel Jiang, Samuel Daulton, Ben Letham, Andrew G Wilson, and Eytan Bakshy. Botorch: A framework for efficient monte-carlo bayesian optimization. Advances in neural information processing systems, 33:21524–21538, 2020. [4] Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient algorithms and tight error bounds. In2014 IEEE 55th annual symposium on foundations of computer science, pages 464–473. IEEE, 2014. [5] James Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. Algorithms for hyper- parameter optimization. Advances in neural information processing systems, 24, 2011. [6] Mark Bun, Gautam Kamath, Thomas Steinke, and Steven Z Wu. Private hypothesis selection. Advances in Neural Information Processing Systems, 32, 2019. [7] Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer: Evaluating and testing unintended memorization in neural networks. InUSENIX Security Symposium, volume 267, 2019. [8] Kamalika Chaudhuri and Staal A Vinterbo. A stability-based validation procedure for differ- entially private machine learning.Advances in Neural Information Processing Systems, 26, 2013. [9] Edith Cohen, Xin Lyu, Jelani Nelson, Tamás Sarlós, and Uri Stemmer. Generalized private selection and testing with high confidence.arXiv preprint arXiv:2211.12063, 2022. [10] Imre Csiszár and Frantisek Matus. Information projections revisited.IEEE Transactions on Information Theory, 49(6):1474–1490, 2003. [11] Soham De, Leonard Berrada, Jamie Hayes, Samuel L Smith, and Borja Balle. Unlock- ing high-accuracy differentially private image classification through scale. arXiv preprint arXiv:2204.13650, 2022. [12] Jinshuo Dong, Aaron Roth, and Weijie J Su. Gaussian differential privacy.Journal of the Royal Statistical Society Series B: Statistical Methodology, 84(1):3–37, 2022. 13[13] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. InTheory of Cryptography: Third Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3, pages 265–284. Springer, 2006. [14] Cynthia Dwork, Moni Naor, Omer Reingold, Guy N Rothblum, and Salil Vadhan. On the complexity of differentially private data release: efficient algorithms and hardness results. In Proceedings of the forty-first annual ACM symposium on Theory of computing, pages 381–390, 2009. [15] Matthias Feurer and Frank Hutter. Hyperparameter optimization.Automated machine learning: Methods, systems, challenges, pages 3–33, 2019. [16] Yonatan Geifman and Ran El-Yaniv. Deep active learning with a neural architecture search. Advances in Neural Information Processing Systems, 32, 2019. [17] Sivakanth Gopi, Gautam Kamath, Janardhan Kulkarni, Aleksandar Nikolov, Zhiwei Steven Wu, and Huanyu Zhang. Locally private hypothesis selection. InConference on Learning Theory, pages 1785–1816. PMLR, 2020. [18] Xin He, Kaiyong Zhao, and Xiaowen Chu. Automl: A survey of the state-of-the-art.Knowledge- Based Systems, 212:106622, 2021. [19] Andrew Hundt, Varun Jain, and Gregory D Hager. sharpdarts: Faster and more accurate differentiable architecture search.arXiv preprint arXiv:1903.09900, 2019. [20] Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Sequential model-based optimization for general algorithm configuration. InLearning and Intelligent Optimization: 5th International Conference, LION 5, Rome, Italy, January 17-21, 2011. Selected Papers 5, pages 507–523. Springer, 2011. [21] Peter Kairouz, Sewoong Oh, and Pramod Viswanath. The composition theorem for differential privacy. InInternational conference on machine learning, pages 1376–1385. PMLR, 2015. [22] Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabas Poczos, and Eric P Xing. Neural architecture search with bayesian optimisation and optimal transport.Advances in neural information processing systems, 31, 2018. [23] Rajiv Khanna, Joydeep Ghosh, Rusell Poldrack, and Oluwasanmi Koyejo. Information projection and approximate inference for structured sparse variables. InArtificial Intelligence and Statistics, pages 1358–1366. PMLR, 2017. [24] Liam Li and Ameet Talwalkar. Random search and reproducibility for neural architecture search. InUncertainty in artificial intelligence, pages 367–377. PMLR, 2020. [25] Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyper- band: A novel bandit-based approach to hyperparameter optimization.The Journal of Machine Learning Research, 18(1):6765–6816, 2017. [26] Jingcheng Liu and Kunal Talwar. Private selection from private candidates. InProceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing, pages 298–309, 2019. 14[27] Frank McSherry and Kunal Talwar. Mechanism design via differential privacy. In48th Annual IEEE Symposium on Foundations of Computer Science (FOCS’07), pages 94–103. IEEE, 2007. [28] Hector Mendoza, Aaron Klein, Matthias Feurer, Jost Tobias Springenberg, and Frank Hutter. Towards automatically-tuned neural networks. InWorkshop on automatic machine learning, pages 58–65. PMLR, 2016. [29] Ilya Mironov. Rényi differential privacy. In2017 IEEE 30th computer security foundations symposium (CSF), pages 263–275. IEEE, 2017. [30] Shubhankar Mohapatra, Sajin Sasy, Xi He, Gautam Kamath, and Om Thakkar. The role of adaptive optimizers for honest private hyperparameter selection. InProceedings of the aaai conference on artificial intelligence, volume 36, pages 7806–7813, 2022. [31] Milad Nasr, Reza Shokri, and Amir Houmansadr. Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning. In 2019 IEEE symposium on security and privacy (SP), pages 739–753. IEEE, 2019. [32] Renato Negrinho, Matthew Gormley, Geoffrey J Gordon, Darshan Patil, Nghia Le, and Daniel Ferreira. Towards modular and programmable architecture search.Advances in neural informa- tion processing systems, 32, 2019. [33] Ashwinee Panda, Xinyu Tang, Vikash Sehwag, Saeed Mahloujifar, and Prateek Mittal. Dp-raft: A differentially private recipe for accelerated fine-tuning.arXiv preprint arXiv:2212.04486, 2022. [34] Nicolas Papernot and Thomas Steinke. Hyperparameter tuning with renyi differential privacy. In International Conference on Learning Representations, 2021. [35] Carl Edward Rasmussen. Gaussian processes in machine learning. In Advanced Lectures on Machine Learning: ML Summer Schools 2003, Canberra, Australia, February 2-14, 2003, Tübingen, Germany, August 4-16, 2003, Revised Lectures, pages 63–71. Springer, 2004. [36] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando De Freitas. Taking the human out of the loop: A review of bayesian optimization.Proceedings of the IEEE, 104(1):148–175, 2015. [37] Shuang Song, Kamalika Chaudhuri, and Anand D Sarwate. Stochastic gradient descent with differentially private updates. In 2013 IEEE global conference on signal and information processing, pages 245–248. IEEE, 2013. [38] Salil Vadhan. The complexity of differential privacy.Tutorials on the Foundations of Cryptogra- phy: Dedicated to Oded Goldreich, pages 347–450, 2017. [39] Hua Wang, Sheng Gao, Huanyu Zhang, Milan Shen, and Weijie J Su. Analytical composition of differential privacy via the edgeworth accountant.arXiv preprint arXiv:2206.04236, 2022. [40] Christopher KI Williams and Carl Edward Rasmussen.Gaussian processes for machine learning, volume 2. MIT press Cambridge, MA, 2006. 15[41] Da Yu, Huishuai Zhang, Wei Chen, Jian Yin, and Tie-Yan Liu. Large scale private learning via low-rank reparametrization. In International Conference on Machine Learning, pages 12208–12218. PMLR, 2021. [42] Tong Yu and Hong Zhu. Hyper-parameter optimization: A review of algorithms and applications. arXiv preprint arXiv:2003.05689, 2020. [43] Arber Zela, Aaron Klein, Stefan Falkner, and Frank Hutter. Towards automated deep learning: Efficient joint neural architecture and hyperparameter search.arXiv preprint arXiv:1807.06906, 2018. [44] Huanyu Zhang, Ilya Mironov, and Meisam Hejazinia. Wide network learning with differential privacy. arXiv preprint arXiv:2103.01294, 2021. 16A Proofs of the technical results A.1 Proof of Main Results First, we define Rényi divergence as follows. Definition A.1(Rényi Divergences). Let P and Q be probability distributions on a common space Ω. Assume thatP is absolutely continuous with respect toQ - i.e., for all measurableE ⊂ Ω, if Q(E) = 0, thenP(E) = 0. Let P(x) and Q(x) denote the densities ofP and Q respectively. The KL divergence fromP to Q is defined as D1(P∥Q) := E X←P \u0014 log \u0012P(X) Q(X) \u0013\u0015 = Z Ω P(x) log \u0012P(x) Q(x) \u0013 dx. The max divergence fromP to Q is defined as D∞(P∥Q) := sup \u001a log \u0012P(E) Q(E) \u0013 : P(E) > 0 \u001b . For α ∈ (1, ∞), the Rényi divergence fromP to Q of orderα is defined as Dα(P∥Q) := 1 α − 1 log   E X←P \"\u0012P(X) Q(X) \u0013α−1#! = 1 α − 1 log \u0012 E X←Q \u0014\u0012P(X) Q(X) \u0013α\u0015\u0013 = 1 α − 1 log \u0012Z Q P(x)αQ(x)1−αdx \u0013 . We now present the definition of Rényi DP (RDP) in [29]. Definition A.2(Rényi Differential Privacy). A randomized algorithmM : Xn → Yis (α, ε)-Rényi differentially private if, for all neighbouring pairs of inputsD, D′ ∈ Xn, Dα (M(x)∥M (x′)) ≤ ε. We define some additional notations for the sake of the proofs. In algorithm 1, for any1 ≤ j ≤ T, and neighboring datasetD and D′, we define the following notations for anyy = (x, q) ∈ Y, the totally ordered range set. Pj(y) = P˜y∼Q(D,π(j))(˜y = y) and P′ j(y) = P˜y∼Q(D′,π′(j))(˜y = y) Pj(≤ y) = P˜y∼Q(D,π(j))(˜y ≤ y) and P′ j(≤ y) = P˜y∼Q(D′,π′(j))(˜y ≤ y) Pj(< y) = P˜y∼Q(D,π(j))(˜y < y) and P′ j(< y) = P˜y∼Q(D′,π′(j))(˜y < y). By these definitions, we havePj(≤ y) = Pj(< y) + Pj(y), andP′ j(≤ y) = P′ j(< y) + P′ j(y). And additionally, we have Pj(y) P′ j(y) = R λ∈Λ P(Mλ(D) = y)π(j)(λ)dλR λ∈Λ P(Mλ(D′) = y)π′(j)(λ)dλ ≤ sup λ∈Λ P(Mλ(D) = y)π(j)(λ) P(Mλ(D′) = y)π′(j)(λ) ≤ C c · sup λ∈Λ P(Mλ(D) = y) P(Mλ(D′) = y). (A.1) 17Here, the first inequality follows from the simple property of integration, and the second inequality follows from the fact thatπ(j) has bounded density betweenc and C. Similarly, we have Pj(≤ y) P′ j(≤ y) ≤ C c · sup λ∈Λ P(Mλ(D) ≤ y) P(Mλ(D′) ≤ y), (A.2) and Pj(< y) P′ j(< y) ≤ C c · sup λ∈Λ P(Mλ(D) < y) P(Mλ(D′) < y). (A.3) Note thatD and D′ are neighboring datasets, andMλ satisfies some DP guarantees. So the ratio P(Mλ(D)∈E) P(Mλ(D′)∈E) for any eventE can be bounded. For simplicity, we define the inner product of a distribution π with the vector M(D) = (P(Mλ(D) = y) : λ ∈ Λ) as π · M(D) := Z λ∈Λ P(Mλ(D) = y)π(λ)dλ. (A.4) Now, we define additional notations to bound the probabilities. RecallSC,s is given by{f ∈ ΛR+ : ess supf ≤ C, ess inff ≥ c, R α∈Λ f(α)dα = 1.}. It is straightforward to see this is a compact set as it is the intersection of three compact sets. We define P+(y) := sup π∈SC,c Z λ∈Λ P(Mλ(D) = y)π(j)(λ)dλ = π+ · M(D), (A.5) where π+ is the distribution that achieves the supreme in the compact setSC,c. Similarly, we define P′−(y) for D′ as given by P′−(y) := inf π∈SC,c Z λ∈Λ P(Mλ(D′) = y) · π′(j)(λ)dλ = π′− · M. (A.6) Similarly, we can defineP′+(y) and P−(y) accordingly. From the definition, we know that P−(y) ≤ Pj(y) ≤ P+(y) and P′−(y) ≤ P′ j(y) ≤ P′+(y). (A.7) We also have P+(y) P′−(y) = π∗ · M(D) π′− · M(D′) ≤ sup λ P(Mλ(D) = y) P(Mλ(D′) = y) · C c . (A.8) It is similar to define P+(≤ y) := sup π∈SC,c Z λ∈Λ P(Mλ(D) ≤ y) and P′+(≤ y) := sup π∈SC,c Z λ∈Λ P(Mλ(D′) ≤ y) P−(≤ y) := inf π∈SC,c Z λ∈Λ P(Mλ(D) ≤ y) and P′−(≤ y) := inf π∈SC,c Z λ∈Λ P(Mλ(D′) ≤ y) P+(< y) := sup π∈SC,c Z λ∈Λ P(Mλ(D) < y) and P′+(< y) := sup π∈SC,c Z λ∈Λ P(Mλ(D′) < y) 18P−(< y) := inf π∈SC,c Z λ∈Λ P(Mλ(D) < y) and P′−(< y) := inf π∈SC,c Z λ∈Λ P(Mλ(D′) < y). Following the exact same proof, we have P−(≤ y) ≤ Pj(≤ y) ≤ P+(≤ y) and P′−(≤ y) ≤ P′ j(≤ y) ≤ P′+(≤ y) (A.9) P−(< y) ≤ Pj(< y) ≤ P+(< y) and P′−(< y) ≤ P′ j(< y) ≤ P′+(< y) (A.10) P+(≤ y) P′−(≤ y) ≤ sup λ P(Mλ(D) ≤ y) P(Mλ(D′) ≤ y) · C c and P+(< y) P′−(< y) ≤ sup λ P(Mλ(D) < y) P(Mλ(D′) < y) · C c . (A.11) It is also straightforward to verify from the definition that P+(≤ y) = P+(< y) + P+(y) and P′+(≤ y) = P′+(< y) + P′+(y) (A.12) P+ − (≤ y) = P−(< y) + P−(y) and P′−(≤ y) = P′−(< y) + P′−(y). (A.13) Lemma A.3.Suppose ifaλ, bλ are non-negative andcλ, c′ λ are positive for allλ. Then we have P λ aλcλP λ bλc′ λ ≤ P λ aλP λ bλ · sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f. Proof of Lemma A.3.This lemma is pretty straight forward by comparing the coefficient for each term in the full expansion. Specifically, we re-write the inequality as X λ aλcλ X λ′ b′ λ ≤ X λ aλ X λ′ b′ λc′ λ · sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f. (A.14) For each termaλb′ λ, its coefficient on the left hand side of(A.14) is cλ, but its coefficient on the right hand side of(A.14) is c′ λ · supλ,λ′ \f\f\fcλ c′ λ \f\f\f. Since we always havec′ λ · supλ,λ′ \f\f\fcλ c′ λ \f\f\f ≥ cλ, andaλb′ λ ≥ 0, we know the inequality (A.14) holds. Next, in order to present our results in terms of RDP guarantees, we prove the following lemma. Lemma A.4.The Rényi divergence betweenP+ and P− is be bounded as follows: Dα(P+∥P′−) ≤ α α − 1 log C c + sup λ∈Λ Dα \u0000 Mλ(D)∥Mλ(D′) \u0001 Proof of Lemma A.4.We write that e(α−1)Dα(P+∥P′−) = X y∈Y P+(y)α · P′−(y)1−α = X y∈Y (P λ π+(λ)P(Mλ(D) = y))α (P λ π′−(λ)P(Mλ(D′) = y))α−1 (A.15) Here, π+ and π′− are defined in(A.5) and (A.6), so they are essentiallyπ+ y and π′− y as they depend on the value ofy. Therefore, we need to “remove” this dependence ony to leverage the RDP guarantees for each base algorithmMλ. We accomplish this task by bridging viaπ, the uniform 19density onΛ (that isπ(λ) = π(λ′) for anyλ, λ′ ∈ Λ). Specifically, we defineaλ = π(λ)P(Mλ(D) = y), bλ = π(λ)P(Mλ(D′) = y), cλ = π+ y (λ) π(λ) , andc′ λ = π′− y (λ) π(λ) . We see that sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f = sup λ,λ′ \f\f\f\f\f π+ y (λ)/π(λ) π′−y (λ′)/π(λ′) \f\f\f\f\f = sup λ,λ′ \f\f\f\f\f π+ y (λ)) π′−y (λ′) \f\f\f\f\f ≤ C/c, (A.16) since π is the uniform, andπ+ y and π′− y belongs toSC,c. We now apply Lemma A.3 with the above notations for eachy to (A.15), and we have X y∈Y (P λ π+(λ)P(Mλ(D) = y))α (P λ π′−(λ)P(Mλ(D′) = y))α−1 = X y∈Y \u0010P λ π(λ)P(Mλ(D) = y) · π+(λ) π(λ) \u0011α−1 \u0010P λ π(λ)P(Mλ(D) = y) · π+(λ) π(λ) \u0011 \u0010P λ π(λ)P(Mλ(D′) = y) · π′−(λ) π(λ) \u0011α−1 = X y∈Y (P λ aλ · cλ)α−1 \u0010P λ π(λ)P(Mλ(D) = y) · π+(λ) π(λ) \u0011 (P λ bλ · c′ λ)α−1 ≤ X y∈Y sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f α−1 (P λ aλ)α−1 \u0010P λ π(λ)P(Mλ(D) = y) · π+(λ) π(λ) \u0011 (P λ bλ)α−1 = X y∈Y sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f α−1 (P λ aλ)α−1 (P λ aλ · cλ) (P λ bλ)α−1 ≤ X y∈Y sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f α−1 (P λ aλ)α−1 (P λ aλ) · supλ cλ (P λ bλ)α−1 ≤ X y∈Y \u0012C c \u0013α−1 (P λ aλ)α−1 (P λ aλ) · \u0000C c \u0001 (P λ bλ)α−1 = X y∈Y \u0012C c \u0013α · (P λ π(λ)P(Mλ(D) = y))α (P λ π(λ)P(Mλ(D′) = y))α−1 The first inequality is due to Lemma A.3, the second inequality is becauseaλ are non-negative, and the last inequality is because of(A.16) and the fact that bothπ+(λ) and π(λ) are defined inSC,c, and thus their ratio is upper bounded byC c for anyλ. Now we only need to prove that for any fixed distributionπ that doesn’t depend on valuey, we have X y∈Y (P λ π(λ)P(Mλ(D) = y))α (P λ π(λ)P(Mλ(D′) = y))α−1 ≤ sup λ∈Λ e(α−1)Dα(Mλ(D)∥Mλ(D′)). (A.17) With this result, we immediately know the result holds for uniform distributionπ as a special case. To prove this result, we first observe that the functionf(u, v) = uαv1−α is a convex function. This 20is because the Hessian off is \u0012α(α − 1)uα−2v1−α −α(α − 1)uα−1v−α −α(α − 1)uα−1v−α α(α − 1)uαv−α−1 \u0013 , which is easy to see to be positive semi-definite. And now, consider any distributionπ, denote u(λ) = P(Mλ(D) = y) and v(λ) = P(Mλ(D′) = y) by Jensen’s inequality, we have f( X λ π(λ)u(λ), X λ π(λ)v(λ)) ≤ X λ π(λ)f(u(λ), v(λ)). By adding the summation overy on both side of the above inequality, we have X y∈Y (P λ π(λ)P(Mλ(D) = y))α (P λ π(λ)P(Mλ(D′) = y))α−1 ≤ X y∈Y X λ π(λ) P(Mλ(D) = y)α P(Mλ(D′) = y)α−1 = X λ X y∈Y π(λ) P(Mλ(D) = y)α P(Mλ(D′) = y)α−1 ≤ sup λ X y∈Y P(Mλ(D) = y)α P(Mλ(D′) = y)α−1 . The first equality is due to Fubini’s theorem. And the second inequality is straight forward as one observe π(λ) only depends onλ. This concludes the proof as we know that e(α−1)Dα(P+∥P′−) ≤ \u0012C c \u0013α sup λ X y∈Y P(Mλ(D) = y)α P(Mλ(D′) = y)α−1 = \u0012C c \u0013α sup λ e(α−1)Dα(Mλ(D)∥Mλ(D′) or equivalently, Dα(P+∥P′−) ≤ α α − 1 log C c + sup λ∈Λ Dα \u0000 Mλ(D)∥Mλ(D′) \u0001 . We now present our crucial technical lemma for adaptive hyperparameter tuing with any distribution on the number of repetitionsT. This is a generalization from [34]. Lemma A.5.Fix α >1. LetT be a random variable supported onN≥0. Letf : [0, 1] → R be the probability generating function ofK, that is,f(x) = P∞ k=0 P[T = k]xk. Let Mλ and M′ λ be the base algorithm forλ ∈ Λ on Y on D and D′ respectively. Define A1 := A(D, π(0), T , C, c), andA2 := A(D′, π(0), T , C, c). Then Dα (A1∥A2) ≤ sup λ Dα \u0000 Mλ∥M′ λ \u0001 + α α − 1 log C c + 1 α − 1 log \u0010 f′(q)α · f′ \u0000 q′\u00011−α\u0011 , where applying the same postprocessing to the bounding probabilitiesP+ and P′− gives probabilitiesq and q′ respectively. This means that, there exist a function setg : Y →[0, 1] such thatq = E X←P+ [g(X)] and q′ = E X′←P′− [g (X′)]. 21Proof of Lemma A.5.We consider the event thatA1 outputs y. By definition, we have A1(y) = ∞X k=1 P(T = k)[ kY j=1 Pj(≤ y) − kY i=1 Pj(< y)] = ∞X k=1 P(T = k)[ kX i=1 Pi(y) i−1Y j=1 Pj(< y) · kY j=i+1 Pj(≤ y)] ≤ ∞X k=1 P(T = k)[ kX i=1 P+(y) i−1Y j=1 P+(< y) · kY j=i+1 P+(≤ y)] = ∞X k=1 P(T = k)[ kX i=1 P+(y) · P+(< y)i−1 · P+(≤ y)k−i] = ∞X k=1 P(T = k)[P+(≤ y)k − P+(< y)k] = f(P+(≤ y)) − f(P+(< y)) = P+(y) · E X←Uniform([P+(<y),P+(≤y)]) [f′(X)]. The second equality is by partitioning on the events of the first time of gettingy, we usei to index such a time. The third inequality is using(A.7), (A.9), and(A.10). The third to last equality is by (A.12) and algebra. The second to last equality is by definition of the probability generating function f. The last equality follows from definition of integral. Similarly, we have A2(y) ≥ ∞X k=1 P(T = k)[P′−(≤ y)k − P′−(< y)k] = P′−(y) · E X←Uniform([P′−(<y),P′−(≤y)]) [f′(X)]. The rest part of the proof is standard and follows similarly as in [34]. Specifically, we have e(α−1)Dα(A1∥A2) = X y∈Y A1(y)α · A2(y)1−α ≤ X y∈Y P+(y)α · P′−(y)1−α · E X←[P+(<y),P+(≤y)] \u0002 f′(X) \u0003α · E X′←[P′−(<y),P′−(≤y)] \u0002 f′ \u0000 X′\u0001\u00031−α ≤ X y∈Y P+(y)α · P′−(y)1−α · E X←[P+(<y),P+(≤y)] X′←[P′−(<y),P′−(≤y)] h f′(X)α · f′ \u0000 X′\u00011−αi ≤ \u0012C c \u0013α sup λ e(α−1)Dα(Mλ(D)∥Mλ(D′)) · max y∈Y E X←[P+(<y),P+(≤y)] X′←[P′−(<y),P′−(≤y)] h f′(X)α · f′ \u0000 X′\u00011−αi . The last inequality follows from Lemma A.4. The second inequality follows from the fact that, for any α ∈ R, the functionh : (0, ∞)2 → (0, ∞) given byh(u, v) = uα · v1−α is convex. Therefore, E[U]αE[V ]1−α = h(E[(U, V)]) ≤ E[h(U, V)] = E \u0002 Uα · V 1−α\u0003 all positive random variables(U, V). Note that X and X′ are required to be uniform separately, but their joint distribution can be 22arbitrary. As in [34], we will couple them so thatX−P+(<y) P+(y) = X′−P′−(<y) P′−(y) . In particular, this implies that, for eachy ∈ Y, there exists somet ∈ [0, 1] such that E X←[P+(<y),P+(≤y)] X′←[P′−(<y),P′−(≤y)] h f′(X)α · f′ \u0000 X′\u00011−αi ≤ f′(P+(< y)+t·P+(y))α ·f′ \u0000 P′−(< y) + t · P′−(y) \u00011−α Therefore, we have Dα (A1∥A2) ≤sup λ Dα \u0000 Mλ∥M′ λ \u0001 + α α − 1 log C c + 1 α − 1 log  max y∈Y t∈[0,1] f′(P+(< y) + t · P+(y))α · f′ \u0000 P′−(< y) + t · P′−(y) \u00011−α  . To prove the result, we simply fixy∗ ∈ Yand t∗ ∈ [0, 1] achieving the maximum above and define g(y) :=    1 if y < y∗ t∗ if y = y∗ 0 if y > y∗ The result directly follows by settingq = E X←P+ [g(X)] and q′ = E X′←P′− [g (X′)]. Now we can prove Theorem 1, given the previous technical lemma. The proof share similarity to the proof of Theorem 2 in [34] with the key difference from the different form in Lemma A.5. We demonstrate this proof as follows for completeness. Proof of Theorem 1.We first specify the probability generating function of the truncated negative binomial distribution f(x) = E T∼NegBin(θ,γ) \u0002 xT \u0003 = ((1−(1−γ)x)−θ−1 γ−θ−1 if θ ̸= 0 log(1−1−γ)x) log(γ) if θ = 0 Therefore, f′(x) = (1 − (1 − γ)x)−θ−1 · (θ·(1−γ) γ−θ−1 if θ ̸= 0 1−γ log(1/γ) if θ = 0 = (1 − (1 − γ)x)−θ−1 · γθ+1 · E[T]. By Lemma A.5, for appropriate valuesq, q′ ∈ [0, 1] and for allα >1 and all ˆα >1, we have Dα (A1∥A2) ≤ sup λ Dα \u0000 Mλ∥M′ λ \u0001 + α α − 1 log C c + 1 α − 1 log \u0010 f′(q)α · f′ \u0000 q′\u00011−α\u0011 ≤ ε + α α − 1 log C c + 1 α − 1 log \u0010 γθ+1 · E[T] · (1 − (1 − γ)q)−α(θ+1) · \u0000 1 − (1 − γ)q′\u0001−(1−α)(θ+1)\u0011 = ε + α α − 1 log C c 23+ 1 α − 1 log \u0010 γθ+1 · E[T] · \u0010 (γ + (1 − γ)(1 − q))1−ˆα · \u0000 γ + (1 − γ) \u0000 1 − q′\u0001\u0001ˆα\u0011ν · (γ + (1 − γ)(1 − q))u \u0011 (Here, we letˆαν = (α − 1)(1 + θ) and (1 − ˆα)ν + u = −α(θ + 1)) ≤ ε + α α − 1 log C c + 1 α − 1 log \u0010 γθ+1 · E[T] · \u0010 γ + (1 − γ) · e(ˆα−1)Dˆα(P+∥P−)\u0011ν · (γ + (1 − γ)(1 − q))u \u0011 (Here,1 − q and 1 − q′ are postprocessings of someP+ and P′− respectively ande(ˆα−1)Dˆα(·∥·) is convex) ≤ ε + α α − 1 log C c + 1 α − 1 log \u0010 γθ+1 · E[T] · \u0010 γ + (1 − γ) · e(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011ν · (γ + (1 − γ)(1 − q))u \u0011 (By Lemma A.4) ≤ ε + α α − 1 log C c + 1 α − 1 log \u0010 γθ+1 · E[T] · \u0010 γ + (1 − γ) · e(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011ν · (γ + (1 − γ)(1 − q))u \u0011 ≤ ε + α α − 1 log C c + 1 α − 1 log \u0010 γθ+1 · E[T] · \u0010 γ + (1 − γ) · e(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011ν · γu \u0011 (Here γ ≤ γ + (1 − γ)(1 − q) and u ≤ 0) = ε + α α − 1 log C c + ν α − 1 log \u0010 γ + (1 − γ) · e(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011 + 1 α − 1 log \u0010 γθ+1 · E[T] · γu \u0011 = ε + α α − 1 log C c + ν α − 1 \u0012 (ˆα − 1) sup λ Dˆα \u0000 Mλ∥M′ λ \u0001 + ˆα log C c + log \u0010 1 − γ · \u0010 1 − e−(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011\u0011\u0011 + 1 α − 1 log \u0010 γu+θ+1 · E[T] \u0011 = ε + α α − 1 log C c + (1 +θ) \u0012 1 − 1 ˆα \u0013 sup λ Dˆα \u0000 Mλ∥M′ λ \u0001 + (1 +θ) log C c + 1 + θ ˆα log \u0010 1 − γ · \u0010 1 − e−(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011\u0011 + log(E[T]) α − 1 + 1 + θ ˆα log(1/γ) (Here we haveν = (α − 1)(1 + θ) ˆα and u = −(1 + θ) \u0012α − 1 ˆα + 1 \u0013 ) = ε + α α − 1 log C c + (1 +θ) \u0012 1 − 1 ˆα \u0013 sup λ Dˆα \u0000 Mλ∥M′ λ \u0001 + (1 +θ) log C c + 1 + θ ˆα log \u00121 γ − 1 + e−(ˆα−1) supλ Dˆα(Mλ∥M′ λ)−ˆα log C c \u0013 + log(E[T]) α − 1 ≤ ε + α α − 1 log C c + (1 +θ) \u0012 1 − 1 ˆα \u0013 ˆε + (1 +θ) log C c + 1 + θ ˆα log \u00121 γ \u0013 + log(E[T]) α − 1 , which completes the proof. B Truncated Negative Binomial Distribution We introduce the definition of truncated negative binomial distribution [34] in this section. Definition B.1.(Truncated Negative Binomial Distribution [34]). Let γ ∈ (0, 1) and θ ∈ (−1, ∞). Define a distribution NegBin(θ, γ) on N+ as follows: 24• If θ ̸= 0 and T is drawn from NegBin(θ, γ), then ∀k ∈ N P [T = k] = (1 − γ)k γ−θ − 1 · k−1Y ℓ=0 \u0012ℓ + θ ℓ + 1 \u0013 and E[T] = θ·(1−γ) γ·(1−γθ). Note that whenθ = 1, it reduces to the geometric distribution with parameter γ. • If θ = 0 and T is drawn from NegBin(0, γ), then P[T = k] = (1 − γ)k k · log(1/γ) and E[T] = 1/γ−1 log(1/γ). C Privatization of Sampling Distribution C.1 General Functional Projection Framework In section 3.2, we define the projection onto a convex setSC,c as an optimization in terms ofℓ2 loss. More generally, we can perform the following general projection at thej-th iteration by considering an additional penalty term, with a constantν: min f ∥f − π(j)∥2 + νKL(π(j), f) (C.1) s.t. f ∈ SC,c. When ν = 0, we recover the originalℓ2 projection. Moreover, it’s worth noting that our formulation has implications for the information projection literature [10, 23]. Specifically, as the penalty term parameterν approaches infinity, the optimization problem evolves into a minimization of KL divergence, recovering the objective function of information projection (in this instance, moment projection). However, the constraint sets in the literature of information projection are generally much simpler than our setSC,c, making it infeasible to directly borrow methods from its field. To the best of our knowledge, our framework is the first to address this specific problem in functional projection and establish a connection to information projection in the DP community. C.2 Practical Implementation of Functional Projection Optimization program (3.1) is essentially a functional programming sincef is a function onΛ. However, whenΛ represents a non-discrete parameter space, such functional minimization is typically difficulttosolveanalytically. Evenwithintheliteratureofinformationprojection, noneofthemethods considers our constraint setSC,c, which can be viewed as the intersections of uncountable single-point constraints onf. To obtain a feasible solution to the optimization problem, we leverage the idea of discretization. Instead of viewing(3.1) as a functional projection problem, we manually discretize Λ and solve(3.1) as a minimization problem over a discrete set. Note that such approximation is unavoidable in numerical computations since computers can only manage discrete functions, even when we solve the functional projection analytically. Moreover, we also have the freedom of choosing 25the discretization grid without incurring extra privacy loss since the privacy cost is independent of the size of parameter space. By convertingSC,c into a set of finite constraints, we are able to solve the discrete optimization problem efficiently using CVXOPT [2]. D DP-HyPO with General Prior Distribution In the main manuscript, we assumeπ(0) follows a uniform distribution over the parameter spaceΛ for simplicity. In practice, informed priors can be used when we want to integrate knowledge about the parameter space into sampling distribution, which is common in the Bayesian optimization framework. We now present the general DP-HyPO framework under the informed prior distribution. To begin with, we define the space of essentially bounded density functions with respect toπ(0) as SC,c(π(0)) = {f ∈ ΛR+ : ess supf/π(0) ≤ C, ess inff/π(0) ≥ c, Z α∈Λ f(α)dα = 1, f≪ π(0)}. When π(0) = 1 µ(λ), we recover the original definition ofSC,c. Note that heref ≪ π(0) means thatf is absolute continuous with respect to the prior distributionπ(0) and this ensures thatSC,c(π(0)) is non-empty. Note that such condition is automatically satisfied whenπ(0) is the uniform prior over the entire parameter space. To define the projection of a density at thej-th iteration, π(j), into the spaceSC,c(π(0)), we consider the following functional programming problem: min f ∥f − π(j)∥2 s.t. f ∈ SC,c(π(0)), which is a direct generalization of Equation (3.1). As before,SC,c(π(0)) is also convex and closed and the optimization program can be solved efficiently via discretization onΛ. E Experiment Details E.1 MNIST Simulation We now provide the detailed description of the experiment in Section 4.2.1. As specified therein, we consider two variable hyperparameters: the learning rateη and clipping normR, while keeping all the other hyperparameters fixed. We set the training batch size to be256, and the total number of epoch to be10. The value ofσ is determined based on the allocatedε budget for each base algorithm. Specifically,σ = 0.71 for GP andσ = 0.64 for Uniform. For demonstration purposes, we set C to 2 andc to 0.75 in the GP method, so each base algorithm of Uniform haslog C/c more privacy budget than base algorithms in GP method. In Algorithm 3, we setτ to 0.1 andβ to 1. To facilitate the implementation of both methods, we discretize the learning rates and clipping norms as specified in the following setting to allow simple implementation of sampling and projection for Uniform and GP methods. Setting E.1.we set a log-spaced grid discretization onη in the range[0.0001, 10] with a multiplicative factor of 3√ 10, resulting in16 observations forη. We also set a linear-spaced grid discretization onR 26in the range[0.3, 6] with an increment of0.3, resulting in20 observations forR. This gives a total of 320 hyperparameters over the search region. We specify the network structure we used in the simulation as below. It is the standard CNN in Tensorflow Privacy and Opacus. class ConvNet(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 16, 8, 2, padding=3) self.conv2 = nn.Conv2d(16, 32, 4, 2) self.fc1 = nn.Linear(32 * 4 * 4, 32) self.fc2 = nn.Linear(32, 10) def forward(self, x): x = F.relu(self.conv1(x)) x = F.max_pool2d(x, 2, 1) x = F.relu(self.conv2(x)) x = F.max_pool2d(x, 2, 1) x = x.view(-1, 32 * 4 * 4) x = F.relu(self.fc1(x)) x = self.fc2(x) return x Despite the simple nature of MNIST, the simulation of training CNN with the two methods over each different fixedT still take significant computation resources. Due to the constraints on computational resources, we conduct a semi-real simulation using the MNIST dataset. We cache the mean accuracy of5 independently trained models for each discretized hyperparameter and treat that as a proxy for the “actual accuracy” of the hyperparameter. Each time we sample the accuracy of a hyperparameter, we add Gaussian noise with a standard deviation of0.1 to the cached mean. We evaluate the performance of the output model based on the “actual accuracy” corresponding to the selected hyperparameter. E.2 CIFAR-10 Simulation We also provide a description of the experiment in Section 4.2.2. We set the training batch size to be 256, and the total number of epoch to be10. The value ofσ is determined based on the allocatedε budget for each base algorithm. Specifically,σ = 0.65 for GP andσ = 0.6 for Uniform. Regarding our GP method, we adopt the same set of hyperparameters as used in our MNIST experiments, which includeC = 2, c = 0.75, τ = 0.1, andβ = 1. As usual, we discretize the learning rates and clipping norms as specified in the following Setting. Setting E.2.we set a log-spaced grid discretization onη in the range[0.0001, 1] with a multiplicative factor of100.1, resulting in50 observations forη. We also set a linear-spaced grid discretization on R in the range[0, 100] with an increment of2, resulting in50 observations forR. This gives a total of 2500 hyperparameter combinations over the search region. We follow the same CNN model architecture with our MNIST experiments. 27In Figure 2, we provide the hyperparameter landscape forσ = 0.65, as generated by BoTorch [3]. Figure 2: Mean and standard error of the accuracy of DP-SGD over the two hyperparameters for σ = 0.65. The learning rate (log-scale) ranges from0.00001 (left) to 1 (right) while the clipping norm ranges from 0 (top) to 100 (bottom). The landscape forσ = 0.6 is similar, with a better accuracy. E.3 Federated Learning Simulation Figure 3: Mean and Standard Error of the loss of the FL over the two hyperparameters. We now provide the detailed description of the experiment in Section 4.2.3. As specified therein, we considered a FL task on a proprietary dataset5. Our objective is to determine the optimal learning rates for the central server (using AdaGrad) and the individual users (using SGD). To simulate this scenario, we utilize the landscape generated by BoTorch [3], as illustrated in Figure 3, and consider it as our reference landscape for both mean and standard deviation of the loss for each hyperparameter. When the algorithm (GP or Uniform) visits a specific hyperparameterλ, our oracle returns a noisy scoreq(λ) drawn from a normal distributionN(µλ, σλ). Figure 3 displays a heatmap that presents the mean (µλ) and standard error (σλ) structure of the loss over these two hyperparameters, providing insights into the landscape’s characteristics. 5We are unable to report a lot of detail about the proprietary dataset due to confidentiality. 28F Additional Related Work In this section, we delve into a more detailed review of the pertinent literature. We begin with non-private Hyperparameter Optimization, a critical topic in the realm of Auto- mated Machine Learning (AutoML) [18]. The fundamental inquiry revolves around the generation of high-performing models within a specific search space. In historical context, two types of optimiza- tions have proven significant in addressing this inquiry: architecture optimization and hyperparameter optimization. Architecture optimization pertains to model-specific parameters such as the number of neural network layers and their interconnectivity, while hyperparameter optimization concerns training-specific parameters, including the learning rate and minibatch size. In our paper, we incorpo- rate both types of optimizations within our HPO framework. Practically speaking,Λ can encompass various learning rates and network architectures for selection. For HPO, elementary methods include grid search and random search [24, 19, 16]. Progressing beyond non-adaptive random approaches, surrogate model-based optimization presents an adaptive method, leveraging information from preceding results to construct a surrogate model of the objective function [28, 43, 22, 32]. These methods predominantly employs Bayesian optimization techniques, including Gaussian process [35], Random Forest [20], and tree-structured Parzen estimator [5]. Another important topic in this paper is Differential Privacy (DP). DP offers a mathematically robust framework for measuring privacy leakage. A DP algorithm promises that an adversary with perfect information about the entire private dataset in use – except for a single individual – would find it hard to distinguish between its presence or absence based on the output of the algorithm [13]. Historically, DP machine learning research has overlooked the privacy cost associated with HPO [1, 41, 44]. The focus has only recently shifted to the “honest HPO” setting, where this cost is factored in [30]. Addressing this issue directly involves employing a composition-based analysis. If each training run of a hyperparameter upholds DP, then the overall HPO procedure adheres to DP through composition across all attempted hyperparameter values. A plethora of literature on the composition of DP mechanisms attempts to quantify a better DP guarantee of the composition. Vadhan et al. [38] demonstrated that though(ε, δ)-DP possesses a simple mathematical form, deriving the precise privacy parameters of a composition is #-P hard. Despite this obstacle, numerous advanced techniques are available to calculate a reasonably accurate approximation of the privacy parameters, such as Moments Accountant [1], GDP Accountant [12], and Edgeworth Accountant [39]. The efficacy of these accountants is attributed to the fact that it is easier to reason about the privacy guarantees of compositions within the framework of Rényi differential privacy [29] or f-differential privacy [12]. These methods have found widespread application in DP machine learning. For instance, when training deep learning models, one of the most commonly adopted methods to ensure DP is via noisy stochastic gradient descent (noisy SGD) [4, 37], which uses Moments Accountant to better quantify the privacy guarantee. Although using composition for HPO is a simple and straightforward approach, it carries with it a significant challenge. The privacy guarantee derived from composition accounting can be excessively loose, scaling polynomially with the number of runs. Chaudhuri et al. [8] were the first to enhance the DP bounds for HPO by introducing additional stability assumptions on the learning algorithms. [26] made significant progress in enhancing DP bounds for HPO without relying on any stability properties of the learning algorithms. They proposed a simple procedure where a hyperparameter was randomly selected from a uniform distribution for each training run. This selection process was repeated a random number of times according to a geometric distribution, and the best model obtained from these runs was outputted. They showed that this procedure satisfied 29(3ε, 0)-DP as long as each training run of a hyperparameter was(ε, 0)-DP. Building upon this, [34] extended the procedure to accommodate negative binomial or Poisson distributions for the repeated uniform selection. They also offered more precise Rényi DP guarantees for this extended procedure. Furthermore, [9] explored a generalization of the procedure for top-k selection, considering (ε, δ)-DP guarantees. In a related context, [30] explored a setting that appeared superficially similar to ours, as their title mentioned “adaptivity.” However, their primary focus was on improving adaptive optimizers such as DP-Adam, which aimed to reduce the necessity of hyperparameter tuning, rather than the adaptive HPO discussed in this paper. Notably, in terms of privacy accounting, their approach only involved composing the privacy cost of each run without proposing any new method. Another relevant area of research is DP selection, which encompasses well-known methods such as the exponential mechanism [27] and the sparse vector technique [14], along with subsequent studies (e.g., [6] and [17]). However, this line of research always assumes the existence of a low- sensitivity score function for each candidate, which is an unrealistic assumption for hyperparameter optimization. 30",
      "meta_data": {
        "arxiv_id": "2306.05734v2",
        "authors": [
          "Hua Wang",
          "Sheng Gao",
          "Huanyu Zhang",
          "Weijie J. Su",
          "Milan Shen"
        ],
        "published_date": "2023-06-09T07:55:46Z",
        "pdf_url": "https://arxiv.org/pdf/2306.05734v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces DP-HyPO, a pioneering adaptive private hyperparameter optimization framework, to bridge the gap between private and non-private HPO. It enables flexible use of non-DP adaptive HPO methods, such as Gaussian process, while avoiding substantial privacy costs due to composition. The framework provides sharp Differential Privacy (DP) guarantees by accurately characterizing the Rényi divergence between adaptive sampling distributions of neighboring datasets, without relying on stability assumptions, thus strictly generalizing prior non-adaptive methods. Empirically demonstrates that a Gaussian process-based DP-HyPO algorithm outperforms its uniform counterpart across various real-world scenarios and allows practitioners to integrate any non-private adaptive HPO methods and allocate privacy budget for adaptivity.",
        "methodology": "DP-HyPO is designed to achieve adaptive hyperparameter optimization with differential privacy by maintaining an adaptive sampling distribution (π) at each iteration. This distribution reflects accumulated information from previous runs. To control privacy loss, the sampling distribution π(j) is required to have a density bounded by constants 'c' and 'C' relative to a prior distribution π(0). The framework includes a general projection technique to privatize any non-private update rules by projecting π(j) into a space of essentially bounded density functions SC,c. Privacy guarantees are provided using the Rényi Differential Privacy (RDP) framework, with specific analysis for when the total number of repetitions (T) follows a truncated negative binomial distribution. An instantiation of DP-HyPO with Gaussian Process (AGP) is provided, where GP is used to construct a surrogate model, estimate mean and variance of performance, assign scores (estimated Upper Confidence Bound), and transform these scores into a sampling distribution using a softmax function, which is then projected to satisfy the bounded density constraint.",
        "experimental_setup": "The DP-HyPO framework, specifically its Gaussian process-based variant (GP), was empirically evaluated against the Uniform DP-HyPO method (a non-adaptive special case with C=c=1). Experiments were conducted under two pragmatic privacy configurations: the white-box setting and the black-box setting.1. White-box setting (adaptive HPO incurs extra privacy cost):- MNIST Simulation: Trained a standard CNN with DP-SGD on the MNIST dataset, optimizing learning rate (η) and clipping norm (R). A semi-real simulation cached mean accuracies for discretized hyperparameters and added Gaussian noise during sampling. Total privacy budget was ε=15, δ=1e-5. Hyperparameter grids included 16 log-spaced η values and 20 linear-spaced R values (total 320 combinations).- CIFAR-10 Simulation: Used the same CNN model and hyperparameters on the CIFAR-10 dataset. Hyperparameter landscapes (mean and standard error of accuracy) were generated using BoTorch [3] from 50 runs. Total privacy budget was ε=12, δ=1e-5. Hyperparameter grids included 50 log-spaced η values and 50 linear-spaced R values (total 2500 combinations).2. Black-box setting (base algorithm privacy fixed, extra budget for HPO):- Federated Learning (FL) Task: Explored an FL task on a proprietary industrial dataset, optimizing learning rates for the central server (AdaGrad) and individual users (SGD). The loss landscape (mean and standard error) was generated by BoTorch [3]. Experiments varied the degree of adaptivity (different C values) and aggregated performance data over a Geometric distribution with various γ values.",
        "limitations": "The framework requires non-private adaptive HPO update rules to be modified (via a projection technique) to satisfy the bounded adaptive density constraint, which is not automatically met. The benefits of adaptivity were observed to be marginal in datasets with uncomplicated hyperparameter landscapes (e.g., MNIST). Due to computational resource constraints, some experiments (MNIST, CIFAR-10, FL) used semi-real simulations or pre-generated hyperparameter landscapes from BoTorch rather than full end-to-end DP-SGD runs for every trial. Details about the proprietary Federated Learning dataset are limited due to confidentiality constraints.",
        "future_research_directions": "Two main future research directions are identified: 1. Investigating alternative HPO specifications that are more practically favorable and leveraging advanced HPO methods from the extensive literature to further improve empirical performance. 2. Establishing a theoretical utility guarantee for the DP-HyPO framework, potentially by applying proof methodologies similar to those found in prior work like Theorem 3.3 in [26], for either the general framework or specific configurations within it."
      }
    },
    {
      "title": "Hyperparameter Optimization through Neural Network Partitioning",
      "abstract": "Well-tuned hyperparameters are crucial for obtaining good generalization\nbehavior in neural networks. They can enforce appropriate inductive biases,\nregularize the model and improve performance -- especially in the presence of\nlimited data. In this work, we propose a simple and efficient way for\noptimizing hyperparameters inspired by the marginal likelihood, an optimization\nobjective that requires no validation data. Our method partitions the training\ndata and a neural network model into $K$ data shards and parameter partitions,\nrespectively. Each partition is associated with and optimized only on specific\ndata shards. Combining these partitions into subnetworks allows us to define\nthe ``out-of-training-sample\" loss of a subnetwork, i.e., the loss on data\nshards unseen by the subnetwork, as the objective for hyperparameter\noptimization. We demonstrate that we can apply this objective to optimize a\nvariety of different hyperparameters in a single training run while being\nsignificantly computationally cheaper than alternative methods aiming to\noptimize the marginal likelihood for neural networks. Lastly, we also focus on\noptimizing hyperparameters in federated learning, where retraining and\ncross-validation are particularly challenging.",
      "full_text": "Published as a conference paper at ICLR 2023 HYPERPARAMETER OPTIMIZATION THROUGH NEURAL NETWORK PARTITIONING Bruno Mlodozeniec†∗, Matthias Reisser‡, Christos Louizos‡ †University of Cambridge, ‡Qualcomm AI Research bkm28@cam.ac.uk, {mreisser,clouizos}@qti.qualcomm.com ABSTRACT Well-tuned hyperparameters are crucial for obtaining good generalization behavior in neural networks. They can enforce appropriate inductive biases, regularize the model and improve performance — especially in the presence of limited data. In this work, we propose a simple and efﬁcient way for optimizing hyperparameters inspired by the marginal likelihood, an optimization objective that requires no validation data. Our method partitions the training data and a neural network model into K data shards and parameter partitions, respectively. Each partition is associated with and optimized only on speciﬁc data shards. Combining these partitions into subnetworks allows us to deﬁne the “out-of-training-sample” loss of a subnetwork, i.e., the loss on data shards unseen by the subnetwork, as the objective for hyperparameter optimization. We demonstrate that we can apply this objective to optimize a variety of different hyperparameters in a single training run while being signiﬁcantly computationally cheaper than alternative methods aiming to optimize the marginal likelihood for neural networks. Lastly, we also focus on optimizing hyperparameters in federated learning, where retraining and cross-validation are particularly challenging. 1 I NTRODUCTION Due to their remarkable generalization capabilities, deep neural networks have become the de-facto models for a wide range of complex tasks. Combining large models, large-enough datasets, and sufﬁcient computing capabilities enable researchers to train powerful models through gradient descent. Regardless of the data regime, however, the choice of hyperparameters — such as neural architecture, data augmentation strategies, regularization, or which optimizer to choose — plays a crucial role in the ﬁnal model’s generalization capabilities. Hyperparameters allow encoding good inductive biases that effectively constrain the models’ hypothesis space (e.g., convolutions for vision tasks), speed up learning, or prevent overﬁtting in the case of limited data. Whereas gradient descent enables the tuning of model parameters, accessing hyperparameter gradients is more complicated. The traditional and general way to optimize hyperparameters operates as follows; 1) partition the dataset into training and validation data1, 2) pick a set of hyperparameters and optimize the model on the training data, 3) measure the performance of the model on the validation data and ﬁnally 4) use the validation metric as a way to score models or perform search over the space of hyperparameters. This approach inherently requires training multiple models and consequently requires spending resources on models that will be discarded. Furthermore, traditional tuning requires a validation set since optimizing the hyperparameters on the training set alone cannot identify the right inductive biases. A canonical example is data augmentations — they are not expected to improve training set performance, but they greatly help with generalization. In the low data regime, deﬁning a validation set that cannot be used for tuning model parameters is undesirable. Picking the right amount of validation data is a hyperparameter in itself. The conventional rule of thumb to use ∼10% of all data can result in signiﬁcant overﬁtting, as pointed out by Lorraine et al. (2019) , when one has a sufﬁciently large number of hyperparameters to tune. Furthermore, a validation set can be challenging ∗Work done while at Qualcomm AI Research. Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc. and/or its subsidiaries. 1a third partition, the test or holdout set is used to estimate the ﬁnal model performance 1 arXiv:2304.14766v1  [cs.LG]  28 Apr 2023Published as a conference paper at ICLR 2023 to obtain in many use cases. An example is Federated Learning (FL) (McMahan et al., 2017), which we speciﬁcally consider in our experimental section. In FL, each extra training run (for,e.g., a speciﬁc hyperparameter setting) comes with additional, non-trivial costs. Different approaches have been proposed in order to address these challenges. Some schemes optimize hyperparameters during a single training run by making the hyperparameters part of the model (e.g., learning dropout rates with concrete dropout (Gal et al., 2017), learning architectures with DARTs (Liu et al., 2018) and learning data-augmentations with schemes as in Benton et al. (2020); van der Wilk et al. (2018)). In cases where the model does not depend on the hyperparameters directly but only indirectly through their effect on the value of the ﬁnal parameters (through optimization), schemes for differentiating through the training procedures have been proposed, such as Lorraine et al. (2019). Another way of optimizing hyperparameters without a validation set is through the canonical view on model selection (and hence hyperparameter optimization) through the Bayesian lens; the concept of optimizing the marginal likelihood. For deep neural networks, however, the marginal likelihood is difﬁcult to compute. Prior works have therefore developed various approximations for its use in deep learning models and used those to optimize hyperparameters in deep learning, such as those of data augmentation (Schw¨obel et al., 2021; Immer et al., 2022). Still, however, these come at a signiﬁcant added computational expense and do not scale to larger deep learning problems. This paper presents a novel approach to hyperparameter optimization, inspired by the marginal likelihood, that only requires a single training run and no validation set. Our method is more scalable than previous works that rely on marginal likelihood and Laplace approximations (which require computing or inverting a Hessian (Immer et al., 2021)) and is broadly applicable to any hierarchical modelling setup. 2 M ARGINAL LIKELIHOOD AND PRIOR WORK In Bayesian inference, the rules of probability dictate how any unknown, such as parameters w or hyperparameters ψ, should be determined given observed data D. Let p(w) be a prior over w and p(D|w,ψ) be a likelihood for Dwith ψbeing the hyperparameters. We are then interested in the posterior given the data p(w|D,ψ) =p(D|w,ψ)p(w)/p(D|ψ). The denominator term p(D|ψ) is known as the marginal likelihood, as it measures the probability of observing the data given ψ, irrespective of the value of w: p(D|ψ) = ∫ p(w)p(D|w,ψ)dw. Marginal likelihood has many desirable properties that make it a good criterion for model selection and hyperparameter optimization. It intuitively implements the essence of Occam’s Razor principle (MacKay, 2003, § 28). In the PAC-Bayesian literature, it has been shown that higher marginal likelihood gives tighter frequentist upper bounds on the generalization performance of a given model class (McAllester, 1998; Germain et al., 2016). It also has close links to cross-validation (see section 2.1) and can be computed from the training data alone. However, computation of the marginal likelihood in deep learning models is usually prohibitively expensive and many recent works have proposed schemes to approximate the marginal likelihood for differentiable model selection (Lyle et al., 2020; Immer et al., 2021; 2022; Schw¨obel et al., 2021). 2.1 “L EARNING SPEED ” PERSPECTIVE Lyle et al. (2020); Fong and Holmes (2020) pointed out the correspondence between “learning speed” and marginal likelihood. Namely, the marginal likelihood of the data Dconditioned on some hyperparameters ψcan be written as: log p(D|ψ) = ∑ k log Ep(w|D1:k−1,ψ) [p(Dk|w,ψ)] ≥ ∑ k Ep(w|D1:k−1,ψ) [log p(Dk|w,ψ)] (1) where (D1,..., DC) is an arbitrary partitioning of the training dataset Dinto Cshards or chunks2, and p(w|D1:k,ψ) is the posterior over parameters of a function fw : X → Y, from the input domain Xto the target domain Yafter seeing data in shards 1 through k. The right-hand side can be interpreted as a type of cross-validation in which we ﬁx an ordering over the shards and measure the “validation” performance on each shardDk using a model trained on the preceding shards D1:k−1. 2We use the terms “chunk” and “shard” interchangeably. 2Published as a conference paper at ICLR 2023 Alternatively, it can be viewed as the learning speed of a (probabilistic) model: i.e., a measure of how quickly it learns to perform well on new shards of data after only having been ﬁt to the previous shards (through exact Bayesian updating). This perspective neatly illustrates why models with higher marginal likelihood can exhibit good inductive biases, e.g., encoded through ψ, w and fw. Namely, such models can be expected to learn faster and generalize better after seeing fewer samples. For example, if the hypothesis space is constrained3to functions satisfying symmetries present in the data, we need fewer data to identify the correct function (Sokolic et al., 2017; Sannai et al., 2021). We argue that the “learning speed” aspect of marginal likelihood — i.e., measuring how well the model generalizes to new data in the training set, having been trained only on the previous data points — is the key property making marginal likelihood a useful tool for selecting hyperparameters. 2.2 T RAINING SPEED FOR HYPERPARAMETER OPTIMIZATION Computing the “learning speed”, requires samples from the posteriorp(w|D1:k,ψ). Unfortunately, in deep learning settings, such samples are impractical to obtain; thus, prior works have focused on more scalable alternatives. Lyle et al. (2020) propose to approximate the objective in Eq. 1 by looking at the training speed during standard training of a neural network by SGD. Speciﬁcally, they deﬁne the training speed as the reduction in the training loss after a single SGD parameter update, summed over all updates in the ﬁrst epoch. They argue that, during the ﬁrst epoch of training, after the neural network parameters, w, have been updated with SGD steps using data from shards D1:k, they can be approximately used in place of the sample from the posterior p(w|D1:k,ψ) in Eq. 1. They extend the analogy to training past one epoch and use the training speed estimate for model selection (Ru et al., 2021). As pointed out by the authors, however, the analogy between learning speed and training speed somewhat breaks down after 1 epoch of training. The network parameters have “seen” every datapoint in the training set after1 epoch, and hence the connection to measuring the model’s generalization capability is weakened. For the sake of scalability and alignment with deep learning practice, we also focus on simple pointwise approximations qk(w) = δ(w = ˆwk) to the posteriors p(w|D1:k,ψ). However, in contrast to prior work, we explicitly parametrize the learning procedure such that, at any given training iteration, we have access to a model that is trained only on a subset of the dataD1:k. In doing so, we can approximate the objective in Eq. 1, and thus use it to optimize the hyperparameters during the entire training run. 3 P ARTITIONED NEURAL NETWORKS Our goal is to optimize the objective LML (D,ψ) = C∑ k=1 Eqk−1(w) [log p(Dk|w,ψ)] (2) wrt. ψ, which is an approximation to the lower-bound presented in Eq. 1 above. In Appendix A, we show that the left-hand side is also a lower-bound on the marginal likelihood under some unobtrusive conditions. As mentioned in Section 2.2, our goal is to propose an architecture and a training scheme so that we can easily obtain models trained on only subsets of the data D1:k for all k throughout training. We propose that each {qk(w)}C k=1 optimizes a subset of the parameters of the neural network, in a manner that allows us to extract “subnetworks” from the main network that have been trained on speciﬁc chunks of data. We describe the partitioning scheme below. Partitioning the parameters Denote the concatenations of the weights of a neural networkw ∈RN. We can deﬁne a partitioning ((w1,..., wC),P) of the parameters into C partitions, such that w = Pconcat(w1,..., wC) for a permutation matrix P ∈{0,1}N×N. For ease of exposition, we drop the dependence on P, assuming that w is already arranged such that P is identity, P = IN×N. Given the partitioning (w1,..., wC) of the parameters, we then specify Csubnetworks with weights w(1) s ,..., w(C) s such that w(k) s = concat(w1,..., wk, ˆwk+1,..., ˆwC), where ˆwi are some default 3or if the learning algorithm is heavily biased towards returning hypotheses that satisfy a given invariance, e.g., through the use of a prior. 3Published as a conference paper at ICLR 2023 values not optimized during training4. More speciﬁcally, the k-th subnetwork, wk s, retains the ﬁrst kpartitions from the weight partitioning and sets the remaining parameters to ˆwk+1:C. Note that, if each wk is only updated on chunks D1:k, the subnetwork w(k) s is only comprised of weights that have been updated on D1:k. Thus, we can view the parameters of w(k) s as an approximation to qk(w). Although, given that a subset of the parameters in each w(k) s is ﬁxed, this would likely be a poor approximation to the true posterior over the weights given D1:k, it could be, intuitively, a reasonable approximation in function space5. Partitioned training Having partitioned the dataset Dinto Cchunks (D1,..., Dk), we update each partition wk by optimising the negative log-likelihood6on chunks D1:k using subnetwork w(k) s by computing the following gradients: ∇wkL ( D1:k,w(k) s ) = ∑ (x,y)∈D1:k ∇wk log p ( y ⏐⏐⏐x; w(k) s ,ψ ) . (3) We interleave stochastic gradient updates of each partition of the weights with updating the hyperpa- rameters ψusing LML in Eq. 2: ∇ψLML (D,ψ) ≈ C∑ k=2 ∑ (x,y)∈Dk ∇ψlog p ( y ⏐⏐⏐x,w(k−1) s ,ψ ) . (4) This can be seen as the sum of the out-of-sample losses for each subnetwork w(k) s . The scheme is illustrated in Figure 1. For details of how the updates are scheduled in our experiments, see Appendix I. Note that, while we could incorporate the gradient of the ﬁrst term from Eq. 1 corresponding to Eq0(w)[log p(D1|w,ψ)] in Eq. 4, we chose to leave it out. Hence, the gradient of Eq. 4 is of an estimate that can be viewed as an approximation to the conditional marginal likelihood log p(D2:C|D1,ψ). Conditional marginal likelihood has been shown to have many desirable properties for model selection and, in many cases, can be a better proxy for generalization (Lotﬁ et al., 2022). Weights: w = (w1,w2,w3) Alternate: Optimize parameters: log p ( D1 |(w1, ˆw2, ˆw3)   Subnet. 1 ,ψ ) w.r.t. w1 log p ( D1:2|(w1,w2, ˆw3)   Subnet. 2 ,ψ ) w.r.t. w2 log p ( D1:3|(w1,w2,w3)   Subnet. 3 ,ψ ) w.r.t. w3 Optimize hyper parameters ψon: log p ( D2|(w1, ˆw2, ˆw3)   Subnet. 1 ,ψ ) + logp ( D3|(w1,w2, ˆw3)   Subnet. 2 ,ψ ) Figure 1: Best viewed in colour. Illustration of the partitioning scheme for a single hidden layer perceptron with C = 3chunks. This procedure, inspired by the marginal likelihood, has several desirable properties compared to prior work. 1) Our objective is computationally efﬁcient, with a computational cost roughly corresponding to evaluating subnetworks on the training set. There is no need to compute nor invert a Hessian with 4e.g., ˆwi could be the value of the weights at initialization, or ˆwi = 0 corresponding to pruning those parameters and obtaining a proper subnetwork. 5Since a) the mapping from parameters to functions is not bijective and b) neural networks are highly overparameterised and can be heavily pruned while retaining performance (Frankle and Carbin, 2018), obtaining a good ﬁt to a subset of the training data with a subset of the model parameters should be possible. Furthermore, “scaling laws” indicate that the beneﬁt of having more parameters becomes apparent mostly for larger dataset sizes (Kaplan et al., 2020), thus it is reasonable for subnetworks ﬁt to more data to have more learnable parameters. 6Optionally with an added negative log-prior regularization term log p(w(k) s ). 4Published as a conference paper at ICLR 2023 respect to the weights, as in the Laplace approximation (Immer et al., 2021; 2022). 2) Our objective is readily amenable to optimization by stochastic gradient descent; we do not have to iterate over the entire training set to compute a single gradient update for the hyperparameters. 3) Compared to the training speed objective (Lyle et al., 2020), in our method, the training of the weights in each subnetwork progresses independently of the data in future chunks. Hence, it can be seen as more truthfully measuring the generalization capability of a model using a given set of hyperparameters. Partitioning Schemes There are several ways in which the neural network weights can be partitioned. In our experiments in Section 5, we partition the weights before beginning training by assigning a ﬁxed proportion of weights in each layer to a given partition at random. For each subnetwork, for the weight partitions corresponding to future chunks, we use the values of the weights at initialisation. For a discussion of partitioning schemes, see Appendix C. 4 R ELATED WORKS Hyperparameter optimization in deep learning Many works have tackled the challenge of op- timizing hyperparameters in deep learning. Works on implicit differentiation, such as the one by Lorraine et al. (2019), allow for optimizing training hyperparameters such as the learning rate, weight- decay, or other hyperparameters that affect the ﬁnal neural network weights only through the training routine. Other works have proposed ways to parameterize and optimize data-augmentations (Cubuk et al., 2018; Li et al., 2020), search-spaces for neural network architectures, as well as methods to optimize architectures using gradient-based optimization (Liu et al., 2018; Elsken et al., 2019). All of the above works have primarily relied on optimizing hyperparameters on a separate validation set and are compatible with the objective deﬁned in this work. Several works have also aimed to cast learning data augmentations as an invariance learning problem. They do so by parameterizing the model itself with data augmentations, and frame invariance learning as a model selection problem (van der Wilk et al., 2018; Benton et al., 2020; Schw¨obel et al., 2021; Nabarro et al., 2022; Immer et al., 2022). We compare against Benton et al. (2020) (“Augerino”) and Immer et al. (2022) (“Differentiable Laplace”) on this task in the experimental section. Hyperparameter optimization without a validation set A limited number of works consider learning hyperparameters without a validation set in a deep learning context. Benton et al. (2020) propose a simple method for learning invariances without a validation set by regularising invariance hyperparameters to those resulting in higher invariance. They show that the invariances found tend to be insensitive to the regularisation strength, determined by another hyperparameter. However, the method relies on being able to a priori deﬁne which hyperparameters lead to higher invariance through a suitable regularisation function. In more complex invariance learning settings, deﬁning the regulariser can be challenging. For example, if data-augmentation transformations were to be parameterized by a neural network (as proposed in Lorraine et al. (2019)), it is non-trivial to devise an adequate regulariser. We show that our method can be applied to such settings. Other works focus on deriving tractable approximations to the marginal likelihood for deep neural networks. Schw ¨obel et al. (2021) propose only marginalising-out the parameters in the last layer of the neural network by switching it out for a Gaussian Process. They treat the preceding layer effectively as a hyperparameter, and optimize invariance parameters using the marginal likelihood. Although they show promising results on MNIST, they found they “were unable to learn invariances for CIFAR-10” (Schw¨obel et al., 2021, §7) and highlighted the need to marginalise lower layers as well. In contrast, our objective can be seen as being inspired by marginal likelihood where arbitrary network layers can be “marginalised”, and works on datasets like CIFAR-10. Immer et al. (2022) have adapted the Laplace approximation (Immer et al., 2021) to make it tractable for learning data augmentations. In contrast to Schw¨obel et al. (2021), they approximately marginalize out all the network parameters, and performs favourably. Their approximation, however, requires approximations to a Hessian w.r.t. all network parameters; for that reason, their work reports results for architectures only up to a ResNet-14, whereas our method can easily scale to larger architectures. Hyperparameter optimization in FL Improving hyperparameter optimization is especially rele- vant to FL. Given the potential system level constraints (Wang et al., 2021), methods that optimize the hyperparameters and parameters in a single training run are preferred. On this note, Khodak et al. (2021) introduced FedEx and showed that it can successfully optimize the client optimizer 5Published as a conference paper at ICLR 2023 hyperparameters. FedEx relies on a training/validation split on the client level and uses a REIN- FORCE type of gradient (Williams, 1992) estimator, which usually exhibits high variance and needs baselines to reduce it (Mohamed et al., 2020). This is in contrast to partitioned networks, which use standard, low-variance backpropagation for the hyperparameters and no separate validation set per client. To optimize the other hyperparameters, Khodak et al. (2021) wrapped FedEx with a traditional hyperparameter optimization strategy, the successive halving algorithm. This is orthogonal to our method and could be applied to partitioned networks as well. In Zhou et al. (2021), the authors perform a hyperparameter search independently on each client with some off-the-shelf methods and then aggregate the results of the search at the server once in order to identify the best hyperparameter setting. The main drawback of this method compared to partitioned networks is that when the local client datasets are small, a client-speciﬁc validation set is not informative, and the aggregation happens only once. Finally, there is also the recent work from Seng et al. (2022) which performs hyperparameter optimization and neural architecture search in the federated setting. Similarly to prior works, it requires client-speciﬁc validation data in order to optimize the hyperparameters. 5 E XPERIMENTS 1 5 10 15 20 25 30 Num. inputs 0.7 0.8 0.9 1.0 Accuracy 0.5 0.4 0.3 0.2 0.1 0.0 Average Log-likelihoodPosthoc Diagonal Laplace Train T est 1 5 10 15 20 25 30 Num. inputs 103 102 Log Marginal Likelihood Estimate Partitioned (a) 0 400080001200016000200002400028000 Iteration 0 5 10 15 20 25Input Mask Element 0.00 0.25 0.50 0.75 1.00 Mask Probability  (b) Figure 2: (a) Demonstrating the ability of the marginal-likelihood inspired objective LML to identify the correct model on a toy input selection task. We plot the hyperparameter objective, train and test set accuracy, and train and test set log-likelihood with the partitioned networks method (left), and the post-hoc diagonal Laplace method (Immer et al., 2021) (right). (b) Mask over input features learned by partitioned networks over time. The ﬁrst 15 features are correctly identiﬁed. Input Selection To demonstrate that LML is a good objective for model selection that captures the desirable properties of the marginal likelihood, we ﬁrst deploy our method on the toy model selection task of Lyle et al. (2020): there the ﬁrst 15 features are informative, and the remaining15 are spurious y∼Bern (1 2 ) x = [ y+ ϵ1,...,y + ϵ15   Informative ,ϵ16,...,ϵ 30   Spurious ]⊺ ϵ1,...,ϵ 30 iid ∼N(0,1). We specify a ﬁxed mask over the inputs prior to training, where the ﬁrst Kinputs remain unmasked, and the remainder is masked. We expect that, given multiple models with different (ﬁxed) masks over the inputs, the proposed objective will be able to identify the correct one — i.e., the one that keeps only the informative features. We train multiple fully connected neural networks (MLPs) on a training set of 1000 examples using our method and compare the ﬁnal values of the LML objective. The results are shown in Figure 2a. LML correctly identiﬁes 15 input features as the optimum, and correlates well with test accuracy and log-likelihood. Training loss and training accuracy, on the other hand, cannot alone disambiguate whether to use 15 or more input features. Differentiable input selection We further show that we can learn the correct mask over the inputs in a differentiable manner using our method during a single training run. We parameterize a learnable mask over the inputs with a concrete Bernoulli distribution (Maddison et al., 2016) and treat the parameters of the mask distribution as a hyperparameter. We optimize them with respect to the proposed objective using our method. The evolution of the learned mask during training is shown in Figure 2b, where we see that we can correctly identify the ﬁrst 15 informative features. 6Published as a conference paper at ICLR 2023 Learning invariances through data-augmentations Following previous literature on learning soft invariances through learning data augmentations (Nabarro et al., 2022; van der Wilk et al., 2018; Benton et al., 2020; Schw ¨obel et al., 2021; Immer et al., 2022), we show that we can learn useful afﬁne image augmentations, resulting in gains in test accuracy. We specify afﬁne data augmentations as part of a probabilistic model as done by van der Wilk et al. (2018), averaging over multiple data augmentation samples during training and inference. This allows us to treat the data-augmentation distribution as a model hyperparameter rather than a training hyperparameter. For datasets, we consider MNIST, CIFAR10, TinyImagenet along with rotCIFAR10 and rotTinyImagenet, variants where the datapoints are randomly rotated at the beginning of training by angles sampled uniformly from [−π,π] (Immer et al., 2022). Experimental setup details are provided in Appendix I. For the CIFAR10 and rotCIFAR10 datasets, we consider as baselines standard training with no augmentations, Augerino (Benton et al., 2020) and Differentiable Laplace (Immer et al., 2022). Following Immer et al. (2022), we use ﬁxupResNets (Zhang et al., 2019) for the architectures. The results can be seen in Table 1. There, we observe that partitioned networks outperform all baselines in the case of CIFAR10 for both ResNet variants we consider. On RotCIFAR10, we observe that partitioned networks outperform the baseline and Augerino, but it is slightly outperformed by Differentiable Laplace, which optimizes additional prior hyperparameters. To demonstrate the scalability of partitioned networks, for the (rot)TinyImagenet experiments we consider a ResNet-50 architecture with GroupNorm(2). In Table 1 we observe that in both cases, partitioned networks learn invariances successfully and improve upon the baseline. Relative to Augerino, we observe that partitioned networks either improve (TinyImagenet) or are similar (rotTinyImagenet). Table 1: Test accuracy with learning afﬁne augmentations on (rot)CIFAR10 and (rot)TinyImagenet. Method Dataset Architecture Baseline Augerino Diff. Laplace Partitioned RotCIFAR10 ﬁxupResNet-8 54.2±0.4 75.4±0.2 79.5±0.6 79.1±0.0 CIFAR10 ﬁxupResNet-8 74.1±0.5 79.0±1.0 84.2±0.8 86.1±0.4 ﬁxupResNet-14 79.5±0.3 83.0±0.1 88.1±0.2 89.1±0.8 RotTinyImagenet ResNet-50 31.5±0.6 44.5±0.2 OOM7 43.9±0.3 TinyImagenet ResNet-50 44.2±0.5 41.1±0.2 OOM 48.6±0.0 Imbuing a model with useful invariances is particularly useful in the low-data regime, due to better data efﬁciency. To show that, we perform experiments where we artiﬁcially reduce the size of the training dataset. The results can be seen in Figure 3. We see that by learning augmentations with partitioned networks, we can drastically improve performance in the low-data regime upon a baseline that does not learn augmentations, while performing favorably against prior works in most cases. On MNIST, our method outperforms the last-layer marginal-likelihood method (last-layer ML) by Schw¨obel et al. (2021) in the large data regime but underperforms in the low-data regime. That is likely to be expected, as their work ﬁts a Gaussian Process (GP) at the last layer (Wilson et al., 2016), which is better tailored for the low-data regime and results into a more ﬂexible model (due to the GP corresponding to an additional, inﬁnite width, layer). Since the MNIST-CNN is sufﬁciently small to ﬁt multiple networks into memory, we also compare to a variant of our method where, instead of partitioning a single network, we train Cdifferent networks where network kis ﬁt on data D1:k. This serves as an upper bound on the performance of the partitioned networks. We see that by partitioning a single network, we can achieve almost equivalent accuracy. On CIFAR10, partitioned networks outperform all other works on all data sizes we considered. On RotCIFAR10, partitioned networks perform again favourably, but they are marginally outperformed by differentiable Laplace in the low-data regime. Compared to partitioned networks where we only optimize augmentations, differentiable Laplace also optimizes the precision of a Gaussian prior over the weights, which better combats overﬁtting in the low-data regime. On both the TinyImagenet and rotTinyImagenet experiments we observe that partitioned networks either outperform or are similar to the baselines on all data sizes considered. 7Out of memory error on a 32GB Nvidia V100. 7Published as a conference paper at ICLR 2023 5000 20000 60000 Dataset Size 0.98 0.99T est Accuracy Baseline Last-layer ML Augerino Diff. Laplace Partitioned (Ens.) Partitioned (a) MNIST 0.25 0.50 0.75 1 5 10 20 50 Dataset Size (x1000) 0.25 0.50 0.75  (b) (rot)CIFAR10 0.25 0.50 10 50 100 Dataset Size (x1000) 0.2 0.4  (c) (rot)TinyImagenet Figure 3: Learning afﬁne data augmentations on subsets of data. (b) uses a ﬁxupResNet-8 architecture whereas (c) a ResNet-50 architecture. (b,c) Top: normal dataset, bottom: rotated dataset. Comparisons to traditional training / validation split We further perform comparisons between partitioned networks and the more traditional training/validation split (denoted as validation set optimization) with additional ﬁnetuning to the task of learning data augmentations. This is realized as follows; we partition 20kCIFAR10 examples into training and validation data of speciﬁc proportions. We then either train a partitioned network (along with the hyperparameters on LML) on these two chunks of data or train a standard network on the training set while using the validation set loss to obtain gradients for the data augmentation hyperparameters. For the validation set optimization baseline, once the hyperparameters are optimized, the resulting network is ﬁnetuned on the whole dataset for 20 epochs. The results for varying chunk proportions are provided in Table 2. Table 2: Learning afﬁne augmentations with ﬁxupResNet-14 on subset of CIFAR-10 (20kexamples). NaN denotes that a run crashed. Chunk Proportions Method [0.3,0.7] [0 .5,0.5] [0 .7,0.3] [0 .8,0.2] [0 .9,0.1] Partitioned 82.9%±0.3 83.0%±0.01 83.7%±0.2 84.0%±0.6 84.6%±0.05 Validation set optim. NaN 78.9%±0.04 81.5%±0.2 82.6%±0.1 83.4%±0.1 +Finetune NaN 81.3%±0.09 82.5%±0.2 83.5%±0.1 83.8%±0.3 Table 3: Learning a feature extractor (ﬁrst 2 out of 3 stages of a Wide ResNet-20) as a hyperparameter on CIFAR10. Method Chunk Proportions Test accuracy Validation set optim. [0.9,0.1] 59 .6%±0.6 Partitioned [0.1,0.8,0.1] 87.3%±0.8 We can see that partitioned net- works (that do not employ ad- ditional ﬁnetuning) outperform validation set optimization with ﬁnetuning in all settings we tried. The gap does get smaller when we move to the more tra- ditional 90/10 splits for train- ing/validation: a 10% proportion for validation data is enough to optimize a handful of hyper- parameters (just 6 scalars). To corroborate this claim, we set up an additional experiment; we use a Wide ResNet-20 on the full CIFAR10 dataset, where the ﬁrst two out of the three stages (13 convolution layers) are considered as hyperparameters. The results for this setting can be seen in Table 3. We see that 10% validation data are not enough, and the validation set optimization baseline performs poorly. This is in contrast to partitioned networks, where with three chunks, we can learn all of these hyperparameters successfully. Note that, compared to Augerino, applying partitioned networks to this setting is straightforward. To apply Augerino, one would have to come up with a metric that can be used to regularize the feature extractor towards “higher invariance”. Partitioned networks for federated learning We consider federated learning (FL) (McMahan et al., 2017), a setting where data is distributed across many clients. In this setting, there are system properties that make hyperparameter optimization especially challenging (Wang et al., 2021). More speciﬁcally, obtaining a validation set and performing multiple training runs with different 8Published as a conference paper at ICLR 2023 hyperparameter settings might not be possible due to the additional communication and computation costs, and transient client availability (clients join and leave the training process at any time). Optimizing hyperparameters together with the model parameters in a single run is therefore especially beneﬁcial (Wang et al., 2021), and partitioned networks are a good ﬁt for FL. We extend our centralized experimental setup to FL by splitting all N clients into Cnon-overlapping chunks, such that each chunk is understood as the union of all clients’ data shards that belong to that chunk. During federated training, a client belonging to chunk ksequentially optimizes partitions wk:C through sub-networks w(k:C) s and computes a gradient wrt. the hyperparameters ψ. Note that partitions w1:k remain unchanged and do not need to be communicated back to the server. This reduction in upload costs is a welcome property for FL, where upload costs can bottleneck system design. The server receives the (hyper-) parameter updates, averages them, and applies the result as a “gradient” to the server-side model in the traditional federated manner (Reddi et al., 2020). For partitioned networks, the hyperparameters that we optimize are the data augmentation parameters and, since we also include dropout in these architectures, the dropout rates (with the concrete relaxation from Maddison et al. (2016)). As a baseline, we consider the standard federated training without learning hyperparameters (denoted as FedAvg) as well as learning the augmentation parameters with Augerino Benton et al. (2020). Please see Appendix J for a detailed explanation of our FL setup. Table 4 summarizes our results using different sub-sets and variations of MNIST and CIFAR10, where we also included rotMNIST Larochelle et al. (2007) as another dataset. We can see that partitioned networks allow training models that generalize better than both FedAvg and FedAvg with Augerino, at reduced communication costs. Especially when the true data-generating process and underlying source of non-i.i.d.-ness are explicitly accounted for — here in the form of rotation — the beneﬁts of learning the augmentations with partitioned networks become apparent. For example, we observe that on the rotated datasets, partitioned networks learn to correctly increase the rotation angle. Table 4: Validation accuracy averaged over the last10 evaluations, each 10 rounds apart; standard- error is computed across 4 random seeds. All datasets are adapted to the federated setting and are synthetically split to be non-i.i.d. sampled as described in Appendix J.2. Dataset & size ↑MNIST ↑RotMNIST ↓Upload Method 1.25k 5k 50k 1.25k 5k 50k [%] FedAvg 95.4%±0.1 97.4%±0.1 99.0%±0.1 80.5%±0.0 90.4%±0.5 96.8%±0.1 100 FedAvg + Augerino 94.2%±0.5 96.4%±0.1 99.1%±0.0 79.5%±0.3 89.0%±2.0 95.3%±0.2 100 FedAvg + Partitioned97.0%±0.1 98.3%±0.0 99.2%±0.1 85.7%±0.9 93.5%±0.6 97.8%±0.1 77 ↑CIFAR10 ↑RotCIFAR10 ↓Upload 1.25k 5k 45k 1.25k 5k 45k [%] FedAvg 50.2%±0.4 64.5%±0.3 79.2%±0.7 35.6%±0.3 45.2%±0.1 53.9%±1.1 100 FedAvg + Augerino 49.9%±0.8 65.0%±0.2 79.9%±0.4 36.1%±0.2 45.0%±0.2 56.4%±0.7 100 FedAvg + Partitioned50.8%±1.0 64.8%±0.4 81.5%±0.5 37.1%±0.2 45.3%±0.3 60.6%±0.2 91 6 D ISCUSSION We propose partitioned networks as a new method for hyperparameter optimization inspired by the marginal likelihood objective. It provides a general and scalable solution to ﬁnding hyperparameters in a single training run without requiring access to a validation set while introducing less additional overhead to the training task than existing approaches. We showed that partitioned networks are applicable on a wide range of tasks; they can identify the correct model on illustrative toy examples, they can learn data augmentations in a way that improves data efﬁciency, they can optimize general feature extractors as hyperparameters and they can also optimize dropout rates. In the federated setting, partitioned networks allow us to overcome practical challenges, reduce the communication overhead and obtain better models. The notion of partitioned networks we propose in this work is novel to the literature and an orthogonal approach to many existing hyperparameter tuning algorithms. Like any other method, partitioned networks come with their own limitations, e.g., needing a partitioning strategy. We expand upon them in appendix H. We hope to see our method successfully reducing the need to perform hyperparameter search through repeated training and thereby contribute to the community’s effort to reduce its carbon footprint. 9Published as a conference paper at ICLR 2023 REFERENCES Gregory Benton, Marc Finzi, and Andrew G Wilson. Augerino, github, com- mit=fd542eb90ac6b1c0959156c1f6ad2ba8719d8572. https://github.com/g-benton/ learning-invariances/. (on page 18) Gregory Benton, Marc Finzi, Pavel Izmailov, and Andrew G Wilson. Learning invariances in neural networks from training data. Advances in neural information processing systems, 33:17605–17616, 2020. (on page 2, 5, 7, 9, 16, 18, 20, 24, 25) Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018. (on page 5) Kamal Dys. Cifar10 resnet: 90+% accuracy;less than 5 min. https://www.kaggle.com/code/ kmldas/cifar10-resnet-90-accuracy-less-than-5-min . Accessed: 2022-09- 17. (on page 26) Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. The Journal of Machine Learning Research, 20(1):1997–2017, 2019. (on page 5) Edwin Fong and Chris C Holmes. On the marginal likelihood and cross-validation. Biometrika, 107 (2):489–496, 2020. (on page 2) Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018. (on page 4) Yarin Gal, Jiri Hron, and Alex Kendall. Concrete dropout.Advances in neural information processing systems, 30, 2017. (on page 2) Pascal Germain, Francis Bach, Alexandre Lacoste, and Simon Lacoste-Julien. Pac-bayesian theory meets bayesian inference. Advances in Neural Information Processing Systems, 29, 2016. (on page 2) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE international conference on computer vision, pages 1026–1034, 2015. (on page 23) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European conference on computer vision , pages 630–645. Springer, 2016. (on page 23) Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. (on page 23) Alexander Immer and Tycho F. A. van der Ouderaa. Learning invariances with laplace ap- proximations (lila), github, commit=c0c4a09a109ed2f55e887def7d854b8a3a2330ef. https: //github.com/tychovdo/lila. (on page 17) Alexander Immer, Matthias Bauer, Vincent Fortuin, Gunnar R¨atsch, and Khan Mohammad Emtiyaz. Scalable marginal likelihood estimation for model selection in deep learning. In International Conference on Machine Learning, pages 4563–4573. PMLR, 2021. (on page 2, 5, 6, 24) Alexander Immer, Tycho F. A. van der Ouderaa, Gunnar R¨atsch, Vincent Fortuin, and Mark van der Wilk. Invariance learning in deep neural networks with differentiable laplace approximations, 2022. URL https://arxiv.org/abs/2202.10638. (on page 2, 5, 7, 15, 16, 17, 18, 22, 23, 24, 25) Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages 448–456. PMLR, 2015. (on page 23) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. (on page 4) 10Published as a conference paper at ICLR 2023 Mikhail Khodak, Renbo Tu, Tian Li, Liam Li, Maria-Florina F Balcan, Virginia Smith, and Ameet Talwalkar. Federated hyperparameter tuning: Challenges, baselines, and connections to weight- sharing. Advances in Neural Information Processing Systems, 34:19184–19197, 2021. (on page 5, 6) Hugo Larochelle, Dumitru Erhan, Aaron Courville, James Bergstra, and Yoshua Bengio. An empirical evaluation of deep architectures on problems with many factors of variation. In Proceedings of the 24th international conference on Machine learning, pages 473–480, 2007. (on page 9) Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015. (on page 24) Yonggang Li, Guosheng Hu, Yongtao Wang, Timothy Hospedales, Neil M Robertson, and Yongxin Yang. Dada: Differentiable automatic data augmentation. arXiv preprint arXiv:2003.03780, 2020. (on page 5) Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055, 2018. (on page 2, 5) Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by implicit differentiation. CoRR, abs/1911.02590, 2019. URL http://arxiv.org/abs/ 1911.02590. (on page 1, 2, 5) Sanae Lotﬁ, Pavel Izmailov, Gregory Benton, Micah Goldblum, and Andrew Gordon Wilson. Bayesian model selection, the marginal likelihood, and generalization. In Kamalika Chaud- huri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning , volume 162 of Pro- ceedings of Machine Learning Research , pages 14223–14247. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/lotfi22a.html. (on page 4) Clare Lyle, Lisa Schut, Robin Ru, Yarin Gal, and Mark van der Wilk. A bayesian perspective on training speed and model selection. Advances in Neural Information Processing Systems , 33: 10396–10408, 2020. (on page 2, 3, 5, 6) David JC MacKay. Information theory, inference and learning algorithms. Cambridge university press, 2003. (on page 2) Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016. (on page 6, 9, 26, 27) David A McAllester. Some pac-bayesian theorems. In Proceedings of the eleventh annual conference on Computational learning theory, pages 230–234, 1998. (on page 2) Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efﬁcient learning of deep networks from decentralized data. In Artiﬁcial intelli- gence and statistics, pages 1273–1282. PMLR, 2017. (on page 2, 8) Shakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih. Monte carlo gradient estimation in machine learning. J. Mach. Learn. Res., 21(132):1–62, 2020. (on page 6) Seth Nabarro, Stoil Ganev, Adri`a Garriga-Alonso, Vincent Fortuin, Mark van der Wilk, and Laurence Aitchison. Data augmentation in bayesian neural networks and the cold posterior effect. In Uncertainty in Artiﬁcial Intelligence, pages 1434–1444. PMLR, 2022. (on page 5, 7) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Gar- nett, editors, Advances in Neural Information Processing Systems 32, pages 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/9015-pytorch-an- imperative-style-high-performance-deep-learning-library .pdf. (on page 22) 11Published as a conference paper at ICLR 2023 Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Kone ˇcn`y, Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. arXiv preprint arXiv:2003.00295, 2020. (on page 9, 26, 27) Robin Ru, Clare Lyle, Lisa Schut, Miroslav Fil, Mark van der Wilk, and Yarin Gal. Speedy performance estimation for neural architecture search. Advances in Neural Information Processing Systems, 34:4079–4092, 2021. (on page 3) Akiyoshi Sannai, Masaaki Imaizumi, and Makoto Kawano. Improved generalization bounds of group invariant/equivariant deep networks via quotient feature spaces. In Uncertainty in Artiﬁcial Intelligence, pages 771–780. PMLR, 2021. (on page 3) Pola Schw¨obel, Martin Jørgensen, Sebastian W. Ober, and Mark van der Wilk. Last layer marginal likelihood for invariance learning, 2021. URL https://arxiv.org/abs/2106.07512. (on page 2, 5, 7, 15, 16, 23, 24, 26, 27) Jonas Seng, Pooja Prasad, Devendra Singh Dhami, and Kristian Kersting. Hanf: Hyperparameter and neural architecture search in federated learning. arXiv preprint arXiv:2206.12342, 2022. (on page 6) Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel Rodrigues. Generalization error of invariant classiﬁers. In Artiﬁcial Intelligence and Statistics, pages 1094–1103. PMLR, 2017. (on page 3) Mark van der Wilk, Matthias Bauer, ST John, and James Hensman. Learning invariances using the marginal likelihood. Advances in Neural Information Processing Systems, 31, 2018. (on page 2, 5, 7, 16) Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equiv- ariant cnns for digital pathology. In International Conference on Medical image computing and computer-assisted intervention, pages 210–218. Springer, 2018. (on page 15) Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H Brendan McMahan, Maruan Al-Shedivat, Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data, et al. A ﬁeld guide to federated optimization. arXiv preprint arXiv:2107.06917, 2021. (on page 5, 8, 9) Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3):229–256, 1992. (on page 6) Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel learning. In Artiﬁcial intelligence and statistics, pages 370–378. PMLR, 2016. (on page 7) Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on computer vision (ECCV), pages 3–19, 2018. (on page 23, 26) Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016. (on page 23) Hongyi Zhang, Yann N Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without normalization. arXiv preprint arXiv:1901.09321, 2019. (on page 7, 18, 23) Yi Zhou, Parikshit Ram, Theodoros Salonidis, Nathalie Baracaldo, Horst Samulowitz, and Heiko Ludwig. Flora: Single-shot hyper-parameter optimization for federated learning. arXiv preprint arXiv:2112.08524, 2021. (on page 6) 12Published as a conference paper at ICLR 2023 A LML IS A LOWER -BOUND TO THE MARGINAL LIKELIHOOD In this section, we show that the objective in equation 2 is a lower-bound on the marginal likelihood, under a mild assumption on each approximate posterior qk(w). The aim is to approximate: log p(D|ψ) = C∑ k=1 log p(Dk|D1:k−1,ψ) (5) Our partitioned approximation is given by: C∑ k=1 Eqk−1(w) [log p(Dk|w,ψ)] (6) We can get the equation for the gap between quantities in 5 and 6: gap = C∑ k=1 log p(Dk|D1:k−1,ψ) − C∑ k=1 Eqk−1(w) [log p(Dk|w,ψ)] (7) = C∑ k=1 Eqk−1(w) [log p(Dk|D1:k−1,ψ) −log p(Dk|w,ψ)] (8) = C∑ k=1 Eqk−1(w) [ log p(Dk|D1:k−1,ψ) p(Dk|w,ψ) ] (9) = C∑ k=1 Eqk−1(w)  log p(w,Dk|D1:k−1)    p(w|D1:k,ψ)p(Dk|D1:k−1,ψ)p(w|D1:k−1,ψ) p(w|D1:k,ψ)p(Dk|w,ψ)p(w|D1:k−1,ψ)   p(w,Dk|D1:k−1)   (10) = C∑ k=1 Eqk−1(w) [ log p(w|D1:k−1,ψ) p(w|D1:k,ψ) ] (11) = C∑ k=1 DKL [qk−1(w)∥p(w|D1:k,ψ)] −DKL [qk−1(w)∥p(w|D1:k−1,ψ)] (12) We now make two assumptions • DKL [qk−1(w)∥p(w|D1:k,ψ)] ≥DKL [qk(w)∥p(w|D1:k,ψ)]. This is motivated from the fact that qk(w) is trained on all data chunks D1:k so it is expected to be a better approxima- tion to the posterior p(w|D1:k), compared to qk−1(w) which is only trained on D1:k−1. • DKL [qC−1(w)∥p(w|D1:C,ψ)] ≥DKL [q0(w)∥p(w)]. Since we are free to choose the approximate posterior before seeing any data — q0(w)—, we can set it to be equal to the prior p(w) which, together with the positivity of the KL divergence, trivially satisﬁes this assumption. Therefore, by rearranging Eq. 12 and using our two assumptions we have that the gap is positive gap =−DKL [q0(w)∥p(w)] +DKL [qC−1(w)∥p(w|D1:C,ψ)] + C∑ k=1 DKL [qk−1(w)∥p(w|D1:k,ψ)] −DKL [qk(w)∥p(w|D1:k,ψ)] ≥0, (13) and our approximation is a lower bound to the marginal likelihood, i.e., log p(D|ψ) ≥ C∑ k=1 Eqk−1(w) [log p(Dk|w,ψ)] . (14) 13Published as a conference paper at ICLR 2023 B P ARTITIONED NETWORKS AS A SPECIFIC APPROXIMATION TO THE MARGINAL LIKELIHOOD In this section of the appendix, we show that the partitioned neural networks we presented in the paper are a particular instance of the approximation to the marginal likelihood shown in equation 2. Consider a dataset Dcomprised of C shards, i.e. D= (D1,..., DC), along with a model, e.g., a neural network, with parameters w ∈RDw, a prior p(w) = ∏Dw j=1 N(wj|0,λ) and a likelihood p(D|w,ψ) with hyperparameters ψ. Assuming a sequence over the dataset chunks, we can write out the true marginal likelihood as log p(D|ψ) = ∑ k log p(Dk|D1:k−1,ψ) = ∑ k log Ep(w|D1:k−1,ψ) [p(Dk|w,ψ)] (15) ≥ ∑ k Ep(w|D1:k−1,ψ) [log p(Dk|w,ψ)] . (16) Since the true posteriors p(w|D1:j,ψ) for j ∈{1,...,C }are intractable, we can use variational inference to approximate them with qφj(w) for j ∈{1,...,C }, with φj being the to-be-optimized parameters of the j’th variational approximation. Based on the result from Appendix A, whenqφj(w) are optimized to match the respective posteriors p(w|D1:j,ψ), we can use them to approximate the marginal likelihood as log p(D|ψ) ≥ ∑ k Eqφk−1 (w) [log p(Dk|w,ψ)] . (17) Partitioned networks correspond to a speciﬁc choice for the sequence of approximating distribution families qφk(w). Speciﬁcally, we partition the parameter space w into Cchunks, i.e., wk ∈RDwk, such that ∑ kDwk = Dw, and we associate each parameter chunk wk with a data shard Dk. Let rφk(wk) be base variational approximations over wk with parameters φk. Each approximate distribution qφk(w) is then deﬁned in terms of these base approximations, i.e., qφk(w) =   k−1∏ j=1 rφj(wj)  rφk(wk) ( K∏ m=k+1 r0(wm) ) (18) where r0(·) is some base distribution with no free parameters. In accordance with the assumptions in appendix A, we can then ﬁt each qφk(w) by minimising the KL-divergence to p(w|D1:k,ψ) – the posterior after seeing kchunks: DKL [qφk(w)∥p(w|D1:k,ψ)] =−Eqφk(w)[log p(D1:k|w,ψ)] +DKL [qφk(w)∥p(w)] + logp(D1:k|ψ) (19) (20) Finding the optimum with respect to φk: arg min φk DKL [qφk(w)∥p(w|D1:k,ψ)] = (21) = arg min φk −Eqφk(w)[log p(D1:k|w,ψ)] +DKL [qφk(w)∥p(w)] (22) = arg min φk −Eqφk(w)[log p(D1:k|w,ψ)] + DKL     k−1∏ j=1 rφj(wj)  rφk(wk) ( K∏ m=k+1 r0(wm) ) ∥ K∏ i p(wi)   (23) = arg min φk −Eqφk(w)[log p(D1:k|w,ψ)] +DKL [rφk(wk)∥p(wk)] . (24) We can now obtain partitioned networks by assuming that rφk(wk) = N(wk|φk,νI) for k ∈ {1,...,C }, r0(w) = N(w|ˆw,νI), with ˆw being the parameters at initialization (i.e., before we 14Published as a conference paper at ICLR 2023 update them on data) and taking ν →0, i.e., in machine-precision, the weights are deterministic. As noted in Section I.1, we scale the weight-decay regularizer forφk (whenever used) differently for each partition k, such that it can be interpreted as regularization towards a prior. In the experiments where we do not regularize φk according to p(wk) when we optimize them, this implicitly corresponds to λ→∞ (i.e. the limiting behaviour when the variance of p(w) goes to inﬁnity), which makes the contribution of the regularizer negligible. C P ARTITIONING SCHEMES There are several ways in which we could aim to partition the weights of a neural network. Throughout the experimental section 5, we partition the weights by assigning a ﬁxed proportion of weights in each layer to a given partition at random. We call this approach random weight partitioning. We also experimented with other partitioning schemes. For example, we tried assigning a ﬁxed proportion of a layer’s outputs (e.g., channels in a convolution layer) to each partition. All weights in a given layer that a speciﬁc output depends on would then be assigned to that partition. We call this approach node partitioning. Both approaches are illustrated in Figure 4. One beneﬁt of the node partitioning scheme is that it makes it possible to update multiple partitions with a single batch; This is because we can make a forward pass at each linear or convolutional layer with the full network parameters w, and, instead, mask the appropriate inputs and outputs to the layer to retrieve an equivalent computation to that with w(k) s . The gradients also need to be masked on the backward pass adequately. No such simpliﬁcation is possible with the random weight partitioning scheme; if we were to compute a backward pass for a single batch of examples using different subnetworks for each example, the memory overhead would grow linearly with the number of subnetworks used. In initial experiments, we found both random weight partitioning and node partitioning performed similarly. In the experimental section 5, we focused on the former, as it’s easier to reason about with relation to e.g., dropout. Throughout this work, partitioning happens prior to initiating training, and remains ﬁxed throughout. It might also be possible to partition the network parameters dynamically during training, which we leave for future work.   w11 w12 w13 w14 w15 w21 w22 w23 w24 w25 w31 w32 w33 w34 w35 w41 w42 w43 w44 w45 w51 w52 w53 w54 w55 w61 w62 w63 w64 w65   (a) Random weight partitioned In node assignment    Out node assignment      w11 w12 w13 w14 w15 w21 w22 w23 w24 w25 w31 w32 w33 w34 w35 w41 w42 w43 w44 w45 w51 w52 w53 w54 w55 w61 w62 w63 w64 w65   (b) Node partitioned Figure 4: Figures showing how the weights within a single weight matrix W ∈R6×5 for a linear layer would be partitioned. D S CALABILITY In the paper, we claim that our method is scalable compared to Schw¨obel et al. (2021) and Immer et al. (2022). What constraints the scalability of the mentioned prior works, however, is different. For the Last Layer Marginal Likelihood, although the approach works on small datasets such as PCAM (Veeling et al., 2018) and MNIST, the authors report that they were unable to learn invariances 15Published as a conference paper at ICLR 2023 on larger datasets such as CIFAR10. In (Schw¨obel et al., 2021, section 7), they explore the issue of scalability in more detail, and showcase that last layer marginal likelihood is insufﬁcient. Differentiable Laplace performs well, even on more complex datasets, such as CIFAR10. Their scalability, however, is limited by the computational and memory complexity of their method, which we go into in more detail in the section below. D.1 C OMPLEXITY ANALYSIS First, we consider the scalability of our algorithm in terms of computational and memory complexity. In particular, we show that our method scales much more favourably compared to Differentiable Laplace (Immer et al., 2022). We present our analysis for a feed-forward model of depth L, with layer widths D8. In order to directly compare to Immer et al. (2022) and Benton et al. (2020), we consider the complexities in the invariance learning setup (Benton et al., 2020; van der Wilk et al., 2018) withSaugmentation samples. In other experiments, hyperparameter optimization setups, S can be taken to be 1. The notation is summarized in Table 5. N Number of datapoints in dataset D NB Batch size S Number of augmentation samples9 C Output size (number of classes) D Feedforward network layer widths L Feedforward network depth P Number of parameters (s.t. O(P) =O(LD2 + DC)) Table 5: Notation for complexity analysis. We consider the computational and memory costs of 1) obtaining a gradient with respect to the parameters 2) obtaining a gradient with respect to the hyperparameters, and 3) computing the value of the model/hyperparameter selection objective for each method. All analysis assumes computation on a Monte-Carlo estimate of the objective on a single batch of data. In Tables 6 and 7, we assume that C <D, and hence, for the clarity of comparison, sometimes fold a factor depending Cinto a factor depending on Dif it’s clearly smaller. This hiding of the factors was only done for Differentiable Laplace, which is the worst scaling method. D.1.1 C OMPUTATIONAL COMPLEXITY Param. Backward Hyperparam. Backward Hyperparam. Objective Partitioned O(NBPS) O(NBPS) O(NBPS) Augerino O(NBPS) O(NBPS) O(NBPS) Diff. Laplace O(NBPS) O(NBPS+NCP +NCDLS + LD3) O(NPS + NCP +NCDLS + LD3) Table 6: Computational Complexities. The two terms highlighted for Augerino can be computed in a single backward pass. For Differentiable Laplace, the terms in blue can be amortised over multiple hyperparameter backward passes. That is why, in their method, they propose updating the hyperparameters once every epoch on (possibly) multiple batches of data, rather than once on every batch as is done with Partitioned Networks and Augerino. 8This is for the ease of comparison. Same upper bound complexities will hold for a network of variable sizes Dℓ for ℓ∈[L], where D= maxℓ Dℓ 9Only relevant for invariance learning. 16Published as a conference paper at ICLR 2023 D.1.2 M EMORY COMPLEXITY The memory complexities for Partitioned Networks, Augerino, and Differentiable Laplace are shown in Table 7. Crucially, the memory required to update the hyperparameters for Differentiable Laplace scales as O(NBSLD2 + P), with a term depending on the square of the network widths. This can become prohibitively expensive for larger models, and is likely the reason why their paper only considers experiments on architectures with widths up to a maximum of 256. Param. Backward Hyperparam. Backward Hyperparam. Objective Partitioned O(NBSLD+ P) O(NBSLD+ P) O(NBSD+ P) Augerino O(NBSLD+ P) O(NBSLD+ P) O(NBSD+ P) Diff. Laplace O(NBSLD+ P) O(NBSLD2 + P) O(NBSLD2 + P) Table 7: Memory Complexities. Differences are highlighted in red. D.2 P RACTICAL SCALABILITY A complexity analysis in big- Onotation as provided by us in the previous sections allows to understand scalability in the limit, but constant terms that manifest in practice are still of interest. In this section we aim present real timing measurements for our method in comparison to Augerino and Differential Laplace, and elaborate on what overhead might be expected with respect to standard neural network training. The empirical timings measurements on an NVIDIA RTX 3080-10GB GPU are shown in Table 8. We used a batch-size of 250, 200 for the MNIST and CIFAR10 experiments respectively, and 20 augmentation samples, just like in our main experiments in Table 1 and Figure 3. As can be seen, the overhead from using a partitioned network is fairly negligible compared to a standard forward and backward pass. The one difference compared to Augerino is, however, the fact that a separate forward-backward pass needs to be made to update the hyperparameters and regular parameters. This necessity is something that can be side-stepped with alternative partitioning schemes, as preliminarily mentioned in appendix C, and is an interesting direction for future research. MNIST CIFAR10 Method CNN ﬁxupResNet-8 ﬁxupResNet-14 Augerino ×1 ×1 ×1 Diff. Laplace† Param. ×1 ×1 ×1 Hyperparam. ×2015.6 ×18.2 - Partitioned Param. ×1.08 ×1.17 ×1.21 Hyperparam. ×1.08 ×1.08 ×1.09 Table 8: Relative empirical time increase with respect to a regular parameter update during standard training. †The timing multipliers with respect to the baseline for ﬁxupResNet-8 are taken from the timings reported in (Immer et al., 2022, Appendix D.4). On the ResNet-14, we get an out-of- memory error during the hyperparam. update step with Differentiable Laplace on the NVIDIA RTX 3080-10GB GPU when running with the ofﬁcial codebase (Immer and van der Ouderaa). Memory Overhead Our proposed method’s memory consumption scales in the same way as Augerino or vanilla neural network training. There is a minor constant memory overhead due to having to store the assignment of weights to partitions. In general, only log Cbits per parameter are necessary to store the partition assignments, whereCis the number of chunks. In our implementation, we only consider C <28, and hence store the assignments in byte tensors. This means that the partitioned models require extra 25% memory for storing the parameters (when using 32bit ﬂoats to represent the parameters). 17Published as a conference paper at ICLR 2023 If the “default” weight values (i.e. those denoted ˆwi in Figure 1) are non-zero, there is an additional overhead to storing those as well, which doubles the memory required to store the parameters. We observed there was no difference in performance when setting default weight values to 0 in architectures in which normalisation layers are used (i.e. most modern architectures). As such, we would in general recommend to set the default weight values to 0. However, we found setting default values to the initialised values to be necessary for stability of training deep normalisation-free architectures such as the ﬁxup architectures (Zhang et al., 2019) we used to compare with Differentiable Laplace. As their method is not compatible with BatchNorm, we used these architectures in our experiments, and hence used non-zero default values. Lastly, if the default weight values are set to the (random) initialisation values, it is possible to write a cleverer implementation in which only the random seeds are stored in memory, and the default values are re-generated every time they are need in a forward and a backward pass. This would make the memory overhead from storing the default values negligible. E N OTE ON AUGERINO In replicating Augerino (Benton et al., 2020) within our code-base and experimenting with the implementation, we discovered a pathological behaviour that is partly mirrored by the authors of Immer et al. (2022). In particular, note that the loss function (Benton et al., 2020, Equation (5)) proposed by the authors is problematic in the sense that for any regularization strength λ> 0, the optimal loss value is negative inﬁnity since the regularization term (negative L2-norm) is unbounded. In our experiments we observe that for a sufﬁciently-large value of λand after a sufﬁcient number of iterations, this behaviour indeed appears and training diverges. In practice, using Augerino therefore necessitates either careful tuning of λ, clipping the regularisation term (a method that introduces yet another hyperparameter), or other techniques such as early stopping. In the open-source repository for the submission (Benton et al.), it can be seen that on many experiments the authors use a ”safe” variant of the objective, in which they clip the regulariser (without pass-through of the gradient) once the l∞-norm of any of the hyperparameters becomes larger than an arbitrary threshold. Without using this adjustment, we found that the Augerino experiments on MNIST crashed every time with hyperparameters diverging to inﬁnity. F S ENSITIVITY TO PARTITIONING F.1 S ENSITIVITY IN TERMS OF FINAL PERFORMANCE (a) 2 chunks [8,1] [4,1] [1,1] [1,4] [1,8] Param. Proportions [8,1] [4,1] [1,1] [1,4] [1,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy  (b) 3 chunks [8,4,1][3,2,1][1,1,1][1,2,3][1,4,8] Param. Proportions [8,4,1] [3,2,1] [1,1,1] [1,2,3] [1,4,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy  (c) 4 chunks [8,4,2,1][4,3,2,1][1,1,1,1][1,2,3,4][1,2,4,8] Param. Proportions [8,4,2,1] [4,3,2,1] [1,1,1,1] [1,2,3,4] [1,2,4,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy Figure 5: Learning afﬁne augmentations on MNIST with a CNN ﬁt on all data. x- and y−ticks denote the ratios of parameters/datapoints assigned to each partition/chunk respectively. Partitioned networks allow for learning hyperparameters in a single training run, however, they introduce an additional hyperparameter in doing so: the partitioning scheme. The practitioner needs to choose the number of chunks C, the relative proportions of data in each chunk, and the relative proportions of parameters assigned to each of the Cpartitions wk. We investigate the sensitivity to the partitioning scheme here. We show that our results are fairly robust to partitioning through a grid-search over parameter partitions and chunk proportions on the afﬁne augmentation learning task on MNIST with the CNN architecture we use throughout this work. 18Published as a conference paper at ICLR 2023 (a) 2 chunks [8,1] [4,1] [1,1] [1,4] [1,8] Param. Proportions [8,1] [4,1] [1,1] [1,4] [1,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy  (b) 3 chunks [8,4,1][3,2,1][1,1,1][1,2,3][1,4,8] Param. Proportions [8,4,1] [3,2,1] [1,1,1] [1,2,3] [1,4,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy  (c) 4 chunks [8,4,2,1][4,3,2,1][1,1,1,1][1,2,3,4][1,2,4,8] Param. Proportions [8,4,2,1] [4,3,2,1] [1,1,1,1] [1,2,3,4] [1,2,4,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy Figure 6: Learning afﬁne augmentations on RotMNIST with a CNN ﬁt on all data. x- and y−ticks denote the ratios of parameters/datapoints assigned to each partition/chunk respectively. Figure 5 and Figure 6 show the test accuracy for a choice of chunk and parameter proportions across two, three and four chunks. The proportions are to be read as un-normalized distributions; for example, chunk proportions set to [1,8] denotes that there are 8×as many datapoints assigned to the second compared to the ﬁrst. Each conﬁguration was run with 2 random seeds, and we report the mean across those runs in the ﬁgure. The same architecture used was the same as for the main MNIST experiments in section 5 (see Appendix I.4 for details). We observe that for various partition/dataset-chunking conﬁgurations, all models achieve fairly similar ﬁnal test accuracy. There is a trend for models with a lot of parameters assigned to later chunks, but with few datapoints assigned to later chunks, to perform worse. While these results show a high level of robustness against the choice of additional hyperparameters introduced by our method, these results do show an opportunity or necessity for choosing the right partitioning scheme in order to achieve optimal performance. F.2 S ENSITIVITY IN TERMS OF HYPERPARAMETERS FOUND To compare how the different partitioning schemes qualitatively impact the hyperparameters that the method identiﬁes, we also retrain vanilla models from scratch using the hyperparameter values found using partitioned networks. Namely, we take the ﬁnal value of the hyperparameters learned with partitioned networks with a given partitioning scheme, and plot the ﬁnal test set accuracy of a vanilla neural network model trained from scratch with those hyperparameters. The results are shown in Figures 7 and 8. (a) 2 chunks [8,1] [4,1] [1,1] [1,4] [1,8] Param. Proportions [8,1] [4,1] [1,1] [1,4] [1,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy  (b) 3 chunks [8,4,1][3,2,1][1,1,1][1,2,3][1,4,8] Param. Proportions [8,4,1] [3,2,1] [1,1,1] [1,2,3] [1,4,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy  (c) 4 chunks [8,4,2,1][4,3,2,1][1,1,1,1][1,2,3,4][1,2,4,8] Param. Proportions [8,4,2,1] [4,3,2,1] [1,1,1,1] [1,2,3,4] [1,2,4,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy Figure 7: Standard neural network trained onMNIST with a CNN ﬁt on all data, with hyperparameters found using partitioned networks with chunk and parameter proportions corresponding to those in Figure 5. x- and y−ticks denote the ratios of parameters/datapoints assigned to each partition/chunk respectively. G H OW GOOD ARE THE HYPERPARAMETERS FOUND ? Here we show that the hyperparameters found by partitioned networks are also a good set of hyperparameters for vanilla neural networks retrained from scratch. This section expands on the 19Published as a conference paper at ICLR 2023 (a) 2 chunks [8,1] [4,1] [1,1] [1,4] [1,8] Param. Proportions [8,1] [4,1] [1,1] [1,4] [1,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy  (b) 3 chunks [8,4,1][3,2,1][1,1,1][1,2,3][1,4,8] Param. Proportions [8,4,1] [3,2,1] [1,1,1] [1,2,3] [1,4,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy  (c) 4 chunks [8,4,2,1][4,3,2,1][1,1,1,1][1,2,3,4][1,2,4,8] Param. Proportions [8,4,2,1] [4,3,2,1] [1,1,1,1] [1,2,3,4] [1,2,4,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy Figure 8: Standard neural network trained on RotMNIST with a CNN ﬁt on all data, with hyper- parameters found using partitioned networks with chunk and parameter proportions corresponding to those in Figure 6. x- and y−ticks denote the ratios of parameters/datapoints assigned to each partition/chunk respectively. experiment in section F.2. To validate this claim, we conducted a fairly extensive hyperparameter search on the afﬁne augmentation learning task on RotMNIST; we trained 200 models by ﬁrst sampling a set of afﬁne augmentation parameters uniformly at random from a predeﬁned range 10, and then training a neural network model (that averages across augmentation samples at train and test time, as described in Benton et al. (2020)) with standard neural training with those hyperparameters ﬁxed throughout. In Figure 9, we plot the ﬁnal test-set performance of all the models trained with those hyperparameters sampled from a ﬁxed range. Alongside, we show the hyperparameters and test-set performance of the partitioned networks as they progress throughout training. The partitioned networks consistently achieve ﬁnal test-set performance as good as that of the best hyperparameter conﬁgurations iden- tiﬁed through extensive random sampling of the space. We also show the test-set performance of neural network models, trained through standard training, with hyperparameters ﬁxed to the ﬁnal hyperparameter values identiﬁed by the partitioned networks. The hyperparameters identiﬁed by partitioned networks appear to also be good for regular neural networks; the standard neural networks with hyperparameters identiﬁed through partitioned training also outperform the extensive random sampling of the hyperparameter space. Furthermore, Figure 9 shows that partitioned networks do learn full rotation invariance on the RotMNIST task, i.e. when full rotation invariance is present in the data generating distribution. 0.0 0.2 0.4 Translation X 0.96 0.97 0.98 0.99T est Accuracy 0.0 0.2 0.4 Translation Y 0 /2 Rotation Random Sampling Partitioned Runs Partitioned Runs Final Partitioned Runs Final - Retrained 0.0 0.2 0.4 Scale X 0.0 0.2 0.4 0.6 Scale Y 0.0 0.2 0.4 Shear Figure 9: The test-set performance plotted alongside (1D projections of) afﬁne augmentation hyper- parameters on the RotMNIST task with MNIST-CNN. Final test-set accuracies are shown for the hyperparameters sampled randomly for a neural network model trained through standard training with those hyperparameters ﬁxed (+). For multiple partitioned networks runs, the plot shows the progres- sion of the identiﬁed hyperparameters and the test-set performance through the training run ( ), as well as the ﬁnal hyperparameters and test-set performance ( ). Lastly, the plot also shows the ﬁnal test-set accuracies of models trained through standard training on the ﬁnal hyperparameters identiﬁed through partitioned training ( ). 10The ranges were: Uniform(0,π) for the maximum rotation, and Uniform(0,1 2 ) for all the remaining afﬁne augmentation parameters (maximum shear, maximum x−and y−translation, and maximum x−and y−scale). 20Published as a conference paper at ICLR 2023 H L IMITATIONS As mentioned in the main text, our method improves upon existing work, but also comes with its own limitations. Complexity Inherent to our method — as presented in e.g. Figure 1 — is the necessity for an additional forward-backward pass to update the hyperparameters. Consequently, hyperparameter optimization has additional costs which, however, are signiﬁcantly less than the computational costs of existing work, as we discuss in more detail in Appendix D.1 and the experimental section. Furthermore, empirically, partitioned networks usually require more training iterations to converge. Performance Assuming the optimal hyper-parameters are given, training the full, non-partitioned networks based on those optimal values can be expected to yield better performance compared to the ﬁnal model found by partitioned training. Partitioning the network inherently constrains the network capacity, causing some loss of performance. Opportunities for alleviating this performance loss while still enjoying single-run hyperparameter optimization through partitioned training will be left to future work. These include for example adjusting training rounds or increasing network capacity in the ﬁrst place. Partitioning While partitioned networks allows for automatic optimization of, intuitively, hard to tune hyperparameters, such as augmentation parameters, they come with the additional limitation of requiring to partition both the data and the model. This introduces an additional hyperparameter, namely, the partitioning strategy. While our default strategy of assigning more parameters and data to the ﬁrst chunk works reasonably well on all of the experiments we consider, if one targets obtaining the best possible performance on a given task, the partitioning strategy might need additional tuning. We provide some empirical results about the sensitivity to partitioning in appendix F.1 I E XPERIMENTAL DETAILS I.1 P ARTITIONED TRAINING Partitioned parameter update scheduling The gradient computation of Equation 3, as described in the main text, requires that the data-points for updating a given subnetwork w(k) s come from the appropriate dataset chunks (x,y) ∈D1:k for a chunk k. Depending on the partitioning scheme (Appendix C), evaluating different subnetworks for different chunks can or cannot be done in a single mini-batch. More speciﬁcally, the random weight-partitioning we chose for our experiments requires a separate mini-batch per subnetwork (in order to keep the memory cost the same as for standard neural network training). An immediate question arising from a chunked dataset and several partitions is to deﬁne the order and frequency of updates across subnetworks. In our experiments we deﬁne (non-uniform) splits of the training dataset Dacross the Cchunks, which requires a tailored approach to sampling the data. More speciﬁcally, for a given (normalized) ratio of chunk-sizes [u1,...,u C], each iteration of partitioned training proceeds as follows: 1. Sample a partition index k∼Cat(u1,...,u C) 2. Sample a mini-batch ˜Dof examples uniformly from D1:k. 3. Evaluate log p( ˜D|w(k) s ,ψ) using subnetwork w(k) s and 4. compute the (stochastic) gradient wrt. partition parameters wk (Eq. 3). 5. Update partition parameters wk using an optimizer, such as SGD or Adam. This sampling scheme results in a data-point (x,y) ∈Dk from earlier chunks to be sampled more often. Concretely, the probability that an example in chunk kwill be sampled is ∝∑ i≤kui. This is done so that each partition wk is updated with equal probability on each of the examples in D1:k As a result, we use with replacement sampling for the partitioned network training throughout the experimental section. 21Published as a conference paper at ICLR 2023 Gradient optimization of partitioned parameters A consequence of per-partition updates with the random weight partitioning scheme (appendix C) is that, for a chosen partition wk to update, all other partitions do not receive a gradient update. In other words, the gradient at each iteration is sparse. Consequently, many off-the-shelve momentum-based optimizers will not account correctly. Speciﬁcally, we implement modiﬁcations to the PyTorch Paszke et al. (2019) provided optimizers that allow us to track per-partition momenta, number of steps, etc. Note that this creates a disconnect between the number of iterations across all partitions and the number of iterations per-partition. Doing so, however aligns the computational cost of training the partitioned network parameters with the cost of training regular neural network parameters. Regardless, we do not alter the way learning-rate schedulers behave in our experiments and anneal learning-rates according to the total number of iterations. Similarly, we report the total number of iterations when comparing against baselines that update all network-parameters per iteration. While a simple gradient-accumulation scheme across mini-batches would result in a single gradient across all partitions, this approach inherently clashes with non-uniform partitioning [u1,...,u C]. Instead, we chose to sequentially apply gradients computed on a single partition, as described in the previous paragraphs. A further advantage of this approach is that learning progress made by updating partition wk immediately inﬂuences (and can improve) the prediction of subnetworks w(k) s ,w(k+1) s ,..., w(C) s . Gradient optimization of hyperparameters Our partitioned network scheme makes it easy to compute stochastic gradients of the hyperparameter objective LML in Eq. 4 using batch gradient descent optimization methods. After every update to a randomly sampled network partition (see previous paragraph), we update hyperparamters ψas follows: • sample a dataset chunk index k ∼Cat(u2 Z ,..., uC Z ). Ratios are re-normalized to exclude D1. • sample a mini-batch ˜Dof examples uniformly from Dk (Note the choice of Dk instead of D1:k). • Evaluate log p( ˜D|w(k−1) s ,ψ) using subnetwork w(k−1) s and • compute the (stochastic) gradient wrt. hyperparameters ψ(Eq. 4). • Update partition parameters ψusing an optimizer, such as SGD or Adam. The above sampling procedure yields an unbiased estimate of gradients in eq. 4. The fact that we optimize hyperparameters with gradients based on data from a single chunk at a time is again a consequence of the random weight-partitioning scheme for the partitioned networks. It is possible to compute gradients wrt. ψfor mini-batches with examples from multiple chunks at a time. With the random weight partitioning scheme, this would result in an increased memory overhead. Lastly, we could also accumulate gradients from different chunks, similarly to Immer et al. (2022), and this would likely result in a lower-variance estimate per update . It is also possible to reduce the computational overhead of evaluating two mini-batches per iteration (one for updates to wk, one for ψ) as we do in our experiments by interleaving hyperparameter updates at less frequent intervals. We leave an exploration of these design choices to future work. Throughout all experiments, except those in the federated settings (see section J), we use the same batch-size for the hyperparameter udpates as for the regular parameter updates. Weight-decay For partitioned networks, whenever using weight-decay, we scale the weight decay for earlier partitions with the reciprocal of the number of examples in chunks used to optimize them, following the diagonal Gaussian prior interpretation of weight-decay. This makes the training compatible with the variational interpretation in Appendix B. I.2 P ARTITIONED AFFINE TRANSFORMATIONS In Appendix C we described how we realize partitioned versions of fully-connected and convolutional layers. Design choices for other parameterized network layers used in our experiments are described below. 22Published as a conference paper at ICLR 2023 Normalization layers It is common-place in most architectures to follow a normalization layer (such as BatchNorm (Ioffe and Szegedy, 2015), GroupNorm (Wu and He, 2018)) with an element- wise or channel-wise, afﬁne transformation. Namely, such a transformation multiplies its input h by a scale vector s and adds a bias vector b: o = h ∗s + b. For random weight-partitioned networks, we parameterize such afﬁne transformations by deﬁning separate vectors {s1,..., sC} and {b1,..., bC}for each partition; the actual scale and bias used in a given subnetwork w(k) s are s(k) s = ∏ i∈{1,...,k}si and b(k) s = ∑ i∈{1,...,k}bi respectively. This ensures that the ﬁnal afﬁne transformation for each subnetwork w(k) s depends on the parameters in the previous partitions [1,...,k −1]. Doing so increases the parameter count for the partitioned networks in architectures that use those normalization layers by a negligible amount. Scale and bias in FixUp networks The FixUp paper (Zhang et al., 2019) introduces extra scales and biases into the ResNet architecture that transform the entire output of the layers they follow. We turn these into “partitioned” parameters using the same scheme as that for scales and biases of afﬁne transformations following normalization layers. For partitioned networks, through-out the paper, we match the proportion of parameters assigned to each partition kin each layer to the proportion of data examples in the corresponding chunk Dk. I.3 A RCHITECTURE CHOICES Input selection experiments We use a fully-connected feed-forward neural network with2 hidden layers of size [256,256], and with GeLU (Hendrycks and Gimpel, 2016) activation functions. We initialise the weights using the Kaiming uniform scheme (He et al., 2015). For partitioned networks, we use the random-weight partitioning scheme. Fixup Resnet For all experiments using FixUp ResNets we follow Immer et al. (2022); Zhang et al. (2019), and use a 3-stage ResNet with channel-sizes (16,32,64) per stage, with identity skip- connections for the residual blocks as described in He et al. (2016). The residual stages are followed by average pooling and a ﬁnal linear layer with biases. We use 2D average pooling in the residual branches of the downsampling blocks.We initialize all the parameters as described in Zhang et al. (2019). Wide ResNet For all experiments using a Wide-ResNet-N-D (Zagoruyko and Komodakis, 2016), with N being the depth and D the width multiplier, we use a 3 stage ResNet with channel-sizes (16D,32D,64D). We use identity skip-connections for the residual blocks, as described in He et al. (2016), also sometimes known as ResNetV2. ResNet-50 We use the ”V2” version of Wide ResNet as described in (Zagoruyko and Komodakis, 2016) and replace BatchNormalization with GroupNormalization using 2 groups. We use the ’standard’ with withD= 1and three stages of 8 layers for a 50-layer deep ResNet. We use ReLU activations for all ResNet experiments throughout. MNIST CNN For the MNIST experiments, we use the same architecture as Schw¨obel et al. (2021) illustrated in the replicated Table 9. Table 9: CNN architecture for MNIST experiments Layer Speciﬁcation 2D convolution channels=20, kernel size=(5,5), padding=2, activation=ReLU Max pooling pool size=(2,2), stride=2 2D convolution channels=50, kernel size=(5,5), padding=2, activation=ReLU Max pooling pool size=(2,2), stride=2 Fully connected units=500, activation=ReLU Fully connected units=50, activation=ReLU Fully connected units=10, activation=Softmax 23Published as a conference paper at ICLR 2023 I.4 T RAINING DETAILS Learning afﬁne augmentations For the parametrization of the learnable afﬁne augmentation strategies, we follow prior works for a fair comparison. More speciﬁcally, for our MNIST based setup we follow the parametrization proposed in Schw¨obel et al. (2021) whereas for our CIFAR10 based setup we use the generator parametrization from Immer et al. (2022). Input selection experiments For the model selection (non-differentiable) input selection exper- iments, we train all variants with Adam with a learning rate of 0.001 and a batch-size of 256 for 10000 iterations. For both Laplace and partitioned networks, we do early stopping based on the marginal likelihood objective (LML for partitioned networks). We use weight-decay 0.0003 in both cases. For the post-hoc Laplace method, we use the diagonal Hessian approximation, following the recommendation in (Immer et al., 2021). For partitioned networks, we divide the data and parameters into 8 chunks of uniform sizes. We plot results averaged across 3 runs. Mask learning for input selection experiment We use the same optimizer settings as for the input selection experiment. We train for 30000 iterations, and optimize hyperparameters with Adam with a learning rate of 0.001. We divide the data and parameters into 4 uniform chunks. MNIST experiments We follow Schw¨obel et al. (2021), and optimize all methods with Adam with a learning rate of 0.001, no weight decay, and a batch-size of 200. For the partitioned net- works and Augerino results, we use 20 augmentation samples. We use an Adam optimizer for the hyperparameters with a learning rate of 0.001 (and default beta parameters). For Augerino on MNIST, we use the “safe” variant, as otherwise the hyperparameters and the loss diverge on every training run. We elaborate on this phenomenon in Appendix E. Otherwise, we follow the recommended settings from (Benton et al., 2020) and Immer et al. (2022), namely, a regularization strength of 0.01, and a learning rate for the hyperparameters of 0.05. For both MNIST and CIFAR experiments, we found it beneﬁcial to allocate more data to either the earlier, or the later, chunks. Hence, we use 3 chunks with [80%,10%,10%] split of examples for all MNIST and CIFAR experiments. CIFAR variations experiments We again follow Immer et al. (2022), and optimize all ResNet models with SGD with a learning rate of 0.1 decayed by a factor of 100×using Cosine An- nealing, and momentum of 0.9 (as is standard for ResNet models). We use a batch-size of 250. We again use Adam for hyperparameter optimization with a learning rate of 0.001 (and default beta parameters). We train our method for [2400,8000,12000,20000,40000] iterations on subsets [1000,5000,10000,20000,50000] respectively for CIFAR-10, just as in (Immer et al., 2022). For all methods, we used a weight-decay of 1e−4. For partitioned networks, we increase the weight decay for earlier partitions with the square root of the number of examples in chunks used to optimize them, following the diagonal Gaussian prior interpretation of weight-decay. We use3 chunks with [80%,10%,10%] split of examples. For RotCIFAR-10 results, we noticed our method hasn’t fully converged (based on training loss) in this number of iterations, and so we doubled the number of training iterations for the RotMNIST results. This slower convergence can be explained by the fact that, with our method, we only update a fraction of the network parameters at every iteration. TinyImagenet experiments Our experiments with TinyImagenet (Le and Yang, 2015) closely follow the setting for the CIFAR-10 experiments described above. Images are of size64x64 pixels, to be classiﬁed into one of 200 classes. The training-set consists of 100000 images and we compare our method against baselines on subset of [10000,50000,100000] datapoints. For the standard version of TinyImagenet, we train for [80000,80000,40000] steps respectively and for the rotated version of TinyImagenet we train for 120000 steps for all subset sizes. We tuned no other hyper-parameters compared to the CIFAR-10 setup and report our method’s result for a partitioning with[80%,20%] across 2 chunks after ﬁnding it to perform slightly better than a [80%,10%,10%] split across 3 chunks in a preliminary comparison. 24Published as a conference paper at ICLR 2023 Fine-tuning experiments For the ﬁne-tuning experiments in table 2, we trained a FixUp ResNet-14 on a subset of 20000 CIFAR10 examples, while optimizing afﬁne augmentations (following afﬁne augmentations parameterization in (Benton et al., 2020)). We used the same optimizer settings as for all other CIFAR experiments, and trained for 80000 iterations, decaying the learning rate with Cosine Annealing for the ﬁrst 60000 iterations. For ﬁne-tuning of validation-set optimization models, we used SGD with same settings, overriding only the learning rate to 0.01. We tried a learning rate of 0.01 and 0.001, and selected the one that was most favourable for the baseline based on the test accuracy. We also tried training on the full CIFAR-10 dataset, but found that all methods ended up within a standard error of each other when more than 70% of the data was assigned to the ﬁrst chunk (or training set, in the case of validation set optimization). This indicates that CIFAR-10 is sufﬁciently larger that, when combined with afﬁne augmentation learning and the relatively small ResNet-14 architecture used, using the extra data in the 2nd partition (or the validation set) results in negligible gains. I.5 D ATASETS Input selection synthetic dataset For the input selection dataset, we sample 3000 datapoints for the training set as described in section 5, and we use a fresh sample of 1000 datapoints for the test set. RotMNIST Sometimes in the literature, RotMNIST referes to a speciﬁc subset of 12000 MNIST examples, whereas in other works, the full dataset with 60000 examples is used. In this work, following (Benton et al., 2020; Immer et al., 2022) we use the latter. J F EDERATED PARTITIONED TRAINING In this section, we explain how partitioned networks can be applied to the federated setting, as well as the experimental details. J.1 P ARTITIONED NETWORKS IN FL In order to apply partitioned networks to the federated setting, we randomly choose a partition for each client such that the marginal distribution of partitions follows a pre-determined ratio. A given chunk Dk therefore corresponds to the union of several clients’ datasets. Analogous to how “partitioned training” is discussed in the main text and Appendix I, we desire each partition wk to be updated on chunks D1:k. Equation 3 in the main text explains which data chunks are used to compute gradients wrt. parameter partition wk. An analogous perspective to this objective is visualized by the exemplary algorithm in Figure 1 and asks which partitions are inﬂuenced (i,e., updated) by data from chunk Dk: A data chunk Dk is used to compute gradients wrt. partitions wk:C through subnetworks w(k) s to w(C) s respectively. Consequently, a client whose dataset is assigned to chunkDk can compute gradients for all partitions wk:C. Updating network partitions Due to the weight-partitioned construction of the partitioned neural networks, it is not possible to compute gradients with respect to all partitions in a single batched forward-pass through the network. Additionally, a change to the partition parameters wk directly inﬂuences subnetworks w(k+1) s to w(C) s . In order to avoid the choice of ordering indices kto Cfor the client’s local update computation, we update each partition independently while keeping all other partitions initialised to the server-provided values that the client received in that round t: Denote Di,k as the dataset of client iwhere we keep index kto emphasize the client’s assignment to chunkk. Further denote wt+1 j,i as the partition wt j after having been updated by client ion dataset Di,k. wt+1 j,i = arg max wj log p ( Di,k|(wt 1,..., wt j, ˆwt j+1,..., ˆwt j+C),ψ ) ∀j ∈[k,C], (25) where the details of optimization are explained in the following section. We leave an explo- ration for different sequential updating schemes to future work. The ﬁnal update communi- cated by a client to the server consists of the concatenation of all updated parameter partitions 25Published as a conference paper at ICLR 2023 wt+1 .,i = concat(wt+1 k,i ,..., wt+1 C,i ). Note that partitions (wt 1,..., wt k−1) have not been modiﬁed and need not be communicated to the server. The resulting communication reductions make partitioned networks especially attractive to FL as data upload from client to server poses a signiﬁcant bottleneck. In practice, we expect the beneﬁts of these communication reductions to outweigh the additional computation burden of sequentially computing gradients wrt., to multiple partitions. The server receives wt+1 .,i from all clients that participates in round t, computes the delta’s with the global model and proceeds to average them to compute the server-side gradient in the typical federated learning fashion (Reddi et al., 2020). Updating hyperparameters The computation of gradients on a clientiwrt. ψis a straight-forward extension of equation 4 and the exemplary algorithm of Figure 1: ∇ψLML (Di,k,ψ) ≈∇ψlog p ( Di,k|w(t+1),(k−1) s,i ,ψ ) , (26) where Di,k corresponds to client i’s local dataset which is assigned to chunk k and w(t+1),(k−1) s corresponds to the (k−1)’th subnetwork after incorporating all updated partitionsw(t+1),(k−1) s,i = concat(wt 1,..., wt k−1,wt+1 k,i ,..., wt+1 C,i ). Note that we compute a full-batch update to ψin MNIST experiments and use a batch-size equal to the batch-size for the partitioned parameter updates for CIFAR10. Upon receiving these gradients from all clients in this round, the server averages them to form a server-side gradient. Conceptually, this approach to updating ψcorresponds to federated SGD. J.2 F EDERATED SETUP Non-i.i.d. partitioning For our federated experiments, we split the 50kMNIST and 45kCIFAR10 training data-points across 100 clients in a non-i.i.d. way to create the typical challenge to federated learning experiments. In order to simulate label-skew, we follow the recipe proposed in Reddi et al. (2020) with α= 1.0 for CIFAR10 and α= 0.1 for MNIST. Note that with α= 0.1, most clients have data corresponding to only a single digit. For our experiments on rotated versions of CIFAR10 and MNIST, we sample a degree of rotation per data-point and keep it ﬁxed during training. In order to create a non-i.i.d partitioning across the clients, we bin data-points according to their degree of rotation into 10 bins and sample using the same technique as for label-skew with α = 0.1 for both datasets. Learning curves are computed using the 10k MNIST and 5k CIFAR10 validation data-points respectively. For the rotated dataset experiments, we rotate the validation set in the same manner as the training set. Architectures and experimental setup We use the convolutional network provided at Schw¨obel et al. (2021) for MNIST and the ResNet-9 (Dys) model for CIFAR10 but with group normaliza- tion (Wu and He, 2018) instead of batch normalization. We include (learnable) dropout using the continuous relaxation proposed at Maddison et al. (2016) between layers for both architectures. We select 3 chunks for MNIST with a [0.7,0.2,0.1] ratio for both, client-assignments and parameter- partition sizes. For CIFAR10, we found a [0.9,0.1] split across 2 sub-networks to be beneﬁcial. In addition to dropout logits, ψencompasses parameters for afﬁne transformations, i.e., shear, trans- lation, scale and rotation. We report results after 2kand 5krounds, respectively, and the expected communication costs as a percentage of the non-partitioned baseline. Shared setting In order to elaborate on the details to reproduce our results, we ﬁrst focus on the settings that apply across all federated experiments. We randomly sample the corresponding subset of 1.25k, 5kdata-points from the full training set and keep that selection ﬁxed across experiments (i,e., baselines and partitioned networks) as well as seeds. The subsequent partitioning across clients as detailed in the previous paragraph is equally kept ﬁxed across experiments and seeds. Each client computes updates for one epoch of its local dataset, which, for the low data regimes of 1.25k data-points globally, results in single update per client using the entire local dataset. We averaged over 10 augmentation samples for the forward pass in both training and inference. MNIST & RotMNIST For 5k data-points and correspondingly 50 data-points on average per client, most clients perform a single update step. A small selection of clients with more than 64 data- 26Published as a conference paper at ICLR 2023 points performs two updates per round. For the experiments using the full dataset and a mini-batch size of 64, each client performs multiple updates per round. After initial exploration on the baseline FedAvg task, we select a local learning-rate of 5e−2 and apply standard SGD. The server performs Adam Reddi et al. (2020) with a learning rate of 1e−3 for the model parameters. We keep the other parameters of Adam at their standard PyTorch values. We ﬁnd this setting to generalize to the partitioned network experiments but found a higher learning rate of3e−3 for the hyper-parameters to be helpful. We chose the convolutional network from Schw¨obel et al. (2021) with (learned) dropout added between layers. The model’s dropout layers are initialized to drop10% of hidden activations. For the baseline model we keep the dropout-rate ﬁxed and found 10% to be more stable than 30%. CIFAR10 & RotCIFAR10 We ﬁx a mini-batch size of 32, leading to multiple updates per client per round in both, the full dataset regime as well as the5kdata-points setting. Similarly to the MNIST setting, we performed an initial exploration of hyperparameters on the baseline FedAvg task and use the same ones on partitioned networks. We used dropout on the middle layer of each block which was initialized to 0.1 for both the baseline and partitioned networks and whereas partitioned networks optimized it with LML and the concrete relaxation from Maddison et al. (2016), the baseline kept it ﬁxed. For the server side optimizer we used Adam with the default betas and a learning rate of 1e−2, whereas for the hyperparameters we used Adam with the default betas and a learning rate of 1e−3. In both cases we used an ϵ= 1e−7. For the local optimizer we used SGD with a learning rate of 10−0.5 and no momentum. J.3 MNIST LEARNING CURVES In Figure 10 we show learning curves for the three considered dataset sizes on the standard MNIST task. Each learning curve is created by computing a moving average across 10 evaluations, each of which is performed every 10 communication rounds, for each seed. We then compute the average and standard-error across sees and plot those values on the y-axis. On the x-axis we denote the total communication costs (up- and download) to showcase the partitioned networks reduction in communication overhead. We see that especially for the low dataset regime, training has not converged yet and we expect performance to improve for an increased number of iterations. Figure 10: Learning curves for MNIST experiments on 1.25k, 5kand 50kdata-points respectively. 27",
      "meta_data": {
        "arxiv_id": "2304.14766v1",
        "authors": [
          "Bruno Mlodozeniec",
          "Matthias Reisser",
          "Christos Louizos"
        ],
        "published_date": "2023-04-28T11:24:41Z",
        "pdf_url": "https://arxiv.org/pdf/2304.14766v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper proposes \"Partitioned Neural Networks,\" a novel and efficient method for hyperparameter optimization (HPO) that operates in a single training run without needing a separate validation set. Inspired by the marginal likelihood, the method partitions the training data into shards and the neural network into corresponding parameter partitions. It optimizes hyperparameters by minimizing an \"out-of-training-sample\" loss on data shards unseen by a subnetwork, thereby approximating the marginal likelihood. This approach is significantly more computationally efficient and scalable than existing marginal likelihood approximations (e.g., Laplace approximations) and is broadly applicable to various hyperparameters like input masks, data augmentations, dropout rates, and feature extractors. It also demonstrates particular advantages in federated learning settings by reducing communication overhead and improving model generalization, especially in low-data regimes and for handling non-i.i.d. data.",
        "methodology": "The methodology is based on approximating a lower bound of the marginal likelihood, formulated as LML(D,ψ) = Σ_k Eq_k-1(w)[log p(D_k|w,ψ)]. The training data is partitioned into C chunks (D1,..., DC), and the neural network weights (w) are partitioned into C sub-weights (w1,..., wC). A k-th subnetwork, w(k)s, comprises the first k parameter partitions (w1,..., wk) and sets the remaining parameters to default values (e.g., initialization). During training, stochastic gradient updates are interleaved: first, a parameter partition wk is updated on data chunks D1:k using its subnetwork w(k)s; then, hyperparameters ψ are updated using the out-of-training-sample loss, which evaluates log p(Dk|w(k-1)s, ψ) on a newly sampled chunk Dk unseen by the w(k-1)s subnetwork. The primary partitioning scheme used is random weight partitioning, where a fixed proportion of weights in each layer is randomly assigned to a partition. For federated learning, clients are assigned to data chunks, and each client optimizes its relevant parameter partitions (wk:C) and computes hyperparameter gradients, reducing communication by not transmitting unmodified earlier partitions.",
        "experimental_setup": "Experiments were conducted on various tasks including a toy input selection task (identifying 15 informative features out of 30), differentiable input selection using learnable masks, learning affine image augmentations, and learning general feature extractors. The datasets included synthetic data for the toy tasks, MNIST, CIFAR10, TinyImagenet, and their rotated variants (rotMNIST, rotCIFAR10, rotTinyImagenet), as well as experiments in low-data regimes (subsets of these datasets). Architectures used comprised fully-connected MLPs, fixupResNets (ResNet-8, ResNet-14), ResNet-50 with GroupNorm(2), Wide ResNet-20, and a CNN for MNIST. The method was compared against baselines such as standard training with no augmentations, Augerino, Differentiable Laplace, Last-layer Marginal Likelihood, and traditional validation set optimization. For federated learning, FedAvg and FedAvg with Augerino were used as baselines, with non-i.i.d. data partitioning (label-skew and rotation-skew) across clients. Validation methods included test accuracy, log-likelihood, and an analysis of robustness to different partitioning schemes.",
        "limitations": "The method introduces an additional forward-backward pass for hyperparameter updates, leading to some computational overhead, though still less than existing marginal likelihood methods. Empirically, it may require more training iterations to converge. There is an inherent constraint on network capacity due to partitioning, which might cause a performance loss compared to a full, non-partitioned network trained with *optimally given* hyperparameters. Furthermore, the partitioning strategy itself (number of chunks, relative data and parameter proportions) becomes an additional hyperparameter that might require tuning for optimal performance, despite observed robustness. Lastly, if default weight values are non-zero (e.g., in normalization-free architectures like FixUp), storing these adds a constant memory overhead, potentially doubling memory for parameters, though this can be mitigated by clever implementation or setting defaults to zero.",
        "future_research_directions": "Future research could explore dynamic partitioning of network parameters during training, rather than fixing them prior to training. Investigating alternative partitioning schemes beyond random weight and node partitioning is also a promising direction, potentially enabling more efficient gradient computations across multiple partitions or reducing memory overheads. Efforts could be made to alleviate the observed performance loss caused by network partitioning, for instance, by adjusting training rounds or strategically increasing network capacity. Further exploration of different hyperparameter update schedules, such as accumulating gradients from multiple chunks or performing less frequent hyperparameter updates, could lead to improved efficiency or stability. In the federated learning context, investigating alternative sequential updating schemes for client-side partitions is another area for future work."
      }
    },
    {
      "title": "Learning to Mutate with Hypergradient Guided Population"
    },
    {
      "title": "Implicit differentiation of Lasso-type models for hyperparameter optimization",
      "abstract": "Setting regularization parameters for Lasso-type estimators is notoriously\ndifficult, though crucial in practice. The most popular hyperparameter\noptimization approach is grid-search using held-out validation data.\nGrid-search however requires to choose a predefined grid for each parameter,\nwhich scales exponentially in the number of parameters. Another approach is to\ncast hyperparameter optimization as a bi-level optimization problem, one can\nsolve by gradient descent. The key challenge for these methods is the\nestimation of the gradient with respect to the hyperparameters. Computing this\ngradient via forward or backward automatic differentiation is possible yet\nusually suffers from high memory consumption. Alternatively implicit\ndifferentiation typically involves solving a linear system which can be\nprohibitive and numerically unstable in high dimension. In addition, implicit\ndifferentiation usually assumes smooth loss functions, which is not the case\nfor Lasso-type problems. This work introduces an efficient implicit\ndifferentiation algorithm, without matrix inversion, tailored for Lasso-type\nproblems. Our approach scales to high-dimensional data by leveraging the\nsparsity of the solutions. Experiments demonstrate that the proposed method\noutperforms a large number of standard methods to optimize the error on\nheld-out data, or the Stein Unbiased Risk Estimator (SURE).",
      "full_text": "Implicit differentiation of Lasso-type models for hyperparameter optimization Quentin Bertrand* 1 Quentin Klopfenstein* 2 Mathieu Blondel3 Samuel Vaiter4 Alexandre Gramfort1 Joseph Salmon5 Abstract Setting regularization parameters for Lasso-type estimators is notoriously difﬁcult, though cru- cial in practice. The most popular hyperparam- eter optimization approach is grid-search using held-out validation data. Grid-search however re- quires to choose a predeﬁned grid for each pa- rameter, which scales exponentially in the num- ber of parameters. Another approach is to cast hyperparameter optimization as a bi-level opti- mization problem, one can solve by gradient de- scent. The key challenge for these methods is the estimation of the gradient w.r.t.the hyperpa- rameters. Computing this gradient via forward or backward automatic differentiation is possible yet usually suffers from high memory consump- tion. Alternatively implicit differentiation typi- cally involves solving a linear system which can be prohibitive and numerically unstable in high dimension. In addition, implicit differentiation usually assumes smooth loss functions, which is not the case for Lasso-type problems. This work introduces an efﬁcient implicit differentia- tion algorithm, without matrix inversion, tailored for Lasso-type problems. Our approach scales to high-dimensional data by leveraging the sparsity of the solutions. Experiments demonstrate that the proposed method outperforms a large num- ber of standard methods to optimize the error on held-out data, or the Stein Unbiased Risk Esti- mator (SURE). *Equal contribution 1Université Paris-Saclay, Inria, CEA, Palaiseau, France 2Institut Mathématique de Bourgogne, Univer- sité de Bourgogne, Dijon, France 3Google Research, Brain team, Paris, France 4CNRS and Institut Mathématique de Bourgogne, Université de Bourgogne, Dijon, France 5IMAG, Université de Montpellier, CNRS, Montpellier, France. Correspondence to: Quentin Bertrand <quentin.bertrand@inria.fr>, Quentin Klopfen- stein <quentin.klopfenstein@u-bourgogne.fr>. Proceedings of the 37 th International Conference on Machine Learning, Online, PMLR 119, 2020. Copyright 2020 by the au- thor(s). 1. Introduction In many statistical applications, the number of parame- ters p is much larger than the number of observations n. In such scenarios, a popular approach to tackle linear re- gression problems is to consider convex ℓ1-type penalties, used in Lasso (Tibshirani, 1996), Group-Lasso (Yuan and Lin, 2006), Elastic-Net (Zou and Hastie, 2005) or adap- tive Lasso (Zou, 2006). These Lasso-type estimators rely on regularization hyperparameters, trading data ﬁdelity against sparsity. Unfortunately, setting these hyperparame- ters is hard in practice: estimators based on ℓ1-type penal- ties are indeed more sensitive to the choice of hyperparam- eters than ℓ2 regularized estimators. To control for overﬁtting, it is customary to use different datasets for model training ( i.e., computing the regression coefﬁcients) and hyperparameter selection ( i.e., choosing the best regularization parameters). A metric, e.g., hold- out loss , is optimized on a validation dataset (Stone and Ramer, 1965). Alternatively one can rely on a statistical criteria that penalizes complex models such as AIC/BIC (Liu et al., 2011) or SURE (Stein Unbiased Risk Estima- tor, Stein 1981). In all cases, hyperparameters are tuned to optimize a chosen metric. The canonical hyperparameter optimization method is grid-search. It consists in ﬁtting and selecting the best model over a predeﬁned grid of parameter values. The complexity of grid-search is exponential with the number of hyperparameters, making it only competitive when the number of hyperparameters is small. Other hyperparameter selection strategies include random search (Bergstra and Bengio, 2012) and Bayesian optimization (Brochu et al., 2010; Snoek et al., 2012) that aims to learn an approxima- tion of the metric over the parameter space and rely on an exploration policy to ﬁnd the optimum. Another line of work for hyperparameter optimization (HO) relies on gradient descent in the hyperparameter space. This strategy has been widely explored for smooth objective functions (Larsen et al., 1996; Bengio, 2000; Larsen et al., 2012). The main challenge for this class of methods is estimating the gradient w.r.t.the hyperparame- ters. Gradient estimation techniques are mostly divided in two categories. Implicit differentiation requires the exact arXiv:2002.08943v3  [stat.ML]  3 Sep 2020Implicit differentiation of Lasso-type models for hyperparameter optimization solution of the optimization problem and involves the res- olution of a linear system (Bengio, 2000). This can be ex- pensive to compute and lead to numerical instabilities, es- pecially when the system is ill-conditioned (Lorraine et al., 2019). Alternatively, iterative differentiation computes the gradient using the iterates of an optimization algorithm. Backward iterative differentiation (Domke, 2012) is com- putationally efﬁcient when the number of hyperparameters is large. However it is memory consuming since it requires storing all intermediate iterates. In contrast, forward itera- tive differentiation (Deledalle et al., 2014; Franceschi et al., 2017) does not require storing the iterates but can be com- putationally expensive with a large number of hyperparam- eters; see Baydin et al. (2018) for a survey. This article proposes to investigate the use of these meth- ods to set the regularization hyperparameters in an auto- matic fashion for Lasso-type problems. To cover the cases of both low and high number of hyperparameters, two esti- mators are investigated, namely the Lasso and the weighted Lasso which have respectively one or as many parameters as features. Our contributions are as follows: • We show that forward iterative differentiation of block coordinate descent (BCD), a state-of-the-art solver for Lasso-type problems, converges towards the true gra- dient. Crucially, we show that this scheme converges linearly once the support is identiﬁed and that its limit does not depend of the initial starting point. • These results lead to the proposed algorithm (Algo- rithm 2) where the computation of the Jacobian is de- coupled from the computation of the regression co- efﬁcients. The later can be done with state-of-the-art convex solvers, and interestingly, it does not require solving a linear system, potentially ill-conditioned. • We show through an extensive benchmark on simu- lated and real high dimensional data that the proposed method outperforms state-of-the-art HO methods. Our work is somewhat similar to Gregor and LeCun (2010); Xin et al. (2016); Borgerding et al. (2017); Liu et al. (2018); Wu et al. (2019), where the solution is differenti- ated w.r.t. optimization parameters instead of the regular- ization parameter. However the goal is very different as they want to accelerate the optimization algorithm whereas we provide an efﬁcient algorithm to compute the gradient. Notation The design matrix is X ∈Rn×p (corresponding to nsamples and pfeatures) and the observation vector is y ∈Rn. The regularization parameter, possibly multivari- ate, is denoted by λ = (λ1,...,λ r)⊤ ∈Rr. We denote ˆβ(λ) ∈Rp the regression coefﬁcients associated to λ. We denote ˆJ(λ) ≜ (∇λˆβ(λ) 1 ,..., ∇λˆβ(λ) p )⊤∈Rp×r the weak Jacobian (Evans and Gariepy, 1992) of ˆβ(λ) w.r.t.λ. For a function ψ : Rp ×Rr →R with weak derivatives of order two, we denote by∇βψ(β,λ) ∈Rp(resp. ∇λ(β,λ) ∈Rr) its weak gradient w.r.t.the ﬁrst parameter (resp. the second parameter). The weak Hessian ∇2ψ(β,λ) is a matrix in R(p+r)×(p+r) which has a block structure ∇2ψ(β,λ) = (∇2 βψ(β,λ) ∇2 β,λψ(β,λ) ∇2 λ,βψ(β,λ) ∇2 λψ(β,λ) ) . The support of ˆβ(λ) (the indices of non-zero coefﬁcients) is denoted by ˆS(λ), and ˆs(λ) represents its cardinality (i.e., the number of non-zero coefﬁcients). The sign vec- tor sign ˆβ(λ) ∈Rp is the vector of component-wise signs (with the convention thatsign(0) = 0) of ˆβ(λ). Note that to ease the reading, we drop λin the notation when it is clear from the context and use ˆβ, ˆJ, ˆS and ˆs. The Mahalanobis distance of a vector x ∈Rp and a matrix A ≻0 is noted ∥x∥A ≜ √ x⊤A−1x. 2. Background 2.1. Problem setting To favor sparse coefﬁcients, we consider Lasso-type es- timators based on non-smooth regularization functions. Such problems consist in ﬁnding: ˆβ(λ) ∈arg min β∈Rp ψ(β,λ) . (1) The Lasso (Tibshirani, 1996) is recovered, with the number of hyperparameters set to r= 1: ψ(β,λ) = 1 2n∥y−Xβ∥2 2 + eλ∥β∥1 , (2) while the weighted Lasso (wLasso, Zou 2006, introduced to reduce the bias of the Lasso) has r= phyperparameters and reads: ψ(β,λ) = 1 2n∥y−Xβ∥2 2 + p∑ j=1 eλj|βj|. (3) Note that we adopt the hyperparameter parametrization of Pedregosa (2016), i.e., we write the regularization parame- ter as eλ. This avoids working with a positivity constraint in the optimization process and ﬁxes scaling issues in the line search. It is also coherent with the usual choice of a geometric grid for grid-search (Friedman et al., 2010). Remark 1. Other formulations could be investigated like Elastic-Net or non-convex formulation, e.g., MCP (Zhang, 2010). Our theory does not cover non-convex cases, though we illustrate that it behaves properly numerically. Handling such non-convex cases is left as a question for future work. The HO problem can be expressed as a nested bi-level op- timization problem. For a given differentiable criterion C: Rp ↦→R (e.g., hold-out loss or SURE), it reads:Implicit differentiation of Lasso-type models for hyperparameter optimization arg min λ∈Rr { L(λ) ≜ C ( ˆβ(λ) )} s.t. ˆβ(λ) ∈arg min β∈Rp ψ(β,λ) . (4) Note that SURE itself is not necessarily weakly differen- tiable w.r.t. ˆβ(λ). However a weakly differentiable approx- imation can be constructed (Ramani et al., 2008; Deledalle et al., 2014). Under the hypothesis that Problem (1) has a unique solution for every λ∈Rr, the function λ↦→ˆβ(λ) is weakly differentiable (Vaiter et al., 2013). Using the chain rule, the gradient of Lw.r.t.λthen writes: ∇λL(λ) = ˆJ⊤ (λ)∇C ( ˆβ(λ) ) . (5) Computing the weak Jacobian ˆJ(λ) of the inner problem is the main challenge, as once the hypergradient ∇λL(λ) has been computed, one can use usual gradient descent, λ(t+1) = λ(t) −ρ∇λL(λ(t)), for a step size ρ > 0. Note however that Lis usually non-convex and conver- gence towards a global minimum is not guaranteed. In this work, we propose an efﬁcient algorithm to compute ˆJ(λ) for Lasso-type problems, relying on improved forward dif- ferentiation. 2.2. Implicit differentiation (smooth case) Implicit differentiation, which can be traced back to Larsen et al. (1996), is based on the knowledge of ˆβ and requires solving a p×plinear system (Bengio, 2000, Sec. 4). Since then, it has been extensively applied in various contexts. Chapelle et al. (2002); Seeger (2008) used implicit differ- entiation to select hyperparameters of kernel-based mod- els. Kunisch and Pock (2013) applied it to image restora- tion. Pedregosa (2016) showed that each inner optimiza- tion problem could be solved only approximately, leverag- ing noisy gradients. Related to our work, Foo et al. (2008) applied implicit differentiation on a “weighted” Ridge-type estimator (i.e., a Ridge penalty with one λj per feature). Yet, all the aforementioned methods have a common draw- back : they are limited to the smooth setting, since they rely on optimality conditions for smooth optimization. They proceed as follows: if β ↦→ψ(β,λ) is a smooth convex function (for any ﬁxed λ) in Problem (1), then for all λ, the solution ˆβ(λ) satisﬁes the following ﬁxed point equation: ∇βψ ( ˆβ(λ),λ ) = 0 . (6) Then, this equation can be differentiated w.r.t.λ: ∇2 β,λψ( ˆβ(λ),λ) + ˆJ⊤ (λ)∇2 βψ( ˆβ(λ),λ) = 0. (7) Assuming that ∇2 βψ( ˆβ(λ),λ) is invertible this leads to a closed form solution for the weak Jacobian ˆJ(λ): ˆJ⊤ (λ) = −∇2 β,λψ ( ˆβ(λ),λ )( ∇2 βψ(β(λ),λ) )    p×p −1 , (8) which in practice is computed by solving a linear system. Unfortunately this approach cannot be generalized for non- smooth problems since Equation (6) no longer holds. 2.3. Implicit differentiation (non-smooth case) Related to our work Mairal et al. (2012) used implicit dif- ferentiation with respect to the dictionary ( X ∈Rn×p) on Elastic-Net models to perform dictionary learning. Regard- ing Lasso problems, the literature is quite scarce, see (Dos- sal et al., 2013; Zou et al., 2007) and (Vaiter et al., 2013; Tibshirani and Taylor, 2011) for a more generic setting encompassing weighted Lasso. General methods for gra- dient estimation of non-smooth optimization schemes ex- ist (Vaiter et al., 2017) but are not practical since they de- pend on a possibly ill-posed linear system to invert. Amos and Kolter (2017) have applied implicit differentiation on estimators based on quadratic objective function with lin- ear constraints, whereas Niculae and Blondel (2017) have used implicit differentiation on a smooth objective func- tion with simplex constraints. However none of these ap- proaches leverages the sparsity of Lasso-type estimators. 3. Hypergradients for Lasso-type problems To tackle hyperparameter optimization of non-smooth Lasso-type problems, we propose in this section an efﬁcient algorithm for hypergradient estimation. Our algorithm re- lies on implicit differentiation, thus enjoying low-memory cost, yet does not require to naively solve a (potentially ill-conditioned) linear system of equations. In the sequel, we assume access to a (weighted) Lasso solver, such as ISTA (Daubechies et al., 2004) or Block Coordinate De- scent (BCD, Tseng and Yun 2009, see also Algorithm 5). 3.1. Implicit differentiation Our starting point is the key observation that Lasso-type solvers induce a ﬁxed point iteration that we can leverage to compute a Jacobian. Indeed, proximal BCD algorithms (Tseng and Yun, 2009), consist in a local gradient step com- posed with a soft-thresholding step (ST),e.g., for the Lasso, for j ∈1,...,p : βj ←ST ( βj −X⊤ :,j(Xβ −y) ∥X:,j∥2 , neλ ∥X:,j∥2 ) (9) where ST(t,τ) = sign(t)·(|t|−τ)+ for any t∈R and τ ≥ 0 (extended for vectors component-wise). The solution ofImplicit differentiation of Lasso-type models for hyperparameter optimization the optimization problem satisﬁes, for anyα> 0, the ﬁxed- point equation (Combettes and Wajs, 2005, Prop. 3.1), for j ∈1,...,p : ˆβ(λ) j = ST ( ˆβ(λ) j −1 αX⊤ j,:(Xˆβ(λ) −y),neλ α ) . (10) The former can be differentiated w.r.t. λ, see Lemma A.1 in Appendix, leading to a closed form solution for the Ja- cobian J(λ) of the Lasso and the weighted Lasso. Proposition 1(Adapting Vaiter et al. 2013, Thm. 1) . Let ˆSbe the support of the vector ˆβ(λ). Suppose that X⊤ ˆSXˆS ≻0 , then a weak Jacobian ˆJ = ˆJ(λ) of the Lasso writes: ˆJˆS = −neλ( X⊤ ˆSXˆS )−1 sign ˆβˆS, (11) ˆJˆSc = 0 , (12) and for the weighted Lasso: ˆJˆS,ˆS = − ( X⊤ ˆSXˆS )−1 diag ( neλˆS ⊙sign ˆβˆS ) (13) ˆJj1,j2 = 0 if j1 /∈ˆSor if j2 /∈ˆS . (14) The proof of Proposition 1 can be found in Appendix A.1. Note that the positivity condition in Proposition 1 is satis- ﬁed if the (weighted) Lasso has a unique solution. More- over, even for multiple solutions cases, there exists at least one satisfying the positivity condition (Vaiter et al., 2013). Proposition 1 shows that the Jacobian of the weighted Lasso ˆJ(λ) ∈ Rp×p is row and column sparse. This is key for algorithmic efﬁciency. Indeed, a priori, one has to store a possibly dense p×p matrix, which is prohibitive when pis large. Proposition 1 leads to a simple algorithm (see Algorithm 1) to compute the Jacobian in a cheap way, as it only requires storing and inverting an ˆs×ˆs matrix. Even if the linear system to solve is of size ˆs×ˆs, instead of p×pfor smooth objective function, the system to invert can be ill-conditioned, especially when a large support size ˆsis encountered. This leads to numerical instabilities and slows down the resolution (see an illustration in Figure 2). Forward (Algorithm 3 in Appendix) and backward (Algo- rithm 4 in Appendix) iterative differentiation, which do not require solving linear systems, can overcome these issues. 3.2. Link with iterative differentiation Iterative differentiation in the ﬁeld of hyperparameter set- ting can be traced back to Domke (2012) who derived a backward differentiation algorithm for gradient descent, heavy ball and L-BFGS algorithms applied to smooth loss functions. Agrawal et al. (2019) generalized it to a spe- ciﬁc subset of convex programs. Maclaurin et al. (2015) derived a backward differentiation for stochastic gradient Algorithm 1IMPLICIT DIFFERENTIATION input : X ∈Rn×p,y ∈Rn,λ ∈R,niter ∈N // jointly compute coef. and Jacobian if Lasso then Get ˆβ = Lasso(X,y,λ,n iter) and its support ˆS. ˆJ = 0p ˆJˆS = −neλ(X⊤ ˆSXˆS)−1 sign ˆβˆS if wLasso then Get ˆβ = wLasso(X,y,λ,n iter) and its support ˆS. ˆJ= 0p×p ˆJˆS,ˆS = −(X⊤ ˆSXˆS)−1 diag(neλˆS ⊙sign ˆβˆS) return ˆβ, ˆJ descent. On the other hand Deledalle et al. (2014) used forward differentiation of (accelerated) proximal gradient descent for hyperparameter optimization with non-smooth penalties. Franceschi et al. (2017) proposed a benchmark of forward mode versus backward mode, varying the num- ber of hyperparameters to learn. Frecon et al. (2018) cast the problem of inferring the groups in a group-Lasso model as a bi-level optimization problem and solved it using back- ward differentiation. Forward differentiation consists in differentiating each step of the algorithm (w.r.t.λin our case). For the Lasso solved with BCD it amounts differentiating Equation (9), and leads to the following recursive equation for the Jacobian, for j ∈1,...p , with zj = βj −X⊤ :,j(Xβ −y)/∥X:,j∥2: Jj ←∂1 ST ( zj, neλ ∥X:,j∥2 )( Jj − 1 ∥X:,j∥2 X⊤ :,jXJ ) + ∂2 ST ( zj, neλ ∥X:,j∥2 ) neλ ∥X:,j∥2 , (15) see Algorithm 3 (in Appendix) for full details. Our proposed algorithm uses the fact that after a ﬁ- nite number of epochs ∂1 ST(zj,neλ/∥X:,j∥2) and ∂2 ST(zj,neλ/∥X:,j∥2) are constant (they no no longer depends on the current β). Indeed, the sign of ˆβ is iden- tiﬁed after a ﬁnite number of iterations thus the partial derivatives are constant. It is then possible to decouple the computation of the Jacobian by only solving Problem (1) in a ﬁrst step and then apply the forward differentiation recur- sion steps, see Algorithm 2. This can be seen as the forward counterpart in a non-smooth case of the recent paper Lor- raine et al. (2019). An additional beneﬁt of such updates is that they can be restricted to the (current) support, which leads to faster Jacobian computation. We now show that the Jacobian computed using forward differentiation and our method, Algorithm 2, converges to- ward the true Jacobian.Implicit differentiation of Lasso-type models for hyperparameter optimization Proposition 2. Assuming the Lasso solution (Prob- lem (2)) (or weighted Lasso Problem (3)) is unique, then Algorithms 2 and 3 converge toward the Jaco- bian ˆJ deﬁned in Proposition 1. Algorithm 3 com- putes the Jacobian along with the regression coefﬁ- cients, once the support has been identiﬁed, the Jaco- bian converges linearly. Algorithm 2 computes ﬁrst the coefﬁcients ˆβ and then the Jacobian ˆJ, provided that the support has been identiﬁed in the ﬁrst step, the convergence is linear in the second, with the same rate as Algorithm 3: ∥J(k+1) ˆS −ˆJ∥(X⊤ :,ˆSX:,ˆS)−1 ≤Ck∥J(k) ˆS −ˆJ∥(X⊤ :,ˆSX:,ˆS)−1 where C = ∥A(jˆs) ...A (j1)∥2 <1, j1,...,j ˆs are the indices of the support of ˆβin increasing order and A(js) = Idˆs− ( X⊤ :,ˆSX:,ˆS )1/2 :,js ∥X:,js∥ ( X⊤ :,ˆSX:,ˆS )1/2 js,: ∥X:,js∥ ∈Rˆs×ˆs. Proof of Proposition 2 can be found in Appendix A.2 and A.3. Remark 3. Uniqueness. As proved in Tibshirani (2013, Lem. 3 and 4) the set of (pathological) lambdas where the Lasso solution is not unique is typically empty. More- over if the Lasso solution is not unique, there could be a non-continuous solution path λ↦→ˆβ(λ), leaving only non- gradient based methods available. Even if Proposition 2 does not provide theoretical guarantees in such a patholog- ical setting, one can still apply Algorithms 2 and 3, see Appendix E.1 for experiments in this settings. Remark 4. Rate for the backward differentiation. The backward and forward differentiation compute the same quantity: ∇λL(λ), but the backward differentiation di- rectly computes the product given in Equation (5) leading to the gradient ofL(λ). Proposition 2 provides rates for the convergence of the Jacobian Jwhich leads to rates for the gradient i.e., for the backward algorithm as well. As an illustration, Figure 1 shows the times of computa- tion of a single gradient ∇λL(λ) and the distance to “op- timum” of this gradient as a function of the number of it- erations in the inner optimization problem for the forward iterative differentiation (Algorithm 3), the backward iter- ative differentiation (Algorithm 4), and the proposed algo- rithm (Algorithm 2). The backward iterative differentiation is several order of magnitude slower than the forward and our implicit forward method. Moreover, once the support has been identiﬁed (after 20 iterations) the proposed im- plicit forward method converges faster than other methods. Note also that in Propositions 1 and 2 the Jacobian for the Imp. F. Iterdiﬀ. (ours)F. Iterdiﬀ.B. Iterdiﬀ. 20 40 60 Number of iterations 10−1 100 101 Times (s) 20 40 60 Number of iterations 10−7 10−5 Objective minus optimum Figure 1.Time to compute a single gradient(Synthetic data, Lasso, n,p = 1000,2000). Inﬂuence on the number of iterations of BCD (in the inner optimization problem of Problem (4)) on the computation time (left) and the distance to “optimum” of the gra- dient ∇λL(λ)(right) for the Lasso estimator. The “optimum” is here the gradient given by implicit differentiation (Algorithm 1). Lasso only depends on the support (i.e., the indices of the non-zero coefﬁcients) of the regression coefﬁcients ˆβ(λ). In other words, once the support of ˆβ(λ) is correctly identi- ﬁed, even if the value of the non-zeros coefﬁcients are not correctly estimated, the Jacobian is exact, see Sun et al. (2019) for support identiﬁcation guarantees. 4. Experiments Our Python code is released as an open source package: https://github.com/QB3/sparse-ho. All the experiments are written in Python using Numba (Lam et al., 2015) for the critical parts such as the BCD loop. We com- pare our gradient computation technique against other com- petitors (see the competitors section) on the HO problem (Problem (4)). Solving the inner optimization problem.Note that our proposed method, implicit forward differentiation, has the appealing property that it can be used with any solver. For instance for the Lasso one can combine the proposed al- gorithm with state of the art solver such as Massias et al. (2018) which would be tedious to combine with iterative differentiation methods. However for the comparison to be fair, for all methods we have used the same vanilla BCD algorithm (recalled in Algorithm 5). We stop the Lasso- types solver when f(β(k+1))−f(β(k)) f(0) <ϵtol ,where f is the cost function of the Lasso or wLasso and ϵtol a given toler- ance. The tolerance is ﬁxed at ϵtol = 10−5 for all methods throughout the different benchmarks. Line search. For each hypergradient-based method, the gradient step is combined with a line-search strategy fol- lowing the work of Pedregosa (2016)1. Initialization. Since the function to optimize Lis not con- 1see https://github.com/fabianp/hoag for detailsImplicit differentiation of Lasso-type models for hyperparameter optimization Table 1.Summary of cost in time and space for each method Mode Computed Space Time Space Time quantity (Lasso) (Lasso) (wLasso) (wLasso) F. Iterdiff. J O(p) O(2npniter) O(p2) O(np2niter) B. Iterdiff. J⊤v O(2pniter) O(npniter + np2niter) O(p2niter) O(npniter + np2niter) Implicit J⊤v O(p) O(npniter + ˆs3) O(p+ ˆs2) O(npniter + ˆs3) Imp. F. Iterdiff. J O(p) O(npniter + nˆsniter_jac) O(p+ ˆs2) O(npniter + nˆs2nit_jac) Algorithm 2IMP. F. I TERDIFF . (proposed) input : X ∈Rn×p,y ∈Rn,λ ∈R,niter,niter_jac ∈N init : J= 0 // sequentially compute coef. & Jacobian if Lasso then Get ˆβ = Lasso(X,y,λ,n iter) and its support ˆS. dr= −X:,ˆSJˆS // trick for cheap updates if wLasso then Get ˆβ = wLasso(X,y,λ,n iter) and its support ˆS. dr= −X:,ˆSJˆS,ˆS for k= 0,...,n iter_jac −1 do for j ∈ˆSdo if Lasso then Jold = Jj // trick for cheap update // diff. Equation (9) w.r.t. λ Jj += X⊤ :,jdr ∥X:,j∥2 − neλ ∥X:,j∥2 sign ˆβj // O(n) dr−= X:,j(Jj,: −Jold) // O(n) if wLasso then Jold = Jj,: // trick for cheap update // diff. Equation (9) w.r.t. λ Jj,ˆS += 1 ∥X:,j∥2 X⊤ :,jdr // O(n×ˆs) Jj,j −= neλj ∥X:,j∥2 sign ˆβj // O(1) dr−= X:,j ⊗(Jj,: −Jold) // O(n×ˆs) return ˆβ,J vex, initialization plays a crucial role in the ﬁnal solution as well as the convergence of the algorithm. For instance, initializing λ = λinit in a ﬂat zone of L(λ) could lead to slow convergence. In the numerical experiments, the Lasso is initialized with λinit = λmax −log(10), where λmax is the smallest λsuch that 0 is a solution of Problem (2). Competitors. In this section we compare the empirical performance of implicit forward differentiation algorithm to different competitors. Competitors are divided in two categories. Firstly, the ones relying on hyperparameter gra- dient: • Imp. F. Iterdiff. : implicit forward differentiation (proposed) described in Algorithm 2. • Implicit: implicit differentiation, which requires solv- ing a ˆs×ˆslinear system as described in Algorithm 1. • F. Iterdiff.: forward differentiation (Deledalle et al., 2014; Franceschi et al., 2017) which jointly computes the regression coefﬁcients ˆβas well as the Jacobian ˆJ as shown in Algorithm 3. Secondly, the ones not based on hyperparameter gradient: • Grid-search: as recommended by Friedman et al. (2010), we use 100 values on a uniformly-spaced grid from λmax to λmax −4 log(10). • Random-search: we sample uniformly at random 100 values taken on the same interval as for the Grid-search [λmax −4 log(10);λmax], as suggested by Bergstra et al. (2013). • Lattice Hyp.: lattice hypercube sampling (Bousquet et al., 2017), combines the idea of grid-search and random-search. We used the sampling scheme of Bouhlel et al. (2019) and their code 2 to sample the points to evaluate the function on. • Bayesian: sequential model based optimization (SMBO) using a Gaussian process to model the objec- tive function. We used the implementation of Bergstra et al. (2013).3 The constraints space for the hyperpa- rameter search was set in[λmax −4 log(10);λmax], and the expected improvement (EI) was used as aquisition function. The cost and the quantity computed by each algorithm can be found in Table 1. The backward differentiation (Domke, 2012) is not included in the benchmark in Figure 2 since it was several orders of magnitude slower than the other techniques (see Figure 1). This is due to the high cost of the BCD algorithm in backward mode, see Table 1. 4.1. Application to held-out loss When using the held-out loss, each dataset(X,y) is split in 3 equal parts: the training set (Xtrain,ytrain), the validation set (Xval,yval) and the test set (Xtest,ytest). 2https://github.com/SMTorg/smt 3https://github.com/hyperopt/hyperoptImplicit differentiation of Lasso-type models for hyperparameter optimization (Lasso, held-out criterion). For the Lasso and the held-out loss, the bilevel optimization Problem (4) reads: arg min λ∈R ∥yval −Xval ˆβ(λ)∥2 (16) s.t. ˆβ(λ) ∈arg min β∈Rp 1 2n∥ytrain −Xtrainβ∥2 2 + eλ∥β∥1 . Figure 2 (top) shows on 3 datasets (see Appendix D for dataset details) the distance to the “optimum” of ∥yval − Xval ˆβ(λ)∥2 as a function of time. Here the goal is to ﬁnd λ solution of Problem (16). The “optimum” is chosen as the minimum of ∥yval −Xval ˆβ(λ)∥2 among all the meth- ods. Figure 2 (bottom) shows the loss ∥ytest −Xtest ˆβ(λ)∥2 on the test set (independent from the training set and the validation set). This illustrates how well the estimator gen- eralizes. Firstly, it can be seen that on all datasets the pro- posed implicit forward differentiation outperforms forward differentiation which illustrates Proposition 2 and corrobo- rates the cost of each algorithm in Table 1. Secondly, it can be seen that on the 20news dataset (Figure 2, top) the im- plicit differentiation (Algorithm 1) convergence is slower than implicit forward differentiation, forward differentia- tion, and even slower than the grid-search. In this case, this is due to the very slow convergence of the conjugate gra- dient algorithm (Nocedal and Wright, 2006) when solving the ill-conditioned linear system in Algorithm 1. (MCP , held-out criterion). We also applied our algorithm on an estimator based on a non-convex penalty: the MCP (Zhang, 2010) with 2 hyperparameters. Since the penalty is non-convex the estimator may not be continuous w.r.t.hy- perparameters and the theory developed above does not hold. However experimentally implicit forward differen- tiation outperforms forward differentiation for the HO, see Appendix C for full details. 4.2. Application to another criterion: SURE Evaluating models on held-out data makes sense if the de- sign is formed from random samples as it is often consid- ered in supervised learning. However, this assumption does not hold for certain kinds of applications in signal or image processing. For these applications, the held-out loss cannot be used as the criterion for optimizing the hyperparame- ters of a given model. In this case, one may use a proxy of the prediction risk, like the Stein Unbiased Risk Estimation (SURE, Stein (1981)). The SURE is an unbiased estimator of the prediction risk under weak differentiable conditions. The drawback of this criterion is that it requires the knowl- edge of the variance of the noise. The SURE is deﬁned as follows: SURE(λ) =∥y−Xˆβ(λ)∥2−nσ2+2σ2dof( ˆβ(λ)) , where the degrees of freedom (dof Efron 1986) is deﬁned as dof( ˆβ(λ)) =∑n i=1 cov(yi,(Xˆβ(λ))i)/σ2 .The dof can be seen a measure of the complexity of the model, for in- stance for the Lasso dof ( ˆβ(λ)) = ˆs, see Zou et al. (2007). The SURE can thus be seen as a criterion trading data- ﬁdelity against model complexity. However, the dof is not differentiable (not even continuous in the Lasso case), yet it is possible to construct a weakly differentiable ap- proximation of it based on Finite Differences Monte-Carlo (see Deledalle et al. 2014 for full details), with ϵ >0 and δ∼N(0,Idn): dofFDMC(y,λ,δ,ϵ ) =1 ϵ⟨Xˆβ(λ)(y+ ϵδ) −Xˆβ(λ)(y),δ⟩. We use this smooth approximation in the bi-level optimiza- tion problem to ﬁnd the best hyperparameter. The bi-level optimization problem then reads: arg min λ∈R ∥y−Xˆβ(λ)∥2 + 2σ2dofFDMC(y,λ,δ,ϵ ) (17) s.t. ˆβ(λ)(y) ∈arg min β∈Rp 1 2n∥y−Xβ∥2 2 + eλ∥β∥1 ˆβ(λ)(y+ ϵδ) ∈arg min β∈Rp 1 2n∥y+ ϵδ−Xβ∥2 2 + eλ∥β∥1 Note that solving this problem requires the computation of two (instead of one for the held-out loss) Jacobians w.r.t.λ of the solution ˆβ(λ) at the points yand y+ ϵδ. (Lasso, SURE criterion). To investigate the estimation per- formance of the implicit forward differentiation in com- parison to the competitors described above, we used as metric the (normalized) Mean Squared Error (MSE) de- ﬁned as MSE ≜ ∥ˆβ−β∗∥2/∥β∗∥2. The entries of the design matrix X ∈Rn×p are i.i.d. random Gaussian vari- ables N(0,1). The number of rows is ﬁxed to n = 100. Then, we generated β∗with 5 non-zero coefﬁcients equals to 1. The vector y was computed by adding to Xβ∗addi- tive Gaussian noise controlled by the Signal-to-Noise Ra- tio: SNR ≜ ∥Xβ∗∥/∥y−Xβ∗∥(here SNR = 3). Fol- lowing Deledalle et al. (2014), we set ϵ = 2σ/n0.3. We varied the number of featurespbetween 200 and 10,000 on a linear grid of size 10. For a ﬁxed number of features, we performed 50 repetitions and each point of the curves rep- resents the mean of these repetitions. Comparing efﬁciency in time between methods is difﬁcult since they are not di- rectly comparable. Indeed, grid-search and random-search discretize the HO space whereas others methods work in the continuous space which is already an advantage. How- ever, to be able to compare the hypergradient methods and possibly compare them to the others, we computed the to- tal amount of time for a method to return its optimal value of λ. In order to have a fair comparison, we compared 50 evaluations of the line-search for each hypergradient meth- ods, 50 evaluations of the Bayesian methods and ﬁnally 50 evaluations on ﬁxed or random grid. We are aware that the cost of each of these evaluations is not the same but it al- lows to see that our method stays competitive in time with optimizing one parameter. Moreover we will also see that our method scales better with a large number of hyperpa- rameters to optimize.Implicit differentiation of Lasso-type models for hyperparameter optimization Imp. F. Iterdiﬀ. (ours) Implicit F. Iterdiﬀ. Grid-search Bayesian Random-search Lattice Hyp. 0.0 0.5 1.0 10−5 10−4 10−3 10−2 10−1 100 Objective minus optimum rcv1 (p = 19, 959) 0 5 10 15 10−3 10−2 10−1 100 101 102 20news (p = 130, 107) 0 100 200 300 10−4 10−3 10−2 10−1 100 101 ﬁnance (p = 1, 668, 737) 0.0 0.5 1.0 Time (s) 10−1 100 Loss on test set 0 5 10 15 Time (s) 101 102 0 100 200 300 Time (s) 10−1 100 101 Figure 2.Computation time for the HO of the Lasso on real data.Distance to “optimum” (top) and performance (bottom) on the test set for the Lasso for 3 different datasets: rcv1, 20news and ﬁnance. Imp. F. Iterdiﬀ. (ours) Implicit F. Iterdiﬀ. Grid-search Bayesian Random-search 200 2500 5000 7500 10000 Number of features (p) 0.000 0.001 0.002 0.003 0.004 relative MSE 200 2500 5000 7500 10000 Number of features (p) 10−1 100 101 102 Time (s) Figure 3.Lasso: estimation performance. Estimation relative Mean Squared Error (left) and running time (right) as a function of the number of features for the Lasso model. Figure 3 shows the inﬂuence of the number of features on the relative MSE (ie. MSE of a method minus the MSE of our implicit forward method) and the computation time. First, MSE of all gradient based methods is lower than the other methods which means that ˆβ(λ) leads to a better es- timation when λ is chosen via the gradient based meth- ods. This illustrates that continuous optimization for hy- perparameter selection leads to better estimation perfor- mance than discrete or Bayesian optimization. Yet, the running time of our proposed method is the lowest of all hypergradient-based strategies and competes with the grid- search and the random-search. (Weighted Lasso vs Lasso, SURE criterion). As our method leverages the sparsity of the solution, it can be used for HO with a large number of hyperparameters, contrary to classi- cal forward differentiation. The weighted Lasso (wLasso, Zou 2006) has p hyperparameters and was introduced to reduce the bias of the Lasso. However setting the phyper- parameters is impossible with grid-search. Figure 4 shows the estimation MSE and the running time of the different methods to obtain the hyperparameter val- ues as a function of the number of features used to simu- late the data. The simulation setting is here the same as for the Lasso problems investigated in Figure 3 ( n = 100, SNR = 3). We compared the classical Lasso estimator and the weighted Lasso estimator where the regularization hy- perparameter was chosen using implicit forward differenti- ation and the forward iterative differentiation as described in Algorithm 3. Problem (4) is not convex for the weighted Lasso and a descent algorithm like ours can be trapped in local minima, crucially depending on the starting point λinit. To alleviate this problem, we introduced a regular- ized version of Problem (4): arg min λ∈R C ( ˆβ(λ) ) + γ p∑ j λ2 j s.t. ˆβ(λ) ∈arg min β∈Rp ≜ ψ(β,λ) . (18) The solution obtained by solving Equation (18) is then used as the initialization λ(0) for our algorithm. In this experiment the regularization term is constant γ =Implicit differentiation of Lasso-type models for hyperparameter optimization Lasso F. Iterdiﬀ. Lasso Implicit Lasso Backward Lasso Imp. F. Iterdiﬀ. (ours) wLasso F. Iterdiﬀ. wLasso Implicit wLasso Backward wLasso Imp. F. Iterdiﬀ. (ours) 200 2500 5000 7500 10000 Number of features (p) 0.00 0.05 0.10 0.15 MSE 200 2500 5000 7500 10000 Number of features (p) 10−1 100 101 102 103 Time (s) Figure 4.Lasso vs wLasso.Estimation Mean Squared Error (left) and running (right) of competitors as a function of the number of features for the weighted Lasso and Lasso models. C(β(λmax))/10. We see in Figure 4 that the weighted Lasso gives a lower MSE than the Lasso and allows for a better recovery of β∗. This experiment shows that the amount of time needed to obtain the vector of hyperparameters of the weighted Lasso via our algorithm is in the same range as for obtaining the unique hyperparameter of the Lasso prob- lem. It also shows that our proposed method is much faster than the naive way of computing the Jacobian using for- ward or backward iterative differentiation. The implicit dif- ferentiation method stays competitive for the wLasso due to the small support of the solution and hence a small ma- trix to inverse. A maximum running time threshold was used for this experiment checking the running time at each line-search iteration, explaining why the forward differen- tiation and backward differentiation of the wLasso does not explode in time on Figure 4. Conclusion In this work we studied the performance of several methods to select hyperparameters of Lasso-type estimators show- ing results for the Lasso and the weighted Lasso, which have respectively one or phyperparameters. We exploited the sparsity of the solutions and the speciﬁc structure of the iterates of forward differentiation, leading to our im- plicit forward differentiation algorithm that computes efﬁ- ciently the full Jacobian of these estimatorsw.r.t.the hyper- parameters. This allowed us to select them through a stan- dard gradient descent and have an approach that scales to a high number of hyperparameters. Importantly, contrary to a classical implicit differentiation approach, the proposed algorithm does not require solving a linear system. Fi- nally, thanks to its two steps nature, it is possible to lever- age in the ﬁrst step the availability of state-of-the-art Lasso solvers that make use of techniques such as active sets or screening rules. Such algorithms, that involve calls to in- ner solvers run on subsets of features, are discontinuous w.r.t.hyperparameters which would signiﬁcantly challenge a single step approach based on automatic differentiation. Acknowledgments This work was funded by ERC Start- ing Grant SLAB ERC-StG-676943 and ANR GraVa ANR- 18-CE40-0005.Implicit differentiation of Lasso-type models for hyperparameter optimization References A. Agrawal, B. Amos, S. Barratt, S. Boyd, S. Diamond, and J. Z. Kolter. Differentiable convex optimization layers. In Advances in neural information processing systems , pages 9558–9570, 2019. B. Amos and J. Z. Kolter. Optnet: Differentiable optimiza- tion as a layer in neural networks. In ICML, volume 70, pages 136–145, 2017. A. G. Baydin, B. A. Pearlmutter, A. A. Radul, and J. M. Siskind. Automatic differentiation in machine learning: a survey. J. Mach. Learn. Res., 18(153):1–43, 2018. Y . Bengio. Gradient-based optimization of hyperparame- ters. Neural computation, 12(8):1889–1900, 2000. J. Bergstra and Y . Bengio. Random search for hyper- parameter optimization. J. Mach. Learn. Res., 2012. J. Bergstra, D. Yamins, and D. D. Cox. Hyperopt: A python library for optimizing the hyperparameters of machine learning algorithms. In Proceedings of the 12th Python in science conference, pages 13–20, 2013. M. Borgerding, P. Schniter, and S. Rangan. Amp-inspired deep networks for sparse linear inverse problems. IEEE Transactions on Signal Processing , 65(16):4293–4308, 2017. M. A. Bouhlel, J. T. Hwang, N. Bartoli, R. Lafage, J. Mor- lier, and J. R. R. A. Martins. A python surrogate model- ing framework with derivatives. Advances in Engineer- ing Software, page 102662, 2019. ISSN 0965-9978. doi: https://doi.org/10.1016/j.advengsoft.2019.03.005. O. Bousquet, S. Gelly, K. Kurach, O. Teytaud, and D. Vin- cent. Critical hyper-parameters: No random, no cry. arXiv preprint arXiv:1706.03200, 2017. P. Breheny and J. Huang. Coordinate descent algorithms for nonconvex penalized regression, with applications to biological feature selection. Ann. Appl. Stat. , 5(1):232, 2011. E. Brochu, V . M. Cora, and N. De Freitas. A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical re- inforcement learning. 2010. O. Chapelle, V . Vapnik, O. Bousquet, and S. Mukherjee. Choosing multiple parameters for support vector ma- chines. Machine learning, 46(1-3):131–159, 2002. P. L. Combettes and V . R. Wajs. Signal recovery by proxi- mal forward-backward splitting. Multiscale Modeling & Simulation, 4(4):1168–1200, 2005. I. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for linear inverse problems with a sparsity constraint. Comm. Pure Appl. Math., 57(11): 1413–1457, 2004. C.-A. Deledalle, S. Vaiter, J. Fadili, and G. Peyré. Stein Unbiased GrAdient estimator of the Risk (SUGAR) for multiple parameter selection. SIAM J. Imaging Sci. , 7 (4):2448–2487, 2014. J. Domke. Generic methods for optimization-based model- ing. In AISTATS, volume 22, pages 318–326, 2012. C. Dossal, M. Kachour, M.J. Fadili, G. Peyré, and C. Ches- neau. The degrees of freedom of the lasso for general design matrix. Statistica Sinica, 23(2):809–828, 2013. B. Efron. How biased is the apparent error rate of a pre- diction rule? J. Amer. Statist. Assoc., 81(394):461–470, 1986. L. C. Evans and R. F. Gariepy. Measure theory and ﬁne properties of functions. CRC Press, 1992. C. S. Foo, C. B. Do, and A. Y . Ng. Efﬁcient multiple hyper- parameter learning for log-linear models. InAdvances in neural information processing systems, pages 377–384, 2008. L. Franceschi, M. Donini, P. Frasconi, and M. Pontil. For- ward and reverse gradient-based hyperparameter opti- mization. In ICML, pages 1165–1173, 2017. J. Frecon, S. Salzo, and M. Pontil. Bilevel learning of the group lasso structure. InAdvances in Neural Information Processing Systems, pages 8301–8311, 2018. J. Friedman, T. J. Hastie, and R. Tibshirani. Regulariza- tion paths for generalized linear models via coordinate descent. J. Stat. Softw., 33(1):1–22, 2010. K. Gregor and Y . LeCun. Learning fast approximations of sparse coding. In ICML, pages 399–406, 2010. E. Hale, W. Yin, and Y . Zhang. Fixed-point continuation for ℓ1-minimization: Methodology and convergence. SIAM J. Optim., 19(3):1107–1130, 2008. K. Kunisch and T. Pock. A bilevel optimization approach for parameter learning in variational models. SIAM J. Imaging Sci., 6(2):938–983, 2013. S. K. Lam, A. Pitrou, and S. Seibert. Numba: A LLVM- based Python JIT Compiler. In Proceedings of the Sec- ond Workshop on the LLVM Compiler Infrastructure in HPC, pages 1–6. ACM, 2015.Implicit differentiation of Lasso-type models for hyperparameter optimization J. Larsen, L. K. Hansen, C. Svarer, and M. Ohlsson. Design and regularization of neural networks: the optimal use of a validation set. In Neural Networks for Signal Process- ing VI. Proceedings of the 1996 IEEE Signal Processing Society Workshop, 1996. J. Larsen, C. Svarer, L. N. Andersen, and L. K. Hansen. Adaptive regularization in neural network modeling. In Neural Networks: Tricks of the Trade - Second Edition , pages 111–130. Springer, 2012. J. Liu, X. Chen, Z. Wang, and W. Yin. Alista: Analytic weights are as good as learned weights in lista. In Inter- national Conference on Learning Representations, 2018. W. Liu, Y . Yang, et al. Parametric or nonparametric? a parametricness index for model selection. Ann. Statist., 39(4):2074–2102, 2011. J. Lorraine, P. Vicol, and D. Duvenaud. Optimizing millions of hyperparameters by implicit differentiation. arXiv preprint arXiv:1911.02590, 2019. D. Maclaurin, D. Duvenaud, and Ryan Adams. Gradient- based hyperparameter optimization through reversible learning. In ICML, volume 37, pages 2113–2122, 2015. J. Mairal, F. Bach, and J. Ponce. Task-driven dictionary learning. IEEE Trans. Pattern Anal. Mach. Intell., 34(4): 791–804, 2012. M. Massias, A. Gramfort, and J. Salmon. Celer: a Fast Solver for the Lasso with Dual Extrapolation. In ICML, volume 80, pages 3315–3324, 2018. M. Massias, S. Vaiter, A. Gramfort, and J. Salmon. Dual extrapolation for sparse generalized linear models.arXiv preprint arXiv:1907.05830, 2019. V . Niculae and M. Blondel. A regularized framework for sparse and structured neural attention. In Advances in neural information processing systems , pages 3338– 3348, 2017. J. Nocedal and S. J. Wright. Numerical optimization . Springer Series in Operations Research and Financial Engineering. Springer, New York, second edition, 2006. F. Pedregosa. Hyperparameter optimization with approxi- mate gradient. In ICML, 2016. S. Ramani, T. Blu, and M. Unser. Monte-Carlo SURE: a black-box optimization of regularization parameters for general denoising algorithms. IEEE Trans. Image Pro- cess., 17(9):1540–1554, 2008. M. W. Seeger. Cross-validation optimization for large scale structured classiﬁcation kernel methods. J. Mach. Learn. Res., 9:1147–1178, 2008. J. Snoek, H. Larochelle, and R. P. Adams. Practical bayesian optimization of machine learning algorithms. In Advances in neural information processing systems , 2012. E. Soubies, L. Blanc-Féraud, and G. Aubert. A uniﬁed view of exact continuous penalties for ℓ2-ℓ0 minimiza- tion. SIAM J. Optim., 27(3):2034–2060, 2017. C. M. Stein. Estimation of the mean of a multivariate nor- mal distribution. Ann. Statist., 9(6):1135–1151, 1981. L. R. A. Stone and J.C. Ramer. Estimating W AIS IQ from Shipley Scale scores: Another cross-validation. Journal of clinical psychology, 21(3):297–297, 1965. Y . Sun, H. Jeong, J. Nutini, and M. Schmidt. Are we there yet? manifold identiﬁcation of gradient-related proxi- mal methods. In AISTATS, volume 89, pages 1110–1119, 2019. R. Tibshirani. Regression shrinkage and selection via the lasso. J. R. Stat. Soc. Ser. B Stat. Methodol., 58(1):267– 288, 1996. R. J. Tibshirani. The lasso problem and uniqueness. Elec- tron. J. Stat., 7:1456–1490, 2013. R. J. Tibshirani and J. Taylor. The solution path of the generalized lasso. Ann. Statist., 39(3):1335–1371, 2011. P. Tseng and S. Yun. Block-coordinate gradient descent method for linearly constrained nonsmooth separable optimization. J. Optim. Theory Appl., 140(3):513, 2009. S. Vaiter, C.-A. Deledalle, G. Peyré, C. Dossal, and J. Fadili. Local behavior of sparse analysis regulariza- tion: Applications to risk estimation. Appl. Comput. Harmon. Anal., 35(3):433–451, 2013. S. Vaiter, C.-A. Deledalle, G. Peyré, J. M. Fadili, and C. Dossal. The degrees of freedom of partly smooth reg- ularizers. Ann. Inst. Stat. Math., 69(4):791–832, 2017. K. Wu, Y . Guo, Z. Li, and C. Zhang. Sparse coding with gated learned ista. In International Conference on Learning Representations, 2019. B. Xin, Y . Wang, W. Gao, D. Wipf, and B. Wang. Maximal sparsity with deep networks? In Advances in Neural In- formation Processing Systems, pages 4340–4348, 2016. M. Yuan and Y . Lin. Model selection and estimation in regression with grouped variables. J. R. Stat. Soc. Ser. B Stat. Methodol., 68(1):49–67, 2006. C.-H. Zhang. Nearly unbiased variable selection under minimax concave penalty. Ann. Statist., 38(2):894–942, 2010.Implicit differentiation of Lasso-type models for hyperparameter optimization H. Zou. The adaptive lasso and its oracle properties. J. Amer. Statist. Assoc., 101(476):1418–1429, 2006. H. Zou and T. J. Hastie. Regularization and variable se- lection via the elastic net. J. R. Stat. Soc. Ser. B Stat. Methodol., 67(2):301–320, 2005. H. Zou, T. J. Hastie, and R. Tibshirani. On the “degrees of freedom” of the lasso. Ann. Statist., 35(5):2173–2192, 2007.Implicit differentiation of Lasso-type models for hyperparameter optimization A. Proofs A.1. Proof of Proposition 1 We start by a lemma on the weak derivative of the soft-thresholding. Lemma A.1. The soft-thresholding ST :R×R+ ↦→R deﬁned by ST(t,τ) = sign(t) ·(|t|−τ)+ is weakly differentiable with weak derivatives ∂1 ST(t,τ) =1{|t|>τ} , (19) and ∂2 ST(t,τ) =−sign(t) ·1{|t|>τ} , (20) where 1{|t|>τ}= { 1, if |t|>τ, 0, otherwise. (21) Proof. See (Deledalle et al., 2014, Proposition 1) Proof. (Proposition 1, Lasso ISTA) The soft-thresholding is differentiable almost everywhere (a.e.), thus Equation (10) can be differentiated a.e. thanks to the previous lemma, and for any α> 0 ˆJ=   1{|ˆβ1|>0} ... 1{|ˆβp|>0}  ⊙ ( Idp−1 αX⊤X ) ˆJ− neλ α   sign( ˆβ1)1{|ˆβ1|>0} ... sign( ˆβp)1{|ˆβp|>0}   . Inspecting coordinates inside and outside the support of ˆβleads to: { ˆJˆSc = 0 ˆJˆS = ˆJˆS −1 αX⊤ :,ˆSX:,ˆS ˆJˆS −neλ α sign ˆβˆS . (22) Rearranging the term of Equation (22) it yields: X⊤ :,ˆSX:,ˆS ˆJˆS = −neλsign ˆβˆS (23) ˆJˆS = −neλ ( X⊤ :,ˆSX:,ˆS )−1 sign ˆβˆS . (24) (Proposition 1, Lasso BCD) The ﬁxed point equations for the BCD case is ˆβj = ST ( ˆβj − 1 ∥X:j∥2 2 X⊤ :j(Xˆβj −y), neλ ∥X:j∥2 2 ) . (25) As before we can differentiate this ﬁxed point equation Equation (25) ˆJj = 1{|ˆβj|>τ}· ( ˆJj − 1 ∥X:j∥2 2 X⊤ :jX ˆJ ) − neλ ∥X:j∥2 2 sign (ˆβj)1{|ˆβj|>τ} , (26) leading to the same result.Implicit differentiation of Lasso-type models for hyperparameter optimization A.2. Proof of Proposition 2 in the ISTA case Proof. (Lasso case, ISTA) In Algorithm 3, β(k) follows ISTA steps, thus (β(k))l∈N converges toward the solution of the Lasso ˆβ. Let ˆS be the support of the Lasso estimator ˆβ, and ν( ˆS) > 0 the smallest eigenvalue of X⊤ :,ˆSX:,ˆS. Under uniqueness assumption proximal gradient descent ( a.k.a. ISTA) achieves sign identiﬁcation (Hale et al., 2008), i.e., there exists k0 ∈N such that for all k≥k0 −1: sign β(k+1) = signˆβ . (27) Recalling the update of the Jacobian Jfor the Lasso solved with ISTA is the following: J(k+1) = ⏐⏐⏐sign β(k+1) ⏐⏐⏐⊙ ( Id − 1 ∥X∥2 2 X⊤X ) J(k) − neλ ∥X∥2 2 sign β(k+1) , it is clear that J(k) is sparse with the sparsity pattern β(k) for all k≥k0. Thus we have that for all k≥k0: J(k+1) ˆS = J(k) ˆS − 1 ∥X∥2 2 X⊤ :,ˆSXJ(k) − neλ ∥X∥2 2 sign ˆβˆS = J(k) ˆS − 1 ∥X∥2 2 X⊤ :,ˆSX:,ˆSJ(k) ˆS − neλ ∥X∥2 2 sign ˆβˆS = ( IdˆS− 1 ∥X∥2 2 X⊤ :,ˆSX:,ˆS ) J(k) ˆS − neλ ∥X∥2 2 sign ˆβˆS. (28) One can remark that ˆJdeﬁned in Equation (11), satisﬁes the following: ˆJˆS = ( IdˆS− 1 ∥X∥2 2 X⊤ :,ˆSX:,ˆS ) ˆJˆS − neλ ∥X∥2 2 sign ˆβˆS . (29) Combining Equations (28) and (29) and denoting ν( ˆS) >0 the smallest eigenvalue of X⊤ ˆSXˆS, we have for all k≥k0: J(k+1) ˆS − ˆJˆS = ( IdˆS− 1 ∥X∥2 2 X⊤ :,ˆSX:,ˆS )( J(k) ˆS − ˆJˆS ) ∥J(k+1) ˆS − ˆJˆS∥2 ≤ ( 1 − ν( ˆS) ∥X∥2 2 ) ∥J(k) ˆS − ˆJˆS∥2 ∥J(k) ˆS − ˆJˆS∥2 ≤ ( 1 − ν( ˆS) ∥X∥2 2 )k−k0 ∥J(k0) ˆS − ˆJˆS∥2 . Thus the sequence of Jacobian ( J(k)) k∈N converges linearly to ˆJonce the support is identiﬁed. Proof. (wLasso case, ISTA) Recalling the update of the Jacobian J ∈Rp×p for the wLasso solved with ISTA is the following: J(k+1) = ⏐⏐⏐sign β(k+1) ⏐⏐⏐⊙ ( Id − 1 ∥X∥2 2 X⊤X ) J(k) − neλ ∥X∥2 2 diag ( sign β(k+1) ) , (30) The proof follows exactly the same steps as the ISTA Lasso case to show convergence in spectral norm of the sequence (J(k))k∈N toward ˆJ.Implicit differentiation of Lasso-type models for hyperparameter optimization A.3. Proof of Proposition 2 in the BCD case The goal of the proof is to show that iterations of the Jacobian sequence (J(k))k∈N generated by the Block Coordinate Descent algorithm (Algorithm 3) converges toward the true Jacobian ˆJ. The main difﬁculty of the proof is to show that the Jacobian sequence follows a Vector AutoRegressive (V AR, see Massias et al. (2019, Thm. 10) for more detail),i.e., the main difﬁculty is to show that there exists k0 such that for all k≥k0: J(k+1) = AJ(k) + B , (31) with A∈Rp×p a contracting operator and B ∈Rp. We follow exactly the proof of Massias et al. (2019, Thm. 10). Proof. (Lasso, BCD, forward differentiation (Algorithm 3)) Let j1,...,j S be the indices of the support of ˆβ, in increasing order. As the sign is identiﬁed, coefﬁcients outside the support are 0 and remain 0. We decompose the k-th epoch of coordinate descent into individual coordinate updates: Let ˜β(0) ∈Rp denote the initialization (i.e., the beginning of the epoch, ), ˜β(1) = β(k) the iterate after coordinate j1 has been updated, etc., up to ˜β(S) after coordinate jS has been updated, i.e., at the end of the epoch ( ˜β(S) = β(k+1)). Let s ∈S, then ˜β(s) and ˜β(s−1) are equal everywhere, except at coordinate js: ˜J(s) js = ˜J(s−1) js − 1 ∥X:,js∥2 X⊤ :,jsX ˜J(s−1) − 1 ∥Xjs∥2 sign βjs after sign identiﬁcation we have: = ˜J(s−1) js − 1 ∥X:,js∥2 X⊤ :,jsX:,ˆS ˜J(s−1) ˆS − 1 ∥X:,js∥2 sign ˆβjs ˜J(s) ˆS = ( Idˆs− 1 ∥X:,js∥2 ejse⊤ jsX⊤ :,ˆSX:,ˆS )    As ˜J(s−1) ˆS − 1 ∥X:,js∥2 sign ˆβjs ( X⊤ :,ˆSX:,ˆS )1/2 ˜J(s) ˆS =  Idˆs− ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejse⊤ js ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥      A(s) ( X⊤ :,ˆSX:,ˆS )1/2 ˜J(s−1) ˆS − ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥2 sign ˆβjs    b(s) We thus have: ( X⊤ :,ˆSX:,ˆS )1/2 ˜J(ˆs) ˆS = A(ˆs) ...A (1)    A∈Rˆs×ˆs ( X⊤ :,ˆSX:,ˆS )1/2 J(1) ˆS + AS...A 2b1 + ··· + ASbS−1 + bS   b∈Rˆs . After sign identiﬁcation and a full update of coordinate descent we thus have: ( X⊤ :,ˆSX:,ˆS )1/2 J(t+1) ˆS = A ( X⊤ :,ˆSX:,ˆS )1/2 J(t) ˆS + b . (32) Lemma A.2. ∥As∥2 ≤1 , Moreover if A(s)x = ∥x∥then x∈vect   ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejs   ⊤ (33) Proof. ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejse⊤ js ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥Implicit differentiation of Lasso-type models for hyperparameter optimization is a symmetric rank 1 matrix, its non-zero eigenvalue is e⊤ js ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejs = e⊤ js X⊤ :,ˆSX:,ˆS ∥X:,js∥2 ejs = 1 . An eigenvector associated to this non-zeros eigenvalue is ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejs . Asis symmetric and real, is diagonalisable in an orthogonal basis, it has eigenvalue1 with multiplicity ˆs−1 and eigenvalue 0 with multiplicity 1. Moreover if ∥Ax∥= ∥x∥, then x∈vect (( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejs )⊤ . Lemma A.3. ∥A∥2 <1 . Proof. A= A(ˆs) ...A (1) We have ∥A∥≤∥ A(ˆs)∥   ≤1 ... ∥A(1)∥   ≤1 ≤1 . Let x∈Rˆs such that ∥Ax∥= ∥x∥, we thus have for all s∈1,..., ˆs, A(s)x = ∥x∥. Using Lemma A.3 we have that for all s∈1,..., ˆsx ∈vect (( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejs )⊤ , i.e., x∈vect (( X⊤ :,ˆSX:,ˆS )1/2)⊤ = {0}because X⊤ :,ˆSX:,ˆS ≻0 Using Equation (32) we have: ∥J(t+1) ˆS − ˆJ∥(X⊤ :,ˆSX:,ˆS)−1 ≤∥A∥2∥J(t) ˆS − ˆJ∥(X⊤ :,ˆSX:,ˆS)−1 , (34) with ∥A∥2 < 1, which leads to the desire result. Since the recursion of the Jacobian sequences of Algorithm 2 and Algorithm 2 are the same once the support is identiﬁed, the proof of convergence of Algorithm 2 is the same (provided that support identiﬁcation has been achieved). Proof. (wLasso case, BCD) As for the Lasso case: ˜J(s) js,: = ˜J(s−1) js,: − 1 ∥X:,js∥2 X⊤ :,jsX ˜J(s−1) − 1 ∥Xjs∥2 sign βjsejse⊤ js after sign identiﬁcation we have: ˜J(s) js,ˆS = ˜J(s−1) js,ˆS − 1 ∥X:,js∥2 X⊤ :,jsX:,ˆS ˜J(s−1) ˆS,ˆS − 1 ∥X:,js∥2 sign ˆβjsejse⊤ js (X⊤ :,ˆSX:,ˆS)1/2 ˜J(s) ˆS,ˆS = ( Idn− (X⊤ :,ˆSX:,ˆS)1/2ejse⊤ js(X⊤ :,ˆSX:,ˆS)1/2 ∥X:,js∥2 )    A(s) (X⊤ :,ˆSX:,ˆS)1/2 ˜J(s−1) ˆS,ˆS −sign ˆβjs ∥X:,js∥2 (X⊤ :,ˆSX:,ˆS)1/2    B(s) ejse⊤ js (X⊤ :,ˆSX:,ˆS)1/2 ˜J(ˆs) ˆS,ˆS = A(ˆs) ...A (1)    A∈Rˆs×ˆs (X⊤ :,ˆSX:,ˆS)1/2 ˜J(0) ˆS,ˆS + A(ˆs) ...A (2)B(1)ej1 e⊤ j1 + ··· + B(ˆs)ejˆse⊤ jˆs    D∈Rˆs×ˆs . (35) As in the Lasso case, Equation (35) leads to linear convergence once the support is identiﬁed for Algorithms 2 and 3.Implicit differentiation of Lasso-type models for hyperparameter optimization B. Block coordinate descent algorithms Algorithm 3 presents the forward iteration scheme which computes iteratively the solution of the Lasso or wLasso jointly with the Jacobian computation. This is the naive way of computing the Jacobian without taking advantage of its sparsity. Eventually, it requires to differentiate every lines of code w.r.t. to λ and take advantage of the BCD updates for cheap updates on the Jacobian as well. Algorithm 3FORWARD ITERDIFF (Deledalle et al., 2014; Franceschi et al., 2017) input : X ∈Rn×p,y ∈Rn,λ ∈R,niter ∈N // jointly compute coef. & Jacobian β = 0 // potentially warm started J= 0 // potentially warm started r= y−Xβ dr= −XJ for k= 0,...,n iter −1 do for j = 0,...,p −1 do // update the regression coefficients βold = βj zj = βj + 1 ∥X:,j∥2 X⊤ :,jr // gradient step βj = ST(zj,neλ/∥X:,j∥2) // proximal step r−= X:,j(βj −βold) // update the Jacobian if Lasso then Jold = Jj Jj = |sign βj| ( Jj + 1 ∥X:,j∥2 X⊤ :,jdr ) // diff. w.r.t. λ Jj −= neλ ∥X:,j∥2 sign βj // diff. w.r.t. λ drj −= X:,j(Jj −Jold) if wLasso then Jold = Jj,: Jj,: = |sign βj| ( Jj,: + 1 ∥X:,j∥2 X⊤ :,jdr ) // diff. w.r.t. λ1,...,λ p Jj,j −= neλj ∥X:,j∥2 sign βj // diff. w.r.t. λ1,...,λ p dr−= X:,j(Jj −Jold) return βniter ,Jniter (λ) Algorithm 4 describes the backward iterative differentiation algorithm used for benchmark. Backward differentiation requires the storage of every updates on β. As Figure 1 shows, this algorithm is not efﬁcient for our case because the function to differentiate f : R →Rp ( f : Rp →Rp, for the wLasso) has a higher dimension output space than the input space. The storage is also an issue mainly for the wLasso case which makes this algorithm difﬁcult to use in practice in our context. Algorithm 5 presents the classical BCD iterative scheme for solving the Lasso problem using the composition of a gradient step with the soft-thresholding operator.Implicit differentiation of Lasso-type models for hyperparameter optimization Algorithm 4BACKWARD ITERDIFF (Domke, 2012) input : X ∈Rn×p,y ∈Rn,λ ∈R,niter ∈N // backward computation of ˆβ and ˆJ⊤ (λ)α β = 0 // potentially warm started // compute the regression coefficients and store the iterates for k= 0,...,n iter −1 do for j = 0,...,p −1 do βold = βj zj = βj + 1 ∥X:,j∥2 X⊤ :,jr // gradient step βj = ST(zj,neλ/∥X:,j∥2) // proximal step r−= X:,j(βj −βold) // Init. backward differentiation g= 0 // g stores ˆJ⊤ λ α // compute the Jacobian for k= niter down to 1 do for j = 0,...,p −1 do if Lasso then g−= neλ ∥X:,j∥2 αjsign β(k) j αj ∗= |sign β(k) j | α−= 1 ∥X:,j∥2 αjX⊤ :,jX // O(np) if wLasso then gj −= neλj ∥X:,j∥2 αjsign β(k) j αj ∗= |sign β(k) j | α−= 1 ∥X:,j∥2 αjX⊤ :,jX return βniter ,g(1) Algorithm 5BCD FOR THE LASSO (Friedman et al., 2010) input : X ∈Rn×p,y ∈Rn,λ ∈R,β(0) ∈Rp,niter ∈N β = β(0) // warm start for k= 0,...,n iter −1 do for j = 0,...,p −1 do βold = βj zj = βj + 1 ∥X:,j∥2 X⊤ :,jr // gradient step βj = ST(zj,neλ/∥X:,j∥2) // proximal step r−= X:,j(βj −βold) return βniterImplicit differentiation of Lasso-type models for hyperparameter optimization C. Derivations for MCP Let us remind the deﬁnition of the Minimax Concave Penalty (MCP) estimator introduced by Zhang (2010), also analyzed under the name CELE0 by Soubies et al. (2017). First of all, for any t∈R: pMCP λ,γ (t) = { λ|t|− t2 2γ, if |t|≤ γλ 1 2 γλ2, if |t|>γλ . (36) The proximity operator of pλ,γ for parameters λ >0 and γ >1 is deﬁned as follow (see Breheny and Huang 2011, Sec. 2.1): proxMCP λ,γ (t) = {ST(t,λ) 1−1 γ if |t|≤ γλ t if |t|>γλ . (37) For ourselves we choose as for the Lasso an exponential parametrization of the coefﬁcients, for λ∈R and γ >0: ˆβ(λ,γ)(y) ≜ arg min β∈Rp 1 2n∥y−Xβ∥2 2 + p∑ j=1 pMCP eλ,eγ (|βj|) . (38) Update rule for Coordinate Descent Below, we provide equation to update the coefﬁcient in the coordinate descent algorithm of the MCP: βj ←arg min βj∈R 1 2n∥y−βjX:,j − ∑ j′̸=j βj′X:,j′∥2 2 + p∑ j′̸=j pMCP eλ,eγ(βj′) +pMCP eλ,eγ(βj) = arg min βj∈R 1 2n∥y−βjX:,j − ∑ j′̸=j βj′X:,j′∥2 2 + pMCP eλ,eγ(βj) = arg min βj∈R ∥X:,j∥2 2   1 2n  βj − 1 ∥X:,j∥2 2 ⟨ y− ∑ j′̸=j βj′X:,j′,X:,j ⟩  2 + 1 ∥X:,j∥2 2 pMCP eλ,eγ(βj)   = arg min βj∈R   1 2n  βj − 1 ∥X:,j∥2 2 ⟨ y− ∑ j′̸=j βj′X:,j′,X:,j ⟩  2 + 1 ∥X:,j∥2 2 pMCP eλ,eγ(βj)   = arg min βj∈R   1 2Lj  βj − 1 ∥X:,j∥2 2 ⟨ y− ∑ j′̸=j βj′X:,j′,X:,j ⟩  2 + pMCP eλ,eγ(βj)  ,with Lj ≜ n ∥X:,j∥2 2 = proxMCP eλ/Lj,eγLj ( βj − 1X2 :,j X⊤ :,j(Xβ −y),λ ) . (39) One can write the following ﬁxed point equation satisﬁed by the estimator ˆβ, with Lj = ∥X:,j∥2 /n: ˆβj = proxMCP eλ/Lj,eγLj   ⟨ y− ∑ k̸=j ˆβkX:,k, X:,j ∥X:,j∥2 ⟩  = proxMCP eλ/Lj,eγLj ( ˆβj − 1 ∥X:,j∥2 X⊤ :,j ( Xˆβ−y )) . (40) Since the MCP penalty is non-convex, the estimator may not be continuous w.r.t. hyperparameters and gradient based hyperparameter optimization may not be theoretically justiﬁed. However we can differentiate the ﬁxed point equationImplicit differentiation of Lasso-type models for hyperparameter optimization Imp. F. iterdiﬀ. (ours) F. iterdiﬀ. Grid-search 0 2 4 10−4 10−3 10−2 10−1 100 Objective minus optimum rcv1 (p=19,959) 0 10 20 30 10−2 10−1 100 101 102  20news (p=130,107) 0 2 4 Time (s) 10−1 100 Loss on test set 0 10 20 30 Time (s) 101 102 Figure 5.Computation time for the HO of the MCP on real dataDistance to “optimum” (top) and performance (bottom) on the test set for the MCP. Equation (40) almost everywhere: ˆJj = ( ˆJj − 1 ∥X:j∥2 2 X⊤ :jX ˆJ ) · ∂proxMCP eλ/Lj,eγLj ∂t ( ˆβj − 1X2 :,j X⊤ :,j(Xβ −y) ) + eλ Lj ∂proxMCP eλ/Lj,eγLj ∂λ ( ˆβj − 1X2 :,j X⊤ :,j(Xβ −y) ) + eγLj ∂proxMCP eλ/Lj,eγLj ∂γ ( ˆβj − 1X2 :,j X⊤ :,j(Xβ −y) ) . (41) where ∂proxMCP λ,γ ∂t (t) = { |sign t| 1−1 γ , if |t|≤ λγ 1, otherwise , (42) ∂proxMCP λ,γ ∂λ (t) =    0, if |t|≤ λ −sign t 1−1 γ , if λ≤|t|≤ λγ 0, if |t|>λγ , (43) ∂proxMCP λ,γ ∂γ (t) = { −ST(t,λ) (γ−1)2 if |t|≤ λγ 0 if |t|>λγ . (44) Contrary to other methods, HO based algorithms do not scale exponentially in the number of hyperparameters. Here we propose experiments on the held-out loss with the MCP estimator (Zhang, 2010), which has 2 hyperparameters λand γ. Our algorithm can generalize to such non-smooth proximity-based estimator. Comments on Figure 5 (MCP , held-out criterion). Figure 5 (top) shows the convergence of the optimum on 2 datasets (rcv1 and 20news) for the MCP estimator. As before implicit forward differentiation outperforms forward differentiation illustrating Proposition 2 and Table 1.Implicit differentiation of Lasso-type models for hyperparameter optimization D. Datasets and implementation details The code used to produce all the ﬁgures as well as the implementation details can be found in the supplementary material in the forward_implicit/expesfolder. In particular in all experiments, for our algorithm, implicit forward differentiation, the size of the loop computing the Jacobian is ﬁxed: n_iter_jac = 100. Reminding that the goal is to compute the gradient: ˆJ⊤ (λ)∇C ( ˆβ(λ) ) , (45) we break the loop if ∥(J(k+1) −J(k))∇C( ˆβ(λ))∥≤∥∇C ( ˆβ(λ))∥×ϵjac , (46) with ϵjac = 10−3. All methods beneﬁt from warm start. D.1. Details on Figure 1 Figure 1 is done using synthetic data. As described in Section 4.2, X ∈ Rn×p is a Toeplitz correlated ma- trix, with correlation coefﬁcient ρ = 0 .9, (n,p) = (1000 ,2000). β ∈ Rp is chosen with 5 non-zero coefﬁ- cients chosen at random. Then y ∈ Rn is chosen to be equal to Xβ contaminated by some i.i.d. random Gaus- sian noise, we chose SNR = 3. For Figure 1 all the implementation details can be found in the joint code in the forward_implicit/examples/plot_time_to_compute_single_gradient.py ﬁle. Figure 1 shows the time of compu- tation of one gradient and the distance to ”optimum”. For this ﬁgure we evaluated the gradient in λ= λmax −ln(10). The ”optimum” is the gradient obtained using the implicit differentiation method. D.2. Details on Figure 2 Let us ﬁrst begin by a description of all the datasets and where they can be downloaded. rcv1. The rcv1 dataset can be downloaded here: https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/ datasets/multilabel.html#rcv1v2%20(topics;%20subsets). The dataset contains n = 20,242 sam- ples and p= 19,959 features. 20news. The 20news dataset can be downloaded here https://www.csie.ntu.edu.tw/~cjlin/ libsvmtools/datasets/multiclass.html#news20. The dataset contains n = 11 ,314 samples and p= 130,107 features. ﬁnance. The ﬁnance (E2006-log1p on libsvm) dataset can be downloaded here: https://www.csie.ntu.edu. tw/~cjlin/libsvmtools/datasets/regression.html#E2006-log1p. The dataset contains n= 16,087 samples and p= 1,668,737 features. All the implementation details can be found in the code: forward_implicit/expes/main_lasso_pred.py. D.3. Details on Figure 3 Figure 3 was performed using simulated data. The matrix X ∈Rn×p was obtained by simulated n×pi.i.d. Gaussian variables N(0,1). The number of rows was ﬁxed atn= 100and we changed the number of columnspfrom 200 to 10,000 on a linear grid of size 10. Then , we generated β∗with 5 coefﬁcients equal to 1 and the rest equals to 0. The vector y is equal to Xβ∗contaminated by some i.i.d. random Gaussian noise controlled by a SNR value of 3. We performed 50 repetitions for each value of pand computed the average MSE on these repetitions. The initial value for the line-search algorithm was set at λmax + ln(0.7) and the number of iterations for the Jacobian at 500 for the whole experiment. All the implementation details can be found in the code : forward_implicit/expes/main_lasso_est.py. D.4. Details on Figure 4 Figure 4 was performed using the same simulating process as described above only this time we performed only 25 repeti- tions for each value of p. We had to deal with the fact that Problem (4) is not convex for the weighted Lasso which means that our line-search algorithm could get stuck in local minima. In order to alleviate this problem, we introduced Equa- tion (18) to obtain an initial point for the line-search algorithm. We chose the regularization term to be constant and equalsImplicit differentiation of Lasso-type models for hyperparameter optimization to C(β(λmax))/10. We used a time treshold of 500 seconds which was hit only by the forward differentiation algorithm for the wLasso. The details about this experiment can be found in the code : forward_implicit/expes/main_wLasso.py.Implicit differentiation of Lasso-type models for hyperparameter optimization E. Supplementary experiments E.1. Experiments with a non-unique solution to the inner problem We recall here that the bi-level optimization Problem (4) is solved using gradient descent. We recall also that gradient descent may not converge toward a global minima since the optimized function λ↦→L(λ) may not be convex. It may be even worse: if the inner optimization problem has not a unique solution, the function λ ↦→L(λ) may not be continuous. However our algorithm can still be applied to compute the hypergradient. Figure 6 shows the time to compute a single (hyper)gradient when the solution to the inner problem is not unique. As proved for instance in Tibshirani (2013, Lemma 3 and 4), the set of parameters where the Lasso solution is not unique is typically ∅or a set whose Lebesgue measure is zero. Moreover, there exist settings such that the solution path (as a multivalued mapping) could be non-continuous, which leaves only non-gradient based methods available. Thus, we decided to not investigate the theory in such pathological settings. The authors are not aware of a classical dataset where non-uniqueness arises. Nevertheless, in the case where there existsλsuch that the solution set is not reduced to a singleton, our proposed algorithm can still be applied to any solution without theoretical guarantees. Experimental setting for non-uniqueness.For completeness, we run our methods on the following toy example Tibshi- rani (2013): we consider a design Xsuch that n= 100, p= 10000and X1,X2,X3 are generated iid following a standard normal distribution, X4 = (X2 +X3)/2 and X5,...,X p are generated i.i.d. following a standard normal distribution, then orthogonalized X5,...,X p w.r.t.Span(X1,...,X 4). We let y= −X1 +X2 +X3. We let the reader observe that when λ = 1/n, the solution set is not reduced to a singleton. In this case, similar conclusions are observed in Figure 6 as for other datasets: Imp. F. Iterdiff (ours) still outperforms its competitors. Imp. F. Iterdiﬀ. (ours) F. Iterdiﬀ. B. Iterdiﬀ. 6 8 10 Number of iterations 100 102 Times (s) 6 8 10 Number of iterations 10−11 10−8 10−5 Objective minus optimum Figure 6.Time to compute a single gradient with non-unique solution(Synthetic data, Lasso, n,p = 1000,10000). Inﬂuence on the number of iterations of BCD (in the inner optimization problem of Problem (4)) on the computation time (left) and the distance to “optimum” of the gradient∇λL(λ)(right) for the Lasso estimator.",
      "meta_data": {
        "arxiv_id": "2002.08943v3",
        "authors": [
          "Quentin Bertrand",
          "Quentin Klopfenstein",
          "Mathieu Blondel",
          "Samuel Vaiter",
          "Alexandre Gramfort",
          "Joseph Salmon"
        ],
        "published_date": "2020-02-20T18:43:42Z",
        "pdf_url": "https://arxiv.org/pdf/2002.08943v3.pdf",
        "github_url": "https://github.com/QB3/sparse-ho"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the challenge of setting regularization parameters for Lasso-type estimators, which is crucial but difficult, especially for high-dimensional data. Existing gradient-based hyperparameter optimization (HO) methods suffer from high memory consumption (forward/backward automatic differentiation) or numerical instability and the assumption of smooth loss functions (implicit differentiation). The main contributions are: 1) Introduction of an efficient implicit differentiation algorithm, without matrix inversion, specifically tailored for non-smooth Lasso-type problems. 2) The proposed method scales to high-dimensional data by leveraging the sparsity of the solutions. 3) It demonstrates that forward iterative differentiation of Block Coordinate Descent (BCD) converges linearly to the true gradient once the support is identified, and its limit is independent of the initial point. 4) The proposed algorithm (Algorithm 2) decouples the computation of the Jacobian from the regression coefficients, allowing state-of-the-art convex solvers for coefficient computation and avoiding potentially ill-conditioned linear systems. 5) Experiments show that the method outperforms many standard HO techniques for optimizing error on held-out data or the Stein Unbiased Risk Estimator (SURE).",
        "methodology": "The hyperparameter optimization (HO) problem is framed as a nested bi-level optimization. The inner problem finds the regression coefficients for Lasso-type models (Lasso or weighted Lasso) given hyperparameters, while the outer problem optimizes a criterion (e.g., held-out loss, SURE) with respect to these hyperparameters using gradient descent. The core methodology relies on an efficient implicit differentiation technique adapted for non-smooth Lasso-type problems. The key insight is leveraging the fixed-point iteration induced by proximal Block Coordinate Descent (BCD) algorithms. The proposed algorithm, 'Implicit Forward Iterdiff' (Algorithm 2), operates in two phases: first, it computes the regression coefficients (e.g., using BCD) to identify the solution's support; second, it computes the Jacobian of the coefficients with respect to hyperparameters using forward differentiation recursion steps, restricted to the identified support. This decoupling and support-restricted computation avoids solving a large (p x p) linear system (as in classical implicit differentiation, Algorithm 1) or storing all intermediate iterates (as in backward iterative differentiation). The convergence of this approach to the true Jacobian is theoretically proven for unique Lasso solutions.",
        "experimental_setup": "The Python code for the proposed method is open-source (sparse-ho, using Numba for critical parts). For fair comparison, all hypergradient-based methods (proposed Implicit Forward Iterdiff, Implicit, Forward Iterdiff) and non-gradient-based competitors (Grid-search, Random-search, Lattice Hyp., Bayesian optimization) utilized the same vanilla BCD algorithm for the inner optimization problem. The inner solver termination criteria was a relative change in the cost function below 10^-5. Hypergradient methods used a line-search strategy. Lasso initializations were set at λ_max - log(10). The studies included: 1) Held-out loss on real, high-dimensional datasets: rcv1 (p=19,959), 20news (p=130,107), and finance (p=1,668,737), splitting data into training, validation, and test sets. The objective was minimizing validation loss, and test loss measured generalization. Non-convex MCP (Minimax Concave Penalty) with two hyperparameters was also tested. 2) Stein Unbiased Risk Estimator (SURE) for Lasso and weighted Lasso models using synthetic data (n=100, p varied from 200 to 10,000, SNR=3, 50 repetitions). The metric used was normalized Mean Squared Error (MSE). For the weighted Lasso with p hyperparameters, a regularized version of the HO problem was used to find a robust initialization.",
        "limitations": "The theoretical guarantees for convergence (Proposition 2) assume a unique solution to the inner Lasso problem. While the set of parameters leading to non-unique solutions is typically of measure zero, such pathological settings could lead to non-continuous solution paths for ˆβ(λ), making gradient-based methods theoretically challenging. The current theory does not cover non-convex cases (e.g., MCP penalty), though numerical experiments suggest proper behavior. The hyperparameter optimization problem L(λ) is generally non-convex, meaning gradient descent might converge to local minima rather than a global optimum, making initialization crucial. Backward iterative differentiation (Algorithm 4) was found to be computationally inefficient and memory-intensive for the BCD algorithm in this context, rendering it impractical for extensive benchmarking. The SURE criterion requires knowledge of the noise variance, which might not always be available in practice.",
        "future_research_directions": "Future research directions include extending the theoretical framework of efficient implicit differentiation to cover non-convex penalty functions (e.g., MCP, Elastic-Net) for Lasso-type estimators. Further investigation into handling other regularization formulations beyond Lasso and weighted Lasso is also suggested. A promising area is to leverage the two-step nature of the proposed algorithm to integrate with advanced state-of-the-art Lasso solvers that employ techniques like active sets or screening rules. Such solvers introduce discontinuities with respect to hyperparameters, and fully combining them with gradient-based HO methods would be a significant advancement, challenging single-step automatic differentiation approaches.",
        "experimental_code": "class ImplicitForward():\n    \"\"\"Algorithm to compute the hypergradient using implicit forward\n    differentiation.\n\n    First the algorithm computes the regression coefficients.\n    Then the iterations of the forward differentiation are applied to compute\n    the Jacobian.\n\n    Parameters\n    ----------\n    tol_jac: float\n        Tolerance for the Jacobian computation.\n    max_iter: int\n        Maximum number of iterations for the inner solver.\n    n_iter_jac: int\n        Maximum number of iterations for the Jacobian computation.\n    use_stop_crit: bool, optional (default=True)\n        Use stopping criterion in hypergradient computation. If False,\n        run to maximum number of iterations.\n    verbose: bool, optional (default=False)\n        Verbosity of the algorithm.\n    \"\"\"\n\n    def __init__(\n            self, tol_jac=1e-3, max_iter=100, n_iter_jac=100,\n            use_stop_crit=True, verbose=False):\n        self.max_iter = max_iter\n        self.tol_jac = tol_jac\n        self.n_iter_jac = n_iter_jac\n        self.use_stop_crit = use_stop_crit\n        self.verbose = verbose\n\n    def get_beta_jac(\n            self, X, y, log_alpha, model, get_grad_outer, mask0=None,\n            dense0=None, quantity_to_warm_start=None, max_iter=1000, tol=1e-3,\n            full_jac_v=False):\n        \"\"\"Compute beta and hypergradient using implicit forward\n        differentiation.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            Design matrix.\n        y: ndarray, shape (n_samples,)\n            Observation vector.\n        log_alpha: float or np.array, shape (n_features,)\n            Logarithm of hyperparameter.\n        model:  instance of ``sparse_ho.base.BaseModel``\n            A model that follows the sparse_ho API.\n        get_grad_outer: callable\n            Function which returns the gradient of the outer criterion.\n        mask0: ndarray, shape (n_features,)\n            Boolean of active feature of the previous regression coefficients\n            beta for warm start.\n        dense0: ndarray, shape (mask.sum(),)\n            Initial value of the previous regression coefficients\n            beta for warm start.\n        quantity_to_warm_start: ndarray\n            Previous Jacobian of the inner optimization problem.\n        max_iter: int\n            Maximum number of iteration for the inner solver.\n        tol: float\n            The tolerance for the inner optimization problem.\n        full_jac_v: bool\n            TODO\n        \"\"\"\n\n        mask, dense, jac = get_bet_jac_implicit_forward(\n            X, y, log_alpha, mask0=mask0, dense0=dense0,\n            jac0=quantity_to_warm_start,\n            tol_jac=tol, tol=tol, niter_jac=self.n_iter_jac, model=model,\n            max_iter=self.max_iter, verbose=self.verbose)\n        return mask, dense, jac\n\n    def compute_beta_grad(\n            self, X, y, log_alpha, model, get_grad_outer, mask0=None,\n            dense0=None, quantity_to_warm_start=None, max_iter=1000, tol=1e-3,\n            full_jac_v=False):\n        mask, dense, jac = get_bet_jac_implicit_forward(\n            X, y, log_alpha, mask0=mask0, dense0=dense0,\n            jac0=quantity_to_warm_start,\n            tol_jac=self.tol_jac, tol=tol, niter_jac=self.n_iter_jac,\n            model=model, max_iter=self.max_iter, verbose=self.verbose,\n            use_stop_crit=self.use_stop_crit)\n        jac_v = model.get_jac_v(X, y, mask, dense, jac, get_grad_outer)\n        if full_jac_v:\n            jac_v = model.get_full_jac_v(mask, jac_v, X.shape[1])\n\n        return mask, dense, jac_v, jac\n\n\ndef get_bet_jac_implicit_forward(\n        X, y, log_alpha, model, mask0=None, dense0=None, jac0=None,\n        tol=1e-3, max_iter=1000, niter_jac=1000, tol_jac=1e-6, verbose=False,\n        use_stop_crit=True):\n\n    mask, dense, _ = compute_beta(\n        X, y, log_alpha, mask0=mask0, dense0=dense0, jac0=jac0, tol=tol,\n        max_iter=max_iter, compute_jac=False, model=model, verbose=verbose,\n        use_stop_crit=use_stop_crit)\n    dbeta0_new = model._init_dbeta0(mask, mask0, jac0)\n    reduce_alpha = model._reduce_alpha(np.exp(log_alpha), mask)\n\n    _, dual_var = model._init_beta_dual_var(X, y, mask, dense)\n    jac = get_only_jac(\n        model.reduce_X(X, mask), model.reduce_y(y, mask), dual_var,\n        reduce_alpha, model.sign(dense, log_alpha), dbeta=dbeta0_new,\n        niter_jac=niter_jac, tol_jac=tol_jac, model=model, mask=mask,\n        dense=dense, verbose=verbose, use_stop_crit=use_stop_crit)\n\n    return mask, dense, jac\n\n\ndef get_only_jac(\n        Xs, y, dual_var, alpha, sign_beta, dbeta=None, niter_jac=100,\n        tol_jac=1e-4, model=\"lasso\", mask=None, dense=None, verbose=False,\n        use_stop_crit=True):\n    n_samples, n_features = Xs.shape\n\n    L = model.get_L(Xs)\n\n    residual_norm = []\n\n    if hasattr(model, 'dual'):\n        ddual_var = model._init_ddual_var(dbeta, Xs, y, sign_beta, alpha)\n        dbeta = model.dbeta\n    else:\n        if dbeta is None:\n            dbeta = model._init_dbeta(n_features)\n        ddual_var = model._init_ddual_var(dbeta, Xs, y, sign_beta, alpha)\n\n    for i in range(niter_jac):\n        if verbose:\n            print(\"%i -st iterations over %i\" % (i, niter_jac))\n        if issparse(Xs):\n            model._update_only_jac_sparse(\n                Xs.data, Xs.indptr, Xs.indices, y, n_samples,\n                n_features, dbeta, dual_var, ddual_var, L, alpha, sign_beta)\n        else:\n            model._update_only_jac(\n                Xs, y, dual_var, dbeta, ddual_var, L, alpha, sign_beta)\n        residual_norm.append(\n            model.get_jac_residual_norm(\n                Xs, y, n_samples, sign_beta, dbeta, dual_var,\n                ddual_var, alpha))\n        if use_stop_crit and i > 1:\n            # relative stopping criterion for the computation of the jacobian\n            # and absolute stopping criterion to handle warm start\n            rel_tol = np.abs(residual_norm[-2] - residual_norm[-1])\n            if (rel_tol < np.abs(residual_norm[-1]) * tol_jac\n                    or residual_norm[-1] < 1e-10):\n                break\n    # HACK we only need this for one test, do not rely on it\n    get_only_jac.n_iter = i\n\n    return dbeta",
        "experimental_info": "The 'Implicit Forward Iterdiff' algorithm is used for hyperparameter optimization (HO) of Lasso-type models, including Lasso, ElasticNet, and Sparse Logistic Regression. The methodology involves a nested bi-level optimization problem, where the inner problem solves for model coefficients and the outer problem optimizes a criterion using gradient-based methods.\n\nKey experimental settings across various examples and experiments include:\n\n1.  **Models & Inner Solvers**: The method is applied to:\n    *   **Lasso**: Typically uses `celer.Lasso(fit_intercept=False, warm_start=True)` or `sklearn.linear_model.Lasso` as the inner solver.\n    *   **ElasticNet**: Uses `celer.ElasticNet(fit_intercept=False, warm_start=True)` or `sklearn.linear_model.ElasticNet`.\n    *   **Sparse Logistic Regression**: Uses `celer.LogisticRegression(penalty='l1', fit_intercept=False)` or `sklearn.linear_model.LogisticRegression` (with 'saga' solver for L1 penalty).\n    Inner solvers are configured with `max_iter` ranging from 50 to 100,000 and `tol` from 1e-5 to 1e-8.\n\n2.  **Outer Optimizers**: Gradient-based methods are employed for the outer optimization problem:\n    *   `LineSearch`: Common settings include `n_outer=10` to `30`, `tol=1e-7`.\n    *   `GradientDescent`: Configured with `n_outer=10` to `75`, `p_grad_norm` (e.g., `1`, `1.5`, `1.9`), `step_size` (often adaptive), and `tol=1e-8`.\n    *   `Adam`: Used with `n_outer=10`, `lr=0.11`, `beta_1=0.9`, `beta_2=0.999`.\n\n3.  **Outer Criteria**: The objective function for HO can be:\n    *   `HeldOutMSE`: Mean Squared Error on a validation set.\n    *   `HeldOutLogistic`: Logistic loss on a validation set.\n    *   `CrossVal`: Cross-validation scores (e.g., 5-fold) based on `HeldOutMSE` or `HeldOutLogistic`.\n    *   `FiniteDiffMonteCarloSure`: Stein Unbiased Risk Estimator.\n\n4.  **Implicit Forward Iterdiff (Algorithm 2) Specifics**: The `ImplicitForward` class parameters are varied:\n    *   `tol_jac`: Tolerance for Jacobian computation ranges from `1e-3` to `1e-8` (and `1e-32` in hypergradient accuracy experiments).\n    *   `n_iter_jac`: Maximum iterations for Jacobian computation, typically `100` to `2000` (up to `5000` for some hypergradient experiments).\n    *   `max_iter`: Maximum iterations for the inner solver, passed to `ImplicitForward`, can be `100` to `50000`.\n    *   `use_stop_crit`: Generally `True` (default), but set to `False` in some hypergradient accuracy tests.\n\n5.  **Datasets**: Experiments are conducted on a variety of datasets:\n    *   **Real-world**: `rcv1` (binary/multiclass), `real-sim`, `news20` (20newsgroups), `leukemia`, `finance`, `mnist`, `usps`, `sector_scale`, `aloi`, MEG data.\n    *   **Synthetic**: `simu` (generated with `make_classification` or `make_correlated_data`).\n    Data splitting typically uses `n_samples // 2` for training and validation, or `KFold` for cross-validation.\n\n6.  **Hyperparameter Initialization & Search Space**:\n    *   **Initialization**: Initial `alpha` or `log_alpha` values (`alpha0`, `log_alpha0`) are commonly set as a fraction of `alpha_max` (e.g., `alpha_max / 5`, `alpha_max / 10`, `0.1 * alpha_max`, `0.9 * alpha_max`).\n    *   **Search Space**: For single hyperparameters (Lasso, LogReg), a logarithmic grid (`np.geomspace`) is used, ranging from `alpha_max` down to `alpha_min` (e.g., `alpha_max / 1_000` to `alpha_max / 100_000`). For two hyperparameters (ElasticNet), 2D grids are constructed (e.g., `n_grid=15` for `alphas_l1` and `alphas_l2`).\n\n7.  **Performance Metrics**: Hyperparameter optimization processes are monitored for:\n    *   Outer criterion value (e.g., validation loss).\n    *   Gradient norm of the outer criterion.\n    *   Computational time.\n    *   Accuracy on validation/test sets (for classification).\n\nOverall, experiments systematically compare `ImplicitForward` against zero-order methods (grid-search, random-search, Bayesian optimization) and other first-order methods (`Forward`, `Implicit`, `Backward`) to demonstrate its efficiency and accuracy, especially in terms of convergence speed and hypergradient computation."
      }
    },
    {
      "title": "Bayesian Optimization for Iterative Learning",
      "abstract": "The performance of deep (reinforcement) learning systems crucially depends on\nthe choice of hyperparameters. Their tuning is notoriously expensive, typically\nrequiring an iterative training process to run for numerous steps to\nconvergence. Traditional tuning algorithms only consider the final performance\nof hyperparameters acquired after many expensive iterations and ignore\nintermediate information from earlier training steps. In this paper, we present\na Bayesian optimization (BO) approach which exploits the iterative structure of\nlearning algorithms for efficient hyperparameter tuning. We propose to learn an\nevaluation function compressing learning progress at any stage of the training\nprocess into a single numeric score according to both training success and\nstability. Our BO framework is then balancing the benefit of assessing a\nhyperparameter setting over additional training steps against their computation\ncost. We further increase model efficiency by selectively including scores from\ndifferent training steps for any evaluated hyperparameter set. We demonstrate\nthe efficiency of our algorithm by tuning hyperparameters for the training of\ndeep reinforcement learning agents and convolutional neural networks. Our\nalgorithm outperforms all existing baselines in identifying optimal\nhyperparameters in minimal time.",
      "full_text": "Bayesian Optimization for Iterative Learning Vu Nguyen ∗ University of Oxford vu@robots.ox.ac.uk Sebastian Schulze ∗ University of Oxford sebastian.schulze@eng.ox.ac.uk Michael A. Osborne University of Oxford mosb@robots.ox.ac.uk Abstract The performance of deep (reinforcement) learning systems crucially depends on the choice of hyperparameters. Their tuning is notoriously expensive, typically requiring an iterative training process to run for numerous steps to convergence. Traditional tuning algorithms only consider the ﬁnal performance of hyperparam- eters acquired after many expensive iterations and ignore intermediate information from earlier training steps. In this paper, we present a Bayesian optimization (BO) approach which exploits the iterative structure of learning algorithms for efﬁcient hyperparameter tuning. We propose to learn an evaluation function compress- ing learning progress at any stage of the training process into a single numeric score according to both training success and stability. Our BO framework is then balancing the beneﬁt of assessing a hyperparameter setting over additional train- ing steps against their computation cost. We further increase model efﬁciency by selectively including scores from different training steps for any evaluated hyper- parameter set. We demonstrate the efﬁciency of our algorithm by tuning hyperpa- rameters for the training of deep reinforcement learning agents and convolutional neural networks. Our algorithm outperforms all existing baselines in identifying optimal hyperparameters in minimal time. 1 Introduction Deep learning (DL) and deep reinforcement learning (DRL) have led to impressive breakthroughs in a broad range of applications such as game play [26, 36], motor control [43], and image recognition [20]. To maintain general applicability, these algorithms expose sets of hyperparameters to adapt their behavior to any particular task at hand. This ﬂexibility comes at the price of having to tune an additional set of parameters – poor settings lead to drastic performance losses [11, 30, 37]. On top of being notoriously sensitive to these choices, deep (reinforcement) learning systems often have high training costs, in computational resources and time. For example, a single training run on the Atari Breakout game took approximately 75 hours on a GPU cluster [26]. Tuning DRL parameters is further complicated as only noisy evaluations of an agent’s ﬁnal performance are obtainable. Bayesian optimization (BO) [12, 28, 35] has recently achieved considerable success in optimizing these hyperparameters. This approach casts the tuning process as a global optimization problem based on noisy evaluations of a black-box function f . BO constructs a surrogate model typically using a Gaussian process (GP) [31], over this unknown function. This GP surrogate is used to build an acquisition function [13, 44] which suggests the next hyperparameter to evaluate. In modern machine learning (ML) algorithms [15], the training process is often conducted in an iterative manner. A natural example is given by deep learning where training is usually based on stochastic gradient descent and other iterative procedures. Similarly, the training of reinforcement learning agents is mostly carried out using multiple episodes. The knowledge accumulated during these training iterations can be useful to inform BO. However, most existing BO approaches [35] ∗These authors contributed equally. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:1909.09593v5  [cs.LG]  16 Jan 2021deﬁne the objective function as the average performance over the ﬁnal training iterations. In doing so, they ignore the information contained in the preceding training steps. In this paper, we present a Bayesian optimization approach for tuning algorithms where iterative learning is available – the cases of deep learning and deep reinforcement learning. First, we consider the joint space of input hyperparameters and number of training iterations to capture the learning progress at different time steps in the training process. We then propose to transform the whole training curve into a numeric score according to user preference. To learn across the joint space efﬁciently, we introduce a data augmentation technique leveraging intermediate information from the iterative process. By exploiting the iterative structure of training procedures, we encourage our algorithm to consider running a larger number of cheap (but high-utility) experiments, when cost- ignorant algorithms would only be able to run a few expensive ones. We demonstrate the efﬁciency of our algorithm on training DRL agents on several well-known benchmarks as well as the training of convolutional neural networks. In particular, our algorithm outperforms existing baselines in ﬁnding the best hyperparameter in terms of wall-clock time. Our main contributions are: • an algorithm to optimize the learning curve of a ML algorithm by using training curve compression, instead of averaged ﬁnal performance; • an approach to learn the compression curve from the data and a data augmentation tech- nique for increased sample-efﬁciency; • demonstration on tuning DRL and convolutional neural networks. 2 Related Work in Iteration-Efﬁcient Bayesian Optimization The ﬁrst algorithm category employs stopping criteria to terminate some training runs early and allo- cate resources towards more promising settings. These criteria typically involve projecting towards a ﬁnal score from early training stages. Freeze-thaw BO [42] models the training loss over time us- ing a GP regressor under the assumption that the training loss roughly follows an exponential decay. Based on this projection, training resources are allocated to the most promising settings. Hyperband [8, 23] dynamically allocates computational resources (e.g. training epochs or dataset size) through random sampling and eliminates under-performing hyperparameter settings by successive halving. Attempts have also been made to improve the epoch efﬁciency of other hyperparameter optimization algorithms in [5, 7, 18] which predict the ﬁnal learning outcome based on partially trained learning curves to identify hyperparameter settings that are expected to under-perform and early-stop them. In the context of DRL, however, these stopping criteria, including the exponential decay assumed in Freeze-thaw BO [42], may not be applicable, due to the unpredictable ﬂuctuations of DRL reward curves. In the supplement, we illustrate the noisiness of DRL training. The second category [16, 17, 23, 41, 48] aims to reduce the resource consumption of BO by utilizing low-ﬁdelity functions which can be obtained by using a subset of the training data or by training the ML model for a small number of iterations. Multi-task BO [41] requires the user to deﬁne a division of the dataset into pre-deﬁned and discrete subtasks. Multi-ﬁdelity BO with continuous approximation (BOCA) [16] and hierarchical partition [34] extend this idea to continuous settings. Speciﬁcally, BOCA ﬁrst selects the hyperparameter input and then the corresponding ﬁdelity to be evaluated at. The ﬁdelity in this context refers to the use of different number of learning iterations. Analogous to BOCA’s consideration of continuous ﬁdelities, Fabolas [17] proposes to model the combined space of input hyperparameter and dataset size and then select the optimal input and dataset size jointly. The above approaches typically identify performance of hyperparameters via the average (either training or validation) loss of the last learning iterations. Thereby, they do not account for potential noise in the learning process (e.g., they might select unstable settings that jumped to high perfor- mance in the last couple of iterations). 3 Bayesian Optimization for Iterative Learning (BOIL) Problem setting. We consider training a machine learning algorithm given a d-dimensional hy- perparameter x ∈X ⊂Rd for t iterations. This process has a training time costc(x,t) and produces 20 100 200 300 400 500 #Episode t 0.80 0.85 0.90 0.95 1.00x Tmin Tmax Augmented Obs Observation 0 100 200 300 400 500 0 50 100 150 200Score 4 18 34 8 45 5 14 26Reward Curve Sigmoid Func 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . |m * 0 ,g* 0 ) for Cifar10 m* 0 =-4.0 g* 0 =1.476 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . |m * 0 ,g* 0 ) for Reacher m* 0 =2.779 g* 0 =1.973 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . |m * 0 ,g* 0 ) for CartPole m* 0 =-3.266 g* 0 =3.0 Figure 1: Left: the score in pink box is a convolution of the reward curve r(·| x = 0.9,t = 500) and a Sigmoid function l(u |g0,m0) = 1 1+exp(−g0[u−m0]) up to time step t. Bottom: observations are selected to augment the dataset (red dots). The heatmap indicates the GP predictive mean µ for f across the number of episodest used to train an agent. Tmin and Tmax are two user-deﬁned thresholds for the number of training episodes. x is a hyperparameter to be tuned. Right: we learn the optimal parameter g∗ 0 and m∗ 0 for each experiment separately. training evaluations r(·| x,t) for t iterations, t ∈[Tmin,Tmax]. These could be episode rewards in DRL or training accuracies in DL. An important property of iterative training is that we know the whole curve at preceding steps r(t′|x,t), ∀t′≤t. Given the raw training curve r(·| x,t), we assume an underlying smoothed black-box function f , deﬁned in Sec. 3.2. Formally, we aim to ﬁnd x∗= argmaxx∈X f (x,Tmax); at the same time, we want to keep the overall training time, ∑N i=1 c(xi,ti), of evaluated settings [xi,ti] as low as possible. We summarize our variables in Table 1 in the supplement for ease of reading. 3.1 Selecting a next point using iteration-efﬁcient modeling We follow popular designs in [17, 19, 39, 41] and model the cost-sensitive black-box function as f (x,t) ∼GP(0,k([x,t],[x′,t′])), where k is an appropriate covariance functions and [x,t] ∈Rd+1. For simplicity and robustness, the cost function c(x,t) is approximated by a linear regressor. De- pending on the setting, it may be more appropriate to employ a second GP or different parametric model if the cost has a more complex dependence on hyperparameters x and iterations t. We regu- larly (re-)optimize both kernel and cost function parameters in between point acquisitions. More speciﬁcally, we choose the covariance function as a productk ([x,t],[x′,t′]) =k(x,x′)×k(t,t′) to induce joint similarities over parameter and iteration space. We estimate the predictive mean and uncertainty for a GP [31] at any input z∗= [x∗,t∗] as µ (z∗) =k∗ [ K +σ2 y I ]−1 y (1) σ2 (z∗) =k∗∗−k∗ [ K +σ2 y I ]−1 kT ∗ (2) where y = [yi]∀i, k∗= [k (z∗,zi)]∀i, K = [k (zi,zj)]∀i, j, k∗∗= k (z∗,z∗), and σ2 y is the noise variance of f . Cost predictions at any particular parameter x and time t are given by µc([x∗,t∗]) =βT [x,t], where β is directly computed from data {Z = [xi,ti],c = [ci]}∀i as β = (ZT Z)−1Zc [1]. Our goal is to select a point with high function value (exploitation), high uncertainty (exploration) and low cost (cheap). At each iteration n, we query the input parameter xn and the number of iteration tn [38, 48]: zn = [xn,tn] = argmax x∈X ,t∈[Tmin,Tmax] α(x,t)/µc(x,t). (3) 3Although our framework is available for any acquisition choices [13, 22, 47], to cope with output noise, we follow [45] and slight modify the expected improvement criterion using the maximum mean GP prediction µmax n . Let λ = µn(z)−µmaxn σn(z) , we then have a closed-form for the new expected improvement (EI) as αEI n (z) =σn (z)φ (λ) + [µn (z)−µmax n ]Φ(λ) where φ is the standard normal p.d.f., Φ is the c.d.f, µn and σn are the GP predictive mean and variance deﬁned in Eq. (1) and Eq. (2), respectively. 3.2 Training curve compression and estimating the transformation function Existing BO approaches [4, 23] typically deﬁne the objective function as an average loss over the ﬁnal learning episodes. However, this does not take into consideration how stable performance is or the training stage at which it has been achieved. We argue that averaging learning losses is likely misleading due to the noise and ﬂuctuations of our observations (learning curves) – particularly during the early stages of training. We propose to compress the whole learning curve into a numeric score via a preference function representing the user’s desired training curve. In the following, we use the Sigmoid function (speciﬁcally the Logistic function) to compute the utility score as y = ˆy(r,m0,g0) =r(·|x,t)•l(·|m0,g0) = t ∑ u=1 r(u |x,t) 1 +exp(−g0 [u −m0]) (4) where •is a dot product, a Logistic function l(·| m0,g0) is parameterized by a growth parameter g0 deﬁning a slope and the middle point of the curve m0. The optimal parameters g0 and m0 are estimated directly from the data. We illustrate different shapes of l parameterized by g0 and m0 in the appendix. The Sigmoid preference has a number of desirable properties. As early weights are small, less credit is given to ﬂuctuations at the initial stages, making it less likely for our surrogate to be biased towards randomly well performing settings. However, as weights monotonically increase, hyperparameters with improving performance are preferred. As weights saturate over time, stable, high performing conﬁgurations are preferred over short “performance spikes” characteristic of un- stable training. Lastly, this utility score assigns higher values to the same performance if it is being maintained over more episodes. Learning the transformation function from data. Different compression curves l(), parameter- ized by different choices of g0 and m0 in Eq. (4), may lead to different utilities y and thus affect the performance. The optimal values of g∗ 0 and m∗ 0 are unknown in advance. Therefore, we propose to learn these values g∗ 0 and m∗ 0 directly from the data. Our intuition is that the ‘optimal’ compression curve l(m∗ 0,g∗ 0) will lead to a better ﬁt of the GP. This better GP surrogate model, thus, will result in better prediction as well as optimization performance. We parameterize the GP log marginal likelihood L [31] as the function of m0 and g0: L(m0,g0) =1 2 ˆyT ( K +σ2 y I )−1 ˆy −1 2 ln ⏐⏐K +σ2 y I ⏐⏐ +const (5) where σ2 y is the output noise variance, ˆy is the function of m0 and g0 deﬁned in Eq. (4). We optimize m0 and g0 (jointly with other GP hyperparameters) using multi-start gradient descent. We derive the derivative ∂L ∂m0 = ∂L ∂ ˆy ∂ ˆy ∂m0 and ∂L ∂g0 = ∂L ∂ ˆy ∂ ˆy ∂g0 which can be computed analytically as: ∂L ∂ ˆy = ( K +σ2 y IN )−1 ˆy; ∂ ˆy ∂m0 = −g0 ×exp(−g0 [u −m0]) [1 +exp(−g0 [u −m0])]2 ; ∂ ˆy ∂g0 = −m0 ×exp(−g0 [u −m0]) [1 +exp(−g0 [u −m0])]2 . The estimated compression curves are illustrated in Right Fig. 1 and in Sec. 4.1. 3.3 Augmenting the training data When evaluating a parameter x over t iterations, we obtain not only a ﬁnal score but also all reward sequences r(t′|x,t),∀t′= 1,..., t. The auxiliary information from the curve can be useful for BO. Therefore, we propose to augment the information from the curve into the sample set of our GP model. A naïve approach for augmentation is to add a full curve of points {[x, j],yj}t j=1 where yj is computed using Eq. (4). However, this approach can be redundant and may im- pose serious issues in the conditioning of the GP covariance matrix. As we cluster 40.80 0.85 0.90 0.95 1.00 200 300 400 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 x GP variance 2 0 2 4 6 8 10 12 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.80 0.85 0.90 0.95 1.00 200 300 400 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 GP variance 400 320 240 160 80 0 80 160 240 0.000 0.005 0.010 0.015 0.020 0.025 0.030 0.035 0.040 x Figure 2: GP with different settings. Left: our augmentation. Right: using a full curve. If we add too many observations, the GP covariance matrix becomes ill-conditioned. On the right, the GP ﬁt is poor with a large mean estimate range of [−400,240] even though the output is standardized N (0,1). All x-axis are over x, a hyperparameter to be tuned. more evaluations closely, the conditioning of the GP covariance degrades further, as dis- cussed in [24]. This conditioning issue is especially serious in our noisy DRL settings. 0 10 20 30 40 50 60 Iterations 0 5 10 15 20 25Log of Condition Number Condition Number of GP Covariance Augmentation No Augmentation Full Curve Reasonable Threshold Figure 3: The condition number of GP covari- ance matrix deteriorates if we add the whole curve of points into a GP. The large condition number indicates the nearness to singularity. We highlight this effect on GP estimation in Fig. 2 wherein the GP mean varies erratically when the natural log of the condition number of the GP co- variance matrix goes above 25 (see Fig. 3) as we include the whole curve. Selecting subset of points from the curve. Dif- ferent solutions, such as the addition of artiﬁcial noise or altering the kernel’s length-scales, have been proposed. We decide to use an active learn- ing approach [10, 29] as sampled data points are expected to contain a lot of redundant informa- tion. As a consequence, the loss of information from sub-sampling the data should be minimal and information-eroding modiﬁcation of the ker- nel matrix itself can be avoided. As a side beneﬁt, the reduced number of sampled points speeds up inference in our GP models. In particular, we select samples at the maximum of the GP predictive uncertainty. Formally, we sequentially select a set Z = [z1,...zM], zm = [x,tm], by varying tm while keeping x ﬁxed as zm =argmax ∀t′≤t σ([x,t′] |D′),∀m ≤M s.t. lnof cond(K) ≤δ (6) where D′= D∪{zj = [x,tj]}m−1 j=1 . This sub-optimisation problem is done in a one-dimensional space of t′∈{Tmin,..., t}, thus it is cheap to optimize using (multi-start) gradient descent (the derivative of GP predictive variance is available [31]). Alternatively, a ﬁxed-size grid could be considered, but this could cause conditioning issues when a point in the grid [ x,tgrid ] is placed near another existing point [ x′,tgrid ] , i.e., ||x −x′||2 ≤ε for some small ε. These generated points Z are used to calculate the output r(zm) and augmented into the observation set (X,Y ) for ﬁtting the GP. The number of samplesM is adaptively chosen such that the natural log of the condition number of the covariance matrix is less than a threshold. This is to ensure that the GP covariance matrix condition number behaves well by reducing the number of unnecessary points added to the GP at later stages. We compute the utility score ym given zm for each augmented point using Eq. (4). In addition, we can estimate the running time cm using the predictive mean µc(zm). We illustrate the augmented observations and estimated scores in Fig. 1. We summarize the overall algorithm in Alg. 1. To enforce non-negativity and numerical stability, we make use of the transformations α ←log[1 +exp(α)] and µc ←log[1 +exp(µc)]. 4 Experiments We assess our model by tuning hyperparameters for two DRL agents on three environments and a CNN on two datasets. We provide additional illustrations and experiments in the appendix. 5Algorithm 1 Bayesian Optimization with Iterative Learning (BOIL) Input: #iter N, initial data D0, z = [x,t]. Output: optimal x∗and y∗= max∀y∈DN y 1: for n = 1....N do 2: Fit a GP to estimate µf (),σf () from Eqs. (1,2) and a LR for cost µc() 3: Select zn = argmaxx,t α(x,t)/µc(x,t) and observe a curve r and a cost c from f (zn) 4: Compressing the learning curve r(zn) into numeric score using Eq. (4). 5: Sample augmented points zn,m,yn,m,cn,m,∀m ≤M given the curve and Dn in Eq. (6) 6: Augment the data into Dn and estimate Logistic curve hyperparameters m0 and g0. 7: end for Experimental setup. All experimental results are averaged over 20 independent runs with differ- ent random seeds. Final performance is estimated by evaluating the chosen hyperparameter over the maximum number of iterations. All experiments are executed on a NVIDIA 1080 GTX GPU using the tensorﬂow-gpu Python package. The DRL environments are available through the OpenAI gym [3] and Mujoco [43]. Our DRL implementations are based on the open source from Open AI Baselines [6]. We release our implementation at https://github.com/ntienvu/BOIL. We use square-exponential kernels for the GP in our model and estimate their parameters by maxi- mizing the marginal likelihood [31]. We set the maximum number of augmented points to beM = 15 and a threshold for a natural log of GP condition numberδ = 20. We note that the optimization over- head is much less than the black-box function evaluation time. Baselines. We compare with Hyperband [23] which demonstrated empirical successes in tuning deep learning applications in an iteration-efﬁcient manner. We extend the discrete multi-task BO [41] to the continuous case – which can also be seen as continuous multi-ﬁdelity BO [16, 39] as in our setting, they both consider cost-sensitivity and iteration-efﬁciency. We, therefore, label the two baselines as continuous multi-task/ﬁdelity BO (CM-T/F-BO). We have ignored the minor difference in these settings, such as multi-task approaches jointly optimizes the ﬁdelity and input while BOCA [16] ﬁrst selects the input and then the ﬁdelity. Our focus is to demonstrate the effectiveness of optimizing the learning curve using compression and augmentation techniques. We therefore omit the comparison of various acquisition functions and kernel choices which can easily be used in our model. We also do not compare with Fabolas [17] which is designed to vary dataset sizes, not iteration numbers. We would expect the performance of Fabolas to be close to CM-T/F-BO. We are unable to compare with FreezeThaw as the code is not available. However, the curves in our setting are not exponential decays and thus ill-suited to their model (see last ﬁgure in the appendix). We have considered an ablation study in the appendix using a time kernel following the exponential decay proposed in Freeze-thaw method [42]. Task descriptions. We consider three DRL settings including a Dueling DQN (DDQN) [46] agent in the CartPole-v0 environment and Advantage Actor Critic (A2C) [25] agents in the InvertedPendulum-v2 and Reacher-v2 environments. In addition to the DRL applications, we tune 6 hyperparameters for training a convolutional neural network [21] on the SVHN dataset and CI- FAR10. Due to space considerations, we refer to the appendix for further details. 4.1 Model illustration /uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018/uni00000006/uni00000024/uni00000058/uni0000004a/uni00000050/uni00000048/uni00000051/uni00000057/uni00000048/uni00000047/uni00000003/uni00000032/uni00000045/uni00000056 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni00000048/uni00000047/uni00000003/uni00000024/uni00000058/uni0000004a/uni00000050/uni00000048/uni00000051/uni00000057/uni00000048/uni00000047/uni00000003/uni00000032/uni00000045/uni00000056 Figure 4: DDQN on CartPole. The number of augmented observations reduces over time. We ﬁrst illustrate the estimated compression func- tion l(m∗ 0,g∗ 0) in Right Fig. 1 from different experi- ments. These Logistic parameters g∗ 0 and m∗ 0 are es- timated by maximizing the GP marginal likelihood and used for compressing the curve. We show that the estimated curve from CartPole tends to reach the highest performance much earlier than Reacher because CartPole is somewhat easier to train than Reacher. We next examine the count of augmented observa- tions generated per iteration in Fig. 4. Although this number is ﬂuctuating, it tends to reduce over 6/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013 /uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048 /uni0000001a/uni00000013 /uni00000019/uni00000013 /uni00000018/uni00000013 /uni00000017/uni00000013 /uni00000016/uni00000013 /uni00000015/uni00000013 /uni00000014/uni00000013 /uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000003/uni00000058/uni00000056/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000032/uni00000053/uni00000057/uni0000004c/uni00000050/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013/uni00000015/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000014/uni00000015/uni00000013/uni00000013/uni00000014/uni00000017/uni00000013/uni00000013 /uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048 /uni00000015/uni00000018 /uni00000018/uni00000013 /uni0000001a/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000018 /uni00000014/uni00000018/uni00000013 /uni00000014/uni0000001a/uni00000018 /uni00000015/uni00000013/uni00000013/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000059/uni00000033/uni00000048/uni00000051/uni00000040/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000003/uni00000058/uni00000056/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000032/uni00000053/uni00000057/uni0000004c/uni00000050/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000025/uni00000032/uni0000002c/uni0000002f Figure 5: The learning curves of the best found parameters by different approaches. The curves show that BO-L and BOIL reliably identify parameters leading to stable training. BOIL takes only half total time to ﬁnd this optimal curve. time. BOIL does not add more augmented observations at the later stage when we have gained sufﬁcient information and GP covariance conditioning falls below our threshold δ = 20. 4.2 Ablation study of curve compression To demonstrate the impact of our training curve compression, we compare BOIL to vanilla Bayesian optimization (BO) and with compression (BO-L) given the same number of iterations at Tmax. We show that using the curve compression leads to stable performance, as opposed to the existing tech- nique of averaging the last iterations. We plot the learning curves of the best hyperparameters identiﬁed by BO, BO-L and BOIL. Fig. 5 shows the learning progress over Tmax episodes for each of these. The curves are smoothed by averaging over 100 consecutive episodes for increased clarity. We ﬁrst note that all three algorithms eventually obtain similar performance at the end of learning. However, since BO-L and BOIL take into account the preceding learning steps, they achieve higher performance more quickly. Furthermore, they achieve this more reliably as evidenced by the smaller error bars (shaded regions). 4.3 Tuning deep reinforcement learning and CNN We now optimize hyperparameters for deep reinforcement learning algorithms; in fact, this applica- tion motivated the development of BOIL. The combinations of hyperparameters to be tuned, target DRL algorithm and environment can be found in the appendix. Comparisons by iterations and real-time. Fig. 6 illustrates the performance of different algo- rithms against the number of iterations as well as real-time (the plots for CIFAR10 are in the ap- pendix). The performance is the utility score of the best hyperparameters identiﬁed by the baselines. Across all three tasks, BOIL identiﬁes optimal hyperparameters using signiﬁcantly less computation time than other approaches. The plots show that other approaches such as BO and BO-L can identify well-performing hyperpa- rameters in fewer iterations than BOIL. However, they do so only considering costly, high-ﬁdelity evaluations resulting in signiﬁcantly higher evaluation times. In contrast to this behavior, BOIL ac- counts for the evaluation costs and chooses to initially evaluate low-ﬁdelity settings consuming less time. This allows fast assessments of a multitude of hyperparameters. The information gathered here is then used to inform later point acquisitions. Hereby, the inclusion of augmented observations is crucial in offering useful information readily available from the data. In addition, this augmenta- tion is essential to prevent from the GP kernel issue instead of adding the full curve of points into our GP model. Hyperband [23] exhibits similar behavior in that it uses low ﬁdelity (small t) evaluations to reduce a pool of randomly sampled conﬁgurations before evaluating at high ﬁdelity (large t). To deal with noisy evaluations and other effects, this process is repeated several times. This puts Hyperband at a disadvantage particularly in the noisy DRL tasks. Since early performance ﬂuctuates hugely, Hyperband can be misled in where to allocate evaluation effort. It is then incapable of revising 7/uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000015/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000016/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000016/uni0000001a/uni00000011/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000027/uni00000027/uni00000034/uni00000031/uni00000010/uni00000026/uni00000044/uni00000055/uni00000057/uni00000033/uni00000052/uni0000004f/uni00000048/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000003/uni00000044/uni00000051/uni00000047/uni00000003 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013/uni00000015/uni00000018/uni00000013/uni00000016/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000015/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000016/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000016/uni0000001a/uni00000011/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000027/uni00000027/uni00000034/uni00000031/uni00000010/uni00000026/uni00000044/uni00000055/uni00000057/uni00000033/uni00000052/uni0000004f/uni00000048/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000003/uni00000044/uni00000051/uni00000047/uni00000003 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000015/uni00000018 /uni00000016/uni00000013 /uni00000016/uni00000018 /uni00000017/uni00000013 /uni00000017/uni00000018 /uni00000018/uni00000013 /uni00000018/uni00000018 /uni00000019/uni00000013 /uni00000019/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000036/uni00000039/uni0000002b/uni00000031/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000015/uni00000018 /uni00000016/uni00000013 /uni00000016/uni00000018 /uni00000017/uni00000013 /uni00000017/uni00000018 /uni00000018/uni00000013 /uni00000018/uni00000018 /uni00000019/uni00000013 /uni00000019/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000036/uni00000039/uni0000002b/uni00000031/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f Figure 6: Comparison over BO evaluations (Left) and real-time (Right). Given the same time bud- get, CM-T/F-BO, Hyperband and BOIL can take more evaluations than vanilla BO, BO-L and Rand. BOIL outperforms other competitors in ﬁnding the optimal parameters in an iteration-efﬁcient man- ner. CM-T/F-BO does not augment the observations from the curve and requires more evaluations. The results of InvertedPendulum and CNN-CIFAR10 are in the appendix. these choices until an entirely new pool of hyperparameters is sampled and evaluated from scratch. In contrast to this, BOIL is more ﬂexible than Hyperband in that it can freely explore-exploit the whole joint space. The GP surrogate hereby allows BOIL to generalize across hyperparameters and propagate information through the joint space. 5 Conclusion and Future work Our framework complements the existing BO toolbox for hyperparameter tuning with iterative learn- ing. We present a way of leveraging our understanding that later stages of the training process are informed by progress made in earlier ones. This results in a more iteration-efﬁcient hyperparame- ter tuning algorithm that is applicable to a broad range of machine learning systems. We evaluate its performance on a set of diverse benchmarks. The results demonstrate that our model surpasses the performance of well-established alternatives while consuming signiﬁcantly fewer resources. Fi- nally, we note that our approach is not necessarily speciﬁc to machine learning algorithms, but more generally applies to any process exhibiting an iterative structure to be exploited. 86 Broader Impact Our work aims at making the optimization of processes operating in a step-wise fashion more efﬁ- cient. As demonstrated this makes BOIL particularly well-suited to supporting supervised learning models and RL systems. By increasing training efﬁcience of these models, we hope to contribute to their widespread deployment whilst reducing the computational and therefore environmental cost their implementation has. Deep (reinforcement) learning systems ﬁnd application in a wide range of settings that directly contribute to real world decisions, e.g., natural language processing, visual task, autonomous driving and many more. As machine learning models building on our contributions are being deployed in the real world, we encourage practicioners to put in place necessary supervision and override mechanisms as precautions against potential failure. In a more general context, our algorithm may be seen as a step towards the construction of an automated pipeline for the training and deployment of machine learning models. A potential danger is that humans become further and further removed from the modelling process, making it harder to spot (potentially critical) failures. We do not see this as an argument against the construction of such a pipeline in principle, but instead encourage practicioners to reﬂect on potential biases indirectly encoded in the choice of data sets and models, they are feeding into said automated processes. The growing opacity of machine learning models is a concern of its own and which automated training procedures will only contribute to. Opposing this is a rapidly growing corpus of work addressing the interpretability of trained machine learning models and their decision making. These can and should be used to rigorously analyse ﬁnal training outcomes. Only then can we ensure that machine learning algorithm do indeed become a beneﬁcial source of information guiding real world policy making as opposed to opaque, unquestioned entities. While our main interest lies in the hyperparameter optimization of machine learning models, it should be noted that any iterative process depending on a set of parameters can make use of our con- tributions. Possible settings could, for instance, include the optimization of manufacturing pipelines in which factory setting are adjusted to increase productivity. 7 Acknowledgements S. Schulze is supported by an I-CASE studentship funded by the EPSRC and Dyson. References [1] Christopher M Bishop. Pattern recognition and machine learning. springer New York, 2006. [2] Eric Brochu, Vlad M Cora, and Nando De Freitas. A tutorial on Bayesian optimization of ex- pensive cost functions, with application to active user modeling and hierarchical reinforcement learning. arXiv preprint arXiv:1012.2599, 2010. [3] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016. [4] Yutian Chen, Aja Huang, Ziyu Wang, Ioannis Antonoglou, Julian Schrittwieser, David Silver, and Nando de Freitas. Bayesian optimization in AlphaGo. arXiv preprint arXiv:1812.06855, 2018. [5] Zhongxiang Dai, Haibin Yu, Bryan Kian Hsiang Low, and Patrick Jaillet. Bayesian optimiza- tion meets Bayesian optimal stopping. In International Conference on Machine Learning , pages 1496–1506, 2019. [6] Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselines. GitHub, GitHub repository, 2017. [7] Tobias Domhan, Jost Tobias Springenberg, and Frank Hutter. Speeding up automatic hy- perparameter optimization of deep neural networks by extrapolation of learning curves. In Twenty-Fourth International Joint Conference on Artiﬁcial Intelligence, 2015. 9[8] Stefan Falkner, Aaron Klein, and Frank Hutter. Bohb: Robust and efﬁcient hyperparameter optimization at scale. In International Conference on Machine Learning , pages 1436–1445, 2018. [9] Peter I Frazier. A tutorial on Bayesian optimization. arXiv preprint arXiv:1807.02811, 2018. [10] Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep Bayesian active learning with image data. In Proceedings of the 34th International Conference on Machine Learning, pages 1183– 1192, 2017. [11] Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018. [12] Philipp Hennig and Christian J Schuler. Entropy search for information-efﬁcient global opti- mization. Journal of Machine Learning Research, 13:1809–1837, 2012. [13] José Miguel Hernández-Lobato, Matthew W Hoffman, and Zoubin Ghahramani. Predictive entropy search for efﬁcient global optimization of black-box functions. In Advances in Neural Information Processing Systems, pages 918–926, 2014. [14] Donald R Jones, Matthias Schonlau, and William J Welch. Efﬁcient global optimization of expensive black-box functions. Journal of Global optimization, 13(4):455–492, 1998. [15] M. I. Jordan and T. M. Mitchell. Machine learning: Trends, perspectives, and prospects. Science, 349(6245):255–260, 2015. [16] Kirthevasan Kandasamy, Gautam Dasarathy, Jeff Schneider, and Barnabás Póczos. Multi- ﬁdelity Bayesian optimisation with continuous approximations. In Proceedings of the 34th International Conference on Machine Learning, pages 1799–1808, 2017. [17] Aaron Klein, Stefan Falkner, Simon Bartels, Philipp Hennig, and Frank Hutter. Fast Bayesian optimization of machine learning hyperparameters on large datasets. In Artiﬁcial Intelligence and Statistics, pages 528–536, 2017. [18] Aaron Klein, Stefan Falkner, Jost Tobias Springenberg, and Frank Hutter. Learning curve pre- diction with Bayesian neural networks. International Conference on Learning Representations (ICLR), 2017. [19] Andreas Krause and Cheng S Ong. Contextual Gaussian process bandit optimization. In Advances in Neural Information Processing Systems, pages 2447–2455, 2011. [20] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pages 1097–1105, 2012. [21] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. [22] Benjamin Letham, Brian Karrer, Guilherme Ottoni, Eytan Bakshy, et al. Constrained Bayesian optimization with noisy experiments. Bayesian Analysis, 14(2):495–519, 2019. [23] Lisha Li and Kevin Jamieson. Hyperband: A novel bandit-based approach to hyperparameter optimization. Journal of Machine Learning Research, 18:1–52, 2018. [24] Mark McLeod, Stephen Roberts, and Michael A Osborne. Optimization, fast and slow: Op- timally switching between local and Bayesian optimization. In International Conference on Machine Learning, pages 3440–3449, 2018. [25] V olodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforce- ment learning. In International conference on machine learning, pages 1928–1937, 2016. [26] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. NIPS Deep Learning Workshop, 2013. [27] Vu Nguyen, Sunil Gupta, Santu Rana, Cheng Li, and Svetha Venkatesh. Regret for expected improvement over the best-observed value and stopping condition. In Proceedings of The 9th Asian Conference on Machine Learning (ACML), pages 279–294, 2017. [28] Vu Nguyen and Michael A Osborne. Knowing the what but not the where in Bayesian opti- mization. In International Conference on Machine Learning, pages 7317–7326, 2020. 10[29] Michael Osborne, Roman Garnett, Zoubin Ghahramani, David K Duvenaud, Stephen J Roberts, and Carl E Rasmussen. Active learning of model evidence using Bayesian quadrature. In Advances in Neural Information Processing Systems, pages 46–54, 2012. [30] Jack Parker-Holder, Vu Nguyen, and Stephen Roberts. Provably efﬁcient online hyperparame- ter optimization with population-based bandits. In Advances in Neural Information Processing Systems, 2020. [31] Carl Edward Rasmussen. Gaussian processes for machine learning. 2006. [32] Binxin Ru, Mark McLeod, Diego Granziol, and Michael A Osborne. Fast information-theoretic Bayesian optimisation. In International Conference on Machine Learning, pages 4381–4389, 2018. [33] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. International Conference on Learning Representations, 2016. [34] Rajat Sen, Kirthevasan Kandasamy, and Sanjay Shakkottai. Multi-ﬁdelity black-box opti- mization with hierarchical partitions. In International conference on machine learning, pages 4538–4547, 2018. [35] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando de Freitas. Taking the human out of the loop: A review of Bayesian optimization. Proceedings of the IEEE , 104(1):148–175, 2016. [36] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanc- tot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484, 2016. [37] Leslie N Smith. A disciplined approach to neural network hyper-parameters: Part 1–learning rate, batch size, momentum, and weight decay. arXiv preprint arXiv:1803.09820, 2018. [38] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical Bayesian optimization of ma- chine learning algorithms. In Advances in Neural Information Processing Systems , pages 2951–2959, 2012. [39] Jialin Song, Yuxin Chen, and Yisong Yue. A general framework for multi-ﬁdelity Bayesian optimization with Gaussian processes. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pages 3158–3167, 2019. [40] Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In Proceedings of the 27th International Conference on Machine Learning, pages 1015–1022, 2010. [41] Kevin Swersky, Jasper Snoek, and Ryan P Adams. Multi-task Bayesian optimization. In Advances in Neural Information Processing Systems, pages 2004–2012, 2013. [42] Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Freeze-thaw Bayesian optimization. arXiv preprint arXiv:1406.3896, 2014. [43] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026–5033. IEEE, 2012. [44] Zi Wang and Stefanie Jegelka. Max-value entropy search for efﬁcient Bayesian optimization. In International Conference on Machine Learning, pages 3627–3635, 2017. [45] Ziyu Wang and Nando de Freitas. Theoretical analysis of Bayesian optimisation with unknown Gaussian process hyper-parameters. arXiv preprint arXiv:1406.7758, 2014. [46] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling network architectures for deep reinforcement learning. In International Conference on Machine Learning, pages 1995–2003, 2016. [47] Jian Wu and Peter Frazier. The parallel knowledge gradient method for batch Bayesian opti- mization. In Advances In Neural Information Processing Systems, pages 3126–3134, 2016. [48] Jian Wu, Saul Toscano-Palmerin, Peter I Frazier, and Andrew Gordon Wilson. Practical multi- ﬁdelity Bayesian optimization for hyperparameter tuning. In 35th Conference on Uncertainty in Artiﬁcial Intelligence, 2019. 11The following sections are intended to give the reader further insights into our design choices and a deeper understanding of the algorithms properties. First, we give a brief overview of Bayesian optimization with Gaussian processes. We then illustrate our models behavior on a two dimensional problem. Last, we give further details of our experiments for reproducibility purposes. A Bayesian Optimization Preliminaries Bayesian optimization is a sequential approach to global optimization of black-box functions with- out making use of derivatives. It uses two components: a learned surrogate model of the objective function and an acquisition function derived from the surrogate for selecting new points to inform the surrogate with. In-depth discussions beyond our brief overview can be found in recent surveys [2, 9, 35]. Notation. We summarize all of the notations used in our model in Table 1 for ease of reading. A.1 Gaussian processes We present the GP surrogate model for the black-box function f [31]. A GP deﬁnes a probability distribution over functions f under the assumption that any subset of points {(xi, f (xi)}is normally distributed. Formally, this is denoted as: f (x) ∼GP ( m(x),k ( x,x′)) , where m(x) and k (x,x′) are the mean and covariance functions, given by m(x) =E[ f (x)] and k(x,x′) =E [ ( f (x)−m(x))( f (x′)−m(x′))T ] . Typically, the mean of the GP is assumed to be zero everywhere. The kernel k(x,x′) can be thought of as a similarity measure relating f (x) and f (x′). Numerous kernels encoding different prior be- liefs about f (x) have been proposed. A popular choice is given by the square exponential kernel k(x,x′) =σ2 f exp [ −(x −x′)2/2σ2 l ] . The length- and output-scales σ2 l and σ2 f regulate the maximal covariance between two points and can be estimated using maximum marginal likelihood. The SE kernel encodes the belief that nearby points are highly correlated as it is maximized at k(x,x) =σ2 f and decays the further x and x′are separated. For predicting f∗= f (x∗) at a new data point x∗, assuming a zero mean m(x) =0, we have: [ f f∗ ] ∼N ( 0, [ K kT ∗ k∗ k∗∗ ]) (7) where k∗∗= k (x∗,x∗), k∗= [k (x∗,xi)]∀i≤N and K = [k (xi,xj)]∀i, j≤N . The conditional probability of p( f∗|f ) follows a univariate Gaussian distribution as p( f∗|f ) ∼N ( µ (x∗),σ2 (x∗) ) . Its mean and variance are given by: µ (x∗) =k∗K−1y σ2 (x∗) =k∗∗−k∗K−1kT ∗. As GPs give full uncertainty information with any prediction, they provide a ﬂexible nonparametric prior for Bayesian optimization. We refer the interested readers to [31] for further details on GPs. A.2 Acquisition function Bayesian optimization is typically applied in settings in which the objective function is expensive to evaluate. To minimize interactions with that objective, an acquisition function is deﬁned to reason about the selection of the next evaluation point xt+1 = argmaxx∈X αt (x). The acquisition func- tion is constructed from the predictive mean and variance of the surrogate to be easy to evaluate and represents the trade-off between exploration (of points with high predictive uncertainty) and exploitation (of points with high predictive mean). Thus, by design the acquisition function can be maximized with standard global optimization toolboxes. Among the many acquisition functions [12, 13, 14, 32, 40, 44] available in the literature, the expected improvement [14, 27, 45] is one of the most popular. 12Table 1: Notation List Parameter Domain Meaning d integer, N dimension, no. of hyperparameters to be optimized x vector,Rd input hyperparameter N integer, N maximum number of BO iterations Tmin, Tmax integer, N the min/max no of iterations for training a ML algorithm t ∈[Tmin,...Tmax] index of training steps M integer, N the maximum number of augmentation. We set M = 15. δ scalar, R threshold for rejecting augmentation when ln of cond(K) > δ m ∈{1,...M} index of augmenting variables n ∈{1,..., N} index of BO iterations z = [x,t] vector, Rd+1 concatenation of the parameter x and iteration t cn,m scalar, R training cost (sec) yn scalar, R transformed score at the BO iteration n yn,m scalar, R transformed score at the BO iteration n, training step m α(x,t) function acquisition function for performance µc(x,t) function estimation of the cost by LR given x and t r(. |x,t) function a raw learning curve, r(x,t) = [r(1 |x,t),...r(t′|x,t),r(t |x,t)] f (x,t) function a black-box function which is compressed from the above f () l (. |m0,g0) function Logistic curve l(u |m0,g0) = 1 1+exp(−g0[u−m0]) g0, g∗ 0 scalar, R a growth parameter deﬁning a slope, g∗ 0 = argmaxg0 L m0, m∗ 0 scalar, R a middle point parameter, m∗ 0 = argmaxm0 L L scalar, R Gaussian process log marginal likelihood A.3 GP kernels and treatment of GP hyperparameters We present the GP kernels and treatment of GP hyperparameters for the black-box function f . Although the raw learning curve in DRL is noisy, the transformed version using our proposed curve compression is smooth. Therefore, we use two squared exponential kernels for input hyperparameter and training iteration, respectively. That iskx(x,x′) =exp ( −||x−x′||2 2σ2x ) and kt (t,t′) =exp ( −||t−t′||2 2σ2t ) where the observation x and t are normalized to [0,1]d and the outcome y is standardized y ∼ N (0,1) for robustness. As a result, our product kernel becomes k ( [x,t],[x′,t′] ) = k(x,x′)×k(t,t′) =exp ( −||x −x′||2 2σ2x −||t −t′||2 2σ2t ) . The length-scales σx and σt are learnable parameters indicating the variability of the function with regards to the hyperparameter input x and number of training iterations t. Estimating appropriate values for them is critical as this represents the GPs prior regarding the sensitivity of performance w.r.t. changes in the number of training iterations and hyperparameters. For extremely large σt we expect the objective function to change very little for different numbers of training iterations. For small σt by contrast we expect drastic changes even for small differences. We estimate these GP hyperparameters (including the length-scalesσx, σt and the output noise varianceσy) by maximizing their log marginal likelihood [31]. We optimize Eq. (5) with a gradient-based optimizer, providing the analytical gradient to the algo- rithm. We start the optimization from the previous hyperparameter values θprev. If the optimization fails due to numerical issues, we keep the previous value of the hyperparameters. We reﬁt the hy- perparameters every 3×d function evaluations where d is the dimension. B Algorithm Illustration and Further Experiments Fig. 7 and Fig. 8 illustrate the behavior of our proposed algorithm BOIL on the example of opti- mizing the discount factor γ of Dueling DQN [46] on the CartPole problem. The two settings differ in the inclusion augmented observations into BOIL in Fig. 7 and CM-T/F-BO (or BOIL without augmented observations) in Fig. 8. 130.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.2 0.8 0.4 0.0 0.4 0.8 1.2 1.6 2.0 2.4 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.6 0.3 0.6 0.9 1.2 1.5 1.8 2.1 2.4 2.7 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 2.5 0.00 0.06 0.12 0.18 0.24 0.30 0.36 0.42 0.48 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 1.36 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 1.6 2.0 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 1.36 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.4 1.6 0.8 0.0 0.8 1.6 2.4 3.2 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 1.36 0.0 0.6 1.2 1.8 2.4 3.0 3.6 4.2 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.6 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.6 Figure 7: Illustration of BOIL on a 2-dimensional optimization task of DDQN on CartPole. The augmented observations ﬁll the joint hyperparameter-iteration space quickly to inform our surrogate. Our decision balances utility α against cost τ for iteration-efﬁciency. Especially in situations of multiple locations sharing the same utility value, our algorithm prefers to select the cheapest option. Table 2: Dueling DQN algorithm on CartPole problem. Variables Min Max Best Found x∗ γ discount factor 0 .8 1 0 .95586 learning rate model 1 e−6 0.01 0 .00589 #Episodes 300 800 - 140.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.0 1.6 1.2 0.8 0.4 0.0 0.4 0.8 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 1.80 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.0 0.3 0.6 0.9 1.2 1.5 1.8 2.1 2.4 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 1.80 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 1.6 0.00 0.08 0.16 0.24 0.32 0.40 0.48 0.56 0.64 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.30 0.45 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.0 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 Figure 8: Illustration of the Continuous Multi task/ﬁdelity BO (CM-T/F-BO) -- this is the case of BOIL without using augmented observations (same setting as Fig. 7). This version leads to less efﬁcient optimization as the additional iteration dimension requires more evaluation than optimizing the hyperparameters on their own. 15Table 3: A2C algorithm on Reacher (left) and InvertedPendulum (right). Variables Min Max Best Found x∗ γ discount factor 0 .8 1 0 .8 learning rate actor 1 e−6 0.01 0 .00071 learning rate critic 1 e−6 0.01 0 .00042 #Episodes 200 500 - Min Max Best Found x∗ 0.8 1 0 .95586 1e−6 0.01 0 .00589 1e−6 0.01 0 .00037 700 1500 - Table 4: Convolutional Neural Network. Variables Min Max Best Found x∗ ﬁlter size 1 8 5 pool size 1 5 5 batch size 16 1000 8 learning rate 1 e−6 0.01 0 .000484 momentum 0 .8 0 .999 0 .82852 decay 0 .9 0 .999 0 .9746 number of epoch 30 150 - In both cases, we plot the GP predictive mean in Eq. (1), GP predictive variance in Eq. (2), the acquisition function in Eq. (3), the predicted function and the ﬁnal decision function in Eq. (8). These equations are deﬁned in the main manuscript. As shown in the respective ﬁgures the ﬁnal decision function balances between utility and cost of any pair (γ,t) to achieve iteration efﬁciency. Especially in situations where multiple locations share the same utility value, our decision will prefer to select the cheapest option. Using the augmented observations in Fig. 7, our joint space is ﬁlled quicker with points and the uncertainty (GP variance) across it reduces faster than in Fig. 8 – the case of vanilla CM-T/F-BO without augmenting obser- vations. A second advantage of having augmented observations is that the algorithm is discouraged to select the same hyperparameter setting at lower ﬁdelity than a previous evaluation. We do not add the full curve as it can be redundant while causing the conditioning problem of the GP covariance matrix. B.1 Experiment settings We summarize the hyperparameter search ranges for A2C on Reacher and InvertedPendulum in Table 3, CNN on SHVN in Table 4 and DDQN on CartPole in Table 2. Additionally, we present the best found parameter x∗for these problems. Further details of the DRL agents are listed in Table 5. B.2 Learning Logistic Function We ﬁrst present the Logistic curve l(u |x,t) = 1 1+exp(−g0[u−m0]) using different choices of g0 and m0 in Fig. 10. We then learn from the data to get the optimal choices g∗ 0 and m∗ 0 presented in Fig. 11. Table 5: Further speciﬁcation for DRL agents Hyperparameter Value A2C Critic-network architecture [32,32] Actor-network architecture [32,32] Entropy coefﬁcient 0 .01 Dueling DQN Q-network architecture [50,50] ε-greedy (start, ﬁnal, number of steps) (1.0,0.05,10000) Buffer size 10000 Batch size 64 PER-α [33] 1 .0 PER-β (start, ﬁnal, number of steps) (1.0,0.6,1000) 160 100 200 300 400 500 Episodes 70 60 50 40 30 20 Average Reward Preference Curve as Sigmoid Best Found Reward Curve Sigmoid Curve 0 100 200 300 400 500 Episodes 90 80 70 60 50 40 30 20 10 Reward Preference Curve as Sigmoid Best Found Reward Curve Sigmoid Curve 0 100 200 300 400 500 Episodes 60 50 40 30 20 10 Average Reward Preference Curve as Log Best Found Reward Curve Log Curve 0 100 200 300 400 500 Episodes 100 80 60 40 20 0 Reward Preference Curve as Log Best Found Reward Curve Log Curve 0 100 200 300 400 500 Episodes 60 50 40 30 20 Average Reward Preference Curve as Average Best Found Reward Curve Average Curve 0 100 200 300 400 500 Episodes 80 60 40 20 0 Reward Preference Curve as Average Best Found Reward Curve Average Curve Figure 9: To highlight the robustness, we examine the results using different preference functions such as Sigmoid curve, Log curve, and Average curve on Reacher experiments. The results include the best found reward curve with different preference choices that show the robustness of our model. Left column: the best found curve using averaged reward over 100 consecutive episodes. Right column: the best found curve using the original reward. 17/uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001a m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001a m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000016 Figure 10: Examples of Logistic function l(u) = 1 1+exp(−g0[u−m0]) with different values of middle parameter m0 and growth parameter g0. B.3 Robustness over Different Preference Functions We next study the learning effects with respect to different choices of the preference functions. We pick three preference functions including the Sigmoid, Log and Average to compute the utility score for each learning curve. Then, we report the best found reward curve under such choices. The experiments are tested using A2C on Reacher-v2. The results presented in Fig. 9 demonstrate the robustness of our model with the preference functions. B.4 Applying Freeze-Thaw BO in the settings considered While both the exponential decay in Freeze-Thaw BO [42] and our compression function encode preferences regarding training development, there is an important distinction between the two ap- proaches. Freeze-thaw BO utilises the exponential decay property to terminate the training curve, while BOIL only uses the sigmoid curve to guide the search. We refer to Fig. 13 for further illustra- tion of why Freeze-thaw BO struggles in DRL settings. B.5 Ablation Study using Freeze-Thraw Kernel for Time In the joint modeling framework of hyperparameter and time (iteration), we can replace the kernel either k(x,x) or k(t,t) with different choices. We, therefore, set up a new baseline of using the time- kernel k(t,t′) in Freeze-Thaw approach [42] which encodes the monotonously exponential decay from the curve. Particularly, we use the kernel deﬁned as k(t,t′) = βα (t +t′+β)α for parameters α,β > 0 which are optimized in the GP models. 186  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . | m * 0 ,g * 0 ) for CartPole m * 0 =-3.266   g * 0 =3.0 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l(m * 0 ,g * 0 ) for CNN_SHVN m * 0 =2.245   g * 0 =2.092 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l(m * 0 ,g * 0 ) for InvPendulum m * 0 =1.649   g * 0 =1.833 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . | m * 0 ,g * 0 ) for Cifar10 m * 0 =-4.0   g * 0 =1.476 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . | m * 0 ,g * 0 ) for Reacher m * 0 =2.779   g * 0 =1.973 Figure 11: We learn the suitable transformation curve directly from the data. We parameterized the Logistic curve as l (m0,g0) = 1 1+exp(−g0[1−m0]) then estimate g0 and m0. The estimated function l(m∗ 0,g∗ 0) is then used to compress our curve. The above plots are the estimated l() at different environments and datasets. /uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni0000001b/uni00000013 /uni0000001c/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000015/uni00000013/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni0000001b/uni00000013 /uni0000001c/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000015/uni00000013/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000015/uni00000013/uni00000011/uni00000013 /uni00000015/uni00000015/uni00000011/uni00000018 /uni00000015/uni00000018/uni00000011/uni00000013 /uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000016/uni00000013/uni00000011/uni00000013 /uni00000016/uni00000015/uni00000011/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000026/uni0000004c/uni00000049/uni00000044/uni00000055/uni00000014/uni00000013/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000015/uni00000013/uni00000011/uni00000013 /uni00000015/uni00000015/uni00000011/uni00000018 /uni00000015/uni00000018/uni00000011/uni00000013 /uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000016/uni00000013/uni00000011/uni00000013 /uni00000016/uni00000015/uni00000011/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000026/uni0000004c/uni00000049/uni00000044/uni00000055/uni00000014/uni00000013/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f Figure 12: Tuning hyperparameters of a DRL on InvertedPendulum and a CNN model on CIFAR10. 190 250 500 750 1000 1250 1500 Epoch 0 20 40 60 80 100 120Reward Reward Curve Freeze-thaw 0 250 500 750 1000 1250 1500 Epoch 0 50 100 150 0 250 500 750 1000 1250 1500 Epoch 0 50 100 150 0 250 500 750 1000 1250 1500 Epoch 0 50 100 150 Reward Curves Examples using A2C on Inverted Pendulum Figure 13: Illustration of Freeze-thaw BO in DRL. Freeze-thaw BO will terminate training processes when training performance (in blue) signiﬁcantly drops (i.e. at the red locations) as the exponential decay model will predict low ﬁnal performance. In most RL enviroments noisy training curves are unavoidable. Thus, Freeze-thaw BO will dismiss all curves including good setting, never completing a single training run before the ﬁnal epoch. /uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni0000001c/uni00000013 /uni0000001c/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000018 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000014/uni00000018 /uni00000014/uni00000015/uni00000013 /uni00000014/uni00000015/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000010/uni00000030/uni00000012/uni00000037/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000013 /uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013/uni00000015/uni00000018/uni00000013/uni00000016/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni0000001c/uni00000013 /uni0000001c/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000018 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000014/uni00000018 /uni00000014/uni00000015/uni00000013 /uni00000014/uni00000015/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a Figure 14: Comparison using freezethaw kernel for time component. We present the result in Fig. 14 that CM-T/F-BO is still less competitive to BOIL using this speciﬁc time kernel. The results again validate the robustness our approach cross different choices of kernel. B.6 Additional Experiments for Tuning DRL and CNN We present the additional experiments for tuning a DRL model using InvertedPendulum environ- ment and a CNN model using a subset of CIFAR10 in Fig. 12. Again, we show that the proposed model clearly gain advantages against the baselines in tuning hyperparameters for model with itera- tive learning information available. B.7 Examples of Deep Reinforcement Learning Training Curves Finally, we present examples of training curves produced by the deep reinforcement learning al- gorithm A2C in Fig. 15. These ﬂuctuate widely and it may not be trivial to deﬁne good stopping criteria as done for other applications in previous work [42]. 200 200 400 80 70 60 50 40 0 200 400 110 100 90 80 70 60 50 0 200 400 110 100 90 80 70 60 50 0 200 400 70 60 50 40 30 0 200 400 70 60 50 40 30 20 10 0 200 400 50 40 30 20 0 200 400 90 80 70 60 50 0 200 400 70 60 50 40 30 20 10 0 200 400 85 80 75 70 0 200 400 60 50 40 30 20 0 200 400 100 90 80 70 60 0 200 400 100 90 80 70 0 200 400 80 60 40 20 0 200 400 100 90 80 70 60 0 200 400 100 90 80 70 0 200 400 80 70 60 50 40 30 20 0 200 400 70 60 50 40 30 20 0 200 400 70 60 50 40 30 20 0 200 400 100 90 80 70 60 50 0 200 400 70 60 50 40 30 20 0 200 400 100 90 80 70 60 50 0 200 400 100 90 80 70 60 0 200 400 60 50 40 30 20 0 200 400 80 60 40 20 0 200 400 85 80 75 70 65 60 0 200 400 100 95 90 85 80 0 200 400 80 60 40 20 0 200 400 100 90 80 70 60 50 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 Figure 15: Examples of reward curves using A2C on Reacher-v2 (rows 1 −3) and on InvertedPendulum-v2 (rows 4 −6). Y-axis is the reward averaged over 100 consecutive episodes. X-axis is the episode. The noisy performance illustrated is typical of DRL settings and complicates the design of early stopping criteria. Due to the property of DRL, it is not trivial to decide when to stop the training curve. In addition, it will be misleading if we only take average over the last 100 iterations. 21",
      "meta_data": {
        "arxiv_id": "1909.09593v5",
        "authors": [
          "Vu Nguyen",
          "Sebastian Schulze",
          "Michael A Osborne"
        ],
        "published_date": "2019-09-20T16:14:34Z",
        "pdf_url": "https://arxiv.org/pdf/1909.09593v5.pdf",
        "github_url": "https://github.com/ntienvu/BOIL"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Bayesian Optimization for Iterative Learning (BOIL), an approach for efficient hyperparameter tuning that exploits the iterative structure of deep (reinforcement) learning algorithms. It addresses the problem that traditional Bayesian Optimization (BO) often ignores intermediate training information and relies solely on final performance, which is expensive and noisy. BOIL proposes to learn an evaluation function that compresses the entire learning progress into a single numeric score, considering both training success and stability. The framework balances the benefit of assessing hyperparameters over additional training steps against their computational cost and increases efficiency through a selective data augmentation technique. BOIL demonstrates superior performance in identifying optimal hyperparameters in minimal wall-clock time for deep reinforcement learning agents and convolutional neural networks.",
        "methodology": "BOIL is based on Bayesian Optimization (BO), modeling the cost-sensitive objective function over input hyperparameters (x) and training iterations (t) using a Gaussian Process (GP) with a product covariance kernel. Training time cost c(x,t) is approximated by a linear regressor. The acquisition function maximizes expected improvement while balancing utility against cost. A key innovation is 'training curve compression,' where the entire learning curve is transformed into a single numeric score using a learnable Sigmoid preference function (Logistic function). The parameters of this Sigmoid function are optimized by maximizing the GP's log marginal likelihood. To improve sample efficiency and prevent ill-conditioning of the GP covariance matrix, a selective data augmentation method is employed. This method actively samples a subset of intermediate training points based on maximum GP predictive uncertainty, dynamically adjusting the number of augmented points to ensure the covariance matrix remains well-conditioned.",
        "experimental_setup": "Experiments were conducted on an NVIDIA 1080 GTX GPU, with results averaged over 20 independent runs. Final performance was estimated by evaluating chosen hyperparameters at the maximum number of iterations. Square-exponential kernels were used for the Gaussian Process. The augmentation strategy used a maximum of 15 augmented points with a threshold of 20 for the natural log of the GP condition number. The method was evaluated by tuning hyperparameters for Dueling DQN on the CartPole-v0 environment, Advantage Actor Critic (A2C) on InvertedPendulum-v2 and Reacher-v2 environments (all from OpenAI gym and Mujoco), and a Convolutional Neural Network on SVHN and CIFAR10 datasets. Baselines for comparison included Hyperband and Continuous Multi-Task/Fidelity BO (CM-T/F-BO), along with ablation studies involving vanilla BO and BO with curve compression (BO-L).",
        "limitations": "The paper highlights that many existing hyperparameter optimization methods struggle with the unpredictable fluctuations and noisiness of Deep Reinforcement Learning (DRL) reward curves, making techniques like early stopping (e.g., Freeze-thaw BO's exponential decay assumption) often inapplicable. A naive approach of augmenting training data by adding the full learning curve can lead to redundancy and serious ill-conditioning issues for the Gaussian Process covariance matrix, necessitating BOIL's selective augmentation strategy. Furthermore, methods like Hyperband can be misled by early performance fluctuations in noisy DRL tasks. From a broader impact perspective, the authors acknowledge the general dangers of increased automation in ML, such as humans being further removed from the modeling process, potentially making critical failures harder to spot, and contributing to the growing opacity of ML models.",
        "future_research_directions": "The authors suggest that the BOIL approach is not limited to machine learning algorithms but is generally applicable to any process exhibiting an iterative structure that can be exploited. This opens avenues for applying BOIL to diverse domains, such as optimizing manufacturing pipelines by adjusting factory settings to increase productivity. The work also contributes to the broader goal of constructing automated pipelines for the training and deployment of machine learning models, implying further research into making such pipelines robust and interpretable.",
        "experimental_code": "import numpy as np\nfrom bayes_opt.acquisition_functions import AcquisitionFunction, unique_rows\nfrom bayes_opt import ProductGaussianProcess\nfrom bayes_opt.acquisition_maximization import acq_max_with_name,acq_min_scipy_kwargs\nimport time\nfrom sklearn import linear_model\nimport copy\nfrom bayes_opt.curve_compression import transform_logistic\nfrom sklearn.preprocessing import MinMaxScaler\n\n\n\nclass BOIL(object):\n\n    def __init__(self, func, SearchSpace,acq_name=\"ei_mu_max\",verbose=1):\n\n        self.method='boil'\n        self.verbose=verbose\n        if isinstance(SearchSpace,dict):\n            self.keys = list(SearchSpace.keys())\n            \n            self.SearchSpace = []\n            for key in list(SearchSpace.keys()):\n                self.SearchSpace.append(SearchSpace[key])\n            self.SearchSpace = np.asarray(self.SearchSpace)\n        else:\n            self.SearchSpace=np.asarray(SearchSpace)\n            \n            \n        self.dim = len(SearchSpace)\n\n        scaler = MinMaxScaler()\n        scaler.fit(self.SearchSpace[:-1,:].T)\n        \n        scalerT = MinMaxScaler()\n        SearchSpace_T=np.atleast_2d(self.SearchSpace[-1,:]).T\n        scalerT.fit(SearchSpace_T)\n\n        self.Xscaler=scaler\n        self.Tscaler=scalerT\n\n        self.scaleSearchSpace=np.array([np.zeros(self.dim), np.ones(self.dim)]).T\n                \n        self.f = func\n    \n        self.X_ori= None\n        self.X = None\n        self.Y = None\n        self.Y_ori = None\n        self.T=None\n        self.T_original=None\n        self.Y_cost_original=None\n        \n        self.time_opt=0\n         \n        self.max_min_gap=self.SearchSpace[:,1]-self.SearchSpace[:,0]\n\n        self.acq_name = acq_name\n        self.logmarginal=0\n\n        self.gp=ProductGaussianProcess(self.scaleSearchSpace,verbose=verbose)\n\n        self.Y_curves=[]\n        self.Y_cost_original=None\n        self.time_opt=0\n        \n        self.acq_func = None\n   \n        self.logmarginal=0\n        \n        self.markVirtualObs=[]\n        \n        self.countVirtual=[]\n\n        self.linear_regression = linear_model.LinearRegression()\n\n        self.condition_number=[]\n        \n        self.max_n_augmentation=10\n        self.threshold_cond=15\n        \n    def init(self, n_init_points=3, seed=1):\n        np.random.seed(seed)\n\n        SearchSpace=np.copy(self.SearchSpace)\n        SearchSpace[-1,0]=SearchSpace[-1,1]\n\n        l = [np.random.uniform(x[0], x[1]) for _ in range(n_init_points) for x in SearchSpace] \n\n        temp=np.asarray(l)\n        temp=temp.T\n        init_X=list(temp.reshape((n_init_points,-1)))\n        \n        self.X_original = np.asarray(init_X)\n        self.T_original=self.X_original[:,-1]\n        self.T_original=np.reshape(self.T_original,(n_init_points,-1))\n        \n        self.X_original=self.X_original[:,:-1]\n        self.X_original=np.reshape(self.X_original,(n_init_points,-1))\n\n        y_init_curves, y_init_cost=self.f(init_X)\n\n        y_init_cost=np.atleast_2d(np.asarray(y_init_cost))\n\n        self.Y_curves+=y_init_curves\n\n        y_init=transform_logistic(y_init_curves,self.gp.logistic_hyper['midpoint'],\\\n                                  self.gp.logistic_hyper['growth'], self.SearchSpace[-1,1])\n        y_init=np.reshape(y_init,(n_init_points,1))\n        \n        self.Y_original = np.asarray(y_init)      \n        self.Y_cost_original=np.reshape(y_init_cost,(-1,1))\n\n        self.X = self.Xscaler.transform(np.asarray(init_X)[:,:-1])\n        self.X=np.reshape(self.X,(n_init_points,-1))\n\n        self.T = self.Tscaler.transform(self.T_original)\n\n        self.markVirtualObs+=[0]*n_init_points\n\n        for ii in range(n_init_points):\n            self.generating_virtual_observations(self.X[ii,:],\\\n                         self.T[ii],[y_init_curves[ii]],y_init_cost[0][ii],IsRandom=False)\n\n        self.Y_cost=(self.Y_cost_original-np.min(self.Y_cost_original))/(np.max(self.Y_cost_original)-np.min(self.Y_cost_original))\n\n        if np.std(self.Y_original)==0:\n            self.Y=(self.Y_original-np.mean(self.Y_original))\n        else:\n            self.Y=(self.Y_original-np.mean(self.Y_original))/np.std(self.Y_original)\n\n       \n    def utility_cost_evaluation(self,x,acq_func,isDebug=False):\n        \n        def utility_cost_evaluation_single(x,acq_func,isDebug=False):\n            utility=acq_func.acq_kind(x,gp=self.gp)\n            \n            try:\n                mean_cost=self.linear_regression.predict(np.reshape(x,(1,-1)))\n                \n            except:\n                print(x)\n                print(\"bug\")\n    \n            mean_cost=max(0,mean_cost)+0.1\n            \n            if 'ei' in acq_func.acq_name:\n                acquisition_function_value= np.log(utility)-np.log(mean_cost)\n            else:\n                acquisition_function_value= np.log(1+np.exp(utility))/np.log(1+np.exp(mean_cost))\n\n            if isDebug==True:\n                print(\"acq_func at the selected point \\t utility:\",np.round(utility,decimals=4),\"\\t cost:\",mean_cost)\n                if utility==0:\n                    print(\"utility =0===============================================================================\")\n       \n            return acquisition_function_value*(-1)\n        \n        \n        if len(x)==self.dim:\n            temp=utility_cost_evaluation_single(x,acq_func,isDebug=isDebug)\n            if isDebug==True:\n                return temp\n            else:\n                utility=np.mean(temp)\n        \n        else:\n            utility=[0]*len(x)\n            for idx,val in enumerate(x):\n                temp=utility_cost_evaluation_single(x=val,acq_func=acq_func,isDebug=isDebug)\n                                                     \n                utility[idx]=np.mean(temp)\n                \n            utility=np.asarray(utility)    \t\t\t\t               \n        return utility   \n    \n        \n    def acq_utility_cost(self):\n\n        acq={}\n        acq['name']=self.acq_name\n        acq['dim']=self.scaleSearchSpace.shape[0]\n        acq['scaleSearchSpace']=self.scaleSearchSpace   \n    \n        if self.acq_name=='ei_mu_max':\n            x_mu_max,mu_max_val=acq_max_with_name(gp=self.gp,scaleSearchSpace=self.scaleSearchSpace,acq_name='mu',IsReturnY=True)\n            acq['mu_max']=  mu_max_val\n\n        myacq=AcquisitionFunction(acq)\n        \n        x_min = acq_min_scipy_kwargs(myfunc=self.utility_cost_evaluation,SearchSpace=self.scaleSearchSpace,\n                        acq_func=myacq, isDebug=False)\n        \n        if self.verbose==True:\n            acq_val=self.utility_cost_evaluation(x_min,myacq,isDebug=False)\n            print(\"selected point from acq func:\",np.round(x_min,decimals=4),\"acq val=log(Utility/Cost)=\",(-1)*np.round(acq_val,decimals=4))\n            if np.round(acq_val,decimals=4)==0:\n                print(\"acq value =0\")\n            \n        return x_min\n    \n    \n    def select_informative_location_by_uncertainty(self,n_virtual_obs,x_max,t_max):\n        \n        SearchSpace=np.copy(self.scaleSearchSpace)\n        for dd in range(self.dim-1):\n            SearchSpace[dd,0],SearchSpace[dd,1]=x_max[dd],x_max[dd]\n            \n        SearchSpace[-1,1]=t_max\n        \n        temp_X,temp_T=self.X.copy(),self.T.copy()\n        temp_gp=copy.deepcopy(self.gp )\n        \n        temp_Y=np.random.random(size=(len(temp_T),1))\n        \n        temp_gp.fit(temp_X,temp_T,temp_Y,self.Y_curves)\n        \n        new_batch_T=None\n\n        pred_var_value=[0]*n_virtual_obs\n        for ii in range(n_virtual_obs):\n            x_max_pred_variance, pred_var_value[ii]=acq_max_with_name(gp=temp_gp,\n                              scaleSearchSpace=SearchSpace,acq_name='pure_exploration',IsReturnY=True)\n            \n            log_cond=np.log( temp_gp.compute_condition_number() )\n            if log_cond>self.threshold_cond or pred_var_value[ii]<(self.gp.noise_delta+1e-3):\n                break\n          \n            if x_max_pred_variance[-1] in temp_T[-ii:]:\n                break\n            \n            temp_X = np.vstack((temp_X, x_max.reshape((1, -1))))\n            temp_T = np.vstack((temp_T, x_max_pred_variance[-1].reshape((1, -1))))\n            temp_gp.X,temp_gp.T=temp_X,temp_T\n            temp_Y=np.random.random(size=(len(temp_T),1))\n            \n            temp_gp.fit(temp_X,temp_T,temp_Y,self.Y_curves)\n\n            if new_batch_T is None:\n                new_batch_T=x_max_pred_variance[-1].reshape((1, -1))\n            else:\n                new_batch_T= np.vstack((new_batch_T, x_max_pred_variance[-1].reshape((1, -1))))\n        \n        if new_batch_T is None:\n            return [],0\n\n        else:\n            output=np.sort(new_batch_T.ravel()).tolist()\n            return output, len(output)\n\n    \n    def generating_virtual_observations(self,x_max,t_max,y_original_curves,y_cost_original,IsRandom=False):\n        \n        temp_X_new_original=self.Xscaler.inverse_transform(np.reshape(x_max,(-1,self.dim-1)))\n\n        max_n_virtual_obs=np.int(t_max*self.max_n_augmentation)\n        if max_n_virtual_obs==0:\n            self.countVirtual.append(0)\n            return\n        \n        if IsRandom==True:\n            l = [np.random.uniform(0, t_max) for _ in range(max_n_virtual_obs)]\n        else:\n            l,n_virtual_obs=self.select_informative_location_by_uncertainty(max_n_virtual_obs,x_max,t_max)\n            \n        self.countVirtual.append(n_virtual_obs)\n        \n        if self.verbose:\n            np.set_printoptions(suppress=True)\n            print(\"Max #augmented points\",max_n_virtual_obs, \"\\t #augmented points \",len(l),\n                  \"\\t Augmented points: \",np.round(l,decimals=3))\n            \n        l_original=[self.SearchSpace[-1,0]+val*self.max_min_gap[-1] for val in l]\n                           \n        virtual_obs_t_original=np.asarray(l_original).T\n        virtual_obs_t=np.asarray(l).T\n        \n        y_virtual_original=[0]*n_virtual_obs\n        for ii in range(n_virtual_obs):\n            \n            idx=np.int(virtual_obs_t_original[ii])\n            \n            temp_curve=y_original_curves[0][:idx+1]\n            self.markVirtualObs.append(1)\n\n            y_virtual_original[ii]=transform_logistic([temp_curve],\\\n                      self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])\n           \n            self.X = np.vstack((self.X, x_max.reshape((1, -1))))\n            self.X_original=np.vstack((self.X_original, temp_X_new_original))\n        \n            self.T = np.vstack((self.T, virtual_obs_t[ii].reshape((1, -1))))\n            temp=np.asarray(virtual_obs_t_original[ii])\n            self.T_original=np.vstack((self.T_original, temp.reshape((1, -1))))\n\n\n            self.Y_original = np.append(self.Y_original,[y_virtual_original[ii]])\n            self.Y_curves.append(temp_curve)\n            \n            y_cost_estimate=y_cost_original*virtual_obs_t[ii]\n            self.Y_cost_original = np.append(self.Y_cost_original,[y_cost_estimate])\n            \n        \n    def suggest_nextpoint(self):\n \n        self.gp=ProductGaussianProcess(self.scaleSearchSpace,self.gp.hyper,self.gp.logistic_hyper)\n        self.gp.fit(self.X, self.T,self.Y,self.Y_curves)\n            \n        self.condition_number.append(self.gp.cond_num)\n        if self.verbose:\n            print(\"ln of conditioning number of GP covariance matrix\", np.round(np.log(self.gp.cond_num),decimals=1))\n\n        count=len(self.markVirtualObs)-np.sum(self.markVirtualObs)\n        count=np.int(count)\n\n        if  len(self.Y)%(2*self.dim)==0:\n\n            hyper=[self.gp.hyper['lengthscale_x'],self.gp.hyper['lengthscale_t'], \\\n                   self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth']]\n            newlengthscale_x,newlengthscale_t,new_midpoint, new_growth = self.gp.optimize_lengthscale_logistic_hyper(hyper,self.gp.noise_delta)\n            \n            self.gp.hyper['lengthscale_x']=newlengthscale_x\n            self.gp.hyper['lengthscale_t']=self.gp.hyper['lengthscale_t']\n            self.gp.logistic_hyper['midpoint']=new_midpoint\n            self.gp.logistic_hyper['growth']=new_growth\n          \n            if self.verbose:\n                print(\"==estimated lengthscale_x={:.4f}   lengthscale_t={:.3f}   Logistic_m0={:.1f}   Logistic_g0={:.1f}\".format(\n                    newlengthscale_x,newlengthscale_t,new_midpoint,new_growth))\n                \n        start_opt=time.time()\n\n        combine_input=np.hstack((self.X,self.T))\n        self.linear_regression.fit(combine_input,self.Y_cost)\n        \n        x_max_temp=self.acq_utility_cost()\n        x_max=x_max_temp[:-1]\n        t_max=x_max_temp[-1]       \n            \n        finished_opt=time.time()\n        elapse_opt=finished_opt-start_opt\n        self.time_opt=np.hstack((self.time_opt,elapse_opt))\n\n        self.markVirtualObs.append(0)\n\n        self.X = np.vstack((self.X, x_max.reshape((1, -1))))\n        self.T = np.vstack((self.T, t_max.reshape((1, -1))))\n\n        temp_X_new_original=self.Xscaler.inverse_transform(np.reshape(x_max,(-1,self.dim-1)))\n        self.X_original=np.vstack((self.X_original, temp_X_new_original))\n        \n        temp_T_new_original=self.Tscaler.inverse_transform(np.reshape(t_max,(-1,1)))\n        self.T_original=np.vstack((self.T_original, temp_T_new_original))\n\n        x_original_to_test=x_max_temp*self.max_min_gap+self.SearchSpace[:,0]\n\n        y_original_curves, y_cost_original= self.f(x_original_to_test)\n        \n        y_original=transform_logistic(y_original_curves,\\\n              self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])\n        \n        if len(y_original_curves)==1:\n            self.Y_curves.append(y_original_curves[0])\n        else:\n            self.Y_curves.append(y_original_curves)\n\n        \n        self.Y_original = np.append(self.Y_original,y_original)\n        self.Y_cost_original = np.append(self.Y_cost_original,y_cost_original)\n\n        self.generating_virtual_observations(x_max,t_max,y_original_curves,y_cost_original[0])\n        \n        if np.std(self.Y_original)==0:\n            self.Y=(self.Y_original-np.mean(self.Y_original))\n        else:\n            self.Y=(self.Y_original-np.mean(self.Y_original))/np.std(self.Y_original)\n            \n        self.Y_cost=(self.Y_cost_original-np.min(self.Y_cost_original))/(np.max(self.Y_cost_original)-np.min(self.Y_cost_original))\n                    \n        np.set_printoptions(suppress=True)\n\n        print(\"[original scale] x={} t={:.0f} current y={:.2f}, ybest={:.2f}\".format( np.round(self.X_original[-1],decimals=4),\\\n              np.asscalar(self.T_original[-1]),np.asscalar(self.Y_original[-1]), np.asscalar(self.Y_original.max())))\n",
        "experimental_info": "BOIL (Bayesian Optimization for Incremental Learning) is implemented in the `BOIL` class. It uses a `ProductGaussianProcess` for modeling the objective function over hyperparameters (x) and training iterations (t), and a `linear_model.LinearRegression` to approximate the training time cost c(x,t).\n\nThe acquisition function, specified as `acq_name=\"ei_mu_max\"`, maximizes expected improvement while balancing utility against cost. This balance is calculated as `np.log(utility) - np.log(mean_cost)` (for 'ei' acquisition) or `np.log(1 + np.exp(utility)) / np.log(1 + np.exp(mean_cost))` (otherwise). The `ei_mu_max` variant uses the maximum of the GP's predictive mean function as the incumbent value `y_max` for expected improvement calculation.\n\nTraining curve compression is performed using a learnable Sigmoid preference function (Logistic function). The parameters of this function (`midpoint` and `growth`) are optimized alongside the Gaussian Process hyperparameters (`lengthscale_x`, `lengthscale_t`) by maximizing the GP's log marginal likelihood. This joint optimization occurs every `2 * self.dim` iterations.\n\nSelective data augmentation is employed to improve sample efficiency and maintain a well-conditioned GP covariance matrix. Intermediate training points are actively sampled based on maximum GP predictive uncertainty using the `pure_exploration` acquisition function. The number of augmented points for a given observation is dynamically adjusted, capped by `self.max_n_augmentation=10` times the selected `t_max` (normalized), and stops if the logarithm of the GP's covariance matrix condition number (`np.log(self.gp.cond_num)`) exceeds `self.threshold_cond=15` or if the predictive variance is very small. Initial observations are `n_init_points=3` and each initial point also generates virtual observations. The maximum episode length for curve compression is derived from `self.SearchSpace[-1,1]`."
      }
    }
  ],
  "evaluated_hypothesis_history": [
    {
      "hypothesis": {
        "open_problems": "Spectral Adapter tunes only a fixed top-r subset of singular vectors. If r is chosen too small, capacity is insufficient; if too large, many trainable parameters are wasted. Currently the user must hand-pick r for every layer/task.",
        "method": "Weighted Spectral Adapter (WSA).  \n1. Keep exactly the same re-parameterisation as Spectral-AdapterA (additive update of the r leading singular vectors).  \n2. Introduce a learnable gating vector g∈R^r (one scalar per adapted singular direction). The effective singular-value scaling becomes S’ = S * (1 + g⊙Δ), where Δ=softsign(s) and s is an unconstrained parameter initialised at 0.  \n3. Add an ℓ1 penalty λ‖g‖₁ to the training objective.  \n4. At inference time directions whose |g_i|<τ (e.g. 0.05) are pruned, giving an automatically selected effective rank.  \nMotivation:  the sparsity penalty lets the model decide how many spectral directions are worth adapting instead of fixing this hyper-parameter manually; unused parameters naturally shrink to zero, so no extra inference cost is incurred.",
        "experimental_setup": "Model: DeBERTa-v3-base (185 M) fine-tuned on the single-sentence SST-2 task.  \nBaselines: (a) Full fine-tune, (b) Spectral-AdapterA with r=8 (original), (c) LoRA r=8.  \nProposed: WSA with initial r=16, λ=1e-3.  \nTraining: 3 epochs, batch 32, AdamW lr 2e-5, same for all methods.  \nAfter training, compute the effective rank r_eff = |{i : |g_i| ≥ 0.05}|.  \nMetric: validation accuracy on SST-2 dev set.  \nHardware: single RTX-A6000; total run time ≤30 min.",
        "primary_metric": "accuracy",
        "experimental_code": "import torch, torch.nn as nn, torch.nn.functional as F\nclass WeightedSpectralAdapter(nn.Module):\n    def __init__(self, base_linear, r_init=16):\n        super().__init__()\n        w = base_linear.weight.data\n        U,S,Vt = torch.linalg.svd(w, full_matrices=False)\n        self.register_buffer('U',U[:,:r_init])\n        self.register_buffer('Vt',Vt[:r_init])\n        self.register_buffer('S',S[:r_init])\n        # trainable params\n        self.delta_U = nn.Parameter(torch.zeros_like(self.U))\n        self.delta_V = nn.Parameter(torch.zeros_like(self.Vt))\n        self.scores   = nn.Parameter(torch.zeros(r_init))  # produces g via softsign\n        self.base_linear = base_linear  # for bias & fallback\n    def forward(self,x):\n        g = torch.tanh(self.scores)          # in (-1,1)\n        S_eff = self.S * (1+g)               # 1 means no change\n        U_eff = self.U + self.delta_U\n        V_eff = self.Vt + self.delta_V\n        W_delta = (U_eff * S_eff) @ V_eff    # r x d  * d x r\n        W = self.base_linear.weight + W_delta\n        return F.linear(x, W, self.base_linear.bias)\n# loss regulariser to be added each step\n# reg_loss = lambda_l1 * torch.abs(torch.tanh(scores)).sum()",
        "expected_result": "WSA should reach ≈94.3 % dev accuracy vs 93.7 % for fixed-rank Spectral-AdapterA and 93.0 % for LoRA, while selecting r_eff≈6–7 directions on average (starting from 16). Parameter count therefore stays comparable to Spectral-AdapterA despite starting with twice the rank.",
        "expected_conclusion": "A simple ℓ1-gated scaling of singular values lets the model automatically choose its effective spectral rank. This removes the manual r hyper-parameter, yields slightly better accuracy, and keeps parameter-efficiency because unused directions are pruned. The modification touches only the objective (add ℓ1 term) and two small vectors, so it can be dropped into any existing Spectral Adapter codebase with minimal effort."
      },
      "evaluation": {
        "novelty_reason": "Existing PEFT methods already adapt a fixed or dynamically–pruned low-rank subspace, but none of the cited work lets the model itself decide which individual singular directions of Spectral-AdapterA to keep.  Spectral Adapter (and SVFT, S4, ReFT, etc.) always fixes r or sparsity pattern beforehand; AdaLoRA prunes LoRA’s rank but operates in weight space, not along pre-computed singular vectors.  The proposed WSA introduces per-direction learnable gates with an ℓ1 sparsity penalty so that (1) the top-r SVD basis is still used, preserving Spectral-Adapter’s theory, yet (2) the effective rank is automatically selected during fine-tuning and can vary across layers and tasks, a capability absent from prior spectral PEFT papers.  This fine-grained ‘spectral rank search’ therefore represents an incremental but clear technical novelty over the closest prior art.",
        "novelty_score": 6,
        "significance_reason": "Choosing the rank hyper-parameter is a practical pain-point for Spectral Adapter users; WSA removes this manual sweep at virtually no extra memory/compute and can even yield slightly higher accuracy while keeping inference cost constant through pruning.  Academically, it extends the spectral-PEFT line with the first adaptive-rank variant, potentially inspiring follow-up work on spectral sparsification and adaptive capacity control.  Societally, the impact is modest: it offers a more user-friendly and possibly better PEFT recipe for moderate-size language models, but does not open a fundamentally new research area or unlock qualitatively new capabilities.",
        "significance_score": 6
      }
    },
    {
      "hypothesis": {
        "open_problems": "1. Current PEFT methods that work in the spectral domain still (a) require the user to decide a global budget (rank or sparsity) and (b) enforce the same budget for all layers and all inputs.  \n2. Even adaptive‐rank approaches (e.g. AdaLoRA) operate in weight space and therefore lose the theoretical benefits of spectral parametrisation and cannot guarantee FLOP savings at inference time.  \n3. None of the existing methods can optimise a user–defined resource target (e.g. latency, MACs) jointly with task loss, nor can they condition their capacity on the input or on the task in a multi-task setting.",
        "method": "Conditional Hard-Concrete Spectral Adapter (CHC-SA).  \n1. For every linear weight W we pre-compute its top-R (e.g. 16) left/right singular vectors U_R , V_R and store them frozen.  \n2. Each singular direction i gets a learnable log-α_i which is transformed with the hard-concrete distribution (Louizos et al., 2018) to yield a stochastic binary mask z_i∈{0,1}.  At training time we use the differentiable sample \\tilde z_i∈(0,1); at inference we threshold to obtain an exact 0/1 mask, so the effective rank r_eff is the number of kept directions.  \n3. Instead of a fixed per-layer mask we predict the log-α vector from a lightweight hyper-network h_φ(c) where c is a task / domain embedding (or simply a learned identifier).  This lets the same base model allocate different spectral capacity to different tasks without storing multiple adapters.  \n4. The training objective is  \n    L_total = L_task + λ E[∑_layers ∑_i \\tilde z_i] + μ | MACs(\\tilde z) − B_target |,  \nwhere the first regulariser encourages sparsity and the second softly enforces a global budget B_target measured in multiply–accumulates.  Because MACs is a linear function of the binary masks it is differentiable through the hard-concrete samples.  \n5. After convergence we run one epoch with straight-through deterministic masks to finetune only the kept directions; then we permanently prune the zeroed directions, yielding a model whose runtime exactly meets the budget without extra indirection.",
        "experimental_setup": "Models:  \n• DeBERTa-v3-base (185 M) for single-task SST-2, CoLA, RTE.  \n• Multi-task setup on GLUE (8 tasks) with a shared CHC-SA.  \nBaselines: full fine-tune, LoRA r=8, Spectral-AdapterA r=8, AdaLoRA (budget-matched).  \nOur variants: CHC-SA with R=16 and B_target set to 40 %, 60 %, 80 % of full fine-tune MACs.  \nOptimiser: AdamW 2 e-5, 3 epochs.  Hyper-network h_φ is a 2-layer MLP of size 128 taking a learned task embedding (8-way).  \nMetrics: dev accuracy/Matthews; realised MACs and wall-clock latency on RTX-A6000 measured with NVIDIA Nsight.  All runs < 90 min.",
        "primary_metric": "1. Task score (accuracy or Matthews correlation).  2. Realised MACs at inference.",
        "experimental_code": "import torch, torch.nn as nn\nclass HardConcreteGate(nn.Module):\n    def __init__(self, R, init_log_alpha=5.):\n        super().__init__()\n        self.log_alpha = nn.Parameter(torch.full((R,), init_log_alpha))\n        self.beta = 2/3; self.gamma = -0.1; self.zeta = 1.1\n    def _sample_u(self):\n        return torch.rand_like(self.log_alpha)\n    def forward(self, train):\n        if train:\n            u = self._sample_u()\n            s = torch.sigmoid((torch.log(u)-torch.log(1-u)+self.log_alpha)/self.beta)\n        else:\n            s = torch.sigmoid(self.log_alpha)\n        s_bar = s*(self.zeta-self.gamma)+self.gamma\n        z = s_bar.clamp(0,1)\n        return z\nclass CHCSALinear(nn.Module):\n    def __init__(self, base_linear: nn.Linear, R=16, hyper_net=None):\n        super().__init__()\n        W = base_linear.weight.data\n        U,S,Vt = torch.linalg.svd(W, full_matrices=False)\n        self.register_buffer('U',U[:,:R])\n        self.register_buffer('Vt',Vt[:R])\n        self.register_buffer('S',S[:R])\n        self.base = base_linear\n        self.R = R\n        self.hyper = hyper_net or (lambda ctx: None)   # ctx→log_alpha shift\n        self.gate = HardConcreteGate(R)\n    def forward(self,x,ctx=None,train=True):\n        if ctx is not None:\n            self.gate.log_alpha.data += self.hyper(ctx)\n        z = self.gate(train)\n        S_eff = self.S * z\n        deltaW = (self.U * S_eff) @ self.Vt\n        return nn.functional.linear(x, self.base.weight + deltaW, self.base.bias)",
        "expected_result": "1. For a 60 % MACs budget CHC-SA matches or exceeds Spectral-AdapterA accuracy on all single-task runs (e.g. SST-2 94.5 % vs 94.0 %).  \n2. On GLUE the shared CHC-SA outperforms task-agnostic Spectral-Adapter by +0.8 average score while using 35 % fewer MACs.  \n3. The realised r_eff varies across layers (1–10) and across tasks, demonstrating contextual capacity allocation.  \n4. Zero-cost pruning yields identical latency to a manually fixed-rank model with the same MACs, validating the hard-concrete objective.",
        "expected_conclusion": "A hard-concrete gating mechanism in spectral space lets language models learn WHEN and WHERE to spend adaptation capacity, subject to an explicit compute budget. This yields better task performance under the same (or lower) inference cost, removes manual rank tuning, and enables a single multi-task adapter whose per-task footprint is determined automatically. The approach is generic, hyper-parameter light, and can replace existing Spectral-Adapter code with ~30 lines of PyTorch."
      },
      "evaluation": {
        "novelty_reason": "1. It is the first PEFT method that combines (i) spectral-space parameterisation, (ii) hard-concrete stochastic gates and (iii) an explicit, differentiable global compute/latency constraint.  Previous spectral approaches (Spectral-Adapter, SVFT) fix a user-chosen rank and cannot optimise FLOPs; adaptive-rank methods (AdaLoRA, LoRA-based pruning) operate in weight space and lose the FLOP guarantees.\n2. The learnable gates are not static but produced by a lightweight hyper-network conditioned on a task/domain embedding, so the same base model can allocate different spectral capacities per task without storing separate adapters.  Neither Spectral-Adapter nor AdaLoRA nor CA-MTL style adapters provide task-conditioned rank selection in spectral space.\n3. Because MACs is a linear function of the binary spectral masks, the objective jointly optimises task loss and a user-specified MAC or latency budget – a formulation absent from all listed related works.\n4. The proposal shows how to translate the stochastic gates into exact 0/1 masks after training, giving a FLOP-exact model with no run-time indirection – again not present in prior spectral or hard-concrete PEFT papers.\nTaken together these elements constitute a non-trivial conceptual advance over the closest works.",
        "novelty_score": 8,
        "significance_reason": "Academically, the hypothesis tackles three open pain-points in PEFT—manual rank tuning, layer-wise uniformity, and lack of compute-aware objectives—using a principled probabilistic mechanism that retains the theoretical benefits of spectral parameterisation.  If validated, it would provide a unified recipe that simultaneously improves accuracy, guarantees inference efficiency and supports multi-task adaptation without extra storage, which is highly relevant as LMs are increasingly deployed on resource-constrained devices.\nSocietally, automatic compute-budget compliance and per-task capacity allocation can lower the barrier for deploying language models on edge hardware and reduce energy consumption, aligning with sustainability goals.\nThe idea is however an incremental improvement within the PEFT landscape rather than a paradigm shift, and its impact will depend on empirical robustness and ease of integration into existing stacks.",
        "significance_score": 7
      }
    },
    {
      "hypothesis": {
        "open_problems": "1. Existing PEFT methods – including hard-concrete spectral gating – decide the adaptation rank only once per task (or per batch). Thus every token, irrespective of its difficulty, pays the same compute price. 2. This leads to (a) wasted FLOPs on easy, high-confidence tokens and (b) insufficient capacity for rare or ambiguous tokens, hurting accuracy–efficiency trade-offs. 3. Current approaches also ignore sequence-length variation: a long input always incurs a proportional compute cost although many later tokens may already be well predicted. 4. There is no mechanism to translate a user-supplied global compute budget (e.g. 6 GFLOPs per sentence on edge hardware) into adaptive, per-token spectral ranks while keeping the model fully differentiable and free of reinforcement-learning tricks.",
        "method": "Token-Adaptive Hard-Concrete Spectral Adapter (TA-HCSA). 1. For every linear weight W we pre-compute the top-R left/right singular vectors (U_R , V_R). 2. Each singular direction i owns a trainable log-α_i, but the log-α is no longer static: at timestep t layer ℓ we set log-α_i^{(ℓ,t)} = log-α_i  + h_φ^{(ℓ)}(s_t), where s_t is the current token’s hidden state summary (mean-pooled over heads) and h_φ^{(ℓ)} is a 2-layer MLP (64→R). 3. The hard-concrete sampler then produces a per-token binary mask z_{i,ℓ,t}∈{0,1}. At inference masks are hardened, so the effective rank r_{ℓ,t}=∑_i z_{i,ℓ,t} varies with token difficulty. 4. We accumulate expected MACs over the whole sequence: MACs_seq = Σ_{t,ℓ} r_{ℓ,t}·c_ℓ, with c_ℓ a pre-computed cost per singular direction. 5. Training minimises L_total = L_task + λ E[MACs_seq] + μ·|E[MACs_seq]−B_target|, where B_target is a user budget in FLOPs per sequence. 6. Because MACs_seq is linear in the binary masks the gradient flows through the hard-concrete relaxation; no REINFORCE is required. 7. After convergence we run one epoch with straight-through masks, permanently prune directions whose empirical keep-rate <5 %, yielding a sparse, token-adaptive model with static weights and zero controller overhead beyond small MLPs (≈0.2 % params).",
        "experimental_setup": "Models:   • RoBERTa-base (125 M) on single-sentence GLUE tasks.   • Llama-2-7B on Natural-Instructions (multi-task). Budgets: B_target = {25 %, 50 %, 75 %} of full fine-tune FLOPs. Baselines: full fine-tune, static Spectral-Adapter r=8, CHC-SA (task-conditioned). Training: 3 epochs, AdamW 2e-5, batch 32. Controller MLP uses hidden size 768→64→R (R=16). Metrics: dev accuracy/F1; realised MACs measured with fvcore; latency on RTX-A6000 and Apple M1. Ablations: (i) no token conditioning, (ii) Gumbel-softmax vs hard-concrete.",
        "primary_metric": "1. Accuracy (or exact-match/F1) versus realised MACs per sequence. 2. Area-under-Accuracy-FLOP curve.",
        "experimental_code": "import torch, torch.nn as nn\nclass TokenGate(nn.Module):\n    def __init__(self, hidden_dim, R):\n        super().__init__()\n        self.mlp = nn.Sequential(nn.Linear(hidden_dim,64), nn.ReLU(), nn.Linear(64,R))\n    def forward(self, h):            # h: (batch, hidden)\n        return self.mlp(h)             # (batch,R)\nclass HardConcrete(nn.Module):\n    def __init__(self,R):\n        super().__init__(); self.R=R; self.log_alpha = nn.Parameter(torch.zeros(R))\n        self.beta=2/3; self.gamma=-0.1; self.zeta=1.1\n    def forward(self, log_alpha_cond, train=True):\n        a = self.log_alpha + log_alpha_cond      # broadcast over batch\n        if train:\n            u=torch.rand_like(a); s=torch.sigmoid((torch.log(u)-torch.log1p(-u)+a)/self.beta)\n        else:\n            s=torch.sigmoid(a)\n        s_bar=s*(self.zeta-self.gamma)+self.gamma\n        return s_bar.clamp(0,1)                  # (batch,R)\nclass TA_HCSA_Linear(nn.Module):\n    def __init__(self, base_linear, R=16, hidden_dim=768):\n        super().__init__()\n        W=base_linear.weight.data; U,S,Vt=torch.linalg.svd(W, full_matrices=False)\n        self.register_buffer('U',U[:,:R]); self.register_buffer('Vt',Vt[:R]); self.register_buffer('S',S[:R])\n        self.base=base_linear; self.R=R\n        self.token_gate=TokenGate(hidden_dim,R); self.hc=HardConcrete(R)\n    def forward(self,x, h_token, train=True):      # h_token: (batch,hidden)\n        log_a_cond=self.token_gate(h_token)        # (batch,R)\n        z=self.hc(log_a_cond, train)               # (batch,R)\n        S_eff=self.S*z                             # broadcast\n        deltaW=(self.U*S_eff.unsqueeze(-1))*self.Vt  # outer product per batch\n        W_eff=self.base.weight+deltaW.sum(0)       # aggregate over batch\n        return nn.functional.linear(x,W_eff,self.base.bias), z.sum(-1).mean()",
        "expected_result": "1. At 50 % FLOPs TA-HCSA matches static Spectral-Adapter accuracy on SST-2 (94.5 %) and exceeds CHC-SA by +0.4 pp while using 20 % fewer MACs due to intra-sentence variation. 2. On Natural-Instructions, token-adaptive model yields +1.2 Rouge-L at the same 60 % MACs. 3. Difficulty-stratified analysis shows easy tokens (top-10 % confidence) use median rank = 2, hard tokens rank = 10. 4. Inference throughput improves by 1.7× on M1 while preserving perplexity.",
        "expected_conclusion": "Dynamically gating spectral directions at the token level unlocks a new axis of compute adaptivity for PEFT. TA-HCSA learns to spend capacity where the model is uncertain and skip work where it is confident, satisfying explicit FLOP budgets without reinforcement learning or architectural changes. This fine-grained conditional compute delivers state-of-the-art accuracy-efficiency trade-offs and brings budget-aware PEFT closer to practical deployment on edge devices."
      },
      "evaluation": {
        "novelty_reason": "Existing PEFT papers that work in spectral space (Spectral-Adapter, SVFT) or adapt rank (AdaLoRA, CHC-SA) decide the adaptation rank once per task, dataset, or at most once per input. None of them make the rank a function of the *current token’s hidden state*. The new hypothesis proposes (1) a per-token, per-layer binary gating of individual singular directions, (2) learned through a hard-concrete relaxation conditioned by a small controller MLP that sees the token representation, and (3) trained with a fully differentiable FLOP-budget regulariser that lets the user specify a global MAC budget in advance – avoiding REINFORCE or discrete search. This fine-grained, token-level compute allocation inside a spectral adapter is not present in any of the related works; prior adaptive-compute methods in LM fine-tuning either prune weights statically, vary rank at epoch/batch granularity, or use early-exit techniques rather than dynamically changing the low-rank space for every token. Therefore the mechanism and the budgeting objective together constitute a clear methodological novelty.",
        "novelty_score": 8,
        "significance_reason": "If the method works, it offers a new axis of efficiency for PEFT: the model can skip most singular vectors on high-confidence tokens and lavish capacity on ambiguous ones, realising large FLOP savings (1.7× speed-up on edge hardware in the proposal) while matching or improving accuracy. This directly addresses an increasingly important practical constraint – running LLM variants on mobile and embedded devices under strict power budgets – while remaining compatible with standard training pipelines (no RL, no architecture change). Academically, it links conditional computation and spectral PEFT, opening a research path to instance-adaptive parameter-efficient tuning. The idea is narrow in scope (applies to spectral low-rank adapters, not to all PEFT forms) and its gains are yet to be proven on very large generative models, so the overall impact, while high, is not transformative.",
        "significance_score": 7
      }
    },
    {
      "hypothesis": {
        "open_problems": "Current PEFT research offers two orthogonal ways to cut inference cost: (i) low-rank spectral adapters that reduce *per-layer* arithmetic; (ii) early-exit (EE) schemes that skip *later layers* once the model is confident.  Both families decide their compute budget at sentence- or batch-level and apply it uniformly to every token.  This causes three gaps: 1) tokens that are already predictable early still traverse all layers until the EE test is triggered, wasting depth-wise FLOPs; 2) tokens that *do* finish early still pay the full rank in the layers they executed; 3) current EE thresholds are non-differentiable, forcing heuristic tuning and preventing explicit FLOP-budget compliance.  No existing method jointly optimises depth-skipping and rank-skipping under a single, differentiable, user-specified compute budget.",
        "method": "Dual-Granularity Adaptive Spectral–Exit (DiGASE). 1) Uncertainty signal: after each Transformer block ℓ we compute the token-level next-token entropy h_{ℓ,t}. 2) Shared gate head g_θ(h_{ℓ,t}) → [0,1] produces a *depth gate* e_{ℓ,t} (hard-concrete) and a vector of *rank gates* z_{ℓ,t,i} for the R predefined singular directions of that block.  The same two-layer MLP therefore decides both whether to *continue to the next block* and how many singular directions to keep in the current one. 3) Early-exit: if e_{ℓ,t}=0 we stop processing token t and copy its hidden state to all remaining layers (straight-through). 4) Budget term: expected MACs per sentence equals Σ_{ℓ,t} [e_{ℓ-1,t}·c_exit + Σ_i z_{ℓ,t,i}·c_{ℓ,i}], where c_exit counts the cost of executing block ℓ and c_{ℓ,i} the i-th singular direction cost. 5) Training objective:  L_total = L_task + λ·E[MACs] + μ·|E[MACs]−B_target|.  Because both e and z use hard-concrete, gradients flow without REINFORCE. 6) Distilled warm-up: for the first epoch we minimise KL to the full-rank, full-depth teacher to stabilise gates. 7) After training we harden gates, prune directions with keep-rate<5 % and encode per-layer early-exit thresholds as static entropy cut-offs—zero controller overhead at inference.",
        "experimental_setup": "Models: RoBERTa-base on SST-2 and CoLA; Llama-2-7B on Natural-Instructions.  Budgets: B_target ∈ {25 %, 50 %, 75 %} of full fine-tune FLOPs.  Baselines: (a) full fine-tune, (b) static Spectral-Adapter r=8, (c) TA-HCSA (token-adaptive rank only), (d) FastBERT early-exit.  Training: 3 epochs, AdamW 2e-5, batch 32.  Gates: R=16, MLP 768→64→(1+R).  Metrics: dev accuracy/Matthews vs realised MACs; median executed layers; latency and energy on RTX-A6000 and Apple M2.  Ablations: i) depth only, ii) rank only, iii) shared vs separate gate heads.",
        "primary_metric": "Area under Accuracy-vs-MAC curve (AUC-FLOPs); wall-clock speed-up at equal accuracy.",
        "experimental_code": "import torch, torch.nn as nn\nclass DualGate(nn.Module):\n    def __init__(self,hid_dim,R):\n        super().__init__()\n        self.mlp=nn.Sequential(nn.Linear(hid_dim,64),nn.ReLU(),nn.Linear(64,R+1))\n        self.beta=2/3; self.gamma=-0.1; self.zeta=1.1\n        self.log_alpha=nn.Parameter(torch.zeros(R+1))\n    def hard_concrete(self,log_a,train):\n        if train:\n            u=torch.rand_like(log_a); s=torch.sigmoid((torch.log(u)-torch.log1p(-u)+log_a)/self.beta)\n        else:\n            s=torch.sigmoid(log_a)\n        s_bar=s*(self.zeta-self.gamma)+self.gamma\n        return s_bar.clamp(0,1)\n    def forward(self,h,train=True):\n        cond=self.mlp(h)+self.log_alpha\n        z_all=self.hard_concrete(cond,train)\n        e=z_all[:,:1]          # depth gate\n        z=z_all[:,1:]          # rank gates\n        return e,z",
        "expected_result": "On SST-2 at 50 % FLOPs DiGASE attains 94.7 % (+0.5 pp over TA-HCSA, +0.9 pp over FastBERT) and runs 1.9× faster on M2.  Median executed layers drop from 12 to 6 for easy tokens, while rank median falls from 8 to 3.  Joint gating yields a 27 % lower MAC than applying depth- or rank-only gating for the same accuracy.  Similar trends hold for Llama-2-7B on Natural-Instructions (+1.4 Rouge-L at equal MACs).  Distillation warm-up shortens convergence by 20 %.",
        "expected_conclusion": "A single uncertainty-conditioned gate can harmonise *where* (layer depth) and *how much* (spectral rank) computation a PEFT-adapted LM expends, delivering larger accuracy-efficiency gains than either axis alone.  The fully differentiable budget objective meets strict FLOP limits without heuristic thresholds, making the method attractive for on-device NLP and sustainable AI."
      },
      "evaluation": {
        "novelty_reason": "Existing PEFT literature separates compute-reduction along two orthogonal axes: (1) low-rank / spectral adaptation that keeps full depth (LoRA, Spectral Adapter, SVFT, TA-HCSA) and (2) early-exit mechanisms that keep full rank in executed layers (FastBERT, ReFT’s token gating, MEFT’s reversible skipping).  None of the cited works (nor broader PEFT papers) present a single mechanism that  • makes a token-level decision after every block,  • jointly prunes spectrum *inside* the current block and decides whether to execute the *next* block,  • enforces a user-specified FLOP budget through a *differentiable* expectation term, and  • learns both kinds of gates with back-prop friendly hard-concrete distributions.  DiGASE therefore introduces a genuinely new compute-allocation granularity (depth × rank per token) and couples it with a differentiable budget regulariser; this combination is absent from prior art.",
        "novelty_score": 8,
        "significance_reason": "If the method works as described, it can almost double the effective search space for saving inference cost—depth *and* rank—while keeping training fully differentiable.  That can translate into lower latency and energy for on-device LMs and into tighter, predictable cost envelopes for cloud deployment, both of which have practical value.  Academically, it pushes the PEFT field beyond parameter count toward fine-grained FLOP control, a dimension that has received comparatively little formal treatment.  Still, the idea builds on well-known ingredients (hard-concrete, early-exit, spectral adapters), so its conceptual leap is incremental rather than paradigm-shifting.",
        "significance_score": 7
      }
    },
    {
      "hypothesis": {
        "open_problems": "1. Existing PEFT research still allocates compute on crude proxies (MACs or layer count). But equal MAC budgets can translate into very different *real* latencies and energy on heterogeneous devices (GPU vs. phone NPU). No method makes the *hardware-measured* cost itself differentiable.\n2. Depth- and rank-adaptive schemes treat every attention head inside a block identically, although heads differ greatly in importance.\n3. Controller overhead grows linearly with gates (per token × per layer). This starts to dominate when we also gate ranks.\n4. There is no publicly available benchmark that couples PEFT accuracy with energy measurements across devices, hindering reproducibility.",
        "method": "Energy-Constrained Head–Rank–Depth Adapter (EChorda).\n\nA.  Hardware-aware differentiable cost model\n   1. During training we sample 2 % of mini-batches, run them *twice* on the target device (e.g. Jetson Orin, Apple M2): once full-precision, once with the current gates hardened. We record wall-clock time and SoC power via nvml/pmset.\n   2. A two–layer MLP ψ_φ takes token length, mean kept heads, mean kept ranks and produces a scalar energy prediction Ē. φ is updated to minimise |Ē − E_measured|. Gradients from the budget loss are propagated *through* ψ_φ, making real energy approximately differentiable.\n\nB.  Tri-granular gating\n   For every block ℓ and token t a shared MLP g_θ outputs\n   • e_{ℓ,t} ∈ {0,1}  – continue to next block\n   • h_{ℓ,t,j} ∈ {0,1}  – keep head j (j=1…H)\n   • r_{ℓ,t,j,i} ∈ {0,1}  – keep singular direction i inside kept heads (i=1…R)\n   Hard-concrete is used for all gates with a single temperature.\n   Head gates switch off the corresponding KV–projection; rank gates act like Spectral-Adapter on that head.\n   Controller reuse: the same logits produce head and rank gates via grouped 1×1 convolution, adding <0.05 % params.\n\nC.  Budget objective\n   L_total = L_task + λ·E[Ē] + μ·|E[Ē] − B_target|\n   where Ē is ψ_φ’s prediction; gradients flow to θ via ψ_φ.\n\nD.  Gate distillation\n   To cut token×layer overhead, after training we learn a tiny decision-tree (max depth = 3) that mimics g_θ on a held-out set and replace the MLP at inference.\n\nE.  Benchmark\n   We contribute Energy-PEFT-Bench: scripts to flash power-logging firmware, datasets <1 GB, and reference curves for three devices (RTX-A6000, Jetson-Orin-NX, M2-Air).",
        "experimental_setup": "Models: RoBERTa-base (125 M) and Llama-2-7B.\nDatasets: SST-2, CoLA, Natural-Instructions.\nDevices: desktop RTX-A6000 (300 W), Jetson-Orin-NX (25 W), MacBook Air M2 (15 W).\nBudgets: B_target = {3 J, 6 J, 12 J} per sentence.\nBaselines: DiGASE, FastBERT, TA-HCSA, static LoRA.\nMetrics: Accuracy/F1 vs. true joules; AUC-Energy curve; average gate-decision latency.\nAblations: (i) no head gating, (ii) no rank gating, (iii) learned vs constant energy model, (iv) tree vs MLP controller.",
        "primary_metric": "Area under Accuracy-vs-Energy curve (AUC-E); secondary – real-time factor on Jetson.",
        "experimental_code": "import torch, torch.nn as nn\nclass EnergyPredictor(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mlp=nn.Sequential(nn.Linear(4,32),nn.ReLU(),nn.Linear(32,1))\n    def forward(self,stats):            # stats=(len, heads, ranks, layers)\n        return self.mlp(stats)\n\ndef budget_loss(stats, E_measured, B_target):\n    Ē=energy_model(stats)\n    return lam*Ē.mean()+mu*((Ē.mean()-B_target).abs())",
        "expected_result": "At 6 J on Jetson-Orin-NX EChorda attains 94.6 % on SST-2 vs 93.8 % for DiGASE and 92.9 % for FastBERT. Energy predictor MAE <3 %. Decision-tree replacement cuts controller latency by 35 µs without accuracy loss. On M2 the model stays within a 15 W envelope, enabling real-time (≥15 sentences/s).",
        "expected_conclusion": "Modelling the hardware energy surface inside the optimisation loop lets PEFT methods obey *actual* deployment budgets instead of proxy MAC counts. Joint depth-head-rank gating delivers finer control and better accuracy-per-joule than previous two-axis approaches. Energy-PEFT-Bench standardises evaluation and should accelerate research towards green, on-device NLP."
      },
      "evaluation": {
        "novelty_reason": "1. None of the cited PEFT papers (Spectral-Adapter, ReFT, SVFT, S4-design-space, MEFT, TempBalance, etc.) make the *measured* hardware cost (energy/latency) differentiable and part of the training loss. They all rely on proxy metrics such as parameter count, FLOPs, or memory.\n2. Prior adaptive pruning/gating works such as DiGASE or FastBERT gate layers or heads, but do not further gate *rank* inside each head. The proposed tri-granular depth-head-rank gating is therefore a new control dimension within PEFT.\n3. Replacing the per-token controller by a post-hoc learned decision tree to remove runtime overhead is not present in related PEFT literature.\n4. An open benchmark that couples PEFT accuracy with energy measurements across heterogeneous devices (GPU, Jetson, Apple-silicon) is also absent from the listed works.\n5. While hardware-aware NAS literature exists, bringing a differentiable energy surrogate into *parameter-efficient fine-tuning* is, to the best of current comparisons, unique.",
        "novelty_score": 8,
        "significance_reason": "1. Fine-tuning large LMs for edge devices is increasingly needed (phones, IoT). Using real energy rather than proxy MACs directly targets deployment constraints, giving the work practical relevance.\n2. The differentiable energy constraint can influence future PEFT research towards greener NLP, addressing societal concerns about model carbon footprint.\n3. Tri-granular gating could improve the accuracy-per-joule frontier compared with current two-axis methods, a concrete academic contribution.\n4. The public Energy-PEFT-Bench would standardise evaluation and accelerate reproducible research, magnifying impact.\n5. Limitations: concept builds on known ideas (gating, energy prediction in NAS); gains may depend on accurate on-device sampling which adds engineering complexity. Hence significance is high but not transformative.",
        "significance_score": 8
      }
    },
    {
      "hypothesis": {
        "open_problems": "1. Building a differentiable cost term by naively sampling the target device (as in EChorda) still needs thousands of real on-device measurements and must be re-done for every new phone/GPU. 2. Existing PEFT work optimises one proxy (MACs) or one real metric (energy) at a time – practitioners actually care about several, often conflicting, hardware objectives (latency, peak-power, energy). 3. Current gating methods have no mechanism for transferring a learned energy/latency trade-off from one device to another without extra measurements. 4. There is no public surrogate that predicts all three metrics jointly from simple model statistics, nor a benchmark that tests cross-device generalisation.",
        "method": "MetaSurrogate-PEFT (MetaS-PEFT)\nA. Universal differentiable hardware surrogate\n   1. Represent a forward pass by the vector s=(seq_len, kept_layers, kept_heads, kept_ranks, param_bits).\n   2. Off-line, collect <3 k measurements of (energy, latency, max_power) on a pool of 6 devices (RTX-A6000, Jetson-Orin-NX, Mac-M2, iPhone-15-NPU, Pixel-8-TPU, Raspberry-Pi-4).\n   3. Train a 3-headed MLP  ψ_φ(s, h_id)→(Ē, L̂ , P̂) where h_id is a one-hot hardware tag. Mixup on h_id regularises cross-device features.\n   4. Meta-learn φ with leave-one-device-out objective so that the surrogate works zero-shot on unseen hardware when given 5 calibration samples.\nB. Multi-objective tri-granular gating\n   1. Keep EChorda’s depth/head/rank hard-concrete gates g_θ.\n   2. New budget loss:  L_total=L_task +λ Ē+μ L̂ +ν P̂  with user-specified weights (λ,μ,ν).\n   3. Gradients flow through ψ_φ ; no device calls during training.\nC. Few-shot calibration for new hardware\n   • At deployment, run 5 random gating configs, fit a linear correction α,β so that  ψ′(s)=α⊙ψ_φ(s)+β ; no further fine-tuning of θ.\nD. Pareto controller distillation\n   • After training, fit a small DecisionTree to predict a Pareto-optimal (Ē ,L̂ ,P̂) triple from token entropy and position, removing the MLP at inference (≤1 kB).",
        "experimental_setup": "Models: RoBERTa-base and Llama-2-7B.\nDatasets: SST-2, CoLA, Natural-Instructions.\nTraining devices (surrogate pool): RTX-A6000, Jetson-Orin-NX, MacBook-M2, iPhone-15.\nUnseen test device: Google Pixel-8 Edge-TPU (20 W cap).\nBudgets: energy≤6 J, latency≤80 ms, power≤11 W.\nBaselines: (i) Original EChorda retrained on Pixel-8, (ii) DiGASE (depth+rank), (iii) Static LoRA.\nMetrics: Hyper-volume of (Accuracy,-Ē,-L̂ ,-P̂) on Pixel-8; MAE of surrogate on unseen device; #real measurements required.",
        "primary_metric": "4-D hyper-volume (HV) of accuracy vs (energy, latency, power) on the unseen Pixel-8.",
        "experimental_code": "# meta-surrogate (device-agnostic)\nclass MetaSurrogate(torch.nn.Module):\n    def __init__(self, n_hw, d_stat=5):\n        super().__init__()\n        self.hw_embed=torch.nn.Embedding(n_hw,8)\n        self.net=torch.nn.Sequential(\n            torch.nn.Linear(d_stat+8,64),torch.nn.ReLU(),\n            torch.nn.Linear(64,64),torch.nn.ReLU(),\n            torch.nn.Linear(64,3))            # E, L, P\n    def forward(self,stats, hw_id):\n        h=self.hw_embed(hw_id)\n        x=torch.cat([stats,h],-1)\n        return self.net(x)\n# calibration\nwith torch.no_grad():\n    α,β=torch.linalg.lstsq(preds, meas).solution   # 5×3 least squares",
        "expected_result": "1. Zero-shot on Pixel-8 with only 5 calibration runs MetaS-PEFT reaches 95.0 % SST-2 accuracy at 5.9 J / 74 ms / 10.4 W, beating EChorda-retrained (94.6 %, 6.3 J / 78 ms / 11.2 W) while using 2 × fewer on-device measurements (5 vs 200). 2. Surrogate MAE on unseen hardware: energy 2.4 %, latency 3.1 %, power 4.0 %. 3. Pareto tree adds <40 µs overhead and retains ≥99 % hyper-volume.",
        "expected_conclusion": "A meta-learned, device-conditioned surrogate lets PEFT methods optimise *multiple* real hardware objectives without costly per-device sampling. The approach generalises across heterogeneous accelerators with negligible calibration and sets a new accuracy-/-energy-/-latency frontier for on-device NLP, pushing PEFT towards practical, sustainable deployment."
      },
      "evaluation": {
        "novelty_reason": "None of the listed related works learns a differentiable surrogate that maps simple model statistics to real hardware metrics, nor do they attempt meta-learning such a surrogate for cross-device generalisation. Existing PEFT papers (Spectral Adapter, S4-design spaces, ReFT, SVFT, MEFT, LoRA-library, etc.) focus on parameter, memory or representation efficiency but leave hardware-level objectives (energy, latency, peak-power) untouched. Likewise, optimisation frameworks such as AutoLRS, BOIL, DP-HyPO optimise hyper-parameters or learning rates, not hardware costs. The proposed MetaSurrogate-PEFT introduces: (1) a 3-headed MLP surrogate conditioned on a one-hot hardware tag, meta-trained with leave-one-device-out so that five calibration runs are enough for an unseen accelerator, (2) direct back-propagation of accuracy-energy-latency-power multi-objective loss through hard-concrete depth/head/rank gates without any in-loop measurements, and (3) a tiny decision-tree controller distilled from the Pareto set. This combination of meta-learned cross-device hardware prediction with tri-granular PEFT gating and multi-objective differentiable budgeting is absent from prior art, giving the hypothesis substantial novelty.",
        "novelty_score": 8,
        "significance_reason": "If successful, MetaSurrogate-PEFT would let practitioners ship language models onto new phones, NPUs or edge TPUs after only five empirical measurements, while simultaneously meeting strict energy, latency and peak-power budgets. This directly addresses a pressing deployment barrier for on-device NLP and could cut the costly trial-and-error process (hundreds or thousands of device runs) outlined in EChorda-type approaches. Academically, it opens a new research axis linking hardware-aware compression, meta-learning and multi-objective optimisation; societally, it enables greener, faster and privacy-preserving on-device inference. While practical impact depends on surrogate accuracy and on maintaining task performance, the expected 2× measurement reduction and improved Pareto frontier reported in the hypothesis suggest meaningful benefits. Therefore the proposed work is of high but not paradigm-shifting significance.",
        "significance_score": 8
      }
    }
  ],
  "research_session": {
    "hypothesis": {
      "open_problems": "1. Building a differentiable cost term by naively sampling the target device (as in EChorda) still needs thousands of real on-device measurements and must be re-done for every new phone/GPU. 2. Existing PEFT work optimises one proxy (MACs) or one real metric (energy) at a time – practitioners actually care about several, often conflicting, hardware objectives (latency, peak-power, energy). 3. Current gating methods have no mechanism for transferring a learned energy/latency trade-off from one device to another without extra measurements. 4. There is no public surrogate that predicts all three metrics jointly from simple model statistics, nor a benchmark that tests cross-device generalisation.",
      "method": "MetaSurrogate-PEFT (MetaS-PEFT)\nA. Universal differentiable hardware surrogate\n   1. Represent a forward pass by the vector s=(seq_len, kept_layers, kept_heads, kept_ranks, param_bits).\n   2. Off-line, collect <3 k measurements of (energy, latency, max_power) on a pool of 6 devices (RTX-A6000, Jetson-Orin-NX, Mac-M2, iPhone-15-NPU, Pixel-8-TPU, Raspberry-Pi-4).\n   3. Train a 3-headed MLP  ψ_φ(s, h_id)→(Ē, L̂ , P̂) where h_id is a one-hot hardware tag. Mixup on h_id regularises cross-device features.\n   4. Meta-learn φ with leave-one-device-out objective so that the surrogate works zero-shot on unseen hardware when given 5 calibration samples.\nB. Multi-objective tri-granular gating\n   1. Keep EChorda’s depth/head/rank hard-concrete gates g_θ.\n   2. New budget loss:  L_total=L_task +λ Ē+μ L̂ +ν P̂  with user-specified weights (λ,μ,ν).\n   3. Gradients flow through ψ_φ ; no device calls during training.\nC. Few-shot calibration for new hardware\n   • At deployment, run 5 random gating configs, fit a linear correction α,β so that  ψ′(s)=α⊙ψ_φ(s)+β ; no further fine-tuning of θ.\nD. Pareto controller distillation\n   • After training, fit a small DecisionTree to predict a Pareto-optimal (Ē ,L̂ ,P̂) triple from token entropy and position, removing the MLP at inference (≤1 kB).",
      "experimental_setup": "Models: RoBERTa-base and Llama-2-7B.\nDatasets: SST-2, CoLA, Natural-Instructions.\nTraining devices (surrogate pool): RTX-A6000, Jetson-Orin-NX, MacBook-M2, iPhone-15.\nUnseen test device: Google Pixel-8 Edge-TPU (20 W cap).\nBudgets: energy≤6 J, latency≤80 ms, power≤11 W.\nBaselines: (i) Original EChorda retrained on Pixel-8, (ii) DiGASE (depth+rank), (iii) Static LoRA.\nMetrics: Hyper-volume of (Accuracy,-Ē,-L̂ ,-P̂) on Pixel-8; MAE of surrogate on unseen device; #real measurements required.",
      "primary_metric": "4-D hyper-volume (HV) of accuracy vs (energy, latency, power) on the unseen Pixel-8.",
      "experimental_code": "# meta-surrogate (device-agnostic)\nclass MetaSurrogate(torch.nn.Module):\n    def __init__(self, n_hw, d_stat=5):\n        super().__init__()\n        self.hw_embed=torch.nn.Embedding(n_hw,8)\n        self.net=torch.nn.Sequential(\n            torch.nn.Linear(d_stat+8,64),torch.nn.ReLU(),\n            torch.nn.Linear(64,64),torch.nn.ReLU(),\n            torch.nn.Linear(64,3))            # E, L, P\n    def forward(self,stats, hw_id):\n        h=self.hw_embed(hw_id)\n        x=torch.cat([stats,h],-1)\n        return self.net(x)\n# calibration\nwith torch.no_grad():\n    α,β=torch.linalg.lstsq(preds, meas).solution   # 5×3 least squares",
      "expected_result": "1. Zero-shot on Pixel-8 with only 5 calibration runs MetaS-PEFT reaches 95.0 % SST-2 accuracy at 5.9 J / 74 ms / 10.4 W, beating EChorda-retrained (94.6 %, 6.3 J / 78 ms / 11.2 W) while using 2 × fewer on-device measurements (5 vs 200). 2. Surrogate MAE on unseen hardware: energy 2.4 %, latency 3.1 %, power 4.0 %. 3. Pareto tree adds <40 µs overhead and retains ≥99 % hyper-volume.",
      "expected_conclusion": "A meta-learned, device-conditioned surrogate lets PEFT methods optimise *multiple* real hardware objectives without costly per-device sampling. The approach generalises across heterogeneous accelerators with negligible calibration and sets a new accuracy-/-energy-/-latency frontier for on-device NLP, pushing PEFT towards practical, sustainable deployment."
    },
    "iterations": []
  }
}