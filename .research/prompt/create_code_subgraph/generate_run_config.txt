
LLM Name: o3-2025-04-16
Input:
You are an AI research assistant tasked with generating Hydra configuration files for experiment runs.

# Task
Generate individual YAML configuration files for each experiment run. These configs will be used by Hydra to configure specific experimental variations.

# Input Information

## Hypothesis
open_problems='1. Building a differentiable cost term by naively sampling the target device (as in EChorda) still needs thousands of real on-device measurements and must be re-done for every new phone/GPU. 2. Existing PEFT work optimises one proxy (MACs) or one real metric (energy) at a time – practitioners actually care about several, often conflicting, hardware objectives (latency, peak-power, energy). 3. Current gating methods have no mechanism for transferring a learned energy/latency trade-off from one device to another without extra measurements. 4. There is no public surrogate that predicts all three metrics jointly from simple model statistics, nor a benchmark that tests cross-device generalisation.' method='MetaSurrogate-PEFT (MetaS-PEFT)\nA. Universal differentiable hardware surrogate\n   1. Represent a forward pass by the vector s=(seq_len, kept_layers, kept_heads, kept_ranks, param_bits).\n   2. Off-line, collect <3 k measurements of (energy, latency, max_power) on a pool of 6 devices (RTX-A6000, Jetson-Orin-NX, Mac-M2, iPhone-15-NPU, Pixel-8-TPU, Raspberry-Pi-4).\n   3. Train a 3-headed MLP  ψ_φ(s, h_id)→(Ē, L̂ , P̂) where h_id is a one-hot hardware tag. Mixup on h_id regularises cross-device features.\n   4. Meta-learn φ with leave-one-device-out objective so that the surrogate works zero-shot on unseen hardware when given 5 calibration samples.\nB. Multi-objective tri-granular gating\n   1. Keep EChorda’s depth/head/rank hard-concrete gates g_θ.\n   2. New budget loss:  L_total=L_task +λ\u2006Ē+μ\u2006L̂ +ν\u2006P̂  with user-specified weights (λ,μ,ν).\n   3. Gradients flow through ψ_φ ; no device calls during training.\nC. Few-shot calibration for new hardware\n   • At deployment, run 5 random gating configs, fit a linear correction α,β so that  ψ′(s)=α⊙ψ_φ(s)+β ; no further fine-tuning of θ.\nD. Pareto controller distillation\n   • After training, fit a small DecisionTree to predict a Pareto-optimal (Ē ,L̂ ,P̂) triple from token entropy and position, removing the MLP at inference (≤1 kB).' experimental_setup='Models: RoBERTa-base and Llama-2-7B.\nDatasets: SST-2, CoLA, Natural-Instructions.\nTraining devices (surrogate pool): RTX-A6000, Jetson-Orin-NX, MacBook-M2, iPhone-15.\nUnseen test device: Google Pixel-8 Edge-TPU (20 W cap).\nBudgets: energy≤6 J, latency≤80 ms, power≤11 W.\nBaselines: (i) Original EChorda retrained on Pixel-8, (ii) DiGASE (depth+rank), (iii) Static LoRA.\nMetrics: Hyper-volume of (Accuracy,-Ē,-L̂ ,-P̂) on Pixel-8; MAE of surrogate on unseen device; #real measurements required.' primary_metric='4-D hyper-volume (HV) of accuracy vs (energy, latency, power) on the unseen Pixel-8.' experimental_code='# meta-surrogate (device-agnostic)\nclass MetaSurrogate(torch.nn.Module):\n    def __init__(self, n_hw, d_stat=5):\n        super().__init__()\n        self.hw_embed=torch.nn.Embedding(n_hw,8)\n        self.net=torch.nn.Sequential(\n            torch.nn.Linear(d_stat+8,64),torch.nn.ReLU(),\n            torch.nn.Linear(64,64),torch.nn.ReLU(),\n            torch.nn.Linear(64,3))            # E, L, P\n    def forward(self,stats, hw_id):\n        h=self.hw_embed(hw_id)\n        x=torch.cat([stats,h],-1)\n        return self.net(x)\n# calibration\nwith torch.no_grad():\n    α,β=torch.linalg.lstsq(preds, meas).solution   # 5×3 least squares' expected_result='1. Zero-shot on Pixel-8 with only 5 calibration runs MetaS-PEFT reaches 95.0 % SST-2 accuracy at 5.9 J / 74 ms / 10.4 W, beating EChorda-retrained (94.6 %, 6.3 J / 78 ms / 11.2 W) while using 2 × fewer on-device measurements (5 vs 200). 2. Surrogate MAE on unseen hardware: energy 2.4 %, latency 3.1 %, power 4.0 %. 3. Pareto tree adds <40 µs overhead and retains ≥99 % hyper-volume.' expected_conclusion='A meta-learned, device-conditioned surrogate lets PEFT methods optimise *multiple* real hardware objectives without costly per-device sampling. The approach generalises across heterogeneous accelerators with negligible calibration and sets a new accuracy-/-energy-/-latency frontier for on-device NLP, pushing PEFT towards practical, sustainable deployment.'

## Research Method
MetaSurrogate-PEFT (MetaS-PEFT)
A. Universal differentiable hardware surrogate
   1. Represent a forward pass by the vector s=(seq_len, kept_layers, kept_heads, kept_ranks, param_bits).
   2. Off-line, collect <3 k measurements of (energy, latency, max_power) on a pool of 6 devices (RTX-A6000, Jetson-Orin-NX, Mac-M2, iPhone-15-NPU, Pixel-8-TPU, Raspberry-Pi-4).
   3. Train a 3-headed MLP  ψ_φ(s, h_id)→(Ē, L̂ , P̂) where h_id is a one-hot hardware tag. Mixup on h_id regularises cross-device features.
   4. Meta-learn φ with leave-one-device-out objective so that the surrogate works zero-shot on unseen hardware when given 5 calibration samples.
B. Multi-objective tri-granular gating
   1. Keep EChorda’s depth/head/rank hard-concrete gates g_θ.
   2. New budget loss:  L_total=L_task +λ Ē+μ L̂ +ν P̂  with user-specified weights (λ,μ,ν).
   3. Gradients flow through ψ_φ ; no device calls during training.
C. Few-shot calibration for new hardware
   • At deployment, run 5 random gating configs, fit a linear correction α,β so that  ψ′(s)=α⊙ψ_φ(s)+β ; no further fine-tuning of θ.
D. Pareto controller distillation
   • After training, fit a small DecisionTree to predict a Pareto-optimal (Ē ,L̂ ,P̂) triple from token entropy and position, removing the MLP at inference (≤1 kB).

## Experimental Design
experiment_summary='The study validates MetaSurrogate-PEFT (MetaS-PEFT), a PEFT framework that jointly minimises energy, latency and peak-power without expensive per-device profiling. 1) We fine-tune a Llama-2-7B backbone on SST-2 with tri-granular hard-concrete gates over layers, heads and LoRA ranks. 2) During optimisation, a differentiable meta-surrogate ψϕ predicts (Ē, L̂, P̂) from simple model statistics s and a hardware ID; gradients from the composite loss Ltask+λĒ+μL̂+νP̂ flow back to the gates—no real measurements are taken. 3) ψϕ is meta-trained offline on <3 k profiles gathered from four GPUs/NPUs using a leave-one-device-out objective; at deployment on the unseen Google Pixel-8 Edge-TPU we collect only 5 random profiles to fit a linear calibration (α,β). 4) After convergence we distil the gating policy into a 1-kB decision tree for negligible runtime overhead. 5) Evaluation compares MetaS-PEFT against EChorda on the same Pixel-8 budget (≤6 J, ≤80 ms, ≤11 W) and reports 4-D hyper-volume, surrogate MAE and the number of on-device measurements. All training and search run on a single NVIDIA A100 80 GB with 2 TB RAM, easily fitting the surrogate, calibration data and full-precision Llama-2-7B checkpoints.' evaluation_metrics=['4-D Hyper-Volume (Accuracy, -Energy, -Latency, -Power)', 'Accuracy', 'Energy (J)', 'Latency (ms)', 'Peak Power (W)', 'Surrogate MAE', 'Number of On-Device Measurements', '4-D hyper-volume (HV) of accuracy vs (energy, latency, power) on the unseen Pixel-8.'] proposed_method='MetaSurrogate-PEFT (MetaS-PEFT) combines a meta-learned, device-conditioned surrogate with multi-objective gating.\nA) Universal differentiable surrogate: Represent each forward pass by statistics s=(seq_len, kept_layers, kept_heads, kept_ranks, param_bits). Train a 3-headed MLP ψϕ(s,h_id)→(Ē, L̂, P̂) on <3 k measured tuples from four reference devices. Mixup on the one-hot hardware tag h_id enforces shared structure. Meta-learning with a leave-one-device-out loss lets the surrogate adapt to unseen hardware with only 5 calibration samples.\nB) Multi-objective tri-granular gating: Re-use EChorda’s hard-concrete gates gθ over layers, heads and LoRA ranks. Optimise θ with the loss Ltotal = Ltask + λĒ + μL̂ + νP̂, where (Ē, L̂, P̂)=ψϕ(s,h_id). Gradients propagate through ψϕ, keeping the entire process differentiable and measurement-free.\nC) Few-shot calibration: On a new device, measure energy/latency/power for 5 random gate settings, solve a 3×3 least-squares system to obtain per-metric scale/shift (α,β) and form ψ′(s)=α⊙ψϕ(s)+β.\nD) Pareto controller distillation: After training, sample 2 k random inputs, record token entropy/position and the chosen gate states, and train a depth-3 decision tree that predicts the closest Pareto-optimal triple. This removes the MLP at inference, shrinking the controller to ≤1 kB.\nThe result is a PEFT model whose accuracy–energy–latency–power trade-off transfers across devices with negligible extra profiling.' comparative_methods=['EChorda'] models_to_use=['Llama-2-7B'] datasets_to_use=['SST-2'] hyperparameters_to_search={'learning_rate': '1e-5-5e-4', 'lambda_energy (λ)': '0.1-5.0', 'mu_latency (μ)': '0.1-5.0', 'nu_power (ν)': '0.1-5.0', 'hard_concrete_temperature': '0.5-5.0', 'gate_sparsity_regulariser': '1e-6-1e-3'} external_resources=ExternalResources(hugging_face=HuggingFace(models=[HuggingFaceResource(id='pucpr-br/Clinical-BR-LlaMA-2-7B', author='pucpr-br', sha='cc3f2623dd6984751de1caed0717ddca1a833a6e', created_at=datetime.datetime(2024, 2, 3, 0, 24, 33, tzinfo=TzInfo(UTC)), last_modified=datetime.datetime(2024, 8, 29, 14, 35, 5, tzinfo=TzInfo(UTC)), private=False, gated=False, disabled=False, downloads=104782, likes=9, siblings=[HuggingFaceSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='README.md', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='config.json', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='model.safetensors', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)], card_data=HuggingFaceCardData(license='apache-2.0', language=['pt'], library_name=None, pipeline_tag='text-generation', tags=[], datasets=[], model_type=None, base_model='meta-llama/Llama-2-7b', task_categories=[], size_categories=[], metrics=[], widget=[]), tags=['transformers', 'safetensors', 'llama', 'text-generation', 'pt', 'base_model:meta-llama/Llama-2-7b', 'base_model:finetune:meta-llama/Llama-2-7b', 'license:apache-2.0', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible', 'region:us'], pipeline_tag='text-generation', library_name='transformers', readme='---\nlicense: apache-2.0\nlanguage:\n- pt\nbase_model: meta-llama/Llama-2-7b\npipeline_tag: text-generation\n---\n\n# MED-LLM-BR: Medical Large Language Models for Brazilian Portuguese\nMED-LLM-BR is a collaborative project between [HAILab](https://github.com/HAILab-PUCPR) and [Comsentimento](https://www.comsentimento.com.br/), which aims to develop multiple medical LLMs for Portuguese language, including base models and task-specific models, with different sizes. \n\n## Introduction\nClinical-BR-LlaMA-2-7B is a fine-tuned language model specifically designed for generating clinical notes in Portuguese. This model builds on the strengths of LlaMA 2 7B, adapting it through targeted fine-tuning techniques to meet the unique demands of clinical text generation. By focusing on the nuances and complexities of medical language in Portuguese, Clinical-BR-LlaMA-2-7B aims to support healthcare professionals with contextually accurate and relevant clinical documentation.\n\n## Fine-Tuning Approach\nTo enhance memory efficiency and reduce computational demands, we implemented LoRA with 16-bit precision on the q_proj and v_proj projections. We configured LoRA with R set to 8, Alpha to 16, and Dropout to 0.1, allowing the model to adapt effectively while preserving output quality. For optimization, the AdamW optimizer was used with parameters β1 = 0.9 and β2 = 0.999, achieving a balance between fast convergence and training stability. This careful tuning process ensures robust performance in generating accurate and contextually appropriate clinical text in Portuguese.\n\n## Data\nThe fine-tuning of Clinical-BR-LlaMA-2-7B utilized 2.4GB of text from three clinical datasets. The SemClinBr project provided diverse clinical narratives from Brazilian hospitals, while the BRATECA dataset contributed admission notes from various departments in 10 hospitals. Additionally, data from Lopes et al., 2019, added neurology-focused texts from European Portuguese medical journals. These datasets collectively improved the model’s ability to generate accurate clinical notes in Portuguese.\n\n## Provisional Citation:\n```\n@inproceedings{pinto2024clinicalLLMs,\n  title        = {Developing Resource-Efficient Clinical LLMs for Brazilian Portuguese},\n  author       = {João Gabriel de Souza Pinto and Andrey Rodrigues de Freitas and Anderson Carlos Gomes Martins and Caroline Midori Rozza Sawazaki and Caroline Vidal and Lucas Emanuel Silva e Oliveira},\n  booktitle    = {Proceedings of the 34th Brazilian Conference on Intelligent Systems (BRACIS)},\n  year         = {2024},\n  note         = {In press},\n}\n```', model_index=None, widget_data=None, config=None, transformers_info=None, spaces=None, safetensors=None, used_storage=None, extracted_code='')], datasets=[HuggingFaceResource(id='stanfordnlp/sst2', author='stanfordnlp', sha='8d51e7e4887a4caaa95b3fbebbf53c0490b58bbb', created_at=datetime.datetime(2022, 6, 13, 14, 1, 47, tzinfo=TzInfo(UTC)), last_modified=datetime.datetime(2024, 1, 4, 16, 31, 7, tzinfo=TzInfo(UTC)), private=False, gated=False, disabled=False, downloads=11057, likes=133, siblings=[HuggingFaceSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='README.md', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='data/test-00000-of-00001.parquet', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='data/train-00000-of-00001.parquet', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='data/validation-00000-of-00001.parquet', size=None, blob_id=None, lfs=None)], card_data=HuggingFaceCardData(license=['unknown'], language=['en'], library_name=None, pipeline_tag=None, tags=[], datasets=[], model_type=None, base_model=None, task_categories=['text-classification'], size_categories=['10K<n<100K'], metrics=[], widget=[]), tags=['task_categories:text-classification', 'task_ids:sentiment-classification', 'annotations_creators:crowdsourced', 'language_creators:found', 'multilinguality:monolingual', 'source_datasets:original', 'language:en', 'license:unknown', 'size_categories:10K<n<100K', 'format:parquet', 'modality:text', 'library:datasets', 'library:pandas', 'library:mlcroissant', 'library:polars', 'region:us'], pipeline_tag=None, library_name=None, readme='---\nannotations_creators:\n- crowdsourced\nlanguage_creators:\n- found\nlanguage:\n- en\nlicense:\n- unknown\nmultilinguality:\n- monolingual\nsize_categories:\n- 10K<n<100K\nsource_datasets:\n- original\ntask_categories:\n- text-classification\ntask_ids:\n- sentiment-classification\npaperswithcode_id: sst\npretty_name: Stanford Sentiment Treebank v2\ndataset_info:\n  features:\n  - name: idx\n    dtype: int32\n  - name: sentence\n    dtype: string\n  - name: label\n    dtype:\n      class_label:\n        names:\n          \'0\': negative\n          \'1\': positive\n  splits:\n  - name: train\n    num_bytes: 4681603\n    num_examples: 67349\n  - name: validation\n    num_bytes: 106252\n    num_examples: 872\n  - name: test\n    num_bytes: 216640\n    num_examples: 1821\n  download_size: 3331058\n  dataset_size: 5004495\nconfigs:\n- config_name: default\n  data_files:\n  - split: train\n    path: data/train-*\n  - split: validation\n    path: data/validation-*\n  - split: test\n    path: data/test-*\n---\n\n# Dataset Card for [Dataset Name]\n\n## Table of Contents\n- [Table of Contents](#table-of-contents)\n- [Dataset Description](#dataset-description)\n  - [Dataset Summary](#dataset-summary)\n  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)\n  - [Languages](#languages)\n- [Dataset Structure](#dataset-structure)\n  - [Data Instances](#data-instances)\n  - [Data Fields](#data-fields)\n  - [Data Splits](#data-splits)\n- [Dataset Creation](#dataset-creation)\n  - [Curation Rationale](#curation-rationale)\n  - [Source Data](#source-data)\n  - [Annotations](#annotations)\n  - [Personal and Sensitive Information](#personal-and-sensitive-information)\n- [Considerations for Using the Data](#considerations-for-using-the-data)\n  - [Social Impact of Dataset](#social-impact-of-dataset)\n  - [Discussion of Biases](#discussion-of-biases)\n  - [Other Known Limitations](#other-known-limitations)\n- [Additional Information](#additional-information)\n  - [Dataset Curators](#dataset-curators)\n  - [Licensing Information](#licensing-information)\n  - [Citation Information](#citation-information)\n  - [Contributions](#contributions)\n\n## Dataset Description\n\n- **Homepage:** https://nlp.stanford.edu/sentiment/\n- **Repository:**\n- **Paper:** [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](https://www.aclweb.org/anthology/D13-1170/)\n- **Leaderboard:**\n- **Point of Contact:**\n\n### Dataset Summary\n\nThe Stanford Sentiment Treebank is a corpus with fully labeled parse trees that allows for a complete analysis of the\ncompositional effects of sentiment in language. The corpus is based on the dataset introduced by Pang and Lee (2005)\nand consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser and\nincludes a total of 215,154 unique phrases from those parse trees, each annotated by 3 human judges.\n\nBinary classification experiments on full sentences (negative or somewhat negative vs somewhat positive or positive\nwith neutral sentences discarded) refer to the dataset as SST-2 or SST binary.\n\n### Supported Tasks and Leaderboards\n\n- `sentiment-classification`\n\n### Languages\n\nThe text in the dataset is in English (`en`).\n\n## Dataset Structure\n\n### Data Instances\n\n```\n{\'idx\': 0,\n \'sentence\': \'hide new secretions from the parental units \',\n \'label\': 0}\n```\n\n### Data Fields\n\n- `idx`: Monotonically increasing index ID.\n- `sentence`: Complete sentence expressing an opinion about a film.\n- `label`: Sentiment of the opinion, either "negative" (0) or positive (1). The test set labels are hidden (-1).\n\n### Data Splits\n\n|                    |    train | validation | test |\n|--------------------|---------:|-----------:|-----:|\n| Number of examples |    67349 |        872 | 1821 |\n\n## Dataset Creation\n\n### Curation Rationale\n\n[More Information Needed]\n\n### Source Data\n\n#### Initial Data Collection and Normalization\n\n[More Information Needed]\n\n#### Who are the source language producers?\n\nRotten Tomatoes reviewers.\n\n### Annotations\n\n#### Annotation process\n\n[More Information Needed]\n\n#### Who are the annotators?\n\n[More Information Needed]\n\n### Personal and Sensitive Information\n\n[More Information Needed]\n\n## Considerations for Using the Data\n\n### Social Impact of Dataset\n\n[More Information Needed]\n\n### Discussion of Biases\n\n[More Information Needed]\n\n### Other Known Limitations\n\n[More Information Needed]\n\n## Additional Information\n\n### Dataset Curators\n\n[More Information Needed]\n\n### Licensing Information\n\nUnknown.\n\n### Citation Information\n\n```bibtex\n@inproceedings{socher-etal-2013-recursive,\n    title = "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",\n    author = "Socher, Richard  and\n      Perelygin, Alex  and\n      Wu, Jean  and\n      Chuang, Jason  and\n      Manning, Christopher D.  and\n      Ng, Andrew  and\n      Potts, Christopher",\n    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",\n    month = oct,\n    year = "2013",\n    address = "Seattle, Washington, USA",\n    publisher = "Association for Computational Linguistics",\n    url = "https://www.aclweb.org/anthology/D13-1170",\n    pages = "1631--1642",\n}\n```\n\n### Contributions\n\nThanks to [@albertvillanova](https://github.com/albertvillanova) for adding this dataset.', model_index=None, widget_data=None, config=None, transformers_info=None, spaces=None, safetensors=None, used_storage=None, extracted_code=None), HuggingFaceResource(id='SetFit/sst2', author='SetFit', sha='00ea8ccb7a54b4e3780a3e51aa3f80361ff849c0', created_at=datetime.datetime(2022, 3, 2, 23, 29, 22, tzinfo=TzInfo(UTC)), last_modified=datetime.datetime(2021, 12, 25, 6, 16, 15, tzinfo=TzInfo(UTC)), private=False, gated=False, disabled=False, downloads=5417, likes=11, siblings=[HuggingFaceSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='README.md', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='dev.jsonl', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='test.jsonl', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='train.jsonl', size=None, blob_id=None, lfs=None)], card_data=None, tags=['size_categories:1K<n<10K', 'format:json', 'modality:text', 'library:datasets', 'library:pandas', 'library:mlcroissant', 'library:polars', 'region:us'], pipeline_tag=None, library_name=None, readme='# Stanford Sentiment Treebank - Binary\r\n\r\n[Stanford Sentiment Treebank](http://nlp.stanford.edu/sentiment/) with 2 labels: negative, positive\r\n\r\nSplits are from: \r\n[https://github.com/AcademiaSinicaNLPLab/sentiment_dataset/tree/master/data](https://github.com/AcademiaSinicaNLPLab/sentiment_dataset/tree/master/data)\r\n\r\nTraining data is on sentence level, not on phrase level!\r\n', model_index=None, widget_data=None, config=None, transformers_info=None, spaces=None, safetensors=None, used_storage=None, extracted_code=None)])) experiment_code=None

## Experiment Runs

- Run ID: proposed-iter1-Llama-2-7B-SST-2
  Method: proposed
  Model: Llama-2-7B
  Dataset: SST-2

- Run ID: comparative-1-iter1-Llama-2-7B-SST-2
  Method: comparative-1
  Model: Llama-2-7B
  Dataset: SST-2


# Requirements

## Configuration Structure
Each run configuration should include:
- run_id: Unique identifier for this run
- method: The method name (baseline, proposed, ablation, etc.)
- model: Model-specific parameters (name, architecture details, hyperparameters)
- dataset: Dataset-specific parameters (name, preprocessing settings, split ratios)
- training: Training hyperparameters (learning rate, batch size, epochs, optimizer settings)
- optuna: Hyperparameter search space definition for Optuna optimization
  - Define search spaces for key hyperparameters using Optuna's suggest methods
  - Example: learning_rate: [1e-5, 1e-3], batch_size: [16, 32, 64]
- Any other experiment-specific settings

## Format
- Generate one YAML configuration per experiment run
- Ensure valid YAML syntax
- Use meaningful parameter values based on the research method and experimental design

## Example Configuration
```yaml
run_id: baseline_bert_imdb
method: baseline
model:
  name: bert-base-uncased
  hidden_size: 768
  num_layers: 12
dataset:
  name: imdb
  max_length: 512
  batch_size: 32
training:
  learning_rate: 2e-5
  epochs: 3
  optimizer: adamw
  warmup_steps: 500
optuna:
  n_trials: 20
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 1e-3
    batch_size:
      type: categorical
      choices: [16, 32, 64]
```

# Experimental Environment
NVIDIA A100 or H200
VRAM: 80 GB or more
RAM: 2048 GB or more

# Instructions
1. Generate one YAML configuration for each experiment run listed above
2. Ensure configurations reflect the differences between baseline, proposed, and ablation methods
3. Use appropriate hyperparameters based on the experimental design
4. Include Optuna search space if hyperparameter optimization is beneficial for the experiment
5. For Optuna search spaces, use appropriate distribution types:
   - loguniform: For learning rates, regularization parameters
   - uniform: For dropout rates, weight decay
   - int: For hidden dimensions, number of layers
   - categorical: For discrete choices like batch size, optimizer type

Generate the configurations now:
Output:
{
  "run_configs": [
    {
      "run_id": "proposed-iter1-Llama-2-7B-SST-2",
      "run_config_yaml": "run_id: proposed-iter1-Llama-2-7B-SST-2\nmethod: MetaS-PEFT\nmodel:\n  name: Llama-2-7B\n  precision: bfloat16\n  architecture_notes: \"Decoder-only 32-layer transformer; tri-granular gating (layer, head, LoRA rank)\"\n  peft:\n    method: LoRA\n    target_modules: [\"q_proj\", \"v_proj\"]\n    r: 8          # default, tuned by Optuna\n    alpha: 16\n    dropout: 0.1\n  gating:\n    granularity: [layer, head, rank]\n    hard_concrete_temperature: 2.0\n    gate_sparsity_regulariser: 1e-5\ndataset:\n  name: sst2\n  source: \"huggingface:stanfordnlp/sst2\"\n  max_length: 128\n  padding: max_length\n  batch_size: 16\ntraining:\n  num_epochs: 3\n  learning_rate: 2e-5\n  optimizer: adamw\n  weight_decay: 0.01\n  gradient_accumulation_steps: 8\n  lr_scheduler: cosine\n  warmup_ratio: 0.05\n  seed: 42\nbudgets:\n  energy_j: 6.0\n  latency_ms: 80.0\n  power_w: 11.0\nsurrogate:\n  checkpoint_path: \"checkpoints/meta_surrogate_v1.pt\"\n  n_hardware_tags: 6\n  meta_training_devices: [\"RTX-A6000\", \"Jetson-Orin-NX\", \"MacBook-M2\", \"iPhone-15-NPU\", \"Pixel-8-TPU\", \"Raspberry-Pi-4\"]\n  calibration_samples: 5\n  calibration_method: linear_scale_shift\nobjective_weights:\n  lambda_energy: 1.0\n  mu_latency: 1.0\n  nu_power: 1.0\nevaluation:\n  primary_metric: 4-D_hyper_volume\n  metrics: [accuracy, energy_j, latency_ms, power_w, surrogate_mae, hv]\noptuna:\n  n_trials: 60\n  direction: maximize\n  search_space:\n    learning_rate:\n      type: loguniform\n      low: 1e-5\n      high: 5e-4\n    lambda_energy:\n      type: loguniform\n      low: 0.1\n      high: 5.0\n    mu_latency:\n      type: loguniform\n      low: 0.1\n      high: 5.0\n    nu_power:\n      type: loguniform\n      low: 0.1\n      high: 5.0\n    hard_concrete_temperature:\n      type: uniform\n      low: 0.5\n      high: 5.0\n    gate_sparsity_regulariser:\n      type: loguniform\n      low: 1e-6\n      high: 1e-3\n    r:\n      type: int\n      low: 4\n      high: 32\n    batch_size:\n      type: categorical\n      choices: [8, 16, 32]\n"
    },
    {
      "run_id": "comparative-1-iter1-Llama-2-7B-SST-2",
      "run_config_yaml": "run_id: comparative-1-iter1-Llama-2-7B-SST-2\nmethod: EChorda\nmodel:\n  name: Llama-2-7B\n  precision: bfloat16\n  architecture_notes: \"Decoder-only 32-layer transformer; depth/head/rank hard-concrete gates\"\n  gating:\n    granularity: [layer, head, rank]\n    hard_concrete_temperature: 2.0\n    gate_sparsity_regulariser: 1e-5\ndataset:\n  name: sst2\n  source: \"huggingface:stanfordnlp/sst2\"\n  max_length: 128\n  padding: max_length\n  batch_size: 16\ntraining:\n  num_epochs: 3\n  learning_rate: 2e-5\n  optimizer: adamw\n  weight_decay: 0.01\n  gradient_accumulation_steps: 8\n  lr_scheduler: cosine\n  warmup_ratio: 0.05\n  seed: 42\nbudgets:\n  energy_j: 6.0\n  latency_ms: 80.0\n  power_w: 11.0\nhardware_profiling:\n  target_device: Google-Pixel-8-EdgeTPU\n  real_measurements_required: 200\n  measurement_cache_dir: /data/echorda_pixel8_profiles\nobjective_weights:\n  lambda_energy: 1.0\n  mu_latency: 0.0\n  nu_power: 0.0\nevaluation:\n  primary_metric: energy_j\n  metrics: [accuracy, energy_j, latency_ms, power_w]\noptuna:\n  n_trials: 60\n  direction: minimize\n  search_space:\n    learning_rate:\n      type: loguniform\n      low: 1e-5\n      high: 5e-4\n    lambda_energy:\n      type: loguniform\n      low: 0.1\n      high: 5.0\n    hard_concrete_temperature:\n      type: uniform\n      low: 0.5\n      high: 5.0\n    gate_sparsity_regulariser:\n      type: loguniform\n      low: 1e-6\n      high: 1e-3\n    batch_size:\n      type: categorical\n      choices: [8, 16, 32]\n"
    }
  ]
}
