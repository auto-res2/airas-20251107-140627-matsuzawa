
LLM Name: o3-2025-04-16
Input:
You are an AI code reviewer validating production-ready experiment code for research papers.

Analyze the provided experiment code and determine if it meets all requirements for immediate execution in research experiments.

# Instructions

## Core Validation Criteria
Check if the generated experiment code meets ALL of the following requirements:

1. **Complete Implementation**:
   - Every component is fully functional, production-ready, publication-worthy code
   - No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
   - All functions and classes are completely implemented
   - No truncated code sections or incomplete implementations

2. **Hydra Integration**:
   - Uses Hydra to manage all experiment configurations from `config/runs/*.yaml` files
   - All parameters are loaded from run configs dynamically
   - Proper configuration structure with run_id, method, model, dataset, training, and optuna sections
   - CLI interface matches:
     * Training (full): `uv run python -u -m src.main run={run_id} results_dir={path} mode=full`
     * Training (trial): `uv run python -u -m src.main run={run_id} results_dir={path} mode=trial`
     * Evaluation: `uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2", ...]'` (independent execution)
   - Code must automatically configure based on mode:
     * When `cfg.mode == "trial"`: Set `cfg.wandb.mode = "disabled"`, `cfg.optuna.n_trials = 0`, epochs=1
     * When `cfg.mode == "full"`: Set `cfg.wandb.mode = "online"` and use full configuration

3. **Complete Data Pipeline**:
   - Full data loading and preprocessing implementation
   - Dataset-specific preprocessing is properly implemented
   - No placeholder dataset loading code
   - Proper error handling for data operations
   - Uses `.cache/` as the cache directory for all datasets and models

4. **Model Implementation**:
   - Complete model architectures for all methods (proposed and comparative methods)
   - No placeholders (TODO, PLACEHOLDER, pass, or incomplete implementations)
   - When External Resources specify HuggingFace models: properly use and customize them (acceptable to wrap AutoModel, add adapters, etc.)
   - When no external models specified: implement architectures from scratch using PyTorch primitives
   - Model-specific configurations correctly applied
   - Proper PyTorch usage throughout

5. **File Structure Compliance**:
   - Contains EXACTLY these required files (and NO other files):
     * `src/train.py`
     * `src/evaluate.py`
     * `src/preprocess.py`
     * `src/model.py`
     * `src/main.py`
     * `pyproject.toml`
     * `config/config.yaml`
   - NO additional files (e.g., NO `src/__init__.py`, NO `setup.py`, NO other Python files)
   - No missing files from the structure
   - All functionality contained within specified files

6. **WandB Integration**:
   - train.py initializes WandB and logs ALL metrics comprehensively:
     * Use `wandb.log()` at each training step/batch/epoch with ALL relevant time-series metrics
     * Log as frequently as possible (per-batch or per-epoch) to capture complete training dynamics
     * Use `wandb.summary["key"] = value` to save final/best metrics (best_val_acc, final_test_acc, best_epoch, etc.)
     * Metric names in train.py's wandb.log() MUST exactly match the keys used in evaluate.py's run.history()
   - Optuna Integration: If using Optuna, DO NOT log intermediate trial results to WandB - only log the final run with best hyperparameters
   - Code must automatically configure based on mode:
     * When `cfg.mode == "trial"`: Set `cfg.wandb.mode = "disabled"` before any WandB operations
     * When `cfg.mode == "full"`: Set `cfg.wandb.mode = "online"` before any WandB operations
   - NO results.json or stdout JSON dumps in train.py
   - config/config.yaml contains mandatory WandB settings (entity/project)
   - `WANDB_API_KEY` environment variable is available for authentication

7. **Configuration Files**:
   - The generated code properly references config files via Hydra
   - NOTE: config/runs/{run_id}.yaml files are provided separately (not in ExperimentCode)
   - All run configurations match the experiment_runs provided
   - Optuna search spaces are properly defined if applicable

8. **Evaluation Script Independence**:
   - evaluate.py is executed independently via `uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2"]'`
   - Accepts `run_ids` parameter as JSON string list (parse with `json.loads(args.run_ids)`)
   - main.py DOES NOT call evaluate.py
   - evaluate.py loads WandB config from `config/config.yaml` (in repository root)
   - evaluate.py retrieves comprehensive data from WandB API:
     * Use `wandb.Api()` to get run data: `run = api.run(f"{entity}/{project}/{run_id}")`
     * Retrieve: `history = run.history()`, `summary = run.summary._json_dict`, `config = dict(run.config)`
   - **STEP 1: Per-Run Processing** (for each run_id):
     * Export comprehensive run-specific metrics to: `{results_dir}/{run_id}/metrics.json`
     * Generate run-specific figures (learning curves, confusion matrices) to: `{results_dir}/{run_id}/`
     * Each run should have its own subdirectory with its metrics and figures
   - **STEP 2: Aggregated Analysis** (after processing all runs):
     * Export aggregated metrics to: `{results_dir}/comparison/aggregated_metrics.json` with the following structure:
       ```json
       {
         "primary_metric": "4-D hyper-volume (HV) of accuracy vs (energy, latency, power) on the unseen Pixel-8.",
         "metrics": {
           "metric_name_1": {"run_id_1": value1, "run_id_2": value2, ...},
           "metric_name_2": {"run_id_1": value1, "run_id_2": value2, ...}
         },
         "best_proposed": {
           "run_id": "proposed-iter2-model-dataset",
           "value": 0.92
         },
         "best_baseline": {
           "run_id": "comparative-1-model-dataset",
           "value": 0.88
         },
         "gap": 4.55
       }
       ```
       The structure must include:
       - "primary_metric": The primary evaluation metric name from the hypothesis
       - "metrics": All collected metrics organized by metric name, then by run_id
       - "best_proposed": The run_id and value of the proposed method with the best primary_metric performance (run_id contains "proposed")
       - "best_baseline": The run_id and value of the baseline/comparative method with the best primary_metric performance (run_id contains "comparative" or "baseline")
       - "gap": Performance gap calculated as: (best_proposed.value - best_baseline.value) / best_baseline.value * 100
         * Must use the expected results from the hypothesis to determine metric direction (higher vs lower is better)
         * If the metric should be minimized, reverse the sign of the gap
         * The gap represents the percentage improvement of the proposed method over the best baseline
     * Generate comparison figures to: `{results_dir}/comparison/`
     * Cross-run comparison charts (bar charts, box plots)
     * Performance metrics tables
     * Statistical significance tests
   - Proper figure quality: legends, annotations, tight_layout
   - Follows GLOBALLY UNIQUE naming convention to prevent collisions:
     * Per-run figures: `{run_id}_{figure_topic}[_<condition>][_pairN].pdf` (e.g., `run-1-proposed-bert-glue_learning_curve.pdf`)
     * Comparison figures: `comparison_{figure_topic}[_<condition>][_pairN].pdf` (e.g., `comparison_accuracy_bar_chart.pdf`)
   - train.py and main.py generate NO figures
   - evaluate.py cannot run in trial_mode (no WandB data available when WandB disabled)

9. **Mode-Based Implementation**:
   - `mode` parameter controls experiment behavior (required parameter)
   - When `cfg.mode == "trial"`:
     * Properly reduces computational load: epochs=1, batches limited to 1-2, Optuna disabled (n_trials=0), small evaluation subset
     * Automatically sets `cfg.wandb.mode = "disabled"`
     * Purpose: Fast validation that code runs without errors
   - When `cfg.mode == "full"`:
     * Automatically sets `cfg.wandb.mode = "online"`
     * Uses full configuration (full epochs, full Optuna trials, etc.)

## Output Format
Respond with a JSON object containing:
- `is_code_ready`: boolean - true if ALL criteria are met, false otherwise
- `code_issue`: string - specific issues found if any criteria are not met, focusing on what needs to be fixed

# Hypothesis
open_problems='1. Building a differentiable cost term by naively sampling the target device (as in EChorda) still needs thousands of real on-device measurements and must be re-done for every new phone/GPU. 2. Existing PEFT work optimises one proxy (MACs) or one real metric (energy) at a time – practitioners actually care about several, often conflicting, hardware objectives (latency, peak-power, energy). 3. Current gating methods have no mechanism for transferring a learned energy/latency trade-off from one device to another without extra measurements. 4. There is no public surrogate that predicts all three metrics jointly from simple model statistics, nor a benchmark that tests cross-device generalisation.' method='MetaSurrogate-PEFT (MetaS-PEFT)\nA. Universal differentiable hardware surrogate\n   1. Represent a forward pass by the vector s=(seq_len, kept_layers, kept_heads, kept_ranks, param_bits).\n   2. Off-line, collect <3 k measurements of (energy, latency, max_power) on a pool of 6 devices (RTX-A6000, Jetson-Orin-NX, Mac-M2, iPhone-15-NPU, Pixel-8-TPU, Raspberry-Pi-4).\n   3. Train a 3-headed MLP  ψ_φ(s, h_id)→(Ē, L̂ , P̂) where h_id is a one-hot hardware tag. Mixup on h_id regularises cross-device features.\n   4. Meta-learn φ with leave-one-device-out objective so that the surrogate works zero-shot on unseen hardware when given 5 calibration samples.\nB. Multi-objective tri-granular gating\n   1. Keep EChorda’s depth/head/rank hard-concrete gates g_θ.\n   2. New budget loss:  L_total=L_task +λ\u2006Ē+μ\u2006L̂ +ν\u2006P̂  with user-specified weights (λ,μ,ν).\n   3. Gradients flow through ψ_φ ; no device calls during training.\nC. Few-shot calibration for new hardware\n   • At deployment, run 5 random gating configs, fit a linear correction α,β so that  ψ′(s)=α⊙ψ_φ(s)+β ; no further fine-tuning of θ.\nD. Pareto controller distillation\n   • After training, fit a small DecisionTree to predict a Pareto-optimal (Ē ,L̂ ,P̂) triple from token entropy and position, removing the MLP at inference (≤1 kB).' experimental_setup='Models: RoBERTa-base and Llama-2-7B.\nDatasets: SST-2, CoLA, Natural-Instructions.\nTraining devices (surrogate pool): RTX-A6000, Jetson-Orin-NX, MacBook-M2, iPhone-15.\nUnseen test device: Google Pixel-8 Edge-TPU (20 W cap).\nBudgets: energy≤6 J, latency≤80 ms, power≤11 W.\nBaselines: (i) Original EChorda retrained on Pixel-8, (ii) DiGASE (depth+rank), (iii) Static LoRA.\nMetrics: Hyper-volume of (Accuracy,-Ē,-L̂ ,-P̂) on Pixel-8; MAE of surrogate on unseen device; #real measurements required.' primary_metric='4-D hyper-volume (HV) of accuracy vs (energy, latency, power) on the unseen Pixel-8.' experimental_code='# meta-surrogate (device-agnostic)\nclass MetaSurrogate(torch.nn.Module):\n    def __init__(self, n_hw, d_stat=5):\n        super().__init__()\n        self.hw_embed=torch.nn.Embedding(n_hw,8)\n        self.net=torch.nn.Sequential(\n            torch.nn.Linear(d_stat+8,64),torch.nn.ReLU(),\n            torch.nn.Linear(64,64),torch.nn.ReLU(),\n            torch.nn.Linear(64,3))            # E, L, P\n    def forward(self,stats, hw_id):\n        h=self.hw_embed(hw_id)\n        x=torch.cat([stats,h],-1)\n        return self.net(x)\n# calibration\nwith torch.no_grad():\n    α,β=torch.linalg.lstsq(preds, meas).solution   # 5×3 least squares' expected_result='1. Zero-shot on Pixel-8 with only 5 calibration runs MetaS-PEFT reaches 95.0 % SST-2 accuracy at 5.9 J / 74 ms / 10.4 W, beating EChorda-retrained (94.6 %, 6.3 J / 78 ms / 11.2 W) while using 2 × fewer on-device measurements (5 vs 200). 2. Surrogate MAE on unseen hardware: energy 2.4 %, latency 3.1 %, power 4.0 %. 3. Pareto tree adds <40 µs overhead and retains ≥99 % hyper-volume.' expected_conclusion='A meta-learned, device-conditioned surrogate lets PEFT methods optimise *multiple* real hardware objectives without costly per-device sampling. The approach generalises across heterogeneous accelerators with negligible calibration and sets a new accuracy-/-energy-/-latency frontier for on-device NLP, pushing PEFT towards practical, sustainable deployment.'

# Current Research Method
MetaSurrogate-PEFT (MetaS-PEFT)
A. Universal differentiable hardware surrogate
   1. Represent a forward pass by the vector s=(seq_len, kept_layers, kept_heads, kept_ranks, param_bits).
   2. Off-line, collect <3 k measurements of (energy, latency, max_power) on a pool of 6 devices (RTX-A6000, Jetson-Orin-NX, Mac-M2, iPhone-15-NPU, Pixel-8-TPU, Raspberry-Pi-4).
   3. Train a 3-headed MLP  ψ_φ(s, h_id)→(Ē, L̂ , P̂) where h_id is a one-hot hardware tag. Mixup on h_id regularises cross-device features.
   4. Meta-learn φ with leave-one-device-out objective so that the surrogate works zero-shot on unseen hardware when given 5 calibration samples.
B. Multi-objective tri-granular gating
   1. Keep EChorda’s depth/head/rank hard-concrete gates g_θ.
   2. New budget loss:  L_total=L_task +λ Ē+μ L̂ +ν P̂  with user-specified weights (λ,μ,ν).
   3. Gradients flow through ψ_φ ; no device calls during training.
C. Few-shot calibration for new hardware
   • At deployment, run 5 random gating configs, fit a linear correction α,β so that  ψ′(s)=α⊙ψ_φ(s)+β ; no further fine-tuning of θ.
D. Pareto controller distillation
   • After training, fit a small DecisionTree to predict a Pareto-optimal (Ē ,L̂ ,P̂) triple from token entropy and position, removing the MLP at inference (≤1 kB).

# Experimental Design
- Strategy: The study validates MetaSurrogate-PEFT (MetaS-PEFT), a PEFT framework that jointly minimises energy, latency and peak-power without expensive per-device profiling. 1) We fine-tune a Llama-2-7B backbone on SST-2 with tri-granular hard-concrete gates over layers, heads and LoRA ranks. 2) During optimisation, a differentiable meta-surrogate ψϕ predicts (Ē, L̂, P̂) from simple model statistics s and a hardware ID; gradients from the composite loss Ltask+λĒ+μL̂+νP̂ flow back to the gates—no real measurements are taken. 3) ψϕ is meta-trained offline on <3 k profiles gathered from four GPUs/NPUs using a leave-one-device-out objective; at deployment on the unseen Google Pixel-8 Edge-TPU we collect only 5 random profiles to fit a linear calibration (α,β). 4) After convergence we distil the gating policy into a 1-kB decision tree for negligible runtime overhead. 5) Evaluation compares MetaS-PEFT against EChorda on the same Pixel-8 budget (≤6 J, ≤80 ms, ≤11 W) and reports 4-D hyper-volume, surrogate MAE and the number of on-device measurements. All training and search run on a single NVIDIA A100 80 GB with 2 TB RAM, easily fitting the surrogate, calibration data and full-precision Llama-2-7B checkpoints.
- Proposed Method: MetaSurrogate-PEFT (MetaS-PEFT) combines a meta-learned, device-conditioned surrogate with multi-objective gating.
A) Universal differentiable surrogate: Represent each forward pass by statistics s=(seq_len, kept_layers, kept_heads, kept_ranks, param_bits). Train a 3-headed MLP ψϕ(s,h_id)→(Ē, L̂, P̂) on <3 k measured tuples from four reference devices. Mixup on the one-hot hardware tag h_id enforces shared structure. Meta-learning with a leave-one-device-out loss lets the surrogate adapt to unseen hardware with only 5 calibration samples.
B) Multi-objective tri-granular gating: Re-use EChorda’s hard-concrete gates gθ over layers, heads and LoRA ranks. Optimise θ with the loss Ltotal = Ltask + λĒ + μL̂ + νP̂, where (Ē, L̂, P̂)=ψϕ(s,h_id). Gradients propagate through ψϕ, keeping the entire process differentiable and measurement-free.
C) Few-shot calibration: On a new device, measure energy/latency/power for 5 random gate settings, solve a 3×3 least-squares system to obtain per-metric scale/shift (α,β) and form ψ′(s)=α⊙ψϕ(s)+β.
D) Pareto controller distillation: After training, sample 2 k random inputs, record token entropy/position and the chosen gate states, and train a depth-3 decision tree that predicts the closest Pareto-optimal triple. This removes the MLP at inference, shrinking the controller to ≤1 kB.
The result is a PEFT model whose accuracy–energy–latency–power trade-off transfers across devices with negligible extra profiling.
- Evaluation Metrics: ['4-D Hyper-Volume (Accuracy, -Energy, -Latency, -Power)', 'Accuracy', 'Energy (J)', 'Latency (ms)', 'Peak Power (W)', 'Surrogate MAE', 'Number of On-Device Measurements', '4-D hyper-volume (HV) of accuracy vs (energy, latency, power) on the unseen Pixel-8.']

# Experiment Runs

- Run ID: proposed-iter1-Llama-2-7B-SST-2
  Method: proposed
  Model: Llama-2-7B
  Dataset: SST-2
  
  Config Content:
  ```yaml
  run_id: proposed-iter1-Llama-2-7B-SST-2
method: MetaS-PEFT
model:
  name: Llama-2-7B
  precision: bfloat16
  architecture_notes: "Decoder-only 32-layer transformer; tri-granular gating (layer, head, LoRA rank)"
  peft:
    method: LoRA
    target_modules: ["q_proj", "v_proj"]
    r: 8          # default, tuned by Optuna
    alpha: 16
    dropout: 0.1
  gating:
    granularity: [layer, head, rank]
    hard_concrete_temperature: 2.0
    gate_sparsity_regulariser: 1e-5
dataset:
  name: sst2
  source: "huggingface:stanfordnlp/sst2"
  max_length: 128
  padding: max_length
  batch_size: 16
training:
  num_epochs: 3
  learning_rate: 2e-5
  optimizer: adamw
  weight_decay: 0.01
  gradient_accumulation_steps: 8
  lr_scheduler: cosine
  warmup_ratio: 0.05
  seed: 42
budgets:
  energy_j: 6.0
  latency_ms: 80.0
  power_w: 11.0
surrogate:
  checkpoint_path: "checkpoints/meta_surrogate_v1.pt"
  n_hardware_tags: 6
  meta_training_devices: ["RTX-A6000", "Jetson-Orin-NX", "MacBook-M2", "iPhone-15-NPU", "Pixel-8-TPU", "Raspberry-Pi-4"]
  calibration_samples: 5
  calibration_method: linear_scale_shift
objective_weights:
  lambda_energy: 1.0
  mu_latency: 1.0
  nu_power: 1.0
evaluation:
  primary_metric: 4-D_hyper_volume
  metrics: [accuracy, energy_j, latency_ms, power_w, surrogate_mae, hv]
optuna:
  n_trials: 60
  direction: maximize
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 5e-4
    lambda_energy:
      type: loguniform
      low: 0.1
      high: 5.0
    mu_latency:
      type: loguniform
      low: 0.1
      high: 5.0
    nu_power:
      type: loguniform
      low: 0.1
      high: 5.0
    hard_concrete_temperature:
      type: uniform
      low: 0.5
      high: 5.0
    gate_sparsity_regulariser:
      type: loguniform
      low: 1e-6
      high: 1e-3
    r:
      type: int
      low: 4
      high: 32
    batch_size:
      type: categorical
      choices: [8, 16, 32]

  ```
  

- Run ID: comparative-1-iter1-Llama-2-7B-SST-2
  Method: comparative-1
  Model: Llama-2-7B
  Dataset: SST-2
  
  Config Content:
  ```yaml
  run_id: comparative-1-iter1-Llama-2-7B-SST-2
method: EChorda
model:
  name: Llama-2-7B
  precision: bfloat16
  architecture_notes: "Decoder-only 32-layer transformer; depth/head/rank hard-concrete gates"
  gating:
    granularity: [layer, head, rank]
    hard_concrete_temperature: 2.0
    gate_sparsity_regulariser: 1e-5
dataset:
  name: sst2
  source: "huggingface:stanfordnlp/sst2"
  max_length: 128
  padding: max_length
  batch_size: 16
training:
  num_epochs: 3
  learning_rate: 2e-5
  optimizer: adamw
  weight_decay: 0.01
  gradient_accumulation_steps: 8
  lr_scheduler: cosine
  warmup_ratio: 0.05
  seed: 42
budgets:
  energy_j: 6.0
  latency_ms: 80.0
  power_w: 11.0
hardware_profiling:
  target_device: Google-Pixel-8-EdgeTPU
  real_measurements_required: 200
  measurement_cache_dir: /data/echorda_pixel8_profiles
objective_weights:
  lambda_energy: 1.0
  mu_latency: 0.0
  nu_power: 0.0
evaluation:
  primary_metric: energy_j
  metrics: [accuracy, energy_j, latency_ms, power_w]
optuna:
  n_trials: 60
  direction: minimize
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 5e-4
    lambda_energy:
      type: loguniform
      low: 0.1
      high: 5.0
    hard_concrete_temperature:
      type: uniform
      low: 0.5
      high: 5.0
    gate_sparsity_regulariser:
      type: loguniform
      low: 1e-6
      high: 1e-3
    batch_size:
      type: categorical
      choices: [8, 16, 32]

  ```
  


# Generated Experiment Code (To be validated)
{"config_yaml": "defaults:\n  - _self_\n\n# Placeholder values \u2013 overridden by run YAML ---------------------------------\nrun: null\nresults_dir: ./results\nmode: full  # {trial, full}\n\n# WandB -----------------------------------------------------------------------\nwandb:\n  entity: gengaru617-personal\n  project: 251106-test\n  mode: online\n\nmethod: null\nmodel: {}\ndataset: {}\ntraining: {}\noptuna:\n  n_trials: 0\n  direction: maximize\nbudgets:\n  energy_j: 6.0\n  latency_ms: 80.0\n  power_w: 11.0\nsurrogate:\n  checkpoint_path: checkpoints/meta_surrogate_v1.pt\n  n_hardware_tags: 6\n  target_hw_id: 4  # Pixel-8 index in training list\n  calibration_samples: 5\n  calibration_file: \"calibration/pixel8_5samples.json\"\nobjective_weights:\n  lambda_energy: 1.0\n  mu_latency: 1.0\n  nu_power: 1.0\nhydra:\n  run:\n    dir: .\n  output_subdir: null", "evaluate_py": "import argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport wandb\nfrom omegaconf import OmegaConf\nfrom scipy.stats import ttest_ind\n\n################################################################################\n# Utilities                                                                     #\n################################################################################\n\ndef _save_json(obj: Dict, path: Path):\n    path.parent.mkdir(parents=True, exist_ok=True)\n    with open(path, \"w\") as fp:\n        json.dump(obj, fp, indent=2)\n\n################################################################################\n# Per-run processing                                                            #\n################################################################################\n\ndef _process_run(run, out_dir: Path):\n    rid = run.id\n    run_dir = out_dir / rid\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    history = run.history()\n    summary = dict(run.summary)\n    cfg = dict(run.config)\n\n    # Save metrics -------------------------------------------------------------\n    history.to_csv(run_dir / \"history.csv\", index=False)\n    _save_json({\"summary\": summary, \"config\": cfg}, run_dir / \"metrics.json\")\n\n    # Learning curve -----------------------------------------------------------\n    plt.figure(figsize=(6, 4))\n    for col in [c for c in [\"train_loss\", \"val_acc\"] if c in history.columns]:\n        sns.lineplot(x=history.index, y=history[col], label=col)\n    plt.title(f\"Learning \u2011 {rid}\")\n    plt.tight_layout()\n    fp = run_dir / f\"{rid}_learning_curve.pdf\"\n    plt.savefig(fp)\n    plt.close()\n    print(fp)\n\n    return summary\n\n################################################################################\n# Aggregation                                                                   #\n################################################################################\n\ndef _aggregate(summaries: Dict[str, Dict], out_dir: Path):\n    comp_dir = out_dir / \"comparison\"\n    comp_dir.mkdir(parents=True, exist_ok=True)\n\n    hv_map = {rid: s.get(\"hv\", 0.0) for rid, s in summaries.items()}\n    best_prop = max(((rid, v) for rid, v in hv_map.items() if \"proposed\" in rid), key=lambda x: x[1])\n    best_base = max(((rid, v) for rid, v in hv_map.items() if (\"baseline\" in rid or \"comparative\" in rid)), key=lambda x: x[1])\n    gap = (best_prop[1] - best_base[1]) / best_base[1] * 100 if best_base[1] else 0.0\n\n    out = {\n        \"primary_metric\": \"4-D hyper-volume (HV) of accuracy vs (energy, latency, power) on the unseen Pixel-8.\",\n        \"metrics\": {k: {rid: s.get(k, None) for rid, s in summaries.items()} for k in [\"hv\", \"best_val_acc\"]},\n        \"best_proposed\": {\"run_id\": best_prop[0], \"value\": best_prop[1]},\n        \"best_baseline\": {\"run_id\": best_base[0], \"value\": best_base[1]},\n        \"gap\": gap,\n    }\n    _save_json(out, comp_dir / \"aggregated_metrics.json\")\n    print(comp_dir / \"aggregated_metrics.json\")\n\n    # Bar chart ---------------------------------------------------------------\n    plt.figure(figsize=(8, 4))\n    sns.barplot(x=list(hv_map.keys()), y=list(hv_map.values()))\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.ylabel(\"HV\")\n    plt.tight_layout()\n    fp = comp_dir / \"comparison_hv_bar.pdf\"\n    plt.savefig(fp)\n    plt.close()\n    print(fp)\n\n################################################################################\n# Main                                                                          #\n################################################################################\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"results_dir\", type=str)\n    parser.add_argument(\"run_ids\", type=str)\n    args = parser.parse_args()\n\n    out_dir = Path(args.results_dir)\n    run_ids = json.loads(args.run_ids)\n\n    cfg_path = Path(__file__).resolve().parent.parent / \"config\" / \"config.yaml\"\n    hydra_cfg = OmegaConf.load(cfg_path)\n    entity, project = hydra_cfg.wandb.entity, hydra_cfg.wandb.project\n\n    api = wandb.Api()\n    summs: Dict[str, Dict] = {}\n    for rid in run_ids:\n        run = api.run(f\"{entity}/{project}/{rid}\")\n        summs[rid] = _process_run(run, out_dir)\n    _aggregate(summs, out_dir)\n    print(\"[evaluation] complete\")\n\nif __name__ == \"__main__\":\n    main()", "main_py": "import subprocess\nimport sys\nfrom pathlib import Path\n\nimport hydra\nfrom omegaconf import OmegaConf\n\n################################################################################\n# Launcher                                                                      #\n################################################################################\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef main(cfg):\n    if cfg.mode not in (\"trial\", \"full\"):\n        raise ValueError(\"mode must be trial/full\")\n\n    cmd = [\n        sys.executable,\n        \"-u\",\n        \"-m\",\n        \"src.train\",\n        f\"run={cfg.run}\",\n        f\"results_dir={cfg.results_dir}\",\n        f\"mode={cfg.mode}\",\n    ]\n    print(\"[main] executing:\", \" \".join(cmd))\n    subprocess.run(cmd, check=True)\n\nif __name__ == \"__main__\":\n    main()", "model_py": "import math\nimport re\nfrom typing import Dict, List, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoModel, AutoModelForSequenceClassification\nfrom peft import LoraConfig, get_peft_model\n\n################################################################################\n#  Hard-Concrete (scalar / vector) gate                                         #\n################################################################################\n\nclass HardConcreteGate(nn.Module):\n    \"\"\"Parameterised hard-concrete gate \u2013 can represent scalar or vector.\"\"\"\n\n    def __init__(self, size: Tuple[int, ...] = (1,), temperature: float = 2.0, init_mean: float = 0.5):\n        super().__init__()\n        self.temperature = temperature\n        logit = math.log(init_mean) - math.log(1 - init_mean)\n        self.log_alpha = nn.Parameter(torch.full(size, logit))\n        self.register_buffer(\"eps\", torch.tensor(1e-6))\n\n    def _sample(self):\n        u = torch.rand_like(self.log_alpha)\n        s = torch.sigmoid((torch.log(u + self.eps) - torch.log(1 - u + self.eps) + self.log_alpha) / self.temperature)\n        return torch.clamp(s, 0, 1)\n\n    def prob(self):\n        return torch.sigmoid(self.log_alpha)\n\n    def forward(self):\n        return self._sample() if self.training else self.prob()\n\n################################################################################\n#  Backbone + PEFT                                                              #\n################################################################################\n\ndef build_backbone_with_peft(cfg):\n    try:\n        model = AutoModelForSequenceClassification.from_pretrained(\n            cfg.model.name, num_labels=2, cache_dir=\".cache/\"\n        )\n    except ValueError:\n        base = AutoModel.from_pretrained(cfg.model.name, cache_dir=\".cache/\")\n        model = ClassificationWrapper(base, base.config.hidden_size, 2)\n\n    if cfg.model.get(\"peft\"):\n        p = cfg.model.peft\n        lora_cfg = LoraConfig(\n            r=p.r,\n            lora_alpha=p.alpha,\n            lora_dropout=p.dropout,\n            task_type=\"SEQ_CLS\",\n            target_modules=list(p.target_modules),\n        )\n        model = get_peft_model(model, lora_cfg)\n        model.print_trainable_parameters()\n    return model\n\n################################################################################\n#  Gating injection                                                             #\n################################################################################\n\ndef inject_tri_granular_gates(model: nn.Module, temperature: float):\n    \"\"\"Inject layer-, head- and LoRA-rank gates. Returns dict of gate lists.\"\"\"\n    layer_gates, head_gates, rank_gates = [], [], []\n\n    # Layer gates --------------------------------------------------------------\n    pattern = re.compile(r\"\\.layer\\.[0-9]+|layers\\.[0-9]+|\\.h\\.[0-9]+\")\n    for n, m in model.named_modules():\n        if pattern.search(n) and isinstance(m, nn.Module) and not hasattr(m, \"hc_layer_gate\"):\n            gate = HardConcreteGate((1,), temperature)\n            m.register_buffer(\"hc_layer_gate\", torch.tensor(0.0))  # marker\n\n            def hook(mod, _inp, output, gate=gate):\n                return output * gate().view(1, 1, 1)\n\n            m.register_forward_hook(hook)\n            layer_gates.append(gate)\n\n    # Head gates ---------------------------------------------------------------\n    for m in model.modules():\n        if hasattr(m, \"num_heads\") and not hasattr(m, \"hc_head_gate\"):\n            n_heads = int(m.num_heads)\n            gate = HardConcreteGate((n_heads,), temperature)\n            setattr(m, \"hc_head_gate\", True)\n\n            def head_hook(mod, _inp, output, gate=gate):\n                bs, sl, dim = output.shape\n                head_dim = dim // n_heads\n                out = output.view(bs, sl, n_heads, head_dim)\n                gated = out * gate().view(1, 1, n_heads, 1)\n                return gated.view(bs, sl, dim)\n\n            m.register_forward_hook(head_hook)\n            head_gates.append(gate)\n\n    # Rank gates (LoRA) --------------------------------------------------------\n    for m in model.modules():\n        if hasattr(m, \"lora_A\") and not hasattr(m, \"hc_rank_gate\"):\n            r = m.lora_A[0].weight.size(0)\n            gate = HardConcreteGate((r,), temperature)\n            setattr(m, \"hc_rank_gate\", True)\n\n            def lora_hook(mod, _inp, output, gate=gate):\n                # output shape matches original linear\n                return output * gate().mean()  # simple scalar approximation\n\n            m.register_forward_hook(lora_hook)\n            rank_gates.append(gate)\n\n    return {\n        \"layer\": layer_gates,\n        \"head\": head_gates,\n        \"rank\": rank_gates,\n    }\n\n################################################################################\n#  Statistics helpers for surrogate                                             #\n################################################################################\n\ndef _gate_kept(gates: List[HardConcreteGate], thresh: float = 0.5) -\u003e int:\n    if not gates:\n        return 0\n    keep = [g.prob().gt(thresh).float().sum().item() for g in gates]\n    return int(sum(keep) / len(keep))\n\n\ndef count_kept_gates(gate_dict: Dict[str, List[HardConcreteGate]], batch, cfg):\n    seq_len = batch[\"attention_mask\"].sum(dim=1).float().mean().unsqueeze(0)\n    kept_layers = torch.tensor([_gate_kept(gate_dict[\"layer\"])]).float()\n    kept_heads = torch.tensor([_gate_kept(gate_dict[\"head\"], 0.5)]).float()\n    kept_ranks = torch.tensor([_gate_kept(gate_dict[\"rank\"], 0.5)]).float()\n    precision_bits = 16 if \"16\" in str(cfg.model.precision) else 32\n    prec = torch.tensor([precision_bits]).float()\n    return torch.stack([seq_len, kept_layers, kept_heads, kept_ranks, prec], dim=1)\n\n################################################################################\n#  Classification wrapper                                                       #\n################################################################################\n\nclass ClassificationWrapper(nn.Module):\n    def __init__(self, backbone: AutoModel, hidden: int, num_labels: int = 2):\n        super().__init__()\n        self.backbone = backbone\n        self.classifier = nn.Linear(hidden, num_labels)\n\n    def forward(self, input_ids=None, attention_mask=None, labels=None):\n        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n        pooled = out.last_hidden_state[:, -1]\n        logits = self.classifier(pooled)\n        loss = F.cross_entropy(logits, labels) if labels is not None else None\n        return {\"loss\": loss, \"logits\": logits}\n\n################################################################################\n#  Hyper-volume computation                                                     #\n################################################################################\n\ndef compute_hyper_volume(acc: float, energy: float, latency: float, power: float, budgets) -\u003e float:\n    if energy \u003e budgets.energy_j or latency \u003e budgets.latency_ms or power \u003e budgets.power_w:\n        return 0.0\n    return (\n        max(acc, 0.0)\n        * (budgets.energy_j - energy)\n        * (budgets.latency_ms - latency)\n        * (budgets.power_w - power)\n    )", "preprocess_py": "from typing import Tuple\n\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer\n\n################################################################################\n# Tokeniser                                                                     #\n################################################################################\n\ndef build_tokenizer(cfg):\n    tokenizer = AutoTokenizer.from_pretrained(cfg.model.name, cache_dir=\".cache/\")\n    if tokenizer.pad_token_id is None:\n        tokenizer.add_special_tokens({\"pad_token\": \"\u003cpad\u003e\"})\n    return tokenizer\n\n################################################################################\n# Dataset                                                                       #\n################################################################################\n\ndef _tokenise(batch, tokenizer, cfg):\n    text_col = \"sentence\"\n    if text_col not in batch:\n        text_col = [k for k, v in batch.items() if isinstance(v, str)][0]\n    return tokenizer(\n        batch[text_col],\n        truncation=True,\n        max_length=cfg.dataset.max_length,\n        padding=cfg.dataset.padding,\n    )\n\n\ndef build_dataloaders(cfg, tokenizer) -\u003e Tuple[DataLoader, DataLoader]:\n    name = cfg.dataset.source.split(\"huggingface:\")[-1]\n    raw = load_dataset(name, cache_dir=\".cache/\")\n\n    # rename label\n    if \"label\" in raw[\"train\"].column_names:\n        raw = raw.rename_column(\"label\", \"labels\")\n\n    tokenised = raw.map(lambda b: _tokenise(b, tokenizer, cfg), batched=True)\n    tokenised.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\n    val_split = \"validation\" if \"validation\" in tokenised else \"test\"\n    train_ds, val_ds = tokenised[\"train\"], tokenised[val_split]\n\n    if cfg.mode == \"trial\":\n        train_ds = train_ds.select(range(min(2 * cfg.dataset.batch_size, len(train_ds))))\n        val_ds = val_ds.select(range(min(2 * cfg.dataset.batch_size, len(val_ds))))\n\n    train_loader = DataLoader(\n        train_ds, batch_size=cfg.dataset.batch_size, shuffle=True, drop_last=False\n    )\n    val_loader = DataLoader(\n        val_ds, batch_size=cfg.dataset.batch_size, shuffle=False, drop_last=False\n    )\n    return train_loader, val_loader", "pyproject_toml": "[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n\n[project]\nname = \"metas-peft-experiments\"\nversion = \"0.1.0\"\nrequires-python = \"\u003e=3.9\"\ndescription = \"MetaSurrogate-PEFT experiments\"\n\n[project.dependencies]\nhydra-core = \"^1.3\"\nomegaconf = \"^2.3\"\nwandb = \"^0.15\"\noptuna = \"^3.5\"\ntorch = \"^2.1\"\ntransformers = \"^4.35\"\ndatasets = \"^2.14\"\npeft = \"^0.6\"\nscikit-learn = \"^1.3\"\nscipy = \"^1.11\"\nmatplotlib = \"^3.8\"\nseaborn = \"^0.13\"\npandas = \"^2.1\"", "train_py": "import json\nimport os\nimport random\nimport subprocess\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nimport hydra\nimport numpy as np\nimport optuna\nimport torch\nimport torch.nn.functional as F\nimport wandb\nfrom omegaconf import OmegaConf\n\nfrom .model import (\n    inject_tri_granular_gates,\n    compute_hyper_volume,\n    count_kept_gates,\n    build_backbone_with_peft,\n)\nfrom .preprocess import build_dataloaders, build_tokenizer\nfrom .surrogate import load_and_prepare_surrogate, apply_calibration\n\n################################################################################\n# Reproducibility                                                               #\n################################################################################\n\ndef set_seed(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n################################################################################\n# WandB helper                                                                  #\n################################################################################\n\ndef _init_wandb(cfg):\n    if cfg.wandb.mode == \"disabled\":\n        os.environ[\"WANDB_MODE\"] = \"disabled\"\n        return None\n    run = wandb.init(\n        entity=cfg.wandb.entity,\n        project=cfg.wandb.project,\n        id=cfg.run_id,\n        resume=\"allow\",\n        config=OmegaConf.to_container(cfg, resolve=True),\n    )\n    print(f\"[wandb] URL: {run.get_url()}\")\n    return run\n\n################################################################################\n# Optuna objective                                                              #\n################################################################################\n\ndef _build_trial_objects(cfg, device):\n    \"\"\"Create independent model + gates for every Optuna trial.\"\"\"\n    model = build_backbone_with_peft(cfg)\n    gates = inject_tri_granular_gates(model, cfg.model.gating.hard_concrete_temperature)\n    model.to(device)\n    return model, gates\n\n\ndef _optuna_objective(trial, base_cfg, device, loaders, surrogate, calib):\n    cfg = OmegaConf.create(OmegaConf.to_container(base_cfg, resolve=False))\n\n    # Search space ------------------------------------------------------\n    cfg.training.learning_rate = trial.suggest_float(\"lr\", 1e-5, 5e-4, log=True)\n    cfg.objective_weights.lambda_energy = trial.suggest_float(\n        \"lambda_energy\", 1e-1, 5.0, log=True\n    )\n    cfg.objective_weights.mu_latency = trial.suggest_float(\n        \"mu_latency\", 1e-1, 5.0, log=True\n    )\n    cfg.objective_weights.nu_power = trial.suggest_float(\n        \"nu_power\", 1e-1, 5.0, log=True\n    )\n\n    # Model -------------------------------------------------------------\n    model, gates = _build_trial_objects(cfg, device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.training.learning_rate)\n\n    train_loader, val_loader = loaders\n    model.train()\n    batch = next(iter(train_loader))\n    batch = {k: v.to(device) for k, v in batch.items()}\n    optimizer.zero_grad()\n    out = model(**batch)\n    task_loss = out[\"loss\"]\n    stats_vec = count_kept_gates(gates, batch, cfg).to(device)\n    hw_id = torch.tensor([cfg.surrogate.target_hw_id], device=device)\n    e_pred, l_pred, p_pred = apply_calibration(surrogate(stats_vec, hw_id), calib)\n    loss = (\n        task_loss\n        + cfg.objective_weights.lambda_energy * e_pred\n        + cfg.objective_weights.mu_latency * l_pred\n        + cfg.objective_weights.nu_power * p_pred\n    )\n    loss.backward()\n    optimizer.step()\n\n    # quick val ---------------------------------------------------------\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for vb in val_loader:\n            vb = {k: v.to(device) for k, v in vb.items()}\n            logits = model(**vb)[\"logits\"]\n            preds = logits.argmax(-1)\n            correct += (preds == vb[\"labels\"]).sum().item()\n            total += preds.size(0)\n            break  # light\n    acc = correct / max(total, 1)\n    return acc\n\n################################################################################\n# Training                                                                     #\n################################################################################\n\ndef _train(cfg):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Data --------------------------------------------------------------------\n    tokenizer = build_tokenizer(cfg)\n    train_loader, val_loader = build_dataloaders(cfg, tokenizer)\n\n    # Surrogate (+ calibration) -----------------------------------------------\n    surrogate, calib = load_and_prepare_surrogate(cfg, device)\n\n    # Hyper-parameter search (Optuna) -----------------------------------------\n    if cfg.optuna.n_trials \u003e 0:\n        loaders = (train_loader, val_loader)\n        study = optuna.create_study(direction=cfg.optuna.direction)\n        study.optimize(\n            lambda t: _optuna_objective(\n                t, cfg, device, loaders, surrogate, calib\n            ),\n            n_trials=cfg.optuna.n_trials,\n        )\n        best_params = study.best_params\n        print(\"[optuna] best\", best_params)\n        cfg.training.learning_rate = best_params.get(\"lr\", cfg.training.learning_rate)\n        cfg.objective_weights.lambda_energy = best_params.get(\n            \"lambda_energy\", cfg.objective_weights.lambda_energy\n        )\n        cfg.objective_weights.mu_latency = best_params.get(\n            \"mu_latency\", cfg.objective_weights.mu_latency\n        )\n        cfg.objective_weights.nu_power = best_params.get(\n            \"nu_power\", cfg.objective_weights.nu_power\n        )\n\n    # Model -------------------------------------------------------------------\n    model = build_backbone_with_peft(cfg)\n    gates = inject_tri_granular_gates(model, cfg.model.gating.hard_concrete_temperature)\n    model.to(device)\n\n    optimizer = torch.optim.AdamW(\n        model.parameters(), lr=cfg.training.learning_rate, weight_decay=cfg.training.weight_decay\n    )\n\n    run = _init_wandb(cfg)\n    global_step, best_val_acc = 0, 0.0\n\n    for epoch in range(cfg.training.num_epochs):\n        model.train()\n        for step, batch in enumerate(train_loader):\n            batch = {k: v.to(device) for k, v in batch.items()}\n            optimizer.zero_grad()\n            fwd = model(**batch)\n            task_loss = fwd[\"loss\"]\n            stats_vec = count_kept_gates(gates, batch, cfg).to(device)\n            hw_id = torch.tensor([cfg.surrogate.target_hw_id], device=device)\n            with torch.no_grad():\n                e_pred, l_pred, p_pred = apply_calibration(\n                    surrogate(stats_vec, hw_id), calib\n                )\n            loss = (\n                task_loss\n                + cfg.objective_weights.lambda_energy * e_pred\n                + cfg.objective_weights.mu_latency * l_pred\n                + cfg.objective_weights.nu_power * p_pred\n            )\n            loss.backward()\n            optimizer.step()\n\n            if run is not None:\n                run.log(\n                    {\n                        \"train_loss\": loss.item(),\n                        \"task_loss\": task_loss.item(),\n                        \"pred_energy\": e_pred.item(),\n                        \"pred_latency\": l_pred.item(),\n                        \"pred_power\": p_pred.item(),\n                    },\n                    step=global_step,\n                )\n            global_step += 1\n            if cfg.mode == \"trial\" and step \u003e= 1:\n                break\n\n        # Validation -----------------------------------------------------\n        model.eval()\n        correct, total, val_loss = 0, 0, 0.0\n        with torch.no_grad():\n            for vb in val_loader:\n                vb = {k: v.to(device) for k, v in vb.items()}\n                outputs = model(**vb)\n                logits = outputs[\"logits\"]\n                val_loss += F.cross_entropy(logits, vb[\"labels\"]).item()\n                preds = logits.argmax(-1)\n                correct += (preds == vb[\"labels\"]).sum().item()\n                total += preds.size(0)\n                if cfg.mode == \"trial\":\n                    break\n        val_acc = correct / max(total, 1)\n        best_val_acc = max(best_val_acc, val_acc)\n\n        hv = compute_hyper_volume(\n            val_acc,\n            e_pred.item(),\n            l_pred.item(),\n            p_pred.item(),\n            cfg.budgets,\n        )\n        if run is not None:\n            run.log(\n                {\n                    \"val_loss\": val_loss / max(total, 1),\n                    \"val_acc\": val_acc,\n                    \"hv\": hv,\n                },\n                step=global_step,\n            )\n        print(f\"[Epoch {epoch}] val_acc={val_acc:.4f} hv={hv:.4f}\")\n\n    if run is not None:\n        run.summary[\"best_val_acc\"] = best_val_acc\n        run.summary[\"hv\"] = hv\n        run.finish()\n\n################################################################################\n# Hydra entry-point                                                             #\n################################################################################\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef main(cfg):\n    # Merge run-specific YAML --------------------------------------------------\n    run_cfg_path = Path(__file__).resolve().parent.parent / \"config\" / \"runs\" / f\"{cfg.run}.yaml\"\n    if not run_cfg_path.exists():\n        raise FileNotFoundError(run_cfg_path)\n    run_cfg = OmegaConf.load(run_cfg_path)\n    cfg = OmegaConf.merge(cfg, run_cfg)\n\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.training.num_epochs = 1\n    elif cfg.mode == \"full\":\n        cfg.wandb.mode = \"online\"\n    else:\n        raise ValueError(\"mode must be trial or full\")\n\n    set_seed(cfg.training.seed)\n    _train(cfg)\n\nif __name__ == \"__main__\":\n    main()"}



Analyze the experiment code thoroughly. Ensure it is complete, executable, and ready for publication-quality research experiments.
Output:
{
    "is_code_ready": false,
    "code_issue": "The code cannot run as-is:\n1. Missing file: train.py imports `from .surrogate import load_and_prepare_surrogate, apply_calibration`, but `src/surrogate.py` (or any implementation of these functions) is not provided. This breaks both training and Optuna objective.\n2. Requirement 5 forbids extra files, so the surrogate logic must be included inside one of the allowed files (e.g., model.py) or the missing file must be added while still respecting the fixed file list.\n3. Because of #1 the implementation of the meta-surrogate, calibration and their device handling is incomplete, violating the “complete implementation” criterion.\n\nFix: Provide a full implementation of the meta-surrogate utilities (`load_and_prepare_surrogate`, `apply_calibration`) within the allowed file set and ensure all imports resolve. Only after this the code will meet the execution and publication-ready standards."
}
