run_id: comparative-1-iter1-Llama-2-7B-SST-2
method: EChorda
model:
  name: NousResearch/Llama-2-7b-hf
  precision: bfloat16
  architecture_notes: "Decoder-only 32-layer transformer; depth/head/rank hard-concrete gates"
  gating:
    granularity: [layer, head, rank]
    hard_concrete_temperature: 2.0
    gate_sparsity_regulariser: 1e-5
dataset:
  name: sst2
  source: "huggingface:stanfordnlp/sst2"
  max_length: 128
  padding: max_length
  batch_size: 16
training:
  num_epochs: 3
  learning_rate: 2e-5
  optimizer: adamw
  weight_decay: 0.01
  gradient_accumulation_steps: 8
  lr_scheduler: cosine
  warmup_ratio: 0.05
  seed: 42
budgets:
  energy_j: 6.0
  latency_ms: 80.0
  power_w: 11.0
hardware_profiling:
  target_device: Google-Pixel-8-EdgeTPU
  real_measurements_required: 200
  measurement_cache_dir: /data/echorda_pixel8_profiles
objective_weights:
  lambda_energy: 1.0
  mu_latency: 0.0
  nu_power: 0.0
evaluation:
  primary_metric: energy_j
  metrics: [accuracy, energy_j, latency_ms, power_w]
optuna:
  n_trials: 60
  direction: minimize
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 5e-4
    lambda_energy:
      type: loguniform
      low: 0.1
      high: 5.0
    hard_concrete_temperature:
      type: uniform
      low: 0.5
      high: 5.0
    gate_sparsity_regulariser:
      type: loguniform
      low: 1e-6
      high: 1e-3
    batch_size:
      type: categorical
      choices: [8, 16, 32]
